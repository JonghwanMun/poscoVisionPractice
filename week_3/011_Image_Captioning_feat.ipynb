{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from vgg16 import vgg16\n",
    "from dataLoader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data download\n",
    "  - 다운로드 제대로 된 후에는 다시 실행할 필요 없습니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('resource/vgg16_feat.h5'):\n",
    "    # download images\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/224x224_mscoco_images.tar.gz')\n",
    "    os.system('tar xvzf 224x224_mscoco_images.tar.gz')\n",
    "    os.system('rm 224x224_mscoco_images.tar.gz')\n",
    "    os.system('mv 224x224_mscoco_images resource/224x224_mscoco_images')\n",
    "\n",
    "    # download vgg16 network\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/vgg16_weights.npz')\n",
    "    os.system('mv vgg16_weights.npz resource/vgg16_weights.npz')\n",
    "    \n",
    "    # download pre-extracted vgg16 features\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/vgg16_feat.h5')\n",
    "    os.system('mv vgg16_feat.h5 resource/vgg16_feat.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Captioning Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, params, bias_init_vector=None):\n",
    "        #dim_image, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, bias_init_vector=None):\n",
    "        \n",
    "        self.wtoi = params['wtoi']\n",
    "        self.vgg16_weight_file = params['vgg16_weight_file']\n",
    "        \n",
    "        # captioning model parameter dimensions\n",
    "        self.dim_image_feat = np.int(params['dim_image'])\n",
    "        self.dim_word_embedding = np.int(params['dim_embed'])\n",
    "        self.dim_hidden_lstm = np.int(params['dim_hidden'])\n",
    "        self.batch_size = np.int(params['batch_size'])\n",
    "        self.n_lstm_steps = np.int(params['n_lstm_steps'])\n",
    "        self.n_words = np.int(params['n_words'])\n",
    "        \n",
    "        self.initializer = tf.random_uniform_initializer(minval=-0.8, maxval=0.8)\n",
    "\n",
    "    def build_model(self, sess):\n",
    "        \"\"\" Build captioning model for training\"\"\"\n",
    "\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, self.dim_image_feat])\n",
    "        input_labels = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        target_labels = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "        \n",
    "        # embed image feature to match the dimension of lstm input\n",
    "        with tf.variable_scope(\"image_embedding\") as img_embed_scope:\n",
    "            image_embeddings = tf.contrib.layers.fully_connected(\n",
    "                                    inputs=image,\n",
    "                                    num_outputs=self.dim_hidden_lstm,\n",
    "                                    activation_fn=None,\n",
    "                                    weights_initializer=self.initializer,\n",
    "                                    biases_initializer=None,\n",
    "                                    scope=img_embed_scope)\n",
    "            \n",
    "        # embed input caption labels to obtain word embeddings\n",
    "        with tf.variable_scope(\"word_embedding\"), tf.device('/cpu:0'):\n",
    "            word_embedding_matrix = tf.get_variable(\n",
    "                                        name='word_embedding_matrix',\n",
    "                                        shape=[self.n_words, self.dim_word_embedding],\n",
    "                                        initializer=self.initializer\n",
    "                                        )\n",
    "            word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, input_labels)\n",
    "                                                            \n",
    "        \n",
    "        # for LSTM cell unit\n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden_lstm, state_is_tuple=True)\n",
    "            zero_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "            \n",
    "            # allow the LSTM variables to be resued\n",
    "            lstm_scope.reuse_variables()\n",
    "            \n",
    "            # run the batch of word embeddings throught the LSTM\n",
    "            caption_length = tf.reduce_sum(mask, 1)\n",
    "            lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                               inputs=word_embeddings,\n",
    "                                               sequence_length=caption_length,\n",
    "                                               initial_state=initial_state,\n",
    "                                               dtype=tf.float32,\n",
    "                                               scope=lstm_scope)\n",
    "        \n",
    "        # stack batches vertically\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        # compute probabilities for words\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                        inputs=lstm_outputs,\n",
    "                        num_outputs=self.n_words,\n",
    "                        activation_fn=None,\n",
    "                        weights_initializer=self.initializer,\n",
    "                        scope=logits_scope)\n",
    "        \n",
    "        targets = tf.reshape(target_labels, [-1])\n",
    "        weights = tf.to_float(tf.reshape(mask, [-1]))\n",
    "        \n",
    "        # compute losses\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                              logits=logits)\n",
    "        batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "                            tf.reduce_sum(weights), name=\"batch_loss\")\n",
    "        tf.losses.add_loss(batch_loss)\n",
    "        total_loss = tf.losses.get_total_loss()\n",
    "        \n",
    "        probs = tf.nn.softmax(logits, name='softmax')\n",
    "\n",
    "        return image, input_labels, target_labels, mask, total_loss, probs\n",
    "\n",
    "    def build_generator(self, sess):\n",
    "        \"\"\" Build captioning model for test\"\"\"\n",
    "        \n",
    "        # extract image feature\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, self.dim_image_feat], name='image_raw')\n",
    "        \n",
    "        # embed image feature to match the dimension of lstm input\n",
    "        with tf.variable_scope(\"image_embedding\") as img_embed_scope:\n",
    "            encoded_images = tf.contrib.layers.fully_connected(\n",
    "                                    inputs=image,\n",
    "                                    num_outputs=self.dim_hidden_lstm,\n",
    "                                    activation_fn=None,\n",
    "                                    weights_initializer=self.initializer,\n",
    "                                    biases_initializer=None,\n",
    "                                    scope=img_embed_scope)        \n",
    "        \n",
    "        # Below part is for caption generation (LSTM)\n",
    "        image_embeddings = tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=[self.batch_size, self.dim_hidden_lstm],\n",
    "                                          name='image_feed')\n",
    "        input_feed = tf.placeholder(dtype=tf.int64, shape=[None], name='input_feed')\n",
    "        input_labels = tf.expand_dims(input_feed, 1)\n",
    "\n",
    "        # embed input caption labels to obtain word embeddings\n",
    "        with tf.variable_scope(\"word_embedding\"), tf.device('/cpu:0'):\n",
    "            word_embedding_matrix = tf.get_variable(\n",
    "                                        name='word_embedding_matrix',\n",
    "                                        shape=[self.n_words, self.dim_word_embedding],\n",
    "                                        initializer=self.initializer\n",
    "                                        )\n",
    "            word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, input_labels)                                                            \n",
    "        \n",
    "        # for LSTM cell unit    \n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden_lstm, state_is_tuple=True)\n",
    "            zero_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "            \n",
    "            # allow the LSTM variables to be resued\n",
    "            lstm_scope.reuse_variables()\n",
    "            \n",
    "            # In inference mode, use concatenated states for convenient feeding and fetching.\n",
    "            tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n",
    "        \n",
    "            # Placeholder for feeding a batch of concatenated states.\n",
    "            state_feed = tf.placeholder(dtype=tf.float32,\n",
    "                                        shape=[None, sum(lstm_cell.state_size)],\n",
    "                                        name=\"state_feed\")\n",
    "            state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n",
    "\n",
    "            # Run a single LSTM step.\n",
    "            lstm_outputs, state_tuple = lstm_cell(\n",
    "                inputs=tf.squeeze(word_embeddings, axis=[1]), state=state_tuple)\n",
    "\n",
    "            # Concatentate the resulting state.\n",
    "            tf.concat(axis=1, values=state_tuple, name=\"state\")\n",
    "            \n",
    "        \n",
    "        # stack batches vertically\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        # compute probabilities for words\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                        inputs=lstm_outputs,\n",
    "                        num_outputs=self.n_words,\n",
    "                        activation_fn=None,\n",
    "                        weights_initializer=self.initializer,\n",
    "                        scope=logits_scope)\n",
    "        probs = tf.nn.softmax(logits, name='softmax')\n",
    "\n",
    "        return image, encoded_images, image_embeddings, state_feed, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Parameters #####################\n",
    "train_params = {\n",
    "    'prepared_feature':True,\n",
    "    'feat_path':'resource/vgg16_feat.h5',\n",
    "    'json_path':'resource/train.json',\n",
    "    'h5py_path':'resource/train.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "test_params = {\n",
    "    'prepared_feature':True,\n",
    "    'feat_path':'resource/vgg16_feat.h5',\n",
    "    'json_path':'resource/test.json',\n",
    "    'h5py_path':'resource/test.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "# create data loader for training and test data\n",
    "loaders = {}\n",
    "loaders['train'] = dataLoader(train_params)\n",
    "loaders['test'] = dataLoader(test_params)\n",
    "\n",
    "model_params = {\n",
    "    'wtoi': loaders['train'].getWtoi(),\n",
    "    'vgg16_weight_file': 'resource/vgg16_weights.npz',\n",
    "    'dim_image': 4096, # dimension of vgg16 network output\n",
    "    'dim_embed': 256,\n",
    "    'dim_hidden': 256,\n",
    "    'batch_size': 50,\n",
    "    'n_lstm_steps': loaders['train'].getMaxCaptionLength(),\n",
    "    'n_words': loaders['train'].getVocabSize(),\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    n_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    model_path = './models'\n",
    "    itow = loaders['test'].getItow()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        caption_generator = Caption_Generator(model_params)#, bias_init_vector=bias_init_vector)\n",
    "        image, input_labels, target_labels, mask, loss, prob = caption_generator.build_model(sess)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=50)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        iteration_per_epoch = loaders['train'].getNumCaptions() / model_params['batch_size']\n",
    "        for epoch in range(n_epochs):\n",
    "            for bi in range(iteration_per_epoch):\n",
    "                # load batch data\n",
    "                batch = loaders['train'].getBatch(model_params['batch_size'])\n",
    "\n",
    "                #if bi % 50 == 0: \n",
    "                _, prob_value, loss_value = sess.run([train_op, prob, loss], \n",
    "                                         feed_dict={image: batch['images_feat'],\n",
    "                                                    input_labels: batch['caption_labels'],\n",
    "                                                    target_labels: batch['target_labels'],\n",
    "                                                    mask: batch['caption_masks']}\n",
    "                                        )\n",
    "\n",
    "                if bi % 50 == 0:\n",
    "                    single_sample = []\n",
    "                    word_index = np.argmax(prob_value, 1)\n",
    "                    for wi in range(15):\n",
    "                        single_sample.append(word_index[wi])\n",
    "                    word_index = np.hstack(single_sample)\n",
    "\n",
    "                    generated_sentence = ''\n",
    "                    for w in word_index:\n",
    "                        word = itow[str(w)]\n",
    "                        if word == '<E>': break\n",
    "                        else:\n",
    "                            generated_sentence += (word + ' ')\n",
    "\n",
    "                    print('Sampled caption: ', generated_sentence)\n",
    "                    print \"%d epoch %d iteration Cost: %.4f\" % (epoch+1, bi, loss_value)\n",
    "\n",
    "            print \"Epoch \", epoch+1, \" is done. Saving the model ... \"\n",
    "            if not os.path.isdir(model_path):\n",
    "                os.makedirs(model_path)\n",
    "            saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch+1)\n",
    "            learning_rate *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embedding_images(sess, images, encoded_images):\n",
    "    embedded_images = sess.run(encoded_images, feed_dict={'image_raw:0': images})\n",
    "    return embedded_images\n",
    "\n",
    "def feed_image(sess, encoded_image):\n",
    "    initial_state = sess.run(fetches='lstm/initial_state:0',\n",
    "                            feed_dict={'image_feed:0': encoded_image})\n",
    "    return initial_state\n",
    "    \n",
    "\n",
    "def inference_step(sess, input_feed, state_feed):\n",
    "    softmax_output, state_output = sess.run(\n",
    "        fetches=[\"softmax:0\", \"lstm/state:0\"],\n",
    "        feed_dict={\n",
    "            \"input_feed:0\": input_feed,\n",
    "            \"lstm/state_feed:0\": state_feed,\n",
    "        })\n",
    "    return softmax_output, state_output, None\n",
    "\n",
    "\n",
    "def test(model_path='./models/model-0', maxlen=15):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        # create caption generator for test\n",
    "        model_params['batch_size'] = 1\n",
    "        caption_generator = Caption_Generator(model_params)\n",
    "        image, image_feat, image_embed, s_feed, prob = caption_generator.build_generator(sess=sess)\n",
    "\n",
    "        # load the model parameters\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        itow = loaders['test'].getItow()\n",
    "        start_idx = loaders['test'].getWtoi()['<S>']\n",
    "        n_steps = loaders['train'].getMaxCaptionLength()\n",
    "        predictions = []\n",
    "        iteration_per_epoch = loaders['test'].getNumCaptions() / model_params['batch_size']\n",
    "        for bi in range(100):\n",
    "            # load batch data\n",
    "            batch = loaders['test'].getBatch(model_params['batch_size'])\n",
    "\n",
    "            # extract vgg feature\n",
    "            enmbedded_feats = embedding_images(sess, batch['images_feat'], image_feat)\n",
    "            input_feed = np.full((model_params['batch_size']), start_idx)\n",
    "            state_feed = feed_image(sess, enmbedded_feats) # initial state\n",
    "            \n",
    "            generated_word_index = []\n",
    "            for ci in range(n_steps):\n",
    "                softmax, next_states, metadata = inference_step(sess, input_feed, state_feed)\n",
    "                \n",
    "                input_feed = np.argmax(softmax,1)\n",
    "                state_feed = next_states\n",
    "                \n",
    "                print(input_feed)\n",
    "                generated_word_index.append(input_feed[0])\n",
    "            \n",
    "            generated_sentence = ''\n",
    "            for w in generated_word_index:\n",
    "                word = itow[str(w)]\n",
    "                if word == '<E>': break\n",
    "                else:\n",
    "                    generated_sentence += (word + ' ')\n",
    "\n",
    "            img = np.asarray(batch['images'].reshape(224,224,3) \n",
    "                             + np.array([123.68, 116.779, 103.939]).reshape(1,1,3),dtype='uint8')\n",
    "            I=Image.fromarray(img)\n",
    "            plt.imshow(I)\n",
    "            plt.show()\n",
    "\n",
    "            print('Generated Caption: ', generated_sentence)\n",
    "\n",
    "            ith_prediction = {}\n",
    "            ith_prediction['image_id'] = batch['image_ids'][0]\n",
    "            ith_prediction['caption'] = generated_sentence\n",
    "            predictions.append(ith_prediction)\n",
    "            \n",
    "\n",
    "        # save the prediction outputs\n",
    "        if not os.path.isdir('result'):\n",
    "                os.makedirs('result')\n",
    "        write_json('result/predictions.json', predictions)\n",
    "        print('Save the predictions to result/predictions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training and Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sampled caption: ', u'pops grab building.l upright never $ glistening scar turquoise mills stuffing reading bie <UNK> <UNK> ')\n",
      "1 epoch 0 iteration Cost: 13.7092\n",
      "('Sampled caption: ', u'collared me mickey rd lid summit resemble lifeguard grave african mmm giraffes giraffes ')\n",
      "1 epoch 50 iteration Cost: 11.1940\n",
      "('Sampled caption: ', u'banana puppies bust affable wih clearance distraught boogie blazes flatbed scratched ')\n",
      "1 epoch 100 iteration Cost: 10.9085\n",
      "('Sampled caption: ', u'a competes parent cement lush league armchair gloomy wetsuits upturned castle-inspired peeling flattening building.l flat ')\n",
      "1 epoch 150 iteration Cost: 11.5080\n",
      "('Sampled caption: ', u'a pawing beads completely pinstripe cabinets salad painter furiously assembled nighstand nighstand lice ')\n",
      "1 epoch 200 iteration Cost: 11.4190\n",
      "('Sampled caption: ', u'a man man the bouquet green tell mot green intertwined feasting toast ')\n",
      "1 epoch 250 iteration Cost: 8.5908\n",
      "('Sampled caption: ', u'a man google brothers talking strong waking is worship oj lyign surprise <UNK> ')\n",
      "1 epoch 300 iteration Cost: 9.0009\n",
      "('Sampled caption: ', u'a man is harvest emoticon the the dishes ')\n",
      "1 epoch 350 iteration Cost: 7.6061\n",
      "('Sampled caption: ', u'a man hitting plan restaurnat dog stool passenger tricycle derby shoe a a ')\n",
      "1 epoch 400 iteration Cost: 9.0817\n",
      "('Sampled caption: ', u'a bear storefront cooled grail distraught . mischievous cartoons cartoons bumping ')\n",
      "1 epoch 450 iteration Cost: 8.9280\n",
      "('Sampled caption: ', u'a woman of his mangoes on living large county swims ')\n",
      "1 epoch 500 iteration Cost: 7.9859\n",
      "('Sampled caption: ', u'a person sign harry their identification sight.one chinatown power platform a music dill mall <UNK> ')\n",
      "1 epoch 550 iteration Cost: 8.6509\n",
      "('Sampled caption: ', u'a large a ')\n",
      "1 epoch 600 iteration Cost: 7.6166\n",
      "('Sampled caption: ', u'a woman looking a cage heavily claiming a girl ')\n",
      "1 epoch 650 iteration Cost: 6.0511\n",
      "('Sampled caption: ', u'a person holding a a do-nut surfboard ')\n",
      "1 epoch 700 iteration Cost: 8.0927\n",
      "('Sampled caption: ', u'a stop is skiing a a jommu parasurfer reads handle himself <UNK> . . . ')\n",
      "1 epoch 750 iteration Cost: 8.6618\n",
      "('Sampled caption: ', u'a laying racket either person of hoof clause pees steel a his ')\n",
      "1 epoch 800 iteration Cost: 7.4976\n",
      "('Sampled caption: ', u'a bear junk a bride standing holding . patio game placidly ')\n",
      "1 epoch 850 iteration Cost: 8.2494\n",
      "('Sampled caption: ', u'a cat white lot with a a a a toilet ')\n",
      "1 epoch 900 iteration Cost: 7.0781\n",
      "('Sampled caption: ', u'a little ')\n",
      "1 epoch 950 iteration Cost: 5.3918\n",
      "Epoch  1  is done. Saving the model ... \n",
      "('Sampled caption: ', u'a man large of the in the in lid . ')\n",
      "2 epoch 0 iteration Cost: 5.9185\n",
      "('Sampled caption: ', u'a man couple black tools a ')\n",
      "2 epoch 50 iteration Cost: 6.9244\n",
      "('Sampled caption: ', u'a man in a a black black in on ')\n",
      "2 epoch 100 iteration Cost: 4.8042\n",
      "('Sampled caption: ', u'a dog in a three in in on ')\n",
      "2 epoch 150 iteration Cost: 8.2546\n",
      "('Sampled caption: ', u'a in playing are and ')\n",
      "2 epoch 200 iteration Cost: 8.2761\n"
     ]
    }
   ],
   "source": [
    "# test model that means to predict captions for test images\n",
    "do_train = True\n",
    "if do_train:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test model that means to predict captions for test images\n",
    "if not do_train:\n",
    "    test(model_path='models/model-90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "print_tensors_in_checkpoint_file('models/model.ckpt-1', tensor_name='lstm/basic_lstm_cell/weights', all_tensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('coco-caption')\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ann_file = 'coco-caption/annotations/captions_val2014.json'\n",
    "res_file = 'result/predictions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create coco object and cocoRes object\n",
    "coco = COCO(ann_file)\n",
    "cocoRes = coco.loadRes(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# evaluate on a subset of images by setting\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "\n",
    "# evaluate results\n",
    "cocoEval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
