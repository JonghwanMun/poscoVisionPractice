{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "#import tensorflow.python.platform\n",
    "#from keras.preprocessing import sequence\n",
    "#from collections import Counter\n",
    "#from cnn_util import *\n",
    "\n",
    "from vgg16 import vgg16\n",
    "from dataLoader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "  - 다운로드 제대로 된 후에는 다시 실행할 필요 없습니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('resource/vgg16_weights.npz'):\n",
    "    # download images\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/224x224_mscoco_images.tar.gz')\n",
    "    os.system('tar xvzf 224x224_mscoco_images.tar.gz')\n",
    "    os.system('rm 224x224_mscoco_images.tar.gz')\n",
    "    os.system('mv 224x224_mscoco_images resource/224x224_mscoco_images')\n",
    "\n",
    "    # download vgg16 network\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/vgg16_weights.npz')\n",
    "    os.system('mv vgg16_weights.npz resource/vgg16_weights.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, params, bias_init_vector=None):\n",
    "        #dim_image, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, bias_init_vector=None):\n",
    "        \n",
    "        self.wtoi = params['wtoi']\n",
    "        self.vgg16_weight_file = params['vgg16_weight_file']\n",
    "        \n",
    "        # captioning model parameter dimensions\n",
    "        self.dim_image = np.int(params['dim_image'])\n",
    "        self.dim_embed = np.int(params['dim_embed'])\n",
    "        self.dim_hidden = np.int(params['dim_hidden'])\n",
    "        self.batch_size = np.int(params['batch_size'])\n",
    "        self.n_lstm_steps = np.int(params['n_lstm_steps'])\n",
    "        self.n_words = np.int(params['n_words'])\n",
    "        \n",
    "        # create model parameters\n",
    "        self.create_parameters(bias_init_vector)\n",
    "        \n",
    "    def create_parameters(self, bias_init_vector):\n",
    "        # image embedding\n",
    "        self.encode_img_W = tf.Variable(tf.random_uniform([self.dim_image, self.dim_hidden], \\\n",
    "                                                          -0.1, 0.1), name='encode_img_W')\n",
    "        self.encode_img_b = self.init_bias(self.dim_hidden, name='encode_img_b')\n",
    "\n",
    "        # word embedding\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([self.n_words, self.dim_embed], \\\n",
    "                                                      -0.1, 0.1), name='Wemb')\n",
    "        self.bemb = self.init_bias(self.dim_embed, name='bemb')\n",
    "\n",
    "        # LSTM cell\n",
    "        self.lstm = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden)\n",
    "\n",
    "        # word prediction\n",
    "        self.embed_word_W = tf.Variable(tf.random_uniform([self.dim_hidden, self.n_words], \\\n",
    "                                                          -0.1, 0.1), name='embed_word_W')\n",
    "\n",
    "        if bias_init_vector is not None:\n",
    "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
    "        else:\n",
    "            self.embed_word_b = self.init_bias(self.n_words, name='embed_word_b')\n",
    "\n",
    "    def build_model(self, sess):\n",
    "        \"\"\" Build captioning model for training\"\"\"\n",
    "\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, 224, 224, 3])\n",
    "        sentence = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "\n",
    "        # extract image feature\n",
    "        vgg = vgg16(image, self.vgg16_weight_file, sess)\n",
    "        image_feat = vgg.fc7\n",
    "        \n",
    "        # embedding image feature to match the dimension of lstm input\n",
    "        image_emb = tf.matmul(image_feat, self.encode_img_W) + self.encode_img_b # (batch_size, dim_hidden)\n",
    "\n",
    "        state = self.lstm.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "        loss = 0.0\n",
    "        with tf.variable_scope(\"LSTM\") as lstm_scope:\n",
    "            for i in range(self.n_lstm_steps):\n",
    "                if i == 0:\n",
    "                    # As an initial input for LSTM, we use embedded image feature\n",
    "                    current_emb = image_emb\n",
    "                else:\n",
    "                    # After i=1, we use input words in ground-truth caption\n",
    "                    with tf.device(\"/cpu:0\"):\n",
    "                        current_emb = tf.nn.embedding_lookup(self.Wemb, sentence[:,i-1]) + self.bemb\n",
    "\n",
    "                if i > 0 : \n",
    "                    lstm_scope.reuse_variables()\n",
    "                \n",
    "                # compute hidden state of lstm\n",
    "                output, state = self.lstm(current_emb, state) # (batch_size, dim_hidden)\n",
    "\n",
    "                # At i=0, the predicted word is <S>, so ignore it\n",
    "                if i > 0: \n",
    "                    labels = sentence[:, i]\n",
    "\n",
    "                    logit_words = tf.matmul(output, self.embed_word_W) + self.embed_word_b # (batch_size, n_words)\n",
    "                    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit_words, labels=labels)\n",
    "                    cross_entropy = cross_entropy * mask[:,i]\n",
    "\n",
    "                    current_loss = tf.reduce_sum(cross_entropy)\n",
    "                    loss = loss + current_loss\n",
    "\n",
    "            loss = loss / tf.reduce_sum(mask[:,1:])\n",
    "            return loss, image, sentence, mask\n",
    "\n",
    "    def build_generator(self, maxlen, sess):\n",
    "        \"\"\" Build captioning model for test\"\"\"\n",
    "        \n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, 224, 224, 3])\n",
    "        \n",
    "        # extract image feature\n",
    "        #vgg = vgg16(image, self.vgg16_weight_file, sess)\n",
    "        vgg = vgg16(image)\n",
    "        image_feat = vgg.fc7\n",
    "        \n",
    "        # embedding image feature to match the dimension of lstm input\n",
    "        image_emb = tf.matmul(image_feat, self.encode_img_W) + self.encode_img_b\n",
    "\n",
    "        # set initial state as zero values\n",
    "        init_state = self.lstm.zero_state(self.batch_size, tf.float32)\n",
    "        generated_words = []\n",
    "\n",
    "        with tf.variable_scope(\"LSTM\") as lstm_scope:\n",
    "            output, prev_state = self.lstm(image_emb, init_state)\n",
    "            start_word_labels = tf.fill([self.batch_size], int(self.wtoi['<S>']))\n",
    "            \n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                input_word_vectors = tf.nn.embedding_lookup(self.Wemb, start_word_labels) + self.bemb\n",
    "\n",
    "            for i in range(maxlen):\n",
    "                lstm_scope.reuse_variables()\n",
    "\n",
    "                output, prev_state = self.lstm(input_word_vectors, prev_state)\n",
    "\n",
    "                logit_words = tf.matmul(output, self.embed_word_W) + self.embed_word_b\n",
    "                max_prob_word = tf.argmax(logit_words, 1)\n",
    "\n",
    "                # prepare word vector for next time step\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    input_word_vectors = tf.nn.embedding_lookup(self.Wemb, max_prob_word)\n",
    "                input_word_vectors += self.bemb\n",
    "\n",
    "                generated_words.append(max_prob_word)\n",
    "\n",
    "        return image, generated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################### Parameters #####################\n",
    "train_params = {\n",
    "    'json_path':'resource/train.json',\n",
    "    'h5py_path':'resource/train.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "test_params = {\n",
    "    'json_path':'resource/test.json',\n",
    "    'h5py_path':'resource/test.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "# create data loader for training and test data\n",
    "loaders = {}\n",
    "loaders['train'] = dataLoader(train_params)\n",
    "loaders['test'] = dataLoader(test_params)\n",
    "\n",
    "model_params = {\n",
    "    'wtoi': loaders['train'].getWtoi(),\n",
    "    'vgg16_weight_file': 'resource/vgg16_weights.npz',\n",
    "    'dim_image': 4096, # dimension of vgg16 network output\n",
    "    'dim_embed': 256,\n",
    "    'dim_hidden': 256,\n",
    "    'batch_size': 10,\n",
    "    'n_lstm_steps': loaders['train'].getMaxCaptionLength(),\n",
    "    'n_words': loaders['train'].getVocabSize(),\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    n_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    model_path = './models'\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    caption_generator = Caption_Generator(model_params)#, bias_init_vector=bias_init_vector)\n",
    "    loss, image, sentence, mask = caption_generator.build_model(sess)\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=50)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    iteration_per_epoch = loaders['train'].getNumCaptions() / model_params['batch_size']\n",
    "    for epoch in range(n_epochs):\n",
    "        for bi in range(10):\n",
    "            # load batch data\n",
    "            batch = loaders['train'].getBatch(model_params['batch_size'])\n",
    "\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={image: batch['images'], \n",
    "                                                                  sentence : batch['caption_labels'],\n",
    "                                                                  mask : batch['caption_masks']})\n",
    "\n",
    "            if bi % 100 == 0:\n",
    "                print \"%d epoch %d iteration Cost: %.4f\" % (epoch, bi, loss_value)\n",
    "\n",
    "        print \"Epoch \", epoch, \" is done. Saving the model ... \"\n",
    "        if not os.path.isdir(model_path):\n",
    "            os.makedirs(model_path)\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
    "        learning_rate *= 0.95\n",
    "\n",
    "def labelToCaption(caption_label, itow):\n",
    "    TODO = True\n",
    "    \n",
    "        \n",
    "def test(model_path='./models/model-0', maxlen=15):\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    # create caption generator for test\n",
    "    model_params['batch_size'] = 1\n",
    "    caption_generator = Caption_Generator(model_params)\n",
    "    image, output_caption = caption_generator.build_generator(maxlen=maxlen, sess=sess)\n",
    "    \n",
    "    # load the model parameters\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    itow = loaders['test'].getItow()\n",
    "    predictions = []\n",
    "    iteration_per_epoch = loaders['test'].getNumCaptions() / model_params['batch_size']\n",
    "    for bi in range(iteration_per_epoch):\n",
    "        # load batch data\n",
    "        batch = loaders['test'].getBatch(model_params['batch_size'])\n",
    "        \n",
    "        generated_word_index = sess.run(output_caption, feed_dict={image:batch['images']})\n",
    "        generated_word_index = np.hstack(generated_word_index)\n",
    "\n",
    "        generated_words = [ itow[str(x)] for x in generated_word_index]\n",
    "        punctuation = np.argmax(np.array(generated_words) == '.') + 1\n",
    "\n",
    "        generated_words = generated_words[:punctuation]\n",
    "        generated_sentence = ' '.join(generated_words)\n",
    "        \n",
    "        if bi % 100 == 0:\n",
    "            print('Generated Caption: ', generated_sentence)\n",
    "        \n",
    "        ith_prediction = {}\n",
    "        ith_prediction['image_id'] = batch['image_ids'][0]\n",
    "        ith_prediction['caption'] = generated_sentence\n",
    "        predictions.append(ith_prediction)\n",
    "        \n",
    "        if bi == 100: break\n",
    "        \n",
    "    # save the prediction outputs\n",
    "    if not os.path.isdir('result'):\n",
    "            os.makedirs('result')\n",
    "    write_json('result/predictions.json', predictions)\n",
    "    print('Save the predictions to result/predictions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test model that means to predict captions for test images\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model-4\n",
      "cars driving down a street next to a building .\n",
      "of\n",
      "of\n",
      "of\n",
      "living\n",
      "<S>\n",
      "at a train camera .\n",
      "with a <UNK> of <UNK> .\n",
      "to a <UNK> of <UNK> <S> .\n",
      "at\n",
      "mirror\n",
      "of\n",
      "in the middle of a field .\n",
      "pens a man standing in front of a white cake .\n",
      "view of a street sign and a <UNK> <S> .\n",
      "of\n",
      "of\n",
      "sign\n",
      "at\n",
      "of people walking down a street .\n",
      "<S> in the middle of a field .\n",
      "in\n",
      "of\n",
      "a man and a woman in a hot .\n",
      "of\n",
      "of\n",
      "jumps\n",
      "of\n",
      "mirror with a <UNK> on it <S> <S> .\n",
      "jumps\n",
      "jumps of a man riding a wave on a air .\n",
      "sign\n",
      "a\n",
      "of\n",
      "sign south chew <UNK> on a street sign .\n",
      ".\n",
      "jumps\n",
      "of\n",
      "jumps\n",
      "to\n",
      "close\n",
      "of\n",
      "jumps of a man riding a wave on a air .\n",
      "jumps\n",
      "to a <UNK> of <UNK> .\n",
      "of\n",
      "of\n",
      "at\n",
      "at a train camera .\n",
      "of\n",
      "of\n",
      "of a bathroom with a toilet and line .\n",
      "of\n",
      "a\n",
      "sign bull to a <UNK> .\n",
      "of\n",
      "pro a white plate with a talking and a drink .\n",
      "area\n",
      "of\n",
      "sign bull to a <UNK> <UNK> <UNK> .\n",
      "sign bull to a <UNK> .\n",
      "of\n",
      "jumps\n",
      "from\n",
      "of\n",
      "at\n",
      "pens a man riding a wave on a air .\n",
      "of\n",
      "of\n",
      "of\n",
      "from\n",
      "jumps\n",
      "at\n",
      "jumps is a man riding a wave on a air .\n",
      "to\n",
      "jumps of a man riding a wave on a air .\n",
      "at a table with a <UNK> <S> .\n",
      "of\n",
      "in\n",
      "at a table with a <UNK> <S> .\n",
      "and\n",
      "jumps\n",
      "of\n",
      "of\n",
      "jumps\n",
      "of\n",
      "of\n",
      "of\n",
      "view of a street sign and a <UNK> .\n",
      "view of a street sign and a <UNK> .\n",
      "with\n",
      "jumps\n",
      "jumps is a man and a woman playing giraffes .\n",
      "of\n",
      "<S>\n",
      "of\n",
      "pens a man riding a wave on a air .\n",
      "of\n",
      "sign\n",
      "at\n",
      "jumps of a man standing in front of a mirror .\n",
      "Save the predictions to result/predictions.json\n"
     ]
    }
   ],
   "source": [
    "# test model that means to predict captions for test images\n",
    "test(model_path='models/model-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('coco-caption')\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_file = 'coco-caption/annotations/captions_val2014.json'\n",
    "res_file = 'result/predictions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:00.686633\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# create coco object and cocoRes object\n",
    "coco = COCO(ann_file)\n",
    "cocoRes = coco.loadRes(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "{'reflen': 875, 'guess': [306, 206, 175, 144], 'testlen': 306, 'correct': [131, 25, 5, 1]}\n",
      "ratio: 0.349714285714\n",
      "Bleu_1: 0.067\n",
      "Bleu_2: 0.036\n",
      "Bleu_3: 0.018\n",
      "Bleu_4: 0.009\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.020\n"
     ]
    }
   ],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# evaluate on a subset of images by setting\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "\n",
    "# evaluate results\n",
    "cocoEval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
