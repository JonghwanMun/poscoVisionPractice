{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import tensorflow.python.platform\n",
    "#from keras.preprocessing import sequence\n",
    "#from collections import Counter\n",
    "#from cnn_util import *\n",
    "\n",
    "from vgg16 import vgg16\n",
    "from dataLoader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "  - 다운로드 제대로 된 후에는 다시 실행할 필요 없습니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# download images\n",
    "os.system('wget cvlab.postech.ac.kr/~jonghwan/224x224_mscoco_images.tar.gz')\n",
    "os.system('tar xvzf 224x224_mscoco_images.tar.gz')\n",
    "os.system('rm 224x224_mscoco_images.tar.gz')\n",
    "os.system('mv 224x224_mscoco_images resource/224x224_mscoco_images')\n",
    "\n",
    "# download vgg16 network\n",
    "os.system('wget cvlab.postech.ac.kr/~jonghwan/vgg16_weights.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Captioning Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, params, bias_init_vector=None):\n",
    "        #dim_image, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, bias_init_vector=None):\n",
    "\n",
    "        self.vgg16_weight_file = params['vgg16_weight_file']\n",
    "        self.dim_image = np.int(params['dim_image'])\n",
    "        self.dim_embed = np.int(params['dim_embed'])\n",
    "        self.dim_hidden = np.int(params['dim_hidden'])\n",
    "        self.batch_size = np.int(params['batch_size'])\n",
    "        self.n_lstm_steps = np.int(params['n_lstm_steps'])\n",
    "        self.n_words = np.int(params['n_words'])\n",
    "        \n",
    "        # create model parameters\n",
    "        self.create_parameters(bais_init_vector)\n",
    "        \n",
    "    def create_parameters(self, bias_init_vector):\n",
    "        # image embedding\n",
    "        self.encode_img_W = tf.Variable(tf.random_uniform([self.dim_image, self.dim_hidden], \\\n",
    "                                                          -0.1, 0.1), name='encode_img_W')\n",
    "        self.encode_img_b = self.init_bias(self.dim_hidden, name='encode_img_b')\n",
    "        \n",
    "        # word embedding\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([self.n_words, self.dim_embed], \\\n",
    "                                                      -0.1, 0.1), name='Wemb')\n",
    "        self.bemb = self.init_bias(self.dim_embed, name='bemb')\n",
    "\n",
    "        # LSTM cell\n",
    "        self.lstm = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden)\n",
    "\n",
    "        # word prediction\n",
    "        self.embed_word_W = tf.Variable(tf.random_uniform([self.dim_hidden, self.n_words], \\\n",
    "                                                          -0.1, 0.1), name='embed_word_W')\n",
    "\n",
    "        if bias_init_vector is not None:\n",
    "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
    "        else:\n",
    "            self.embed_word_b = self.init_bias(self.n_words, name='embed_word_b')\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\" Build captioning model for training\"\"\"\n",
    "\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, 224, 224, 3])\n",
    "        sentence = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "\n",
    "        # extract image feature\n",
    "        vgg = vgg16(image, self.vgg16_weight_file, sess)\n",
    "        image_feat = vgg.fc7\n",
    "        \n",
    "        # embedding image feature to match the dimension of lstm input\n",
    "        image_emb = tf.matmul(image_feat, self.encode_img_W) + self.encode_img_b # (batch_size, dim_hidden)\n",
    "\n",
    "        state = tf.zeros([self.batch_size, self.lstm.state_size])\n",
    "\n",
    "        loss = 0.0\n",
    "        with tf.variable_scope(\"LSTM\"):\n",
    "            for i in range(self.n_lstm_steps): # maxlen + 1\n",
    "                if i == 0:\n",
    "                    current_emb = image_emb\n",
    "                else:\n",
    "                    with tf.device(\"/cpu:0\"):\n",
    "                        current_emb = tf.nn.embedding_lookup(self.Wemb, sentence[:,i-1]) + self.bemb\n",
    "\n",
    "                if i > 0 : tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                output, state = self.lstm(current_emb, state) # (batch_size, dim_hidden)\n",
    "\n",
    "                if i > 0: # 이미지 다음 바로 나오는건 #START# 임. 이건 무시.\n",
    "                    labels = sentence[:, i]\n",
    "                    #tf.expand_dims(sentence[:, i], 1) # (batch_size)\n",
    "                    #indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "                    #concated = tf.concat(1, [indices, labels])\n",
    "                    #onehot_labels = tf.sparse_to_dense(\n",
    "                    #        concated, tf.pack([self.batch_size, self.n_words]), 1.0, 0.0) # (batch_size, n_words)\n",
    "\n",
    "                    logit_words = tf.matmul(output, self.embed_word_W) + self.embed_word_b # (batch_size, n_words)\n",
    "                    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logit_words, labels)\n",
    "                    cross_entropy = cross_entropy * mask[:,i]#tf.expand_dims(mask, 1)\n",
    "\n",
    "                    current_loss = tf.reduce_sum(cross_entropy)\n",
    "                    loss = loss + current_loss\n",
    "\n",
    "            loss = loss / tf.reduce_sum(mask[:,1:])\n",
    "            return loss, image, sentence, mask\n",
    "\n",
    "    def build_generator(self, maxlen):\n",
    "        \"\"\" Build captioning model for test\"\"\"\n",
    "        \n",
    "        image = tf.placeholder(tf.float32, [1, self.dim_image])\n",
    "        image_emb = tf.matmul(image, self.encode_img_W) + self.encode_img_b\n",
    "\n",
    "        state = tf.zeros([1, self.lstm.state_size])\n",
    "        #last_word = image_emb # 첫 단어 대신 이미지\n",
    "        generated_words = []\n",
    "\n",
    "        with tf.variable_scope(\"LSTM\"):\n",
    "            output, state = self.lstm(image_emb, state)\n",
    "            last_word = tf.nn.embedding_lookup(self.Wemb, [0]) + self.bemb\n",
    "\n",
    "            for i in range(maxlen):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                output, state = self.lstm(last_word, state)\n",
    "\n",
    "                logit_words = tf.matmul(output, self.embed_word_W) + self.embed_word_b\n",
    "                max_prob_word = tf.argmax(logit_words, 1)\n",
    "\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    last_word = tf.nn.embedding_lookup(self.Wemb, max_prob_word)\n",
    "\n",
    "                last_word += self.bemb\n",
    "\n",
    "                generated_words.append(max_prob_word)\n",
    "\n",
    "        return image, generated_words\n",
    "\n",
    "def get_caption_data(annotation_path, feat_path):\n",
    "    feats = np.load(feat_path)\n",
    "    annotations = pd.read_table(annotation_path, sep='\\t', header=None, names=['image', 'caption'])\n",
    "    captions = annotations['caption'].values\n",
    "\n",
    "    return feats, captions\n",
    "\n",
    "def preProBuildWordVocab(sentence_iterator, word_count_threshold=30): # borrowed this function from NeuralTalk\n",
    "    print 'preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold, )\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in sentence_iterator:\n",
    "        nsents += 1\n",
    "        for w in sent.lower().split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print 'filtered words from %d to %d' % (len(word_counts), len(vocab))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '.'  # period at the end of the sentence. make first dimension be end token\n",
    "    wordtoix = {}\n",
    "    wordtoix['#START#'] = 0 # make first vector be the start token\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "      wordtoix[w] = ix\n",
    "      ixtoword[ix] = w\n",
    "      ix += 1\n",
    "\n",
    "    word_counts['.'] = nsents\n",
    "    bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "    return wordtoix, ixtoword, bias_init_vector\n",
    "\n",
    "\n",
    "################### 학습 관련 Parameters #####################\n",
    "\n",
    "dim_embed = 256\n",
    "dim_hidden = 256\n",
    "dim_image = 4096\n",
    "batch_size = 128\n",
    "\n",
    "#learning_rate = 0.001\n",
    "n_epochs = 1000\n",
    "###############################################################\n",
    "#################### 잡다한 Parameters ########################\n",
    "model_path = './models/tensorflow'\n",
    "vgg_path = './data/vgg16.tfmodel'\n",
    "data_path = './data'\n",
    "feat_path = './data/feats.npy'\n",
    "annotation_path = os.path.join(data_path, 'results_20130124.token')\n",
    "################################################################\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    feats, captions = get_caption_data(annotation_path, feat_path)\n",
    "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(captions)\n",
    "\n",
    "    np.save('data/ixtoword', ixtoword)\n",
    "\n",
    "    index = np.arange(len(feats))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    feats = feats[index]\n",
    "    captions = captions[index]\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    n_words = len(wordtoix)\n",
    "    maxlen = np.max( map(lambda x: len(x.split(' ')), captions) )\n",
    "    caption_generator = Caption_Generator(\n",
    "            dim_image=dim_image,\n",
    "            dim_hidden=dim_hidden,\n",
    "            dim_embed=dim_embed,\n",
    "            batch_size=batch_size,\n",
    "            n_lstm_steps=maxlen+2,\n",
    "            n_words=n_words,\n",
    "            bias_init_vector=bias_init_vector)\n",
    "\n",
    "    loss, image, sentence, mask = caption_generator.build_model()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=50)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        #train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        for start, end in zip( \\\n",
    "                range(0, len(feats), batch_size),\n",
    "                range(batch_size, len(feats), batch_size)\n",
    "                ):\n",
    "\n",
    "            current_feats = feats[start:end]\n",
    "            current_captions = captions[start:end]\n",
    "\n",
    "            current_caption_ind = map(lambda cap: [wordtoix[word] for word in cap.lower().split(' ')[:-1] if word in wordtoix], current_captions)\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=maxlen+1)\n",
    "            current_caption_matrix = np.hstack( [np.full( (len(current_caption_matrix),1), 0), current_caption_matrix] ).astype(int)\n",
    "\n",
    "            current_mask_matrix = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array( map(lambda x: (x != 0).sum()+2, current_caption_matrix ))\n",
    "            #  +2 -> #START# and '.'\n",
    "\n",
    "            for ind, row in enumerate(current_mask_matrix):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={\n",
    "                image: current_feats,\n",
    "                sentence : current_caption_matrix,\n",
    "                mask : current_mask_matrix\n",
    "                })\n",
    "\n",
    "            print \"Current Cost: \", loss_value\n",
    "\n",
    "        print \"Epoch \", epoch, \" is done. Saving the model ... \"\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
    "        learning_rate *= 0.95\n",
    "\n",
    "def test(test_feat='./guitar_player.npy', model_path='./models/tensorflow/model-1', maxlen=30): # Naive greedy search\n",
    "\n",
    "    ixtoword = np.load('data/ixtoword.npy').tolist()\n",
    "    n_words = len(ixtoword)\n",
    "\n",
    "    feat = [np.load(test_feat)]\n",
    "    sess = tf.InteractiveSession()\n",
    "    caption_generator = Caption_Generator(\n",
    "           dim_image=dim_image,\n",
    "           dim_hidden=dim_hidden,\n",
    "           dim_embed=dim_embed,\n",
    "           batch_size=batch_size,\n",
    "           n_lstm_steps=maxlen,\n",
    "           n_words=n_words)\n",
    "\n",
    "    image, generated_words = caption_generator.build_generator(maxlen=maxlen)\n",
    "    # 이 부분이 존나 중요함. 계속 caption_generator를 가져온 뒤 바로 restore를 했었는데,\n",
    "    # TensorFlow의 LSTM은 call을 한 뒤에 weight가 만들어지기 때문에 build_generator보다 뒤쪽에서 restore를 해야 함.\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    generated_word_index= sess.run(generated_words, feed_dict={image:feat})\n",
    "    generated_word_index = np.hstack(generated_word_index)\n",
    "\n",
    "    generated_sentence = [ixtoword[x] for x in generated_word_index]\n",
    "\n",
    "\n",
    "def read_image(path):\n",
    "\n",
    "     img = crop_image(path, target_height=224, target_width=224)\n",
    "     if img.shape[2] == 4:\n",
    "         img = img[:,:,:3]\n",
    "\n",
    "     img = img[None, ...]\n",
    "     return img\n",
    "\n",
    "\n",
    "def test_tf(test_image_path=None, model_path='./models/model-72', maxlen=30):\n",
    "    with open(vgg_path) as f:\n",
    "        fileContent = f.read()\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(fileContent)\n",
    "\n",
    "    images = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    tf.import_graph_def(graph_def, input_map={\"images\":images})\n",
    "\n",
    "    ixtoword = np.load('./data/ixtoword.npy').tolist()\n",
    "    n_words = len(ixtoword)\n",
    "\n",
    "    image_val = read_image(test_image_path)\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    caption_generator = Caption_Generator(\n",
    "           dim_image=dim_image,\n",
    "           dim_hidden=dim_hidden,\n",
    "           dim_embed=dim_embed,\n",
    "           batch_size=batch_size,\n",
    "           n_lstm_steps=maxlen,\n",
    "           n_words=n_words)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    fc7 = sess.run(graph.get_tensor_by_name(\"import/fc7_relu:0\"), feed_dict={images:image_val})\n",
    "\n",
    "    fc7_tf, generated_words = caption_generator.build_generator(maxlen=maxlen)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    generated_word_index= sess.run(generated_words, feed_dict={fc7_tf:fc7})\n",
    "    generated_word_index = np.hstack(generated_word_index)\n",
    "\n",
    "    generated_words = [ixtoword[x] for x in generated_word_index]\n",
    "    punctuation = np.argmax(np.array(generated_words) == '.')+1\n",
    "\n",
    "    generated_words = generated_words[:punctuation]\n",
    "    generated_sentence = ' '.join(generated_words)\n",
    "    print generated_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
