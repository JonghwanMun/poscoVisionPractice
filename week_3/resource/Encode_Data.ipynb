{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import nltk.tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    return json_file\n",
    "\n",
    "def write_json(json_path, json_file):\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_file, f)\n",
    "\n",
    "\n",
    "def process_caption(caption):\n",
    "    \"\"\"Insert start/end token into a list of tokenized words in caption\"\"\"\n",
    "    \n",
    "    tokenized_caption = ['<S>']\n",
    "    tokenized_caption.extend(nltk.tokenize.word_tokenize(caption.lower()))\n",
    "    tokenized_caption.append('<E>')\n",
    "    return tokenized_caption\n",
    "    \n",
    "    \n",
    "def create_vocab(captions, min_word_count=3):\n",
    "    \"\"\"Construct vocabulary\n",
    "    \n",
    "    Args:\n",
    "        captions: A list of caption annotations \n",
    "                    {\n",
    "                    'image_id': 'int',\n",
    "                    'image_path': 'string',\n",
    "                    'captions: 'list'\n",
    "                    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenize the captions\n",
    "    for cap_ann in captions:\n",
    "        cap_ann['tokenized_captions'] = []\n",
    "        for cap in cap_ann['captions']:\n",
    "            cap_ann['tokenized_captions'].append(\n",
    "                    process_caption(cap['caption']))\n",
    "            \n",
    "    # count the words in all captions\n",
    "    word_count = {}\n",
    "    for cap_ann in captions:\n",
    "        for cap in cap_ann['tokenized_captions']:\n",
    "            for word in cap:\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "    print('Total words:', len(word_count))\n",
    "    \n",
    "    # discard the words where the number is smaller than min_word_count\n",
    "    vocab = [x for x in word_count.items() if x[1] >= min_word_count]\n",
    "    vocab.sort(key=lambda x: x[1], reverse=True)\n",
    "    vocab = [x[0] for x in vocab]\n",
    "    vocab.insert(0, '<UNK>')\n",
    "    print('Words in vocabulary:', len(vocab))\n",
    "    print('Top 20 words:\\n', vocab[:20])\n",
    "        \n",
    "    # contruct the vocabulary dictionary\n",
    "    wtoi = {w:i for i,w in enumerate(vocab)}\n",
    "    itow = {i:w for i,w in enumerate(vocab)}\n",
    "    \n",
    "    return vocab, wtoi, itow, num_caption\n",
    "\n",
    "def count_captions(captions):\n",
    "    \"\"\" Count the number of captions \"\"\"\n",
    "    num_caption = 0\n",
    "    for cap_ann in captions:\n",
    "        cap_ann['tokenized_captions'] = []\n",
    "        for cap in cap_ann['captions']:\n",
    "            cap_ann['tokenized_captions'].append(\n",
    "                    process_caption(cap['caption']))\n",
    "            num_caption += 1\n",
    "    print('Number of captions:', num_caption)\n",
    "    return num_caption\n",
    "\n",
    "def encode_annotations(anns, vocab, wtoi, itow, datatype, max_question_length=15):\n",
    "    \"\"\" Encode annotations and save them\n",
    "    Args:\n",
    "        anns: annotations, 'list of dictionaries'\n",
    "        vocab: vocabulary, 'list'\n",
    "        wtoi: dictionary of word to index of vocabulary\n",
    "        itow: dictionary of index to word of vocabulary\n",
    "        datatype: data type (train|test), 'string'\n",
    "        max_question_length: maximum length of each question when converting to the label\n",
    "    \"\"\"\n",
    "    \n",
    "    num_captions = count_captions(anns)\n",
    "    question_length = np.zeros(num_captions, dtype='uint32')\n",
    "    question_labels = np.zeros((num_captions, max_question_length), dtype='uint32')\n",
    "    question_ids = []\n",
    "    image_ids = []\n",
    "    image_file_name = []\n",
    "    \n",
    "    idx_cap  = 0\n",
    "    for ann in anns:\n",
    "        img_id = ann['image_id']\n",
    "        img_path = ann['image_path']\n",
    "        for cap, info in zip(ann['tokenized_captions'], ann['captions']):\n",
    "            # numpy data\n",
    "            question_length[idx_cap] = min(len(cap), max_question_length)\n",
    "            for i,w in enumerate(cap):\n",
    "                if i == (max_question_length-1):\n",
    "                    question_labels[idx_cap, i] = wtoi['<E>']\n",
    "                    break\n",
    "                if w in vocab:\n",
    "                    question_labels[idx_cap, i] = wtoi[w]\n",
    "                else:\n",
    "                    question_labels[idx_cap, i] = wtoi['<UNK>']\n",
    "            \n",
    "            # json data\n",
    "            question_ids.append(info['id'])\n",
    "            image_file_name.append(img_path)\n",
    "            image_ids.append(img_id)\n",
    "            \n",
    "            idx_cap += 1\n",
    "    \n",
    "    \n",
    "    # save the numpy data\n",
    "    save_h5py_path = '%s.h5' % datatype\n",
    "    f = h5py.File(save_h5py_path, 'w')\n",
    "    f.create_dataset('question_labels', dtype='uint32',\n",
    "                    data=question_labels)\n",
    "    f.create_dataset('question_length', dtype='uint32',\n",
    "                    data=question_length)\n",
    "    f.close()\n",
    "    print('Save %s h5py file to %s' % (datatype, save_h5py_path))\n",
    "    \n",
    "    # save the list data\n",
    "    save_json_path = '%s.json' % datatype\n",
    "    save_json_file = {}\n",
    "    save_json_file['wtoi'] = wtoi\n",
    "    save_json_file['itow'] = itow\n",
    "    save_json_file['question_ids'] = question_ids\n",
    "    save_json_file['image_ids'] = image_ids\n",
    "    save_json_file['image_path'] = image_file_name\n",
    "    write_json(save_json_path, save_json_file)\n",
    "    print('Save %s json file to %s' % (datatype, save_json_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MSCOCO Caption Annotations for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_info_file = 'train_caption_info.json'\n",
    "test_info_file = 'test_caption_info.json'\n",
    "\n",
    "train_info = load_json(train_info_file)\n",
    "test_info = load_json(test_info_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 7353\n",
      "Words in vocabulary: 3244\n",
      "Top 20 words:\n",
      " ['<UNK>', 'a', '<E>', '<S>', '.', 'on', 'of', 'the', 'in', 'with', 'and', 'is', 'man', 'to', 'sitting', 'an', 'two', ',', 'at', 'standing']\n"
     ]
    }
   ],
   "source": [
    "# Construct vocabulary for training data\n",
    "vocab, wtoi, itow, num_caption = create_vocab(train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions: 25010\n",
      "Save train h5py file to train.h5\n",
      "Number of captions: 5001\n",
      "Save test h5py file to test.h5\n"
     ]
    }
   ],
   "source": [
    "# encode labels and lengths of questions\n",
    "encode_annotations(train_info, vocab, wtoi, itow, 'train')\n",
    "encode_annotations(test_info, vocab, wtoi, itow, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
