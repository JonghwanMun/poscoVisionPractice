{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import random\n",
    "import nltk.tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    return json_file\n",
    "\n",
    "def write_json(json_path, json_file):\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_file, f)\n",
    "\n",
    "\n",
    "def process_caption(caption):\n",
    "    \"\"\"Insert start/end token into a list of tokenized words in caption\"\"\"\n",
    "    \n",
    "    tokenized_caption = ['<S>']\n",
    "    tokenized_caption.extend(nltk.tokenize.word_tokenize(caption.lower()))\n",
    "    tokenized_caption.append('<E>')\n",
    "    return tokenized_caption\n",
    "    \n",
    "    \n",
    "def create_vocab(captions, min_word_count=3):\n",
    "    \"\"\"Construct vocabulary\n",
    "    \n",
    "    Args:\n",
    "        captions: A list of caption annotations \n",
    "                    {\n",
    "                    'image_id': 'int',\n",
    "                    'image_path': 'string',\n",
    "                    'captions: 'list'\n",
    "                    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenize the captions\n",
    "    for cap_ann in captions:\n",
    "        cap_ann['tokenized_captions'] = []\n",
    "        for cap in cap_ann['captions']:\n",
    "            cap_ann['tokenized_captions'].append(\n",
    "                    process_caption(cap['caption']))\n",
    "            \n",
    "    # count the words in all captions\n",
    "    word_count = {}\n",
    "    for cap_ann in captions:\n",
    "        for cap in cap_ann['tokenized_captions']:\n",
    "            for word in cap:\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "    print('Total words:', len(word_count))\n",
    "    \n",
    "    # discard the words where the number is smaller than min_word_count\n",
    "    vocab = [x for x in word_count.items() if x[1] >= min_word_count]\n",
    "    vocab.sort(key=lambda x: x[1], reverse=True)\n",
    "    vocab = [x[0] for x in vocab]\n",
    "    vocab.insert(0, '<UNK>')\n",
    "    print('Words in vocabulary:', len(vocab))\n",
    "    print('Top 20 words:\\n', vocab[:20])\n",
    "        \n",
    "    # contruct the vocabulary dictionary\n",
    "    wtoi = {w:i for i,w in enumerate(vocab)}\n",
    "    itow = {i:w for i,w in enumerate(vocab)}\n",
    "    \n",
    "    return vocab, wtoi, itow\n",
    "\n",
    "def count_captions(captions):\n",
    "    \"\"\" Count the number of captions \"\"\"\n",
    "    num_caption = 0\n",
    "    for cap_ann in captions:\n",
    "        cap_ann['tokenized_captions'] = []\n",
    "        for cap in cap_ann['captions']:\n",
    "            cap_ann['tokenized_captions'].append(\n",
    "                    process_caption(cap['caption']))\n",
    "            num_caption += 1\n",
    "    print('Number of captions:', num_caption)\n",
    "    return num_caption\n",
    "\n",
    "def encode_annotations(anns, vocab, wtoi, itow, datatype, max_caption_length=15):\n",
    "    \"\"\" Encode annotations and save them\n",
    "    Args:\n",
    "        anns: annotations, 'list of dictionaries'\n",
    "        vocab: vocabulary, 'list'\n",
    "        wtoi: dictionary of word to index of vocabulary\n",
    "        itow: dictionary of index to word of vocabulary\n",
    "        datatype: data type (train|test), 'string'\n",
    "        max_caption_length : maximum length of each caption when converting to the label\n",
    "    \"\"\"\n",
    "    \n",
    "    num_captions = count_captions(anns)\n",
    "    caption_length = np.zeros(num_captions, dtype='uint32')\n",
    "    caption_labels = np.zeros((num_captions, max_caption_length), dtype='uint32')\n",
    "    caption_ids = []\n",
    "    image_ids = []\n",
    "    image_file_name = []\n",
    "    \n",
    "    idx_cap  = 0\n",
    "    for ann in anns:\n",
    "        img_id = ann['image_id']\n",
    "        img_path = ann['image_path']\n",
    "        \n",
    "        if datatype == 'train':\n",
    "            for cap, info in zip(ann['tokenized_captions'], ann['captions']):\n",
    "                # numpy data\n",
    "                caption_length[idx_cap] = min(len(cap), max_caption_length)\n",
    "                for i,w in enumerate(cap):\n",
    "                    if i == (max_caption_length-1):\n",
    "                        caption_labels[idx_cap, i] = wtoi['<E>']\n",
    "                        break\n",
    "                    if w in vocab:\n",
    "                        caption_labels[idx_cap, i] = wtoi[w]\n",
    "                    else:\n",
    "                        caption_labels[idx_cap, i] = wtoi['<UNK>']\n",
    "\n",
    "                # json data\n",
    "                caption_ids.append(info['id'])\n",
    "                image_file_name.append(img_path)\n",
    "                image_ids.append(img_id)\n",
    "\n",
    "                idx_cap += 1\n",
    "        else:\n",
    "            idx = random.randint(0, len(ann['tokenized_captions'])-1)\n",
    "            cap, info = ann['tokenized_captions'][idx], ann['captions'][idx]\n",
    "            # numpy data\n",
    "            caption_length[idx_cap] = min(len(cap), max_caption_length)\n",
    "            for i,w in enumerate(cap):\n",
    "                if i == (max_caption_length-1):\n",
    "                    caption_labels[idx_cap, i] = wtoi['<E>']\n",
    "                    break\n",
    "                if w in vocab:\n",
    "                    caption_labels[idx_cap, i] = wtoi[w]\n",
    "                else:\n",
    "                    caption_labels[idx_cap, i] = wtoi['<UNK>']\n",
    "\n",
    "            # json data\n",
    "            caption_ids.append(info['id'])\n",
    "            image_file_name.append(img_path)\n",
    "            image_ids.append(img_id)           \n",
    "            \n",
    "            idx_cap += 1\n",
    "    \n",
    "    \n",
    "    # save the numpy data\n",
    "    save_h5py_path = 'resource/%s.h5' % datatype\n",
    "    f = h5py.File(save_h5py_path, 'w')\n",
    "    f.create_dataset('caption_labels', dtype='uint32',\n",
    "                    data=caption_labels)\n",
    "    f.create_dataset('caption_length', dtype='uint32',\n",
    "                    data=caption_length)\n",
    "    f.close()\n",
    "    print('Save %s h5py file to %s' % (datatype, save_h5py_path))\n",
    "    \n",
    "    # save the list data\n",
    "    save_json_path = 'resource/%s.json' % datatype\n",
    "    save_json_file = {}\n",
    "    save_json_file['wtoi'] = wtoi\n",
    "    save_json_file['itow'] = itow\n",
    "    save_json_file['caption_ids'] = caption_ids\n",
    "    save_json_file['image_ids'] = image_ids\n",
    "    save_json_file['image_path'] = image_file_name\n",
    "    save_json_file['max_caption_length'] = max_caption_length\n",
    "    write_json(save_json_path, save_json_file)\n",
    "    print('Save %s json file to %s' % (datatype, save_json_path))\n",
    "    print('The number of saved captions: %d\\n' % len(save_json_file['image_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load MSCOCO Caption Annotations for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_info_file = 'resource/train_caption_info.json'\n",
    "test_info_file = 'resource/test_caption_info.json'\n",
    "\n",
    "train_info = load_json(train_info_file)\n",
    "test_info = load_json(test_info_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total words:', 7353)\n",
      "('Words in vocabulary:', 3244)\n",
      "('Top 20 words:\\n', ['<UNK>', u'a', '<S>', '<E>', u'.', u'on', u'of', u'the', u'in', u'with', u'and', u'is', u'man', u'to', u'sitting', u'an', u'two', u',', u'at', u'standing'])\n"
     ]
    }
   ],
   "source": [
    "# Construct vocabulary for training data\n",
    "vocab, wtoi, itow = create_vocab(train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of captions:', 25010)\n",
      "Save train h5py file to resource/train.h5\n",
      "Save train json file to resource/train.json\n",
      "The number of saved captions: 25010\n",
      "\n",
      "('Number of captions:', 5001)\n",
      "Save test h5py file to resource/test.h5\n",
      "Save test json file to resource/test.json\n",
      "The number of saved captions: 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encode labels and lengths of captions\n",
    "encode_annotations(train_info, vocab, wtoi, itow, 'train')\n",
    "encode_annotations(test_info, vocab, wtoi, itow, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
