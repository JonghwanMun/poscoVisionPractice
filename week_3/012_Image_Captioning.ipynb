{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from vgg16 import vgg16\n",
    "from dataLoader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data download\n",
    "  - 다운로드 제대로 된 후에는 다시 실행할 필요 없습니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('resource/vgg16_weights.npz'):\n",
    "    # download images\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/224x224_mscoco_images.tar.gz')\n",
    "    os.system('tar xvzf 224x224_mscoco_images.tar.gz')\n",
    "    os.system('rm 224x224_mscoco_images.tar.gz')\n",
    "    os.system('mv 224x224_mscoco_images resource/224x224_mscoco_images')\n",
    "\n",
    "    # download vgg16 network\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/vgg16_weights.npz')\n",
    "    os.system('mv vgg16_weights.npz resource/vgg16_weights.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Captioning Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, params, bias_init_vector=None):\n",
    "        #dim_image, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, bias_init_vector=None):\n",
    "        \n",
    "        self.wtoi = params['wtoi']\n",
    "        self.vgg16_weight_file = params['vgg16_weight_file']\n",
    "        \n",
    "        # captioning model parameter dimensions\n",
    "        self.dim_image_feat = np.int(params['dim_image'])\n",
    "        self.dim_word_embedding = np.int(params['dim_embed'])\n",
    "        self.dim_hidden_lstm = np.int(params['dim_hidden'])\n",
    "        self.batch_size = np.int(params['batch_size'])\n",
    "        self.n_lstm_steps = np.int(params['n_lstm_steps'])\n",
    "        self.n_words = np.int(params['n_words'])\n",
    "        \n",
    "        self.initializer = tf.random_uniform_initializer(minval=-0.8, maxval=0.8)\n",
    "\n",
    "    def build_model(self, sess):\n",
    "        \"\"\" Build captioning model for training\"\"\"\n",
    "\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, 224, 224, 3])\n",
    "        input_labels = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        target_labels = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "\n",
    "        # extract image feature\n",
    "        vgg = vgg16(image, self.vgg16_weight_file, sess, trainable=False)\n",
    "        image_feat = vgg.fc7\n",
    "        \n",
    "        # embed image feature to match the dimension of lstm input\n",
    "        with tf.variable_scope(\"image_embedding\") as img_embed_scope:\n",
    "            image_embeddings = tf.contrib.layers.fully_connected(\n",
    "                                    inputs=image_feat,\n",
    "                                    num_outputs=self.dim_hidden_lstm,\n",
    "                                    activation_fn=None,\n",
    "                                    weights_initializer=self.initializer,\n",
    "                                    biases_initializer=None,\n",
    "                                    scope=img_embed_scope)\n",
    "            \n",
    "        # embed input caption labels to obtain word embeddings\n",
    "        with tf.variable_scope(\"word_embedding\"), tf.device('/cpu:0'):\n",
    "            word_embedding_matrix = tf.get_variable(\n",
    "                                        name='word_embedding_matrix',\n",
    "                                        shape=[self.n_words, self.dim_word_embedding],\n",
    "                                        initializer=self.initializer\n",
    "                                        )\n",
    "            word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, input_labels)\n",
    "                                                            \n",
    "        \n",
    "        # for LSTM cell unit\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden_lstm, state_is_tuple=True)\n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "            zero_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "            \n",
    "            # allow the LSTM variables to be resued\n",
    "            lstm_scope.reuse_variables()\n",
    "            \n",
    "            # run the batch of word embeddings throught the LSTM\n",
    "            caption_length = tf.reduce_sum(mask, 1)\n",
    "            lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                               inputs=word_embeddings,\n",
    "                                               sequence_length=caption_length,\n",
    "                                               initial_state=initial_state,\n",
    "                                               dtype=tf.float32,\n",
    "                                               scope=lstm_scope)\n",
    "        \n",
    "        # stack batches vertically\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        # compute probabilities for words\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                        inputs=lstm_outputs,\n",
    "                        num_outputs=self.n_words,\n",
    "                        activation_fn=None,\n",
    "                        weights_initializer=self.initializer,\n",
    "                        scope=logits_scope)\n",
    "        \n",
    "        targets = tf.reshape(target_labels, [-1])\n",
    "        weights = tf.to_float(tf.reshape(mask, [-1]))\n",
    "        \n",
    "        # compute losses\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                              logits=logits)\n",
    "        batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "                            tf.reduce_sum(weights), name=\"batch_loss\")\n",
    "        tf.losses.add_loss(batch_loss)\n",
    "        total_loss = tf.losses.get_total_loss()\n",
    "        \n",
    "        probs = tf.nn.softmax(logits, name='softmax')\n",
    "\n",
    "        return image, input_labels, target_labels, mask, total_loss, probs\n",
    "\n",
    "    def build_generator(self, sess):\n",
    "        \"\"\" Build captioning model for test\"\"\"\n",
    "        \n",
    "        # extract image feature\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, 224, 224, 3], name='image_raw')\n",
    "        vgg = vgg16(image, self.vgg16_weight_file, sess, trainable=False)\n",
    "        image_feat = vgg.fc7\n",
    "        \n",
    "        # embed image feature to match the dimension of lstm input\n",
    "        with tf.variable_scope(\"image_embedding\") as img_embed_scope:\n",
    "            encoded_images = tf.contrib.layers.fully_connected(\n",
    "                                    inputs=image_feat,\n",
    "                                    num_outputs=self.dim_hidden_lstm,\n",
    "                                    activation_fn=None,\n",
    "                                    weights_initializer=self.initializer,\n",
    "                                    biases_initializer=None,\n",
    "                                    scope=img_embed_scope)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Below part is for caption generation (LSTM)\n",
    "        image_embeddings = tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=[self.batch_size, self.dim_image_feat],\n",
    "                                          name='image_feed')\n",
    "        input_feed = tf.placeholder(dtype=tf.int64, shape=[None], name='input_feed')\n",
    "        input_labels = tf.expand_dims(input_feed, 1)\n",
    "\n",
    "        # embed input caption labels to obtain word embeddings\n",
    "        with tf.variable_scope(\"word_embedding\"), tf.device('/cpu:0'):\n",
    "            word_embedding_matrix = tf.get_variable(\n",
    "                                        name='word_embedding_matrix',\n",
    "                                        shape=[self.n_words, self.dim_word_embedding],\n",
    "                                        initializer=self.initializer\n",
    "                                        )\n",
    "            word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, input_labels)                                                            \n",
    "        \n",
    "        # for LSTM cell unit\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden_lstm, state_is_tuple=True)\n",
    "        \n",
    "        tmp = [var for var in tf.global_variables() if var.op.name=='lstm/basic_lstm_cell/weights']\n",
    "        print(tmp)\n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "            zero_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "            \n",
    "            # allow the LSTM variables to be resued\n",
    "            lstm_scope.reuse_variables()\n",
    "            \n",
    "            # In inference mode, use concatenated states for convenient feeding and fetching.\n",
    "            tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n",
    "\n",
    "            # Placeholder for feeding a batch of concatenated states.\n",
    "            state_feed = tf.placeholder(dtype=tf.float32,\n",
    "                                        shape=[None, sum(lstm_cell.state_size)],\n",
    "                                        name=\"state_feed\")\n",
    "            state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n",
    "\n",
    "            # Run a single LSTM step.\n",
    "            lstm_outputs, state_tuple = lstm_cell(\n",
    "                inputs=tf.squeeze(word_embeddings, axis=[1]), state=state_tuple)\n",
    "\n",
    "            # Concatentate the resulting state.\n",
    "            tf.concat(axis=1, values=state_tuple, name=\"state\")\n",
    "            \n",
    "        \n",
    "        # stack batches vertically\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        # compute probabilities for words\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                        inputs=lstm_outputs,\n",
    "                        num_outputs=self.n_words,\n",
    "                        activation_fn=None,\n",
    "                        weights_initializer=self.initializer,\n",
    "                        scope=logits_scope)\n",
    "        probs = tf.nn.softmax(logits, name='softmax')\n",
    "\n",
    "        return images, encoded_images, image_embeddings, state_feed, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Parameters #####################\n",
    "train_params = {\n",
    "    'json_path':'resource/train.json',\n",
    "    'h5py_path':'resource/train.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "test_params = {\n",
    "    'json_path':'resource/test.json',\n",
    "    'h5py_path':'resource/test.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "# create data loader for training and test data\n",
    "loaders = {}\n",
    "loaders['train'] = dataLoader(train_params)\n",
    "loaders['test'] = dataLoader(test_params)\n",
    "\n",
    "model_params = {\n",
    "    'wtoi': loaders['train'].getWtoi(),\n",
    "    'vgg16_weight_file': 'resource/vgg16_weights.npz',\n",
    "    'dim_image': 4096, # dimension of vgg16 network output\n",
    "    'dim_embed': 256,\n",
    "    'dim_hidden': 256,\n",
    "    'batch_size': 50,\n",
    "    'n_lstm_steps': loaders['train'].getMaxCaptionLength(),\n",
    "    'n_words': loaders['train'].getVocabSize(),\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    n_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    model_path = './models'\n",
    "    itow = loaders['test'].getItow()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        caption_generator = Caption_Generator(model_params)#, bias_init_vector=bias_init_vector)\n",
    "        image, input_labels, target_labels, mask, loss, prob = caption_generator.build_model(sess)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=50)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        iteration_per_epoch = loaders['train'].getNumCaptions() / model_params['batch_size']\n",
    "        for epoch in range(n_epochs):\n",
    "            for bi in range(iteration_per_epoch):\n",
    "                # load batch data\n",
    "                batch = loaders['train'].getBatch(model_params['batch_size'])\n",
    "\n",
    "                #if bi % 50 == 0: \n",
    "                _, prob_value, loss_value = sess.run([train_op, prob, loss], \n",
    "                                         feed_dict={image: batch['images'],\n",
    "                                                    input_labels: batch['caption_labels'],\n",
    "                                                    target_labels: batch['target_labels'],\n",
    "                                                    mask: batch['caption_masks']}\n",
    "                                        )\n",
    "\n",
    "                if bi % 50 == 0:\n",
    "                    single_sample = []\n",
    "                    word_index = np.argmax(prob_value, 1)\n",
    "                    for wi in range(15):\n",
    "                        single_sample.append(word_index[wi])\n",
    "                    word_index = np.hstack(single_sample)\n",
    "\n",
    "                    generated_sentence = ''\n",
    "                    for w in word_index:\n",
    "                        word = itow[str(w)]\n",
    "                        if word == '<E>': break\n",
    "                        else:\n",
    "                            generated_sentence += (word + ' ')\n",
    "\n",
    "                    print('Sampled caption: ', generated_sentence)\n",
    "                    print \"%d epoch %d iteration Cost: %.4f\" % (epoch+1, bi, loss_value)\n",
    "\n",
    "            print \"Epoch \", epoch+1, \" is done. Saving the model ... \"\n",
    "            if not os.path.isdir(model_path):\n",
    "                os.makedirs(model_path)\n",
    "            saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch+1)\n",
    "            learning_rate *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embedding_images(sess, images, encoded_images):\n",
    "    embedded_images = sess.run(encoded_images, feed_dict={'image_raw:0': images})\n",
    "    return embedded_images\n",
    "\n",
    "def feed_image(sess, encoded_image):\n",
    "    initial_state = sess.run(fetches='lstm/initial_state:0',\n",
    "                            feed_dict={'image_feed:0': encoded_image})\n",
    "    return initial_state\n",
    "    \n",
    "\n",
    "def inference_step(sess, input_feed, state_feed):\n",
    "    softmax_output, state_output = sess.run(\n",
    "        fetches=[\"softmax:0\", \"lstm/state:0\"],\n",
    "        feed_dict={\n",
    "            \"input_feed:0\": input_feed,\n",
    "            \"lstm/state_feed:0\": state_feed,\n",
    "        })\n",
    "    return softmax_output, state_output, None\n",
    "\n",
    "\n",
    "def test(model_path='./models/model-0', maxlen=15):\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        # create caption generator for test\n",
    "        model_params['batch_size'] = 1\n",
    "        caption_generator = Caption_Generator(model_params)\n",
    "        image, image_feat, output_caption = caption_generator.build_generator(sess=sess)\n",
    "\n",
    "        # load the model parameters\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        itow = loaders['test'].getItow()\n",
    "        start_idx = loader['test'].getWtoi()['<S>']\n",
    "        n_steps = loaders['train'].getMaxCaptionLength()\n",
    "        predictions = []\n",
    "        iteration_per_epoch = loaders['test'].getNumCaptions() / model_params['batch_size']\n",
    "        for bi in range(100):\n",
    "            # load batch data\n",
    "            batch = loaders['test'].getBatch(model_params['batch_size'])\n",
    "\n",
    "            # extract vgg feature\n",
    "            encoded_images = embedding_images(sess, batch['images'], image_feat)\n",
    "            input_feed = tf.fill([model_params['batch_size']], start_idx)\n",
    "            state_feed = feed_image(sess, encoded_images) # initial state\n",
    "            \n",
    "            generated_word_index = []\n",
    "            for ci in range(n_steps):\n",
    "                softmax, next_states, metadata = inference_step(sess, input_feed, state_feed)\n",
    "                \n",
    "                input_feed = np.argmax(softmax)\n",
    "                state_feed = next_states\n",
    "                \n",
    "                generated_word_index.append(input_feed)\n",
    "                \n",
    "                input('Continue to enter:')\n",
    "            \n",
    "            \"\"\"\n",
    "            generated_sentence = ''\n",
    "            for w in generated_word_index:\n",
    "                word = itow[str(w)]\n",
    "                if word == '<E>': break\n",
    "                else:\n",
    "                    generated_sentence += (word + ' ')\n",
    "\n",
    "            img = np.asarray(batch['images'].reshape(224,224,3) \n",
    "                             + np.array([123.68, 116.779, 103.939]).reshape(1,1,3),dtype='uint8')\n",
    "            I=Image.fromarray(img)\n",
    "            fig,ax=plt.subplots(1,2)\n",
    "            ax[1].imshow(I)\n",
    "            ax[0].plot(image_feat_calc.reshape(-1))\n",
    "            plt.show()\n",
    "\n",
    "            print('Generated Caption: ', generated_sentence)\n",
    "\n",
    "            ith_prediction = {}\n",
    "            ith_prediction['image_id'] = batch['image_ids'][0]\n",
    "            ith_prediction['caption'] = generated_sentence\n",
    "            predictions.append(ith_prediction)\n",
    "            \"\"\"  \n",
    "\n",
    "        # save the prediction outputs\n",
    "        if not os.path.isdir('result'):\n",
    "                os.makedirs('result')\n",
    "        write_json('result/predictions.json', predictions)\n",
    "        print('Save the predictions to result/predictions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training and Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'conv1_1_W', (3, 3, 3, 64))\n",
      "(1, 'conv1_1_b', (64,))\n",
      "(2, 'conv1_2_W', (3, 3, 64, 64))\n",
      "(3, 'conv1_2_b', (64,))\n",
      "(4, 'conv2_1_W', (3, 3, 64, 128))\n",
      "(5, 'conv2_1_b', (128,))\n",
      "(6, 'conv2_2_W', (3, 3, 128, 128))\n",
      "(7, 'conv2_2_b', (128,))\n",
      "(8, 'conv3_1_W', (3, 3, 128, 256))\n",
      "(9, 'conv3_1_b', (256,))\n",
      "(10, 'conv3_2_W', (3, 3, 256, 256))\n",
      "(11, 'conv3_2_b', (256,))\n",
      "(12, 'conv3_3_W', (3, 3, 256, 256))\n",
      "(13, 'conv3_3_b', (256,))\n",
      "(14, 'conv4_1_W', (3, 3, 256, 512))\n",
      "(15, 'conv4_1_b', (512,))\n",
      "(16, 'conv4_2_W', (3, 3, 512, 512))\n",
      "(17, 'conv4_2_b', (512,))\n",
      "(18, 'conv4_3_W', (3, 3, 512, 512))\n",
      "(19, 'conv4_3_b', (512,))\n",
      "(20, 'conv5_1_W', (3, 3, 512, 512))\n",
      "(21, 'conv5_1_b', (512,))\n",
      "(22, 'conv5_2_W', (3, 3, 512, 512))\n",
      "(23, 'conv5_2_b', (512,))\n",
      "(24, 'conv5_3_W', (3, 3, 512, 512))\n",
      "(25, 'conv5_3_b', (512,))\n",
      "(26, 'fc6_W', (25088, 4096))\n",
      "(27, 'fc6_b', (4096,))\n",
      "(28, 'fc7_W', (4096, 4096))\n",
      "(29, 'fc7_b', (4096,))\n",
      "('Sampled caption: ', u'teeth businessman old tusk smaller populated bound she picking cucumber stylized <UNK> <UNK> <UNK> <UNK> ')\n",
      "1 epoch 0 iteration Cost: 11.6363\n",
      "('Sampled caption: ', u'a surface standing woman woman cake accident pitching pulling headphones belongs self <UNK> sitting sitting ')\n",
      "1 epoch 50 iteration Cost: 9.3395\n",
      "('Sampled caption: ', u'a young standing parking playing so snowy tower wheelbarrow ')\n",
      "1 epoch 100 iteration Cost: 7.9747\n",
      "('Sampled caption: ', u'a a a of a ')\n",
      "1 epoch 150 iteration Cost: 6.9414\n",
      "('Sampled caption: ', u'a group the a in in ')\n",
      "1 epoch 200 iteration Cost: 6.1533\n",
      "('Sampled caption: ', u'a man is . a . . . . in ')\n",
      "1 epoch 250 iteration Cost: 5.3895\n",
      "('Sampled caption: ', u'a a a wearing a green . . . ')\n",
      "1 epoch 300 iteration Cost: 5.1465\n",
      "('Sampled caption: ', u'a woman in a next a the a the a in of the . ')\n",
      "1 epoch 350 iteration Cost: 5.0487\n",
      "('Sampled caption: ', u'a two through to a a of a table with <UNK> other other other other ')\n",
      "1 epoch 400 iteration Cost: 5.0233\n",
      "('Sampled caption: ', u'a man with a on holding , a beach . . ')\n",
      "1 epoch 450 iteration Cost: 4.6889\n",
      "Epoch  1  is done. Saving the model ... \n",
      "('Sampled caption: ', u'a standing the a photo up and of . . with ')\n",
      "2 epoch 0 iteration Cost: 4.5761\n",
      "('Sampled caption: ', u'a people are on his holding <UNK> a are ')\n",
      "2 epoch 50 iteration Cost: 4.6054\n",
      "('Sampled caption: ', u'a man with with that a front ')\n",
      "2 epoch 100 iteration Cost: 4.2785\n",
      "('Sampled caption: ', u'a a <UNK> sitting a in . . a large in . ')\n",
      "2 epoch 150 iteration Cost: 4.1929\n",
      "('Sampled caption: ', u'a young of a in and . on the white ')\n",
      "2 epoch 200 iteration Cost: 4.2058\n",
      "('Sampled caption: ', u'a man with a in and and . . . ')\n",
      "2 epoch 250 iteration Cost: 3.8322\n",
      "('Sampled caption: ', u'a cows in in to a other on . the a at ')\n",
      "2 epoch 300 iteration Cost: 3.9333\n",
      "('Sampled caption: ', u'a man with with yellow next the plate player . ')\n",
      "2 epoch 350 iteration Cost: 3.9194\n",
      "('Sampled caption: ', u'a group a with in a . with a a ')\n",
      "2 epoch 400 iteration Cost: 4.0495\n",
      "('Sampled caption: ', u'a man is sitting on a field with a and ')\n",
      "2 epoch 450 iteration Cost: 3.9143\n",
      "Epoch  2  is done. Saving the model ... \n",
      "('Sampled caption: ', u'a man of people sitting with of the bed and a ')\n",
      "3 epoch 0 iteration Cost: 4.0360\n",
      "('Sampled caption: ', u'a is a man sitting is sitting on a skateboard ')\n",
      "3 epoch 50 iteration Cost: 3.9517\n",
      "('Sampled caption: ', u'a is sitting on a of a a and . <UNK> other other other other ')\n",
      "3 epoch 100 iteration Cost: 3.7027\n",
      "('Sampled caption: ', u'a man is is on on a of a <UNK> . . ')\n",
      "3 epoch 150 iteration Cost: 3.6668\n",
      "('Sampled caption: ', u'a man tower a beach with in are street ')\n",
      "3 epoch 200 iteration Cost: 3.6458\n",
      "('Sampled caption: ', u'a woman is in riding on a table . to <UNK> hydrant . ')\n",
      "3 epoch 250 iteration Cost: 3.3951\n",
      "('Sampled caption: ', u'a man clock of people on of in the field ')\n",
      "3 epoch 300 iteration Cost: 3.4847\n",
      "('Sampled caption: ', u'a man is a a street . . a <UNK> . . . . <UNK> ')\n",
      "3 epoch 350 iteration Cost: 3.6736\n",
      "('Sampled caption: ', u'a skiers walking on the side . <UNK> . . road . <UNK> other other ')\n",
      "3 epoch 400 iteration Cost: 3.6757\n",
      "('Sampled caption: ', u'a man sign with with large and a ')\n",
      "3 epoch 450 iteration Cost: 3.6516\n",
      "Epoch  3  is done. Saving the model ... \n",
      "('Sampled caption: ', u'a man group a large dog bear in a beach . in . ')\n",
      "4 epoch 0 iteration Cost: 3.8001\n",
      "('Sampled caption: ', u'a man with a sitting at a table . . ')\n",
      "4 epoch 50 iteration Cost: 3.5368\n",
      "('Sampled caption: ', u'a man is sitting a skateboard in a side with ')\n",
      "4 epoch 100 iteration Cost: 3.3814\n",
      "('Sampled caption: ', u'a man player is a air and ')\n",
      "4 epoch 150 iteration Cost: 3.3603\n",
      "('Sampled caption: ', u'a man with a <UNK> and a couch and a table . <UNK> other other ')\n",
      "4 epoch 200 iteration Cost: 3.3966\n",
      "('Sampled caption: ', u'a man of with a and and and a ')\n",
      "4 epoch 250 iteration Cost: 3.2721\n",
      "('Sampled caption: ', u'a woman with in a <UNK> of airport and ')\n",
      "4 epoch 300 iteration Cost: 3.3823\n",
      "('Sampled caption: ', u'a man in a side with riding on a . water ')\n",
      "4 epoch 350 iteration Cost: 3.4044\n",
      "('Sampled caption: ', u'a man up of a man of a on and <UNK> other other other other ')\n",
      "4 epoch 400 iteration Cost: 3.3622\n",
      "('Sampled caption: ', u'a man and a grass in a a frisbee game ')\n",
      "4 epoch 450 iteration Cost: 3.3215\n",
      "Epoch  4  is done. Saving the model ... \n",
      "('Sampled caption: ', u'a man child of with a and and a . a . <UNK> other other ')\n",
      "5 epoch 0 iteration Cost: 3.6004\n",
      "('Sampled caption: ', u'a man is on to of a side . ')\n",
      "5 epoch 50 iteration Cost: 3.4084\n",
      "('Sampled caption: ', u'a man sign with a another light in in a a ')\n",
      "5 epoch 100 iteration Cost: 3.3440\n",
      "('Sampled caption: ', u'a man with standing a bed sky ')\n",
      "5 epoch 150 iteration Cost: 3.1087\n",
      "('Sampled caption: ', u'a and with on a of . on in near ')\n",
      "5 epoch 200 iteration Cost: 3.2084\n",
      "('Sampled caption: ', u'a man of of with a a people to . ')\n",
      "5 epoch 250 iteration Cost: 3.1875\n",
      "('Sampled caption: ', u'a men are standing on to a large . a large . ')\n",
      "5 epoch 300 iteration Cost: 3.2332\n",
      "('Sampled caption: ', u'a man with in holding a tennis racquet . <UNK> other other other other other ')\n",
      "5 epoch 350 iteration Cost: 3.2318\n",
      "('Sampled caption: ', u'a man and and on a field bathroom . ')\n",
      "5 epoch 400 iteration Cost: 3.2413\n",
      "('Sampled caption: ', u'a man hydrant is a tennis <UNK> . a field ')\n",
      "5 epoch 450 iteration Cost: 3.1032\n",
      "Epoch  5  is done. Saving the model ... \n",
      "('Sampled caption: ', u'a man player with a <UNK> with to eat . <UNK> other other other other ')\n",
      "6 epoch 0 iteration Cost: 3.2499\n",
      "('Sampled caption: ', u'a man of standing with a in on and a . ')\n",
      "6 epoch 50 iteration Cost: 3.3678\n",
      "('Sampled caption: ', u'a man is of a in on he a baseball ')\n",
      "6 epoch 100 iteration Cost: 3.0931\n",
      "('Sampled caption: ', u'a man of standing down on to a <UNK> . . ')\n",
      "6 epoch 150 iteration Cost: 2.9171\n",
      "('Sampled caption: ', u'a man clock of water with a <UNK> . a ')\n",
      "6 epoch 200 iteration Cost: 3.0800\n",
      "('Sampled caption: ', u'a in through a lush in a ball . ')\n",
      "6 epoch 250 iteration Cost: 3.0547\n",
      "('Sampled caption: ', u'a man <UNK> of large on a on a mirror . ')\n",
      "6 epoch 300 iteration Cost: 3.0921\n",
      "('Sampled caption: ', u'a man child is on on a head . ')\n",
      "6 epoch 350 iteration Cost: 2.9570\n",
      "('Sampled caption: ', u'a man is on a street street . a ')\n",
      "6 epoch 400 iteration Cost: 3.1243\n",
      "('Sampled caption: ', u'a from a is is a frisbee and a head . ')\n",
      "6 epoch 450 iteration Cost: 3.0238\n",
      "Epoch  6  is done. Saving the model ... \n",
      "('Sampled caption: ', u'a man tennis player in at the <UNK> ball . holding . ')\n",
      "7 epoch 0 iteration Cost: 3.0997\n",
      "('Sampled caption: ', u'a <UNK> on a water of a water . ')\n",
      "7 epoch 50 iteration Cost: 3.1934\n",
      "('Sampled caption: ', u'a man with is a large of . . ')\n",
      "7 epoch 100 iteration Cost: 2.8792\n"
     ]
    }
   ],
   "source": [
    "# test model that means to predict captions for test images\n",
    "do_train = True\n",
    "if do_train:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test model that means to predict captions for test images\n",
    "if not do_train:\n",
    "    test(model_path='models/model-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm/basic_lstm_cell/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('coco-caption')\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ann_file = 'coco-caption/annotations/captions_val2014.json'\n",
    "res_file = 'result/predictions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create coco object and cocoRes object\n",
    "coco = COCO(ann_file)\n",
    "cocoRes = coco.loadRes(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# evaluate on a subset of images by setting\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "\n",
    "# evaluate results\n",
    "cocoEval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
