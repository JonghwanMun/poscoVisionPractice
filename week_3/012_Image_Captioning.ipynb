{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from vgg16 import vgg16\n",
    "from dataLoader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data download\n",
    "  - 다운로드 제대로 된 후에는 다시 실행할 필요 없습니다 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('resource/vgg16_weights.npz'):\n",
    "    # download images\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/224x224_mscoco_images.tar.gz')\n",
    "    os.system('tar xvzf 224x224_mscoco_images.tar.gz')\n",
    "    os.system('rm 224x224_mscoco_images.tar.gz')\n",
    "    os.system('mv 224x224_mscoco_images resource/224x224_mscoco_images')\n",
    "\n",
    "    # download vgg16 network\n",
    "    os.system('wget cvlab.postech.ac.kr/~jonghwan/vgg16_weights.npz')\n",
    "    os.system('mv vgg16_weights.npz resource/vgg16_weights.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Captioning Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, params, bias_init_vector=None):\n",
    "        #dim_image, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, bias_init_vector=None):\n",
    "        \n",
    "        self.wtoi = params['wtoi']\n",
    "        self.vgg16_weight_file = params['vgg16_weight_file']\n",
    "        \n",
    "        # captioning model parameter dimensions\n",
    "        self.dim_image_feat = np.int(params['dim_image'])\n",
    "        self.dim_word_embedding = np.int(params['dim_embed'])\n",
    "        self.dim_hidden_lstm = np.int(params['dim_hidden'])\n",
    "        self.batch_size = np.int(params['batch_size'])\n",
    "        self.n_lstm_steps = np.int(params['n_lstm_steps'])\n",
    "        self.n_words = np.int(params['n_words'])\n",
    "        \n",
    "        self.initializer = tf.random_uniform_initializer(minval=-0.8, maxval=0.8)\n",
    "\n",
    "    def build_model(self, sess):\n",
    "        \"\"\" Build captioning model for training\"\"\"\n",
    "\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, 224, 224, 3])\n",
    "        input_labels = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        target_labels = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "\n",
    "        # extract image feature\n",
    "        vgg = vgg16(image, self.vgg16_weight_file, sess, trainable=False)\n",
    "        image_feat = vgg.fc7\n",
    "        \n",
    "        # embed image feature to match the dimension of lstm input\n",
    "        with tf.variable_scope(\"image_embedding\") as img_embed_scope:\n",
    "            image_embeddings = tf.contrib.layers.fully_connected(\n",
    "                                    inputs=image_feat,\n",
    "                                    num_outputs=self.dim_hidden_lstm,\n",
    "                                    activation_fn=None,\n",
    "                                    weights_initializer=self.initializer,\n",
    "                                    biases_initializer=None,\n",
    "                                    scope=img_embed_scope)\n",
    "            \n",
    "        # embed input caption labels to obtain word embeddings\n",
    "        with tf.variable_scope(\"word_embedding\"), tf.device('/cpu:0'):\n",
    "            word_embedding_matrix = tf.get_variable(\n",
    "                                        name='word_embedding_matrix',\n",
    "                                        shape=[self.n_words, self.dim_word_embedding],\n",
    "                                        initializer=self.initializer\n",
    "                                        )\n",
    "            word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, input_labels)\n",
    "                                                            \n",
    "        \n",
    "        # for LSTM cell unit\n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden_lstm, state_is_tuple=True)\n",
    "            zero_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "            \n",
    "            # allow the LSTM variables to be resued\n",
    "            lstm_scope.reuse_variables()\n",
    "            \n",
    "            # run the batch of word embeddings throught the LSTM\n",
    "            caption_length = tf.reduce_sum(mask, 1)\n",
    "            lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                               inputs=word_embeddings,\n",
    "                                               sequence_length=caption_length,\n",
    "                                               initial_state=initial_state,\n",
    "                                               dtype=tf.float32,\n",
    "                                               scope=lstm_scope)\n",
    "        \n",
    "        # stack batches vertically\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        # compute probabilities for words\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                        inputs=lstm_outputs,\n",
    "                        num_outputs=self.n_words,\n",
    "                        activation_fn=None,\n",
    "                        weights_initializer=self.initializer,\n",
    "                        scope=logits_scope)\n",
    "        \n",
    "        targets = tf.reshape(target_labels, [-1])\n",
    "        weights = tf.to_float(tf.reshape(mask, [-1]))\n",
    "        \n",
    "        # compute losses\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,\n",
    "                                                              logits=logits)\n",
    "        batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "                            tf.reduce_sum(weights), name=\"batch_loss\")\n",
    "        tf.losses.add_loss(batch_loss)\n",
    "        total_loss = tf.losses.get_total_loss()\n",
    "        \n",
    "        probs = tf.nn.softmax(logits, name='softmax')\n",
    "\n",
    "        return image, input_labels, target_labels, mask, total_loss, probs\n",
    "\n",
    "    def build_generator(self, sess):\n",
    "        \"\"\" Build captioning model for test\"\"\"\n",
    "        \n",
    "        # extract image feature\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, 224, 224, 3], name='image_raw')\n",
    "        vgg = vgg16(image, self.vgg16_weight_file, sess, trainable=False)\n",
    "        image_feat = vgg.fc7\n",
    "        \n",
    "        # embed image feature to match the dimension of lstm input\n",
    "        with tf.variable_scope(\"image_embedding\") as img_embed_scope:\n",
    "            encoded_images = tf.contrib.layers.fully_connected(\n",
    "                                    inputs=image_feat,\n",
    "                                    num_outputs=self.dim_hidden_lstm,\n",
    "                                    activation_fn=None,\n",
    "                                    weights_initializer=self.initializer,\n",
    "                                    biases_initializer=None,\n",
    "                                    scope=img_embed_scope)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Below part is for caption generation (LSTM)\n",
    "        image_embeddings = tf.placeholder(dtype=tf.float32,\n",
    "                                          shape=[self.batch_size, self.dim_hidden_lstm],\n",
    "                                          name='image_feed')\n",
    "        input_feed = tf.placeholder(dtype=tf.int64, shape=[None], name='input_feed')\n",
    "        input_labels = tf.expand_dims(input_feed, 1)\n",
    "\n",
    "        # embed input caption labels to obtain word embeddings\n",
    "        with tf.variable_scope(\"word_embedding\"), tf.device('/cpu:0'):\n",
    "            word_embedding_matrix = tf.get_variable(\n",
    "                                        name='word_embedding_matrix',\n",
    "                                        shape=[self.n_words, self.dim_word_embedding],\n",
    "                                        initializer=self.initializer\n",
    "                                        )\n",
    "            word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, input_labels)                                                            \n",
    "        \n",
    "        # for LSTM cell unit    \n",
    "        with tf.variable_scope(\"lstm\", initializer=self.initializer) as lstm_scope:\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.dim_hidden_lstm, state_is_tuple=True)\n",
    "            zero_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "            _, initial_state = lstm_cell(image_embeddings, zero_state)\n",
    "            \n",
    "            # allow the LSTM variables to be resued\n",
    "            lstm_scope.reuse_variables()\n",
    "            \n",
    "            # In inference mode, use concatenated states for convenient feeding and fetching.\n",
    "            tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n",
    "        \n",
    "        \n",
    "            # Placeholder for feeding a batch of concatenated states.\n",
    "            state_feed = tf.placeholder(dtype=tf.float32,\n",
    "                                        shape=[None, sum(lstm_cell.state_size)],\n",
    "                                        name=\"state_feed\")\n",
    "            state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\n",
    "\n",
    "            # Run a single LSTM step.\n",
    "            lstm_outputs, state_tuple = lstm_cell(\n",
    "                inputs=tf.squeeze(word_embeddings, axis=[1]), state=state_tuple)\n",
    "\n",
    "            #lstm_outputs, state_tuple = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "            #                                   inputs=word_embeddings,\n",
    "            #                                   initial_state=state_tuple,\n",
    "            #                                   dtype=tf.float32,\n",
    "            #                                   scope=lstm_scope)\n",
    "            \n",
    "            # Concatentate the resulting state.\n",
    "            tf.concat(axis=1, values=state_tuple, name=\"state\")\n",
    "            \n",
    "        \n",
    "        # stack batches vertically\n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        # compute probabilities for words\n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "                        inputs=lstm_outputs,\n",
    "                        num_outputs=self.n_words,\n",
    "                        activation_fn=None,\n",
    "                        weights_initializer=self.initializer,\n",
    "                        scope=logits_scope)\n",
    "        probs = tf.nn.softmax(logits, name='softmax')\n",
    "\n",
    "        return image, encoded_images, image_embeddings, state_feed, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################### Parameters #####################\n",
    "train_params = {\n",
    "    'json_path':'resource/train.json',\n",
    "    'h5py_path':'resource/train.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "test_params = {\n",
    "    'json_path':'resource/test.json',\n",
    "    'h5py_path':'resource/test.h5',\n",
    "    'img_dir': 'resource/224x224_mscoco_images/'\n",
    "}\n",
    "# create data loader for training and test data\n",
    "loaders = {}\n",
    "loaders['train'] = dataLoader(train_params)\n",
    "loaders['test'] = dataLoader(test_params)\n",
    "\n",
    "model_params = {\n",
    "    'wtoi': loaders['train'].getWtoi(),\n",
    "    'vgg16_weight_file': 'resource/vgg16_weights.npz',\n",
    "    'dim_image': 4096, # dimension of vgg16 network output\n",
    "    'dim_embed': 256,\n",
    "    'dim_hidden': 256,\n",
    "    'batch_size': 50,\n",
    "    'n_lstm_steps': loaders['train'].getMaxCaptionLength(),\n",
    "    'n_words': loaders['train'].getVocabSize(),\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    n_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    model_path = './models'\n",
    "    itow = loaders['test'].getItow()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        caption_generator = Caption_Generator(model_params)#, bias_init_vector=bias_init_vector)\n",
    "        image, input_labels, target_labels, mask, loss, prob = caption_generator.build_model(sess)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=50)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        iteration_per_epoch = loaders['train'].getNumCaptions() / model_params['batch_size']\n",
    "        for epoch in range(n_epochs):\n",
    "            for bi in range(iteration_per_epoch):\n",
    "                # load batch data\n",
    "                batch = loaders['train'].getBatch(model_params['batch_size'])\n",
    "\n",
    "                #if bi % 50 == 0: \n",
    "                _, prob_value, loss_value = sess.run([train_op, prob, loss], \n",
    "                                         feed_dict={image: batch['images'],\n",
    "                                                    input_labels: batch['caption_labels'],\n",
    "                                                    target_labels: batch['target_labels'],\n",
    "                                                    mask: batch['caption_masks']}\n",
    "                                        )\n",
    "\n",
    "                if bi % 50 == 0:\n",
    "                    single_sample = []\n",
    "                    word_index = np.argmax(prob_value, 1)\n",
    "                    for wi in range(15):\n",
    "                        single_sample.append(word_index[wi])\n",
    "                    word_index = np.hstack(single_sample)\n",
    "\n",
    "                    generated_sentence = ''\n",
    "                    for w in word_index:\n",
    "                        word = itow[str(w)]\n",
    "                        if word == '<E>': break\n",
    "                        else:\n",
    "                            generated_sentence += (word + ' ')\n",
    "\n",
    "                    print('Sampled caption: ', generated_sentence)\n",
    "                    print \"%d epoch %d iteration Cost: %.4f\" % (epoch+1, bi, loss_value)\n",
    "\n",
    "            print \"Epoch \", epoch+1, \" is done. Saving the model ... \"\n",
    "            if not os.path.isdir(model_path):\n",
    "                os.makedirs(model_path)\n",
    "            saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch+1)\n",
    "            learning_rate *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def embedding_images(sess, images, encoded_images):\n",
    "    embedded_images = sess.run(encoded_images, feed_dict={'image_raw:0': images})\n",
    "    return embedded_images\n",
    "\n",
    "def feed_image(sess, encoded_image):\n",
    "    initial_state = sess.run(fetches='lstm/initial_state:0',\n",
    "                            feed_dict={'image_feed:0': encoded_image})\n",
    "    return initial_state\n",
    "    \n",
    "\n",
    "def inference_step(sess, input_feed, state_feed):\n",
    "    softmax_output, state_output = sess.run(\n",
    "        fetches=[\"softmax:0\", \"lstm/state:0\"],\n",
    "        feed_dict={\n",
    "            \"input_feed:0\": input_feed,\n",
    "            \"lstm/state_feed:0\": state_feed,\n",
    "        })\n",
    "    return softmax_output, state_output, None\n",
    "\n",
    "\n",
    "def test(model_path='./models/model-0', maxlen=15):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        # create caption generator for test\n",
    "        model_params['batch_size'] = 1\n",
    "        caption_generator = Caption_Generator(model_params)\n",
    "        image, image_feat, image_embed, s_feed, prob = caption_generator.build_generator(sess=sess)\n",
    "\n",
    "        # load the model parameters\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        itow = loaders['test'].getItow()\n",
    "        start_idx = loaders['test'].getWtoi()['<S>']\n",
    "        n_steps = loaders['train'].getMaxCaptionLength()\n",
    "        predictions = []\n",
    "        iteration_per_epoch = loaders['test'].getNumCaptions() / model_params['batch_size']\n",
    "        for bi in range(100):\n",
    "            # load batch data\n",
    "            batch = loaders['test'].getBatch(model_params['batch_size'])\n",
    "\n",
    "            # extract vgg feature\n",
    "            encoded_images = embedding_images(sess, batch['images'], image_feat)\n",
    "            input_feed = np.full((model_params['batch_size']), start_idx)\n",
    "            state_feed = feed_image(sess, encoded_images) # initial state\n",
    "            \n",
    "            generated_word_index = []\n",
    "            for ci in range(n_steps):\n",
    "                softmax, next_states, metadata = inference_step(sess, input_feed, state_feed)\n",
    "                \n",
    "                input_feed = np.argmax(softmax,1)\n",
    "                state_feed = next_states\n",
    "                \n",
    "                generated_word_index.append(input_feed)            \n",
    "            \n",
    "            generated_sentence = ''\n",
    "            for w in generated_word_index:\n",
    "                word = itow[str(w[0])]\n",
    "                if word == '<E>': break\n",
    "                else:\n",
    "                    generated_sentence += (word + ' ')\n",
    "\n",
    "            img = np.asarray(batch['images'].reshape(224,224,3) \n",
    "                             + np.array([123.68, 116.779, 103.939]).reshape(1,1,3),dtype='uint8')\n",
    "            I=Image.fromarray(img)\n",
    "            fig,ax=plt.subplots(1,1)\n",
    "            ax[0].imshow(I)\n",
    "            plt.show()\n",
    "\n",
    "            print('Generated Caption: ', generated_sentence)\n",
    "\n",
    "            ith_prediction = {}\n",
    "            ith_prediction['image_id'] = batch['image_ids'][0]\n",
    "            ith_prediction['caption'] = generated_sentence\n",
    "            predictions.append(ith_prediction)\n",
    "            \n",
    "\n",
    "        # save the prediction outputs\n",
    "        if not os.path.isdir('result'):\n",
    "                os.makedirs('result')\n",
    "        write_json('result/predictions.json', predictions)\n",
    "        print('Save the predictions to result/predictions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training and Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test model that means to predict captions for test images\n",
    "do_train = False\n",
    "if do_train:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[12]\n",
      "[11]\n",
      "[14]\n",
      "[5]\n",
      "[1]\n",
      "[25]\n",
      "[9]\n",
      "[1]\n",
      "[97]\n",
      "[4]\n",
      "[3]\n",
      "[0]\n",
      "[4]\n",
      "[3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'AxesSubplot' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d4336ed91e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test model that means to predict captions for test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models/model.ckpt-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6b2d86d8e22c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model_path, maxlen)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mI\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AxesSubplot' object does not support indexing"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nauJ/Acz2XLpusNoKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e601cfa10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test model that means to predict captions for test images\n",
    "if not do_train:\n",
    "    test(model_path='models/model.ckpt-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "print_tensors_in_checkpoint_file('models/model.ckpt-1', tensor_name='lstm/basic_lstm_cell/weights', all_tensors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('coco-caption')\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ann_file = 'coco-caption/annotations/captions_val2014.json'\n",
    "res_file = 'result/predictions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create coco object and cocoRes object\n",
    "coco = COCO(ann_file)\n",
    "cocoRes = coco.loadRes(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# evaluate on a subset of images by setting\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "\n",
    "# evaluate results\n",
    "cocoEval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
