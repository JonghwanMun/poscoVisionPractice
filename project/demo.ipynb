{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow.python.platform\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "\n",
    "import skimage\n",
    "import skimage.io\n",
    "\n",
    "from tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from cider.cider import Cider\n",
    "from cider.cider_scorer import CiderScorer\n",
    "\n",
    "class Caption_Generator():\n",
    "    def init_weight(self, dim_in, dim_out, name=None, stddev=1.0):\n",
    "        return tf.Variable(tf.truncated_normal([dim_in, dim_out], stddev=stddev/math.sqrt(float(dim_in))), name=name)\n",
    "\n",
    "    def init_bias(self, dim_out, name=None):\n",
    "        return tf.Variable(tf.zeros([dim_out]), name=name)\n",
    "\n",
    "    def __init__(self, dim_image, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, bias_init_vector=None):\n",
    "\n",
    "        self.dim_image = np.int(dim_image)\n",
    "        self.dim_embed = np.int(dim_embed)\n",
    "        self.dim_hidden = np.int(dim_hidden)\n",
    "        self.batch_size = np.int(batch_size)\n",
    "        self.n_lstm_steps = np.int(n_lstm_steps)\n",
    "        self.n_words = np.int(n_words)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.Wemb = tf.Variable(tf.random_uniform([n_words, dim_embed], -0.1, 0.1), name='Wemb')\n",
    "\n",
    "        self.bemb = self.init_bias(dim_embed, name='bemb')\n",
    "\n",
    "        self.lstm = tf.contrib.rnn.BasicLSTMCell(dim_hidden, state_is_tuple=False)\n",
    "\n",
    "        #self.encode_img_W = self.init_weight(dim_image, dim_hidden, name='encode_img_W')\n",
    "        self.encode_img_W = tf.Variable(tf.random_uniform([dim_image, dim_hidden], -0.1, 0.1), name='encode_img_W')\n",
    "        self.encode_img_b = self.init_bias(dim_hidden, name='encode_img_b')\n",
    "\n",
    "        self.embed_word_W = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name='embed_word_W')\n",
    "\n",
    "        if bias_init_vector is not None:\n",
    "            self.embed_word_b = tf.Variable(bias_init_vector.astype(np.float32), name='embed_word_b')\n",
    "        else:\n",
    "            self.embed_word_b = self.init_bias(n_words, name='embed_word_b')\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        image = tf.placeholder(tf.float32, [self.batch_size, self.dim_image])\n",
    "        sentence = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "\n",
    "        image_emb = tf.matmul(image, self.encode_img_W) + self.encode_img_b # (batch_size, dim_hidden)\n",
    "\n",
    "        state = tf.zeros([self.batch_size, self.lstm.state_size])\n",
    "\n",
    "        loss = 0.0\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for i in range(self.n_lstm_steps): # maxlen + 1\n",
    "                if i == 0:\n",
    "                    current_emb = image_emb\n",
    "                else:\n",
    "                    with tf.device(\"/cpu:0\"):\n",
    "                        current_emb = tf.nn.embedding_lookup(self.Wemb, sentence[:,i-1]) + self.bemb\n",
    "\n",
    "                if i > 0 : tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                output, state = self.lstm(current_emb, state) # (batch_size, dim_hidden)\n",
    "\n",
    "                if i > 0: # 이미지 다음 바로 나오는건 #START# 임. 이건 무시.\n",
    "                    labels = tf.expand_dims(sentence[:, i], 1) # (batch_size)\n",
    "                    indices = tf.expand_dims(tf.range(0, self.batch_size, 1), 1)\n",
    "                    concated = tf.concat([indices, labels], 1)\n",
    "                    onehot_labels = tf.sparse_to_dense(\n",
    "                            concated, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0) # (batch_size, n_words)\n",
    "\n",
    "                    logit_words = tf.matmul(output, self.embed_word_W) + self.embed_word_b # (batch_size, n_words)\n",
    "                    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_words, labels=onehot_labels)\n",
    "                    cross_entropy = cross_entropy * mask[:,i]#tf.expand_dims(mask, 1)\n",
    "\n",
    "                    current_loss = tf.reduce_sum(cross_entropy)\n",
    "                    loss = loss + current_loss\n",
    "\n",
    "            loss = loss / tf.reduce_sum(mask[:,1:])\n",
    "            return loss, image, sentence, mask\n",
    "\n",
    "    def build_generator(self, maxlen):\n",
    "        image = tf.placeholder(tf.float32, [1, self.dim_image])\n",
    "        image_emb = tf.matmul(image, self.encode_img_W) + self.encode_img_b\n",
    "\n",
    "        state = tf.zeros([1, self.lstm.state_size])\n",
    "        #last_word = image_emb # 첫 단어 대신 이미지\n",
    "        generated_words = []\n",
    "\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            output, state = self.lstm(image_emb, state)\n",
    "            last_word = tf.nn.embedding_lookup(self.Wemb, [0]) + self.bemb\n",
    "\n",
    "            for i in range(maxlen):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                output, state = self.lstm(last_word, state)\n",
    "\n",
    "                logit_words = tf.matmul(output, self.embed_word_W) + self.embed_word_b\n",
    "                max_prob_word = tf.argmax(logit_words, 1)\n",
    "\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    last_word = tf.nn.embedding_lookup(self.Wemb, max_prob_word)\n",
    "\n",
    "                last_word += self.bemb\n",
    "\n",
    "                generated_words.append(max_prob_word)\n",
    "\n",
    "        return image, generated_words\n",
    "\n",
    "def get_caption_data(train_anno_path, train_feat_path):\n",
    "     feats = np.load(train_feat_path)\n",
    "     annotations = np.load(train_anno_path)\n",
    "#     captions = annotations['caption'].values\n",
    "\n",
    "     return feats, annotations\n",
    "\n",
    "def preProBuildWordVocab(sentence_iterator, word_count_threshold=30): # borrowed this function from NeuralTalk\n",
    "    print 'preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold, )\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in sentence_iterator:\n",
    "      nsents += 1\n",
    "      for w in sent.lower().split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    print 'filtered words from %d to %d' % (len(word_counts), len(vocab))\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '.'  # period at the end of the sentence. make first dimension be end token\n",
    "    wordtoix = {}\n",
    "    wordtoix['#START#'] = 0 # make first vector be the start token\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "      wordtoix[w] = ix\n",
    "      ixtoword[ix] = w\n",
    "      ix += 1\n",
    "\n",
    "    word_counts['.'] = nsents\n",
    "    bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n",
    "    return wordtoix, ixtoword, bias_init_vector\n",
    "\n",
    "def crop_image(x, target_height=227, target_width=227, as_float=True):\n",
    "    #image = skimage.img_as_float(skimage.io.imread(x)).astype(np.float32)\n",
    "    image = skimage.io.imread(x)\n",
    "    if as_float:\n",
    "        image = skimage.img_as_float(image).astype(np.float32)\n",
    "\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.tile(image[:,:,None], 3)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[:,:,:,0]\n",
    "\n",
    "    height, width, rgb = image.shape\n",
    "    if width == height:\n",
    "        resized_image = cv2.resize(image, (target_height,target_width))\n",
    "\n",
    "    elif height < width:\n",
    "        resized_image = cv2.resize(image, (int(width * float(target_height)/height), target_width))\n",
    "        cropping_length = int((resized_image.shape[1] - target_height) / 2)\n",
    "        resized_image = resized_image[:,cropping_length:resized_image.shape[1] - cropping_length]\n",
    "\n",
    "    else:\n",
    "        resized_image = cv2.resize(image, (target_height, int(height * float(target_width) / width)))\n",
    "        cropping_length = int((resized_image.shape[0] - target_width) / 2)\n",
    "        resized_image = resized_image[cropping_length:resized_image.shape[0] - cropping_length,:]\n",
    "\n",
    "    return cv2.resize(resized_image, (target_height, target_width))\n",
    "\n",
    "\n",
    "def read_image(path):\n",
    "\n",
    "     img = crop_image(path, target_height=224, target_width=224)\n",
    "     if img.shape[2] == 4:\n",
    "         img = img[:,:,:3]\n",
    "\n",
    "     img = img[None, ...]\n",
    "     return img\n",
    "    \n",
    "  \n",
    "    \n",
    "def test_tf(test_image_path=None, model_path='./models/model-72', maxlen=30):\n",
    "    with open(vgg_path) as f:\n",
    "        fileContent = f.read()\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(fileContent)\n",
    "\n",
    "    images = tf.placeholder(\"float32\", [1, 224, 224, 3])\n",
    "    tf.import_graph_def(graph_def, input_map={\"images\":images})\n",
    "\n",
    "    ixtoword = np.load('./data/ixtoword.npy').tolist()\n",
    "    n_words = len(ixtoword)\n",
    "\n",
    "    image_val = read_image(test_image_path)\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    caption_generator = Caption_Generator(\n",
    "           dim_image=dim_image,\n",
    "           dim_hidden=dim_hidden,\n",
    "           dim_embed=dim_embed,\n",
    "           batch_size=batch_size,\n",
    "           n_lstm_steps=maxlen,\n",
    "           n_words=n_words)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    fc7 = sess.run(graph.get_tensor_by_name(\"import/fc7_relu:0\"), feed_dict={images:image_val})\n",
    "\n",
    "    fc7_tf, generated_words = caption_generator.build_generator(maxlen=maxlen)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    generated_word_index= sess.run(generated_words, feed_dict={fc7_tf:fc7})\n",
    "    generated_word_index = np.hstack(generated_word_index)\n",
    "\n",
    "    generated_words = [ixtoword[x] for x in generated_word_index]\n",
    "    punctuation = np.argmax(np.array(generated_words) == '.')+1\n",
    "\n",
    "    generated_words = generated_words[:punctuation]\n",
    "    generated_sentence = ' '.join(generated_words)\n",
    "    print generated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 학습 관련 Parameters #####################\n",
    "\n",
    "dim_embed = 256\n",
    "dim_hidden = 256\n",
    "dim_image = 4096\n",
    "batch_size = 128\n",
    "\n",
    "#learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "###############################################################\n",
    "#################### 잡다한 Parameters ########################\n",
    "model_path = './models'\n",
    "vgg_path = './data/vgg16.tfmodel'\n",
    "data_path = './data'\n",
    "train_feat_path = './data/train_feat.npy'\n",
    "test_feat_path  = './data/test_feat.npy'\n",
    "train_anno_path = './data/train_caption.npy'\n",
    "test_anno_path  = './data/test_caption.npy'\n",
    "annotation_path = os.path.join(data_path, 'results_20130124.token')\n",
    "################################################################\n",
    "\n",
    "\n",
    "def train(epoch=0):\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    feats, captions = get_caption_data(train_anno_path, train_feat_path)\n",
    "    wordtoix, ixtoword, bias_init_vector = preProBuildWordVocab(captions)\n",
    "\n",
    "    np.save('data/ixtoword', ixtoword)\n",
    "    np.save('data/wordtoix', wordtoix)\n",
    "\n",
    "    index = np.arange(len(feats))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    feats = feats[index]\n",
    "    captions = captions[index]\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    n_words = len(wordtoix)\n",
    "    maxlen = np.max( map(lambda x: len(x.split(' ')), captions) )\n",
    "    caption_generator = Caption_Generator(\n",
    "            dim_image=dim_image,\n",
    "            dim_hidden=dim_hidden,\n",
    "            dim_embed=dim_embed,\n",
    "            batch_size=batch_size,\n",
    "            n_lstm_steps=maxlen+2,\n",
    "            n_words=n_words,\n",
    "            bias_init_vector=bias_init_vector)\n",
    "\n",
    "    loss, image, sentence, mask = caption_generator.build_model()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=50)\n",
    "    if not epoch == 0:\n",
    "        saver.restore(sess, 'models/model-' + str(epoch))\n",
    "\n",
    "    temp = set(tf.all_variables())\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    if epoch == 0:\n",
    "        tf.initialize_all_variables().run()\n",
    "    else:\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - temp))\n",
    "\n",
    "        \n",
    "    for epoch in range(epoch, n_epochs):\n",
    "        #train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        for start, end in zip( \\\n",
    "                range(0, len(feats), batch_size),\n",
    "                range(batch_size, len(feats), batch_size)\n",
    "                ):\n",
    "            current_feats = feats[start:end]\n",
    "            current_captions = captions[start:end]\n",
    "\n",
    "            current_caption_ind = map(lambda cap: [wordtoix[word] for word in cap.lower().split(' ')[:-1] if word in wordtoix], current_captions)\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=maxlen+1)\n",
    "            current_caption_matrix = np.hstack( [np.full( (len(current_caption_matrix),1), 0), current_caption_matrix] ).astype(int)\n",
    "\n",
    "            current_mask_matrix = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array( map(lambda x: (x != 0).sum()+2, current_caption_matrix ))\n",
    "            #  +2 -> #START# and '.'\n",
    "\n",
    "            for ind, row in enumerate(current_mask_matrix):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={\n",
    "                image: current_feats,\n",
    "                sentence : current_caption_matrix,\n",
    "                mask : current_mask_matrix\n",
    "                })\n",
    "\n",
    "            print \"Current Cost: \", loss_value, \", Epoch: \", epoch, \", iteration: \", start, \"/\", len(feats) \n",
    "\n",
    "        print \"Epoch \", epoch, \" is done. Saving the model ... \"\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch+1)\n",
    "        learning_rate *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_1(reference, candidate):\n",
    "    #print reference, candidate\n",
    "    prec = [x for x in candidate if x in reference] \n",
    "    bleu1 = float(len(prec)) / len(candidate)\n",
    "    return bleu1\n",
    "\n",
    "def bleu_2(reference, candidate):\n",
    "    bi_gram_cand = zip(candidate, candidate[1:])\n",
    "    bi_gram_ref = zip(reference, reference[1:])\n",
    "    prec = [[x,y] for x,y in bi_gram_cand if (x,y) in bi_gram_ref]\n",
    "    bleu2 = float(len(prec)) / len(bi_gram_cand)\n",
    "    return bleu2\n",
    "                \n",
    "def bleu_3(reference, candidate):\n",
    "    tri_gram_cand = zip(candidate, candidate[1:], candidate[2:])\n",
    "    tri_gram_ref = zip(reference, reference[1:], reference[2:])\n",
    "    prec = [[x,y,z] for x,y,z in tri_gram_cand if (x,y,z) in tri_gram_ref]\n",
    "    bleu3 = float(len(prec)) / len(tri_gram_cand)\n",
    "    return bleu3\n",
    "\n",
    "def bleu_4(reference, candidate):\n",
    "    quad_gram_cand = zip(candidate, candidate[1:], candidate[2:], candidate[3:])\n",
    "    quad_gram_ref = zip(reference, reference[1:], reference[2:], reference[3:])\n",
    "    prec = [[x,y, z, w] for x,y, z, w in quad_gram_cand if (x,y,w,z) in quad_gram_ref]\n",
    "    bleu4 = float(len(prec)) / len(quad_gram_cand)\n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(test_image_path='data/flickr30k', model_path='./models/model-10', maxlen=30):        \n",
    "    ixtoword = np.load('./data/ixtoword.npy').tolist()\n",
    "    wordtoix = np.load('./data/wordtoix.npy').tolist()\n",
    "\n",
    "    n_words = len(ixtoword)\n",
    "    \n",
    "    filename = np.load('./data/test_file.npy')\n",
    "    test_feat = np.load('./data/test_feat.npy')\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    caption_generator = Caption_Generator(\n",
    "           dim_image=dim_image,\n",
    "           dim_hidden=dim_hidden,\n",
    "           dim_embed=dim_embed,\n",
    "           batch_size=batch_size,\n",
    "           n_lstm_steps=maxlen,\n",
    "           n_words=n_words)\n",
    "\n",
    "    fc7_tf, generated_word = caption_generator.build_generator(maxlen=maxlen)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    gt_cap = np.load('./data/test_caption.npy')\n",
    "    \n",
    "    res = {}\n",
    "    bleu1 = 0.0 ; bleu2 = 0.0 ; bleu3 = 0.0 ; bleu4 = 0.0 ; \n",
    "    for i in range(0, len(test_feat)):\n",
    "        image_feat = np.reshape(test_feat[i], (1, -1))\n",
    "        image = filename[i]\n",
    "        \n",
    "        generated_word_index= sess.run(generated_word, feed_dict={fc7_tf:image_feat})\n",
    "        generated_word_index = np.hstack(generated_word_index)\n",
    "\n",
    "        generated_words = [ixtoword[x] for x in generated_word_index]\n",
    "        punctuation = np.argmax(np.array(generated_words) == '.')+1\n",
    "        \n",
    "        generated_word_index = generated_word_index[:punctuation-1]\n",
    "        generated_words = generated_words[:punctuation]\n",
    "        generated_sentence = ' '.join(generated_words)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            PIL_image = Image.open(os.path.join(test_image_path, image))\n",
    "            fig,ax=plt.subplots(1,1)\n",
    "            ax.imshow(PIL_image)\n",
    "            plt.show()\n",
    "            \n",
    "            print generated_sentence\n",
    "        \n",
    "        res[image] = [{}]\n",
    "        res[image][0]['caption'] = generated_sentence\n",
    "        res[image][0]['image_id'] = image\n",
    "        \n",
    "        gt_sentence = gt_cap[range(i * 5, (i + 1) * 5)]\n",
    "        gt_words_index = [wordtoix[word] for x in gt_sentence for word in x.lower().split(' ') if word in wordtoix]\n",
    "        \n",
    "        bleu1 = bleu1 + bleu_1(gt_words_index, generated_word_index)\n",
    "        bleu2 = bleu2 + bleu_2(gt_words_index, generated_word_index)\n",
    "        bleu3 = bleu3 + bleu_3(gt_words_index, generated_word_index)\n",
    "        bleu4 = bleu4 + bleu_4(gt_words_index, generated_word_index)\n",
    "    \n",
    "    gts = {}\n",
    "    \n",
    "    cnt = 0\n",
    "    for name in filename:\n",
    "        gts[name] = [{}, {}, {}, {}, {}]\n",
    "        for j in range(0, 5):\n",
    "            gts[name][j]['caption'] = gt_cap[cnt]\n",
    "            gts[name][j]['image_id'] = name\n",
    "            cnt += 1\n",
    "    \n",
    "    tokenizer = PTBTokenizer()\n",
    "    gts = tokenizer.tokenize(gts)\n",
    "    res = tokenizer.tokenize(res)\n",
    "    \n",
    "    cider = Cider()\n",
    "    score, scores = cider.compute_score(gts, res)\n",
    "    bleu1 /= len(test_feat) ; bleu2 /= len(test_feat) ; bleu3 /= len(test_feat) ; bleu4 /= len(test_feat) ; #Meteor /= sample_cnt\n",
    "    \n",
    "    print \"----------------------- Evaluation Results --------------------------\"\n",
    "    print \"BLEU 1 :\", bleu1, \"\\nBLEU 2 :\", bleu2, \"\\nBLEU 3 :\", bleu3, \"\\nBLEU 4 :\", bleu4 \n",
    "    print('Cider: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation('data/flickr30k/', 'models/model-2')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
