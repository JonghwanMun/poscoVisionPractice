{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## example code에서 성능이 낮았던 multi-layer NN의 성능을 올려봅시다.\n",
    "### Problem 1. 파라미터 초기화의 중요성 확인\n",
    "  - all zero 초기화는 좋은 weight 초기화 방법이 아닙니다.\n",
    "  - linear layer의 weight를 normal distribution으로 초기화 하도록 구현\n",
    "      - hint: \"normal\"을 tensorflow 홈페이지에서 검색\n",
    "      - mean은 0으로, stv는 0.1로 설정\n",
    "  - 약 92% accuracy를 가지도록 learning rate 찾기\n",
    "  \n",
    "### Problem 2. Activation function의 중요성 확인\n",
    "  - linearLayer 함수에서 linear layer의 결과 값에 ReLu 적용\n",
    "  - linearLayer 함수에서 with_relu=True로 설정\n",
    "  - 약 97% accuracy를 가짐\n",
    "\n",
    "### Problem 3. Dropout 적용\n",
    "  - linearLayer 함수에서 linear layer의 결과 값 (혹은 Relu를 적용한 값)에 dropout 적용\n",
    "  - keep_prob은 0.8로 적용\n",
    "  - linearLayer 함수에서 with_dropout=True로 설정\n",
    "  - 약 97% accuracy를 가짐 (relu만 적용했을때보다 약간 성능이 떨어짐)\n",
    "\n",
    "### Problem 4. \n",
    "  - 자주 쓰이는 layer는 이미 구현되어 제공됩니다. 제공되는 함수를 이용하여 모델을 정의해봅시다.\n",
    "  - linearLayer 함수의 인풋 파라미터 use_contribute를 True로 바꾼 후, linearLayer 함수의 역할을 하는 레이어를 아래 링크에서 찾아서 적용해봅시다.\n",
    "      - https://www.tensorflow.org/api_guides/python/contrib.layers\n",
    "  - linearLayer 함수에서 use_contribute=True로 설정\n",
    "\n",
    "### Problem 5. 더 deep한 NN을 만들어 봅시다.\n",
    "  - 4-layer NN을 구현하고, 성능이 나오는 파라미터 (hidden layer의 dimension들, learning rate)를 찾기\n",
    "  - note: buildModel 함수의 buildInference 함수 호출하는 부분도 수정 필요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Fuctions\n",
    "  - Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearLayer(inp, inp_dim, out_dim, with_relu=False, \\\n",
    "                with_dropout=False, prefix='hidden', use_contribute=False):\n",
    "    \"\"\" Linear layer\n",
    "    Args:\n",
    "        inp: input data, [batch_size, inp_dim]\n",
    "        inp_dim: input dimension\n",
    "        out_dim: output dimension\n",
    "    Returns:\n",
    "        out: Output tensor with the computed logits, [batch_size, out_dim]\n",
    "             return (inp * weight + bias)\n",
    "    \"\"\"\n",
    "    print '%s: %d-dim ->%d-dim (relu: %s, dropout: %s, contribute:%s)' \\\n",
    "            % (prefix, inp_dim, out_dim, with_relu, with_dropout, use_contribute)\n",
    "    \n",
    "    with tf.name_scope(prefix):\n",
    "        if not use_contribute :\n",
    "            # Problem 1\n",
    "            # TODO: initializing weight using normal distribution, mean (1.0) and std (0.1) \n",
    "            \n",
    "            weight = tf.Variable(TODO_FillHere, name='weights') # TODO\n",
    "            bias = tf.Variable(tf.zeros([out_dim]), name='biases')\n",
    "            out = tf.matmul(inp, weight) + bias\n",
    "            \n",
    "            if with_relu : \n",
    "                # Problem 2\n",
    "                # TODO: Apply relu operation\n",
    "                \n",
    "            if with_dropout :\n",
    "                # Problem 3\n",
    "                # TODO: Apply dropout operation\n",
    "                \n",
    "                \n",
    "        else :\n",
    "            # Problem 4\n",
    "            # TODO: Using tf.contrib.layers utility\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildInputHolders():\n",
    "    \"\"\" define placeholders for model inputs\n",
    "    Returns:\n",
    "        img_holder: images placeholder\n",
    "        label_hodler: labels placeholder\n",
    "    \"\"\"\n",
    "    img_holder = tf.placeholder(tf.float32, [None, 28*28])\n",
    "    label_holder = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    return img_holder, label_holder\n",
    "\n",
    "\n",
    "def buildInference(images, img_dim, label_num, hidden1_units, hidden2_units):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "    Args:\n",
    "        images: Images placeholder, from inputs().\n",
    "        hidden1_units: Size of the first hidden layer.\n",
    "        hidden2_units: Size of the second hidden layer.\n",
    "    Returns:\n",
    "        pred: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "    # Hidden 1\n",
    "    hidden1 = linearLayer(images, img_dim, hidden1_units, prefix='hidden1')\n",
    "    # Hidden 2\n",
    "    hidden2 = linearLayer(hidden1, hidden1_units,hidden2_units, prefix='hidden2')\n",
    "    \n",
    "    # Problem 5\n",
    "    # TODO: adding additional linear layer\n",
    "    \n",
    "    # Linear\n",
    "    # 마지막 레이블에 대한 확률값을 예측하는 layer에서는 relu와 dropout을 적용하지 않습니다.\n",
    "    pred = linearLayer(hidden2, hidden2_units, \\\n",
    "                       label_num, with_relu=False, \\\n",
    "                       with_dropout=False, prefix='linear')\n",
    "    \n",
    "    return tf.nn.softmax(pred)\n",
    "\n",
    "def buildLoss(preds, labels):\n",
    "    \"\"\"Calculates the loss from the predictions and the labels.\n",
    "    Args:\n",
    "        preds  : prediction tensor, [batch_size, NUM_CLASSES]\n",
    "        labels : labels tensor, [batch_size]\n",
    "    Returns:\n",
    "        loss : loss tensor of type float\n",
    "    \"\"\"\n",
    "    cross_entropy_loss = -tf.reduce_sum(labels * tf.log(preds), reduction_indices=[1])\n",
    "    return tf.reduce_mean(cross_entropy_loss)\n",
    "\n",
    "def buildAccuracy(pred, labels):\n",
    "    \"\"\"Calculates the accuracy from the predictions and the labels.\n",
    "    Args:\n",
    "        preds  : prediction tensor, [batch_size, NUM_CLASSES]\n",
    "        labels : labels tensor, [batch_size]\n",
    "    Returns:\n",
    "        accuracy : accuracy tensor of type float\n",
    "    \"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def buildModel(params):\n",
    "    \"\"\" Build Model\n",
    "    Args:\n",
    "        params : dictionary data for parameters {\n",
    "            'batch_size' : batch size\n",
    "            'lr' : learning rate\n",
    "            }\n",
    "    Returns:\n",
    "        imgs : input images of model\n",
    "        labels : input labels of model\n",
    "        train_step : one step operation for training\n",
    "        loss : Loss tensor\n",
    "        acc : accuracy\n",
    "    \"\"\"\n",
    "    imgs, labels = buildInputHolders()\n",
    "    preds = buildInference(imgs, 28*28, 10, 128, 32) # for Problem 5, should be changed\n",
    "    loss = buildLoss(preds, labels)\n",
    "    acc = buildAccuracy(preds, labels)\n",
    "    \n",
    "    global_step = tf.Variable(.0, trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "    train_step = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    return imgs, labels, train_step, loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main Function\n",
    "  - 각 문제를 푼 후 아래 두 셀을 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "params = {}\n",
    "params['batch_size'] = 100\n",
    "params['lr'] = 0.5\n",
    "params['total_batch'] = int(mnist.train.num_examples/params['batch_size'])\n",
    "\n",
    "# build model\n",
    "imgs, labels, train_step, loss, acc = buildModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Avg. cost = 0.449\n",
      "Epoch: 0002 Avg. cost = 0.196\n",
      "Epoch: 0003 Avg. cost = 0.142\n",
      "Epoch: 0004 Avg. cost = 0.112\n",
      "Epoch: 0005 Avg. cost = 0.093\n",
      "Epoch: 0006 Avg. cost = 0.077\n",
      "Epoch: 0007 Avg. cost = 0.067\n",
      "Epoch: 0008 Avg. cost = 0.057\n",
      "Epoch: 0009 Avg. cost = 0.050\n",
      "Epoch: 0010 Avg. cost = 0.045\n",
      "Test Accuracy:  0.9744\n"
     ]
    }
   ],
   "source": [
    "batch_size = params['batch_size']\n",
    "total_batch = params['total_batch']\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(10):\n",
    "        total_cost = 0\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train_step, loss], \\\n",
    "                                   feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "            total_cost += cost_val\n",
    "\n",
    "        print \"Epoch:\", \"%04d\" % (epoch + 1), \\\n",
    "            \"Avg. cost =\", '{:.3f}'.format(total_cost / total_batch)\n",
    "\n",
    "    # 테스트 데이터에 대한 정확도\n",
    "    print \"Test Accuracy: \", sess.run(acc, \\\n",
    "                    feed_dict={imgs: mnist.test.images, labels: mnist.test.labels})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
