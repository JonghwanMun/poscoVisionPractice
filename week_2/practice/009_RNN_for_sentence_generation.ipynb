{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for sentence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 단어 단위(Word-level)의 RNN 구현해보자\n",
    "\n",
    "### <학습목표>\n",
    "1. 이번 노트북에서는 단어(word) 단위의 입력값으로 RNN을 학습해 보고, 결과를 Tensorboard를 이용하여 보는 것을 목표로 합니다.\n",
    "2. 학습할 데이터는 Sherlock homes 시리즈 중 The Sign of the Four의 영문책을 이용하여 학습합니다.\n",
    "3. 학습된 모델을 이용하여 새로운 문장을 만들어 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "위에 선언한 함수에서 이제 hyperparameter들을 정합니다. \n",
    "일반적으로 network의 크기가 커질 수록(hidden unit이 많을 수록, layer 수가 많을 수록) 성능이 향상되지만, overfitting(fit to variance)이 되는 현상을 잘 관찰해야 합니다. hyperparameter들이 너무 적을 경우에는 underfitting(fit to bias)되는 현상이 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lstm_size' : 128,\n",
    "    'batch_size': 100,\n",
    "    'time_steps': 15,    \n",
    "    'num_layers' : 3,\n",
    "    'optimizer_params': {'learning_rate': 1e-3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python2 -- python3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import namedtuple\n",
    "from six.moves import urllib\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the data.\n",
    "url = 'http://cvlab.postech.ac.kr/~wgchang/data/others/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        if not os.path.isdir(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        filename, _ = urllib.request.urlretrieve(url + os.path.basename(filename), filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ../data/sherlock.txt\n"
     ]
    }
   ],
   "source": [
    "filename = maybe_download('../data/sherlock.txt', 3377296)\n",
    "# filename = maybe_download('../data/sherlock_short.txt', 609394)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration for prevent whole gpu usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "우선 텍스트 데이터를 불러들인 후 각 단어들을 정수값으로 변환하여 모델이 학습할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_multiple_s(text):\n",
    "    # 여러번 띄어쓰기가 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r' +',r' ',text)\n",
    "    # 여러번 탭이 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\t+',r' ',text)\n",
    "    # 여러번 newline으로된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\n+',r' ',text)\n",
    "    # 특수문자를 제거합니다.\n",
    "    text = re.sub(r'[^A-Za-z0-9.,\\?!\\'\" ]+',r'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    text=f.read()\n",
    "text=remove_multiple_s(text.lower())\n",
    "texts = re.findall(r'([.,\\?!\\'\"0-9]|\\b[a-zA-z]+\\b)',text,re.IGNORECASE)\n",
    "dictionary = set(texts)\n",
    "word_to_int = {c: i for i, c in enumerate(dictionary)}\n",
    "int_to_word = dict(enumerate(dictionary))\n",
    "words = np.array([word_to_int[c] for c in texts], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16387,  9801, 15499, ...,  6579,   848, 13063], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트의 길이와 텍스트가 숫자로 변환되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693263"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20372"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'i', 'mr', '.', 'sherlock', 'holmes', 'in', 'the', 'year', '1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이제 데이터를 training과 validation으로 나누고 각각을 batch로 만들어봅시다. 이번 과제에서는 Test set은 따로 없습니다.\n",
    "문장에서 input과 target의 배열을 만듭니다. 여기서 target은 input과 같은 길이의 글자열이지만 한 글자가 밀려진 글자열입니다.\n",
    "batch 크기를 맞추기 위해서 문장의 뒤에 남는 부분은 버립니다.\n",
    "split_frac은 training과 validation을 나누는 set의 비율을 나타냅니다. 전체 batch갯수중 90%를 training으로, 10%를 validation으로 사용합니다.\n",
    "\n",
    "<img src=\"../resources/dataset.jpeg\" width=\"500\" alt=\"split dataset\">\n",
    "\n",
    "x matrix(행렬)는 (`batch크기 x 글자열 길이`)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(words, **params):\n",
    "    batch_size = params.get('batch_size') or 50\n",
    "    time_steps = params.get('time_steps') or 50\n",
    "    split_frac = params.get('split_frac') or 0.8\n",
    "    \n",
    "    slice_size = batch_size * time_steps\n",
    "    n_batches = int(len(words) / slice_size)\n",
    "    x = words[: n_batches*slice_size]\n",
    "    y = words[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*time_steps], y[:, :split_idx*time_steps]\n",
    "    val_x, val_y = x[:, split_idx*time_steps:], y[:, split_idx*time_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(words, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 나눠졌는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5535)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1395)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16387,  9801, 15499,   848,  4720,  3984, 11012,  6041,  6762,\n",
       "        16845],\n",
       "       [ 6782, 17119, 13681,   848, 10027,  1000,  8628,  5808, 19848,\n",
       "        10465],\n",
       "       [13176,  3632, 11012, 18521, 19091,   848, 13063,  9801,  4258,\n",
       "         4634],\n",
       "       [ 4258,  6724, 10465, 19368, 10465,  4258,  9587,  3881, 10465,\n",
       "         4258],\n",
       "       [  266,  6041, 17112,  8144,   848, 17946,  4258,  2073,  6871,\n",
       "         7560],\n",
       "       [19081,  6464, 19531,  7933,  2453,  7560, 10465,  3944,  7560,\n",
       "         5445],\n",
       "       [ 1281, 10458, 10465,  6230,  2481, 10465,  5439, 10465, 17119,\n",
       "         4061],\n",
       "       [ 2579, 18521, 15527,   848,  5116,  9801,  1000,  2579,  5771,\n",
       "         5439],\n",
       "       [10465, 17119,  2425,  2615, 15378, 11938, 11964, 17920,  1367,\n",
       "        17119],\n",
       "       [ 7777,  9858, 17672, 10138, 18520,   848,  7560, 19961,  3281,\n",
       "        19289],\n",
       "       [  848,  7560,  2138,  6871,  6041, 13234,  7315,  6230,  6871,\n",
       "        19697],\n",
       "       [17431, 10465,    14, 19289,  1779,  2579,  6041, 20080, 17672,\n",
       "         3716],\n",
       "       [17277,  7867,   213, 11012,  6041, 12925, 17672, 11004,   848,\n",
       "        10027],\n",
       "       [16103,  9875,  4757, 18539,   848, 13063, 19081, 19961,  3614,\n",
       "         2495],\n",
       "       [11544, 10465,  4258, 10027,  7771, 10465,  4258, 19697,  7773,\n",
       "         1779],\n",
       "       [15503,  3102,  1000, 18964,  3716,  5841, 19289, 15659, 20080,\n",
       "          848],\n",
       "       [13716,   848,  6041,  9397,  9062,  7867, 12876, 14956,  6724,\n",
       "          848],\n",
       "       [ 1612, 10465,  1073,  6041, 17359, 17119, 15160, 17672,  6041,\n",
       "         9517],\n",
       "       [ 6230, 18457, 11012, 18521, 13118, 19697,  4303, 14211,  1073,\n",
       "         1577],\n",
       "       [ 2425, 10692, 17668,  7867, 13508,   848, 17672,  1612, 10465,\n",
       "         9801],\n",
       "       [  848, 10027,  5943,  6245,  2579, 18521,  9405, 10465, 17119,\n",
       "         5788],\n",
       "       [ 9801, 14476,  6041,  9148,   848, 12206, 15503,  1619, 15309,\n",
       "         4061],\n",
       "       [10465,  9801, 11897, 10465, 12313, 10465,  6871, 19081, 11698,\n",
       "         3956],\n",
       "       [ 6871, 11004,  1000,  6722, 16103, 15994,  5841,  3629,  6041,\n",
       "        14574],\n",
       "       [ 8442,  9801,  3566,  2579, 14476,  6041,  1011, 17672,  6041,\n",
       "        19611],\n",
       "       [11012,  8628,  1577, 10673, 10458, 10465, 17119,  6871,  1637,\n",
       "        15453],\n",
       "       [10465,  2275, 11938, 10465, 17119, 18521,  4088, 10465,  4070,\n",
       "        10465],\n",
       "       [17672,  7867,  4358, 17119, 10677, 14632, 10465,  6230, 10246,\n",
       "        10882],\n",
       "       [18325, 17119,  6041,    65, 17672, 12886, 11975, 10465,  1779,\n",
       "        11993],\n",
       "       [13063, 13063,  9587,  3801,  3656,  6041, 13734,   848, 13063,\n",
       "        13063],\n",
       "       [ 6230,  7867, 16953, 19697, 11002,  7867,   243,  5307,  5116,\n",
       "        15593],\n",
       "       [15453,  6722,  5963,  2967, 11004,  1073,  7867, 13070, 11172,\n",
       "          848],\n",
       "       [ 1022,  6871, 18521, 17831, 11002,  7867,  4724, 10465,  6871,\n",
       "        10027],\n",
       "       [17518, 15033,   848, 16524, 12269,   650, 19289,  8705, 10465,\n",
       "        17119],\n",
       "       [ 4757,  7867,  4498, 13911, 10465, 12035,  2493, 11004, 15453,\n",
       "         6722],\n",
       "       [13063, 13063,  9804, 10465,  9804, 10465,  7560, 11473,  7461,\n",
       "          753],\n",
       "       [ 2825, 12965, 13063, 20051,  8476,  7589,   848, 13063,  7560,\n",
       "         6579],\n",
       "       [17672, 15475, 10465, 17119, 17672, 19360,  7980,  4718,  7867,\n",
       "         7643],\n",
       "       [11436,  7323, 11004, 10465, 15605,  8357, 18521,  9223,  2579,\n",
       "        18521],\n",
       "       [19117, 19697, 13616,  2579,  3614,  2495,  6301, 10465,  6871,\n",
       "         9801],\n",
       "       [15503,  3255, 10465, 16033, 10465, 17119, 11004,  9810,  2260,\n",
       "        15032],\n",
       "       [ 9801,  2490, 19688,  6871, 15503,  5682, 11698,  4505,  7560,\n",
       "        15179],\n",
       "       [17887, 19961, 10465, 17119, 11439, 10465, 17668, 18521, 12994,\n",
       "        11012],\n",
       "       [ 4358,  2579, 15487,  6885,  9801,  6579, 13394,   848, 11018,\n",
       "         9801],\n",
       "       [ 6871, 10518, 17672, 18521,  2172,   848, 18521, 18246, 11002,\n",
       "        10900],\n",
       "       [ 6518,  3629, 16183,  6578, 19697, 10366, 11004,  7867,  1737,\n",
       "         1029],\n",
       "       [13312, 18237, 17231,  2579, 12013, 11002,  6041,  1779, 19697,\n",
       "        11002],\n",
       "       [19697, 15395,  6041,  5873, 16943,   848, 19536, 17672,  4061,\n",
       "        10551],\n",
       "       [13367, 15487,  7867, 12258, 12336,  6724,   848,  4061, 11711,\n",
       "         4258],\n",
       "       [13230,  3791, 13091, 10465, 19697,  6722,  7001, 15925,  2579,\n",
       "        15487],\n",
       "       [ 5963, 17344,  2579,  7560,  4061, 12119,   848,  9801, 18320,\n",
       "         2493],\n",
       "       [ 1741, 10465, 11004,  4258,  2073,  6722, 11012,  7867,  7642,\n",
       "         9871],\n",
       "       [17717,  4258,  2073,  6041,  3335, 17421, 10465, 17119,  6724,\n",
       "         9801],\n",
       "       [ 9801,   650, 15593, 10465, 17119, 10465,  2493,  4961, 11964,\n",
       "        16524],\n",
       "       [13070,    28, 12965, 17946,   616,  9801,  5771,  9927, 13063,\n",
       "        20051],\n",
       "       [ 1133,  9927, 13063, 13063, 16524, 10465, 19368, 10465,  9801,\n",
       "         5963],\n",
       "       [19289,  7461, 20227,  8632,   848, 13063, 13063,  6370, 10465,\n",
       "        15499],\n",
       "       [ 7867, 17486,  3789, 10465, 13063, 10854, 10027,   848, 13063,\n",
       "        15179],\n",
       "       [10465, 16524, 19081, 15486, 13367, 20080,  1916,  2579,  6222,\n",
       "        18539],\n",
       "       [ 9801,  5963, 20286,  3715, 15503, 10692,   848, 15503,  3070,\n",
       "        18571],\n",
       "       [13121,  6871, 10027,  4303,  3566, 18521, 11821,  2453, 12088,\n",
       "          848],\n",
       "       [ 5439, 11004, 11002, 10027, 18667,  7560, 11522,  9927, 13063,\n",
       "        13063],\n",
       "       [  848,  5116,  6041,  2248,  7560, 19961,  3281,  1210,  1073,\n",
       "         6835],\n",
       "       [20141,  2579,  5908, 10465, 15043, 17119, 10119, 15032,  7867,\n",
       "        20015],\n",
       "       [ 4516,  5564, 13874,  5682,  2579,  7560, 10465, 15499,   848,\n",
       "         3984],\n",
       "       [11002,  2425, 16953, 10465, 17119,  6041, 18386, 17672, 11768,\n",
       "         7560],\n",
       "       [ 4303,  6722,  1358, 15487, 11012,  6041,  3354,  6113,   848,\n",
       "         9587],\n",
       "       [16537,   848, 11018,  3984,  1892,  2736, 10027,  6027, 18521,\n",
       "        20227],\n",
       "       [10762,   848, 13063, 13063,  9804, 10465,  5439, 10465, 11052,\n",
       "         6871],\n",
       "       [ 6011, 17119, 10429,  3629,  6041,  4100, 17672,  3591, 14137,\n",
       "          848],\n",
       "       [  848, 13063, 13063,  9801,  6230,    14, 19536,  1073,  5368,\n",
       "        11012],\n",
       "       [ 7251, 17672,  7117, 10465,  4088,  7880, 17668, 12087, 10518,\n",
       "        10465],\n",
       "       [ 7642,   851,  1281,  1073, 11734, 10465, 13063,  2425, 16650,\n",
       "        16578],\n",
       "       [17423,  6871, 10027,  5943,  4061, 11254, 17668,   848, 17946,\n",
       "         5943],\n",
       "       [19368,   848, 19081,  5963, 17633, 18539,  2453,  5116,  6871,\n",
       "        10027],\n",
       "       [17883,  9801,  1862,  6041,  6036,  1035, 16822,  4199,  1073,\n",
       "        13874],\n",
       "       [ 6230, 16524, 15363,  3330,  7080,  6041, 12644, 17119,  6041,\n",
       "        11711],\n",
       "       [10458,  3716,   848, 11004,  6230,  6722,  7867, 11684,  4619,\n",
       "        17119],\n",
       "       [  848, 17119,  6724, 10465, 15503, 11181,  5781, 10465, 19081,\n",
       "         5963],\n",
       "       [11018, 17717,   496,  4258,  7357,  7867,  6248, 11711, 11012,\n",
       "         6041],\n",
       "       [ 6041, 14840, 11012,  5404,  2579, 10366,  6041, 13121, 13101,\n",
       "         1000],\n",
       "       [12965, 13063,  6754,  2453,  1779, 17672, 18521, 16659, 16013,\n",
       "         4641],\n",
       "       [ 9621, 19114,   848, 11004,  5943,  2928,  1044, 18101,  2615,\n",
       "        11012],\n",
       "       [10027, 15623, 11487, 10075,  2495,  6041, 14122, 11012,  6041,\n",
       "         6572],\n",
       "       [13366, 10606, 10465, 17119, 18521, 14449,  2565, 11012,  2453,\n",
       "         6871],\n",
       "       [10027,   818, 10465, 13063, 11698,  9801,  8025, 19707,  5943,\n",
       "        19860],\n",
       "       [10027,  1000, 11915, 16103,   443, 17668, 14211, 17672, 18521,\n",
       "         6353],\n",
       "       [ 9223,   848,  6041,  8067,  1000,  5478,  1073,  7867, 13630,\n",
       "        10465],\n",
       "       [  848,  9801,  4258,  4708,  1000, 10937,  6182, 17672, 11004,\n",
       "          848],\n",
       "       [  848,  4258,  3558,  4258,  6871,  4516,  3614,  7867,  8449,\n",
       "          848],\n",
       "       [11473, 15503, 17366,  2579, 18521, 12722, 10465, 17119,  2799,\n",
       "        15503],\n",
       "       [ 4720,  3984,  2493, 19081, 13550, 15593,  4148, 15864,  6871,\n",
       "         6113],\n",
       "       [17717, 11255, 19531,  5033,   848, 17717, 11002,  1779, 18101,\n",
       "        14226],\n",
       "       [ 6835, 12534,  3715, 11012,  6041, 10673,   848, 13063, 13063,\n",
       "        10060],\n",
       "       [10027, 11002, 13874,  1052, 12789, 17119,  7867, 13312, 16834,\n",
       "        11711],\n",
       "       [  164,   848, 13063,  9380, 12965,  7560, 19961,  7890,   753,\n",
       "         3629],\n",
       "       [13063, 13063,  7560,  1854,  6871,  7560,  5963,  1000, 11004,\n",
       "        20129],\n",
       "       [ 7867, 17887,  4258,  2073,  5955, 17119,  7969,  1043,  8860,\n",
       "        11012],\n",
       "       [16524,  8835,  2579,  3716, 10465, 16033, 10465,  6727, 15453,\n",
       "        19081],\n",
       "       [ 1779, 17672,  7188,  6230, 18521, 19953, 12336, 10465,  8879,\n",
       "         3426]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "학습을 할 때 각각 batch를 순서대로 넣어야 하기때문에 batch하나를 가져오는 함수를 만들어봅시다. 각 batch는 (`batch 크기 X time_steps`)입니다.\n",
    "예를 들면, 우리의 모델이 100개의 문자열에 대해서 학습을 한다면, `time_steps = 100`이 됩니다. 그 다음 batch는 학습한 그 다음 문자열부터 학습됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tensorflow를 이용하여 RNN을 만들어봅시다. tensorflow관련 함수들은 [Tensorflow RNN API](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)를 참조하시면 됩니다.\n",
    "##### 참조 링크\n",
    "- [One-hot vector](https://www.tensorflow.org/api_docs/python/tf/one_hot): \n",
    "<img src='../resources/one_hot.png' width=\"700\" alt=\"one hot encoding\">\n",
    "- [Dropout](https://www.youtube.com/watch?v=NhZVe50QwPM)[참조논문](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf): Dropout은 random하게 특정 node를 0으로 만들어서 back-propagation이 0으로 된 node 이후로 진행되지 않게하여 overfiting을 막아주는 regularization역할을하여 학습을 원할하게합니다. **Advanced Topic: [Batch Normalization](https://arxiv.org/abs/1502.03167)를 추가적으로 공부하시면 overfitting 관련 공부에 도움이 됩니다.**\n",
    "<img src='../resources/dropout.png' width=\"700\" alt=\"dropout\">\n",
    "- [Optimizer](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorBoard에 그래프를 기입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "def define_your_model():\n",
    "    ###\n",
    "    tf.summary.histogram('histogram', histogram)\n",
    "    tf.summary.scalar('scalar', scalar)\n",
    "    ###\n",
    "    merged = tf.summary.merge_all()\n",
    "    ###\n",
    "model = define_your_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ###\n",
    "    file_writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    file_writer.add_summary(summary_to_record, iteration_index)\n",
    "    ###\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def define_rnn_graph(num_classes, **params):\n",
    "    # parameters\n",
    "    lstm_size = params.get('lstm_size') or 128\n",
    "    batch_size = params.get('batch_size') or 50\n",
    "    time_steps = params.get('time_steps') or 50\n",
    "    num_layers = params.get('num_layers') or 2\n",
    "    optimizer_params = params.get('optimizer_params') or {'learning_rate': 1e-3}\n",
    "    grad_clip = params.get('grad_clip') or 10\n",
    "    sampling = params.get('sampling') or False\n",
    "    \n",
    "    if sampling == True:\n",
    "        batch_size, time_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # placeholders를 선언합니다.\n",
    "    # input을 tf.one_hot함수를 이용하여 one_hot vector로 바꿔줍니다.\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, time_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    # target도 비슷한 방식으로 진행합니다\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, time_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "\n",
    "        # Loss를 계산하기위해 one_hot vector들의 matrix를 tf.reshape함수를 이용하여 하나의 긴 vector로 바꾸어줍니다.\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    # Dropout을 위한 확률값을 저장하는 place holder\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # RNN의 한 종류인 LSTM 구현\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            lstm = rnn.BasicLSTMCell(lstm_size)\n",
    "            # rnn.DropoutWrapper를 이용하여 RNN model에 Dropout 추가\n",
    "            drop = rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            # LSTM hidden layer 추가, weight sharing\n",
    "            lstm_layers.append(drop)\n",
    "        cell = rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "    # tf.nn.dynamic_rnn함수를 이용해 RNN을 실행\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # forward propagation\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "\n",
    "    # Output을 Concatenate한 후에 Reshape합니다.\n",
    "    with tf.name_scope('reshaper'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Cost를 계산하기위해 RNN putput을  input으로하는 softmax layer를 제작합니다.\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # weights & bias를 histogram으로 작성\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        # prediction의 확률값을 histogram으로 작성\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        # cost값을 scalar value로 작성\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # 학습을 위한 Optimizer를 정의합니다.\n",
    "    # 대표적인 optimizer로는 SGD(stocastic gradient descent), Adam, RMSprop 등이 있습니다.\n",
    "    # Gradient clipping을 통해 gradient값이 매우 큰 경우는 grad_clip값으로 제한합니다.\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(**optimizer_params)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # summary를 merge합니다.\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 앞에 선언한 노드들을 모두 Graph로 만들어서 결과로 반환합니다.\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer','merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 학습 (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint를 저장할 directory를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('checkpoints/sherlock_word'):\n",
    "    os.makedirs('checkpoints/sherlock_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "    filelist=set([re.search(r'(^i[0-9]+_l[0-9]+_[0-9]+\\.[0-9]+)\\b',l).group(0) \\\n",
    "                  for l in os.listdir(checkpoint_dir) if re.search(r'(^i[0-9]+_l[0-9]+_[0-9]+\\.[0-9]+)\\b',l)])\n",
    "    filelist=list(filelist)\n",
    "    checkpoint_sorted_by_iterations = sorted(filelist, key=lambda pattern:\\\n",
    "                                    int(re.search(r'^i([0-9]+)_l[0-9]+_[0-9]+\\.[0-9]+\\b',pattern).group(1)))\n",
    "    if checkpoint_sorted_by_iterations == []:\n",
    "        return None\n",
    "    else:\n",
    "        return os.path.join(checkpoint_dir,checkpoint_sorted_by_iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "checkpoint_interval = 50\n",
    "checkpoint = None\n",
    "# 기존 checkpoint를 실행하고싶다면 None 대신 checkpoint_path를 넣으면됩니다.\n",
    "checkpoint = load_latest_checkpoint('checkpoints/sherlock_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/7380 Training loss: 9.9219 0.8294 sec/batch\n",
      "Epoch 1/20  Iteration 2/7380 Training loss: 9.9206 0.6049 sec/batch\n",
      "Epoch 1/20  Iteration 3/7380 Training loss: 9.9190 0.5852 sec/batch\n",
      "Epoch 1/20  Iteration 4/7380 Training loss: 9.9168 0.5695 sec/batch\n",
      "Epoch 1/20  Iteration 5/7380 Training loss: 9.9136 0.6527 sec/batch\n",
      "Epoch 1/20  Iteration 6/7380 Training loss: 9.9092 0.6532 sec/batch\n",
      "Epoch 1/20  Iteration 7/7380 Training loss: 9.9023 0.6217 sec/batch\n",
      "Epoch 1/20  Iteration 8/7380 Training loss: 9.8906 0.6690 sec/batch\n",
      "Epoch 1/20  Iteration 9/7380 Training loss: 9.8699 0.7458 sec/batch\n",
      "Epoch 1/20  Iteration 10/7380 Training loss: 9.8386 0.8175 sec/batch\n",
      "Epoch 1/20  Iteration 11/7380 Training loss: 9.7890 0.8779 sec/batch\n",
      "Epoch 1/20  Iteration 12/7380 Training loss: 9.7315 0.9210 sec/batch\n",
      "Epoch 1/20  Iteration 13/7380 Training loss: 9.6735 0.9240 sec/batch\n",
      "Epoch 1/20  Iteration 14/7380 Training loss: 9.6187 0.9306 sec/batch\n",
      "Epoch 1/20  Iteration 15/7380 Training loss: 9.5554 0.9346 sec/batch\n",
      "Epoch 1/20  Iteration 16/7380 Training loss: 9.4903 0.9486 sec/batch\n",
      "Epoch 1/20  Iteration 17/7380 Training loss: 9.4273 1.0072 sec/batch\n",
      "Epoch 1/20  Iteration 18/7380 Training loss: 9.3539 0.9505 sec/batch\n",
      "Epoch 1/20  Iteration 19/7380 Training loss: 9.2823 0.9628 sec/batch\n",
      "Epoch 1/20  Iteration 20/7380 Training loss: 9.2134 0.9749 sec/batch\n",
      "Epoch 1/20  Iteration 21/7380 Training loss: 9.1439 0.9842 sec/batch\n",
      "Epoch 1/20  Iteration 22/7380 Training loss: 9.0784 0.9818 sec/batch\n",
      "Epoch 1/20  Iteration 23/7380 Training loss: 9.0107 0.9849 sec/batch\n",
      "Epoch 1/20  Iteration 24/7380 Training loss: 8.9497 0.9899 sec/batch\n",
      "Epoch 1/20  Iteration 25/7380 Training loss: 8.8868 0.9939 sec/batch\n",
      "Epoch 1/20  Iteration 26/7380 Training loss: 8.8236 0.9891 sec/batch\n",
      "Epoch 1/20  Iteration 27/7380 Training loss: 8.7544 0.9970 sec/batch\n",
      "Epoch 1/20  Iteration 28/7380 Training loss: 8.6984 1.0092 sec/batch\n",
      "Epoch 1/20  Iteration 29/7380 Training loss: 8.6406 1.0022 sec/batch\n",
      "Epoch 1/20  Iteration 30/7380 Training loss: 8.5850 0.9965 sec/batch\n",
      "Epoch 1/20  Iteration 31/7380 Training loss: 8.5303 0.9955 sec/batch\n",
      "Epoch 1/20  Iteration 32/7380 Training loss: 8.4794 0.9965 sec/batch\n",
      "Epoch 1/20  Iteration 33/7380 Training loss: 8.4301 1.0028 sec/batch\n",
      "Epoch 1/20  Iteration 34/7380 Training loss: 8.3838 0.9986 sec/batch\n",
      "Epoch 1/20  Iteration 35/7380 Training loss: 8.3433 0.9895 sec/batch\n",
      "Epoch 1/20  Iteration 36/7380 Training loss: 8.2984 0.9982 sec/batch\n",
      "Epoch 1/20  Iteration 37/7380 Training loss: 8.2571 0.9948 sec/batch\n",
      "Epoch 1/20  Iteration 38/7380 Training loss: 8.2182 0.9907 sec/batch\n",
      "Epoch 1/20  Iteration 39/7380 Training loss: 8.1813 0.9946 sec/batch\n",
      "Epoch 1/20  Iteration 40/7380 Training loss: 8.1450 0.9893 sec/batch\n",
      "Epoch 1/20  Iteration 41/7380 Training loss: 8.1086 0.9898 sec/batch\n",
      "Epoch 1/20  Iteration 42/7380 Training loss: 8.0753 0.9891 sec/batch\n",
      "Epoch 1/20  Iteration 43/7380 Training loss: 8.0442 0.9827 sec/batch\n",
      "Epoch 1/20  Iteration 44/7380 Training loss: 8.0147 0.9816 sec/batch\n",
      "Epoch 1/20  Iteration 45/7380 Training loss: 7.9865 0.9775 sec/batch\n",
      "Epoch 1/20  Iteration 46/7380 Training loss: 7.9578 0.9820 sec/batch\n",
      "Epoch 1/20  Iteration 47/7380 Training loss: 7.9305 0.9858 sec/batch\n",
      "Epoch 1/20  Iteration 48/7380 Training loss: 7.9032 0.9920 sec/batch\n",
      "Epoch 1/20  Iteration 49/7380 Training loss: 7.8795 0.9828 sec/batch\n",
      "Epoch 1/20  Iteration 50/7380 Training loss: 7.8543 0.9780 sec/batch\n",
      "Validation loss: 6.30701 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 51/7380 Training loss: 7.8304 0.9927 sec/batch\n",
      "Epoch 1/20  Iteration 52/7380 Training loss: 7.8077 0.9850 sec/batch\n",
      "Epoch 1/20  Iteration 53/7380 Training loss: 7.7847 0.9923 sec/batch\n",
      "Epoch 1/20  Iteration 54/7380 Training loss: 7.7630 0.9784 sec/batch\n",
      "Epoch 1/20  Iteration 55/7380 Training loss: 7.7416 0.9762 sec/batch\n",
      "Epoch 1/20  Iteration 56/7380 Training loss: 7.7200 0.9923 sec/batch\n",
      "Epoch 1/20  Iteration 57/7380 Training loss: 7.6996 0.9824 sec/batch\n",
      "Epoch 1/20  Iteration 58/7380 Training loss: 7.6806 0.9762 sec/batch\n",
      "Epoch 1/20  Iteration 59/7380 Training loss: 7.6628 0.9816 sec/batch\n",
      "Epoch 1/20  Iteration 60/7380 Training loss: 7.6461 0.9911 sec/batch\n",
      "Epoch 1/20  Iteration 61/7380 Training loss: 7.6278 0.9769 sec/batch\n",
      "Epoch 1/20  Iteration 62/7380 Training loss: 7.6116 0.9786 sec/batch\n",
      "Epoch 1/20  Iteration 63/7380 Training loss: 7.5949 0.9741 sec/batch\n",
      "Epoch 1/20  Iteration 64/7380 Training loss: 7.5770 0.9891 sec/batch\n",
      "Epoch 1/20  Iteration 65/7380 Training loss: 7.5615 0.9769 sec/batch\n",
      "Epoch 1/20  Iteration 66/7380 Training loss: 7.5469 0.9771 sec/batch\n",
      "Epoch 1/20  Iteration 67/7380 Training loss: 7.5335 0.9772 sec/batch\n",
      "Epoch 1/20  Iteration 68/7380 Training loss: 7.5203 0.9765 sec/batch\n",
      "Epoch 1/20  Iteration 69/7380 Training loss: 7.5075 0.9815 sec/batch\n",
      "Epoch 1/20  Iteration 70/7380 Training loss: 7.4926 0.9763 sec/batch\n",
      "Epoch 1/20  Iteration 71/7380 Training loss: 7.4785 0.9764 sec/batch\n",
      "Epoch 1/20  Iteration 72/7380 Training loss: 7.4677 0.9820 sec/batch\n",
      "Epoch 1/20  Iteration 73/7380 Training loss: 7.4538 0.9745 sec/batch\n",
      "Epoch 1/20  Iteration 74/7380 Training loss: 7.4429 0.9745 sec/batch\n",
      "Epoch 1/20  Iteration 75/7380 Training loss: 7.4320 0.9728 sec/batch\n",
      "Epoch 1/20  Iteration 76/7380 Training loss: 7.4186 0.9708 sec/batch\n",
      "Epoch 1/20  Iteration 77/7380 Training loss: 7.4080 0.9720 sec/batch\n",
      "Epoch 1/20  Iteration 78/7380 Training loss: 7.3978 0.9680 sec/batch\n",
      "Epoch 1/20  Iteration 79/7380 Training loss: 7.3876 0.9910 sec/batch\n",
      "Epoch 1/20  Iteration 80/7380 Training loss: 7.3767 0.9728 sec/batch\n",
      "Epoch 1/20  Iteration 81/7380 Training loss: 7.3661 0.9727 sec/batch\n",
      "Epoch 1/20  Iteration 82/7380 Training loss: 7.3551 0.9737 sec/batch\n",
      "Epoch 1/20  Iteration 83/7380 Training loss: 7.3445 0.9807 sec/batch\n",
      "Epoch 1/20  Iteration 84/7380 Training loss: 7.3342 0.9727 sec/batch\n",
      "Epoch 1/20  Iteration 85/7380 Training loss: 7.3250 0.9766 sec/batch\n",
      "Epoch 1/20  Iteration 86/7380 Training loss: 7.3154 0.9754 sec/batch\n",
      "Epoch 1/20  Iteration 87/7380 Training loss: 7.3057 0.9687 sec/batch\n",
      "Epoch 1/20  Iteration 88/7380 Training loss: 7.2965 0.9955 sec/batch\n",
      "Epoch 1/20  Iteration 89/7380 Training loss: 7.2861 0.9824 sec/batch\n",
      "Epoch 1/20  Iteration 90/7380 Training loss: 7.2771 0.9782 sec/batch\n",
      "Epoch 1/20  Iteration 91/7380 Training loss: 7.2687 0.9747 sec/batch\n",
      "Epoch 1/20  Iteration 92/7380 Training loss: 7.2618 0.9757 sec/batch\n",
      "Epoch 1/20  Iteration 93/7380 Training loss: 7.2538 0.9690 sec/batch\n",
      "Epoch 1/20  Iteration 94/7380 Training loss: 7.2458 0.9671 sec/batch\n",
      "Epoch 1/20  Iteration 95/7380 Training loss: 7.2377 0.9707 sec/batch\n",
      "Epoch 1/20  Iteration 96/7380 Training loss: 7.2305 0.9700 sec/batch\n",
      "Epoch 1/20  Iteration 97/7380 Training loss: 7.2224 0.9760 sec/batch\n",
      "Epoch 1/20  Iteration 98/7380 Training loss: 7.2164 0.9835 sec/batch\n",
      "Epoch 1/20  Iteration 99/7380 Training loss: 7.2092 0.9869 sec/batch\n",
      "Epoch 1/20  Iteration 100/7380 Training loss: 7.2019 0.9725 sec/batch\n",
      "Validation loss: 6.24652 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 101/7380 Training loss: 7.1959 0.9703 sec/batch\n",
      "Epoch 1/20  Iteration 102/7380 Training loss: 7.1890 0.9759 sec/batch\n",
      "Epoch 1/20  Iteration 103/7380 Training loss: 7.1824 0.9731 sec/batch\n",
      "Epoch 1/20  Iteration 104/7380 Training loss: 7.1754 0.9824 sec/batch\n",
      "Epoch 1/20  Iteration 105/7380 Training loss: 7.1691 0.9669 sec/batch\n",
      "Epoch 1/20  Iteration 106/7380 Training loss: 7.1627 0.9715 sec/batch\n",
      "Epoch 1/20  Iteration 107/7380 Training loss: 7.1566 0.9768 sec/batch\n",
      "Epoch 1/20  Iteration 108/7380 Training loss: 7.1503 0.9773 sec/batch\n",
      "Epoch 1/20  Iteration 109/7380 Training loss: 7.1426 0.9654 sec/batch\n",
      "Epoch 1/20  Iteration 110/7380 Training loss: 7.1361 0.9802 sec/batch\n",
      "Epoch 1/20  Iteration 111/7380 Training loss: 7.1294 0.9681 sec/batch\n",
      "Epoch 1/20  Iteration 112/7380 Training loss: 7.1236 0.9743 sec/batch\n",
      "Epoch 1/20  Iteration 113/7380 Training loss: 7.1167 0.9701 sec/batch\n",
      "Epoch 1/20  Iteration 114/7380 Training loss: 7.1108 0.9675 sec/batch\n",
      "Epoch 1/20  Iteration 115/7380 Training loss: 7.1044 0.9685 sec/batch\n",
      "Epoch 1/20  Iteration 116/7380 Training loss: 7.0986 0.9698 sec/batch\n",
      "Epoch 1/20  Iteration 117/7380 Training loss: 7.0922 0.9704 sec/batch\n",
      "Epoch 1/20  Iteration 118/7380 Training loss: 7.0861 0.9707 sec/batch\n",
      "Epoch 1/20  Iteration 119/7380 Training loss: 7.0806 0.9665 sec/batch\n",
      "Epoch 1/20  Iteration 120/7380 Training loss: 7.0753 0.9674 sec/batch\n",
      "Epoch 1/20  Iteration 121/7380 Training loss: 7.0704 0.9649 sec/batch\n",
      "Epoch 1/20  Iteration 122/7380 Training loss: 7.0664 0.9703 sec/batch\n",
      "Epoch 1/20  Iteration 123/7380 Training loss: 7.0609 0.9703 sec/batch\n",
      "Epoch 1/20  Iteration 124/7380 Training loss: 7.0557 0.9692 sec/batch\n",
      "Epoch 1/20  Iteration 125/7380 Training loss: 7.0505 0.9684 sec/batch\n",
      "Epoch 1/20  Iteration 126/7380 Training loss: 7.0448 0.9666 sec/batch\n",
      "Epoch 1/20  Iteration 127/7380 Training loss: 7.0394 0.9692 sec/batch\n",
      "Epoch 1/20  Iteration 128/7380 Training loss: 7.0349 0.9652 sec/batch\n",
      "Epoch 1/20  Iteration 129/7380 Training loss: 7.0295 0.9670 sec/batch\n",
      "Epoch 1/20  Iteration 130/7380 Training loss: 7.0249 0.9720 sec/batch\n",
      "Epoch 1/20  Iteration 131/7380 Training loss: 7.0203 0.9757 sec/batch\n",
      "Epoch 1/20  Iteration 132/7380 Training loss: 7.0159 0.9678 sec/batch\n",
      "Epoch 1/20  Iteration 133/7380 Training loss: 7.0113 0.9673 sec/batch\n",
      "Epoch 1/20  Iteration 134/7380 Training loss: 7.0061 0.9753 sec/batch\n",
      "Epoch 1/20  Iteration 135/7380 Training loss: 7.0013 0.9674 sec/batch\n",
      "Epoch 1/20  Iteration 136/7380 Training loss: 6.9966 0.9680 sec/batch\n",
      "Epoch 1/20  Iteration 137/7380 Training loss: 6.9921 0.9759 sec/batch\n",
      "Epoch 1/20  Iteration 138/7380 Training loss: 6.9872 0.9661 sec/batch\n",
      "Epoch 1/20  Iteration 139/7380 Training loss: 6.9824 0.9731 sec/batch\n",
      "Epoch 1/20  Iteration 140/7380 Training loss: 6.9772 0.9724 sec/batch\n",
      "Epoch 1/20  Iteration 141/7380 Training loss: 6.9742 0.9676 sec/batch\n",
      "Epoch 1/20  Iteration 142/7380 Training loss: 6.9699 0.9752 sec/batch\n",
      "Epoch 1/20  Iteration 143/7380 Training loss: 6.9663 0.9702 sec/batch\n",
      "Epoch 1/20  Iteration 144/7380 Training loss: 6.9627 0.9697 sec/batch\n",
      "Epoch 1/20  Iteration 145/7380 Training loss: 6.9588 0.9740 sec/batch\n",
      "Epoch 1/20  Iteration 146/7380 Training loss: 6.9549 0.9719 sec/batch\n",
      "Epoch 1/20  Iteration 147/7380 Training loss: 6.9507 0.9680 sec/batch\n",
      "Epoch 1/20  Iteration 148/7380 Training loss: 6.9466 0.9650 sec/batch\n",
      "Epoch 1/20  Iteration 149/7380 Training loss: 6.9425 0.9707 sec/batch\n",
      "Epoch 1/20  Iteration 150/7380 Training loss: 6.9388 0.9675 sec/batch\n",
      "Validation loss: 6.22784 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 151/7380 Training loss: 6.9347 0.9767 sec/batch\n",
      "Epoch 1/20  Iteration 152/7380 Training loss: 6.9311 0.9662 sec/batch\n",
      "Epoch 1/20  Iteration 153/7380 Training loss: 6.9279 0.9701 sec/batch\n",
      "Epoch 1/20  Iteration 154/7380 Training loss: 6.9248 0.9780 sec/batch\n",
      "Epoch 1/20  Iteration 155/7380 Training loss: 6.9212 0.9661 sec/batch\n",
      "Epoch 1/20  Iteration 156/7380 Training loss: 6.9174 0.9637 sec/batch\n",
      "Epoch 1/20  Iteration 157/7380 Training loss: 6.9133 0.9668 sec/batch\n",
      "Epoch 1/20  Iteration 158/7380 Training loss: 6.9102 0.9677 sec/batch\n",
      "Epoch 1/20  Iteration 159/7380 Training loss: 6.9067 0.9737 sec/batch\n",
      "Epoch 1/20  Iteration 160/7380 Training loss: 6.9034 0.9688 sec/batch\n",
      "Epoch 1/20  Iteration 161/7380 Training loss: 6.9004 0.9678 sec/batch\n",
      "Epoch 1/20  Iteration 162/7380 Training loss: 6.8974 0.9676 sec/batch\n",
      "Epoch 1/20  Iteration 163/7380 Training loss: 6.8945 0.9710 sec/batch\n",
      "Epoch 1/20  Iteration 164/7380 Training loss: 6.8917 0.9674 sec/batch\n",
      "Epoch 1/20  Iteration 165/7380 Training loss: 6.8887 0.9682 sec/batch\n",
      "Epoch 1/20  Iteration 166/7380 Training loss: 6.8856 0.9774 sec/batch\n",
      "Epoch 1/20  Iteration 167/7380 Training loss: 6.8824 0.9760 sec/batch\n",
      "Epoch 1/20  Iteration 168/7380 Training loss: 6.8797 0.9689 sec/batch\n",
      "Epoch 1/20  Iteration 169/7380 Training loss: 6.8761 0.9652 sec/batch\n",
      "Epoch 1/20  Iteration 170/7380 Training loss: 6.8727 0.9686 sec/batch\n",
      "Epoch 1/20  Iteration 171/7380 Training loss: 6.8702 0.9701 sec/batch\n",
      "Epoch 1/20  Iteration 172/7380 Training loss: 6.8677 0.9696 sec/batch\n",
      "Epoch 1/20  Iteration 173/7380 Training loss: 6.8648 0.9689 sec/batch\n",
      "Epoch 1/20  Iteration 174/7380 Training loss: 6.8618 0.9744 sec/batch\n",
      "Epoch 1/20  Iteration 175/7380 Training loss: 6.8588 0.9744 sec/batch\n",
      "Epoch 1/20  Iteration 176/7380 Training loss: 6.8560 0.9844 sec/batch\n",
      "Epoch 1/20  Iteration 177/7380 Training loss: 6.8532 0.9773 sec/batch\n",
      "Epoch 1/20  Iteration 178/7380 Training loss: 6.8503 0.9750 sec/batch\n",
      "Epoch 1/20  Iteration 179/7380 Training loss: 6.8471 0.9689 sec/batch\n",
      "Epoch 1/20  Iteration 180/7380 Training loss: 6.8440 0.9722 sec/batch\n",
      "Epoch 1/20  Iteration 181/7380 Training loss: 6.8413 0.9778 sec/batch\n",
      "Epoch 1/20  Iteration 182/7380 Training loss: 6.8388 0.9727 sec/batch\n",
      "Epoch 1/20  Iteration 183/7380 Training loss: 6.8364 0.9705 sec/batch\n",
      "Epoch 1/20  Iteration 184/7380 Training loss: 6.8333 0.9769 sec/batch\n",
      "Epoch 1/20  Iteration 185/7380 Training loss: 6.8309 0.9776 sec/batch\n",
      "Epoch 1/20  Iteration 186/7380 Training loss: 6.8280 0.9715 sec/batch\n",
      "Epoch 1/20  Iteration 187/7380 Training loss: 6.8253 0.9701 sec/batch\n",
      "Epoch 1/20  Iteration 188/7380 Training loss: 6.8226 0.9700 sec/batch\n",
      "Epoch 1/20  Iteration 189/7380 Training loss: 6.8198 0.9834 sec/batch\n",
      "Epoch 1/20  Iteration 190/7380 Training loss: 6.8175 0.9673 sec/batch\n",
      "Epoch 1/20  Iteration 191/7380 Training loss: 6.8145 0.9693 sec/batch\n",
      "Epoch 1/20  Iteration 192/7380 Training loss: 6.8126 0.9746 sec/batch\n",
      "Epoch 1/20  Iteration 193/7380 Training loss: 6.8105 0.9723 sec/batch\n",
      "Epoch 1/20  Iteration 194/7380 Training loss: 6.8078 0.9713 sec/batch\n",
      "Epoch 1/20  Iteration 195/7380 Training loss: 6.8059 0.9708 sec/batch\n",
      "Epoch 1/20  Iteration 196/7380 Training loss: 6.8034 0.9694 sec/batch\n",
      "Epoch 1/20  Iteration 197/7380 Training loss: 6.8011 0.9844 sec/batch\n",
      "Epoch 1/20  Iteration 198/7380 Training loss: 6.7995 0.9692 sec/batch\n",
      "Epoch 1/20  Iteration 199/7380 Training loss: 6.7973 0.9757 sec/batch\n",
      "Epoch 1/20  Iteration 200/7380 Training loss: 6.7954 0.9803 sec/batch\n",
      "Validation loss: 6.22173 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 201/7380 Training loss: 6.7934 0.9746 sec/batch\n",
      "Epoch 1/20  Iteration 202/7380 Training loss: 6.7912 0.9806 sec/batch\n",
      "Epoch 1/20  Iteration 203/7380 Training loss: 6.7893 0.9710 sec/batch\n",
      "Epoch 1/20  Iteration 204/7380 Training loss: 6.7872 0.9692 sec/batch\n",
      "Epoch 1/20  Iteration 205/7380 Training loss: 6.7852 0.9786 sec/batch\n",
      "Epoch 1/20  Iteration 206/7380 Training loss: 6.7834 0.9730 sec/batch\n",
      "Epoch 1/20  Iteration 207/7380 Training loss: 6.7807 0.9685 sec/batch\n",
      "Epoch 1/20  Iteration 208/7380 Training loss: 6.7784 0.9707 sec/batch\n",
      "Epoch 1/20  Iteration 209/7380 Training loss: 6.7762 0.9854 sec/batch\n",
      "Epoch 1/20  Iteration 210/7380 Training loss: 6.7738 0.9753 sec/batch\n",
      "Epoch 1/20  Iteration 211/7380 Training loss: 6.7717 0.9878 sec/batch\n",
      "Epoch 1/20  Iteration 212/7380 Training loss: 6.7697 1.0008 sec/batch\n",
      "Epoch 1/20  Iteration 213/7380 Training loss: 6.7676 0.9846 sec/batch\n",
      "Epoch 1/20  Iteration 214/7380 Training loss: 6.7655 0.9816 sec/batch\n",
      "Epoch 1/20  Iteration 215/7380 Training loss: 6.7632 0.9849 sec/batch\n",
      "Epoch 1/20  Iteration 216/7380 Training loss: 6.7608 0.9770 sec/batch\n",
      "Epoch 1/20  Iteration 217/7380 Training loss: 6.7585 1.0077 sec/batch\n",
      "Epoch 1/20  Iteration 218/7380 Training loss: 6.7564 0.9875 sec/batch\n",
      "Epoch 1/20  Iteration 219/7380 Training loss: 6.7550 0.9741 sec/batch\n",
      "Epoch 1/20  Iteration 220/7380 Training loss: 6.7533 0.9816 sec/batch\n",
      "Epoch 1/20  Iteration 221/7380 Training loss: 6.7512 0.9712 sec/batch\n",
      "Epoch 1/20  Iteration 222/7380 Training loss: 6.7498 0.9736 sec/batch\n",
      "Epoch 1/20  Iteration 223/7380 Training loss: 6.7480 0.9653 sec/batch\n",
      "Epoch 1/20  Iteration 224/7380 Training loss: 6.7459 0.9727 sec/batch\n",
      "Epoch 1/20  Iteration 225/7380 Training loss: 6.7441 0.9759 sec/batch\n",
      "Epoch 1/20  Iteration 226/7380 Training loss: 6.7421 0.9810 sec/batch\n",
      "Epoch 1/20  Iteration 227/7380 Training loss: 6.7404 0.9686 sec/batch\n",
      "Epoch 1/20  Iteration 228/7380 Training loss: 6.7383 0.9681 sec/batch\n",
      "Epoch 1/20  Iteration 229/7380 Training loss: 6.7365 0.9754 sec/batch\n",
      "Epoch 1/20  Iteration 230/7380 Training loss: 6.7351 0.9850 sec/batch\n",
      "Epoch 1/20  Iteration 231/7380 Training loss: 6.7331 0.9800 sec/batch\n",
      "Epoch 1/20  Iteration 232/7380 Training loss: 6.7311 0.9729 sec/batch\n",
      "Epoch 1/20  Iteration 233/7380 Training loss: 6.7290 0.9780 sec/batch\n",
      "Epoch 1/20  Iteration 234/7380 Training loss: 6.7268 0.9765 sec/batch\n",
      "Epoch 1/20  Iteration 235/7380 Training loss: 6.7251 0.9721 sec/batch\n",
      "Epoch 1/20  Iteration 236/7380 Training loss: 6.7231 0.9716 sec/batch\n",
      "Epoch 1/20  Iteration 237/7380 Training loss: 6.7213 0.9741 sec/batch\n",
      "Epoch 1/20  Iteration 238/7380 Training loss: 6.7192 0.9775 sec/batch\n",
      "Epoch 1/20  Iteration 239/7380 Training loss: 6.7166 0.9751 sec/batch\n",
      "Epoch 1/20  Iteration 240/7380 Training loss: 6.7146 0.9755 sec/batch\n",
      "Epoch 1/20  Iteration 241/7380 Training loss: 6.7130 0.9754 sec/batch\n",
      "Epoch 1/20  Iteration 242/7380 Training loss: 6.7119 0.9735 sec/batch\n",
      "Epoch 1/20  Iteration 243/7380 Training loss: 6.7100 0.9782 sec/batch\n",
      "Epoch 1/20  Iteration 244/7380 Training loss: 6.7086 0.9833 sec/batch\n",
      "Epoch 1/20  Iteration 245/7380 Training loss: 6.7072 0.9733 sec/batch\n",
      "Epoch 1/20  Iteration 246/7380 Training loss: 6.7057 0.9787 sec/batch\n",
      "Epoch 1/20  Iteration 247/7380 Training loss: 6.7041 0.9691 sec/batch\n",
      "Epoch 1/20  Iteration 248/7380 Training loss: 6.7027 0.9708 sec/batch\n",
      "Epoch 1/20  Iteration 249/7380 Training loss: 6.7010 0.9701 sec/batch\n",
      "Epoch 1/20  Iteration 250/7380 Training loss: 6.6996 0.9702 sec/batch\n",
      "Validation loss: 6.21616 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 251/7380 Training loss: 6.6977 0.9784 sec/batch\n",
      "Epoch 1/20  Iteration 252/7380 Training loss: 6.6961 0.9744 sec/batch\n",
      "Epoch 1/20  Iteration 253/7380 Training loss: 6.6947 0.9816 sec/batch\n",
      "Epoch 1/20  Iteration 254/7380 Training loss: 6.6926 0.9772 sec/batch\n",
      "Epoch 1/20  Iteration 255/7380 Training loss: 6.6914 0.9741 sec/batch\n",
      "Epoch 1/20  Iteration 256/7380 Training loss: 6.6899 0.9707 sec/batch\n",
      "Epoch 1/20  Iteration 257/7380 Training loss: 6.6886 0.9718 sec/batch\n",
      "Epoch 1/20  Iteration 258/7380 Training loss: 6.6870 0.9736 sec/batch\n",
      "Epoch 1/20  Iteration 259/7380 Training loss: 6.6859 0.9699 sec/batch\n",
      "Epoch 1/20  Iteration 260/7380 Training loss: 6.6842 0.9748 sec/batch\n",
      "Epoch 1/20  Iteration 261/7380 Training loss: 6.6829 0.9798 sec/batch\n",
      "Epoch 1/20  Iteration 262/7380 Training loss: 6.6817 0.9830 sec/batch\n",
      "Epoch 1/20  Iteration 263/7380 Training loss: 6.6806 0.9750 sec/batch\n",
      "Epoch 1/20  Iteration 264/7380 Training loss: 6.6791 0.9722 sec/batch\n",
      "Epoch 1/20  Iteration 265/7380 Training loss: 6.6777 0.9714 sec/batch\n",
      "Epoch 1/20  Iteration 266/7380 Training loss: 6.6760 0.9722 sec/batch\n",
      "Epoch 1/20  Iteration 267/7380 Training loss: 6.6749 0.9731 sec/batch\n",
      "Epoch 1/20  Iteration 268/7380 Training loss: 6.6736 0.9809 sec/batch\n",
      "Epoch 1/20  Iteration 269/7380 Training loss: 6.6719 0.9767 sec/batch\n",
      "Epoch 1/20  Iteration 270/7380 Training loss: 6.6707 0.9721 sec/batch\n",
      "Epoch 1/20  Iteration 271/7380 Training loss: 6.6695 0.9844 sec/batch\n",
      "Epoch 1/20  Iteration 272/7380 Training loss: 6.6678 0.9729 sec/batch\n",
      "Epoch 1/20  Iteration 273/7380 Training loss: 6.6669 0.9751 sec/batch\n",
      "Epoch 1/20  Iteration 274/7380 Training loss: 6.6657 0.9843 sec/batch\n",
      "Epoch 1/20  Iteration 275/7380 Training loss: 6.6643 0.9685 sec/batch\n",
      "Epoch 1/20  Iteration 276/7380 Training loss: 6.6629 0.9716 sec/batch\n",
      "Epoch 1/20  Iteration 277/7380 Training loss: 6.6620 0.9748 sec/batch\n",
      "Epoch 1/20  Iteration 278/7380 Training loss: 6.6608 0.9728 sec/batch\n",
      "Epoch 1/20  Iteration 279/7380 Training loss: 6.6597 0.9723 sec/batch\n",
      "Epoch 1/20  Iteration 280/7380 Training loss: 6.6583 0.9731 sec/batch\n",
      "Epoch 1/20  Iteration 281/7380 Training loss: 6.6572 0.9752 sec/batch\n",
      "Epoch 1/20  Iteration 282/7380 Training loss: 6.6561 0.9728 sec/batch\n",
      "Epoch 1/20  Iteration 283/7380 Training loss: 6.6550 0.9721 sec/batch\n",
      "Epoch 1/20  Iteration 284/7380 Training loss: 6.6539 0.9865 sec/batch\n",
      "Epoch 1/20  Iteration 285/7380 Training loss: 6.6530 0.9812 sec/batch\n",
      "Epoch 1/20  Iteration 286/7380 Training loss: 6.6517 0.9705 sec/batch\n",
      "Epoch 1/20  Iteration 287/7380 Training loss: 6.6503 0.9714 sec/batch\n",
      "Epoch 1/20  Iteration 288/7380 Training loss: 6.6491 0.9807 sec/batch\n",
      "Epoch 1/20  Iteration 289/7380 Training loss: 6.6476 0.9867 sec/batch\n",
      "Epoch 1/20  Iteration 290/7380 Training loss: 6.6462 0.9722 sec/batch\n",
      "Epoch 1/20  Iteration 291/7380 Training loss: 6.6447 0.9761 sec/batch\n",
      "Epoch 1/20  Iteration 292/7380 Training loss: 6.6436 0.9822 sec/batch\n",
      "Epoch 1/20  Iteration 293/7380 Training loss: 6.6424 0.9779 sec/batch\n",
      "Epoch 1/20  Iteration 294/7380 Training loss: 6.6413 0.9774 sec/batch\n",
      "Epoch 1/20  Iteration 295/7380 Training loss: 6.6403 0.9756 sec/batch\n",
      "Epoch 1/20  Iteration 296/7380 Training loss: 6.6392 0.9843 sec/batch\n",
      "Epoch 1/20  Iteration 297/7380 Training loss: 6.6379 0.9877 sec/batch\n",
      "Epoch 1/20  Iteration 298/7380 Training loss: 6.6368 0.9709 sec/batch\n",
      "Epoch 1/20  Iteration 299/7380 Training loss: 6.6358 0.9761 sec/batch\n",
      "Epoch 1/20  Iteration 300/7380 Training loss: 6.6351 0.9899 sec/batch\n",
      "Validation loss: 6.20875 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 301/7380 Training loss: 6.6341 0.9779 sec/batch\n",
      "Epoch 1/20  Iteration 302/7380 Training loss: 6.6330 0.9842 sec/batch\n",
      "Epoch 1/20  Iteration 303/7380 Training loss: 6.6319 0.9757 sec/batch\n",
      "Epoch 1/20  Iteration 304/7380 Training loss: 6.6313 0.9889 sec/batch\n",
      "Epoch 1/20  Iteration 305/7380 Training loss: 6.6305 0.9908 sec/batch\n",
      "Epoch 1/20  Iteration 306/7380 Training loss: 6.6295 0.9861 sec/batch\n",
      "Epoch 1/20  Iteration 307/7380 Training loss: 6.6282 0.9776 sec/batch\n",
      "Epoch 1/20  Iteration 308/7380 Training loss: 6.6268 0.9735 sec/batch\n",
      "Epoch 1/20  Iteration 309/7380 Training loss: 6.6256 0.9746 sec/batch\n",
      "Epoch 1/20  Iteration 310/7380 Training loss: 6.6241 0.9771 sec/batch\n",
      "Epoch 1/20  Iteration 311/7380 Training loss: 6.6231 0.9842 sec/batch\n",
      "Epoch 1/20  Iteration 312/7380 Training loss: 6.6218 0.9874 sec/batch\n",
      "Epoch 1/20  Iteration 313/7380 Training loss: 6.6206 0.9757 sec/batch\n",
      "Epoch 1/20  Iteration 314/7380 Training loss: 6.6197 0.9774 sec/batch\n",
      "Epoch 1/20  Iteration 315/7380 Training loss: 6.6187 0.9721 sec/batch\n",
      "Epoch 1/20  Iteration 316/7380 Training loss: 6.6177 1.0093 sec/batch\n",
      "Epoch 1/20  Iteration 317/7380 Training loss: 6.6169 0.9724 sec/batch\n",
      "Epoch 1/20  Iteration 318/7380 Training loss: 6.6161 0.9812 sec/batch\n",
      "Epoch 1/20  Iteration 319/7380 Training loss: 6.6156 0.9782 sec/batch\n",
      "Epoch 1/20  Iteration 320/7380 Training loss: 6.6148 0.9807 sec/batch\n",
      "Epoch 1/20  Iteration 321/7380 Training loss: 6.6138 0.9833 sec/batch\n",
      "Epoch 1/20  Iteration 322/7380 Training loss: 6.6127 0.9805 sec/batch\n",
      "Epoch 1/20  Iteration 323/7380 Training loss: 6.6117 0.9747 sec/batch\n",
      "Epoch 1/20  Iteration 324/7380 Training loss: 6.6107 0.9732 sec/batch\n",
      "Epoch 1/20  Iteration 325/7380 Training loss: 6.6098 0.9834 sec/batch\n",
      "Epoch 1/20  Iteration 326/7380 Training loss: 6.6091 0.9911 sec/batch\n",
      "Epoch 1/20  Iteration 327/7380 Training loss: 6.6079 0.9854 sec/batch\n",
      "Epoch 1/20  Iteration 328/7380 Training loss: 6.6067 0.9853 sec/batch\n",
      "Epoch 1/20  Iteration 329/7380 Training loss: 6.6056 0.9787 sec/batch\n",
      "Epoch 1/20  Iteration 330/7380 Training loss: 6.6044 0.9789 sec/batch\n",
      "Epoch 1/20  Iteration 331/7380 Training loss: 6.6032 0.9876 sec/batch\n",
      "Epoch 1/20  Iteration 332/7380 Training loss: 6.6022 0.9772 sec/batch\n",
      "Epoch 1/20  Iteration 333/7380 Training loss: 6.6008 0.9900 sec/batch\n",
      "Epoch 1/20  Iteration 334/7380 Training loss: 6.6000 0.9806 sec/batch\n",
      "Epoch 1/20  Iteration 335/7380 Training loss: 6.5990 0.9751 sec/batch\n",
      "Epoch 1/20  Iteration 336/7380 Training loss: 6.5981 0.9908 sec/batch\n",
      "Epoch 1/20  Iteration 337/7380 Training loss: 6.5974 0.9740 sec/batch\n",
      "Epoch 1/20  Iteration 338/7380 Training loss: 6.5963 0.9772 sec/batch\n",
      "Epoch 1/20  Iteration 339/7380 Training loss: 6.5954 0.9848 sec/batch\n",
      "Epoch 1/20  Iteration 340/7380 Training loss: 6.5941 0.9846 sec/batch\n",
      "Epoch 1/20  Iteration 341/7380 Training loss: 6.5929 0.9812 sec/batch\n",
      "Epoch 1/20  Iteration 342/7380 Training loss: 6.5916 0.9792 sec/batch\n",
      "Epoch 1/20  Iteration 343/7380 Training loss: 6.5907 0.9823 sec/batch\n",
      "Epoch 1/20  Iteration 344/7380 Training loss: 6.5898 0.9890 sec/batch\n",
      "Epoch 1/20  Iteration 345/7380 Training loss: 6.5888 0.9782 sec/batch\n",
      "Epoch 1/20  Iteration 346/7380 Training loss: 6.5877 0.9817 sec/batch\n",
      "Epoch 1/20  Iteration 347/7380 Training loss: 6.5869 0.9792 sec/batch\n",
      "Epoch 1/20  Iteration 348/7380 Training loss: 6.5860 0.9752 sec/batch\n",
      "Epoch 1/20  Iteration 349/7380 Training loss: 6.5851 0.9812 sec/batch\n",
      "Epoch 1/20  Iteration 350/7380 Training loss: 6.5841 0.9805 sec/batch\n",
      "Validation loss: 6.16075 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 351/7380 Training loss: 6.5836 1.0022 sec/batch\n",
      "Epoch 1/20  Iteration 352/7380 Training loss: 6.5823 0.9852 sec/batch\n",
      "Epoch 1/20  Iteration 353/7380 Training loss: 6.5814 0.9836 sec/batch\n",
      "Epoch 1/20  Iteration 354/7380 Training loss: 6.5803 0.9755 sec/batch\n",
      "Epoch 1/20  Iteration 355/7380 Training loss: 6.5793 0.9790 sec/batch\n",
      "Epoch 1/20  Iteration 356/7380 Training loss: 6.5784 0.9802 sec/batch\n",
      "Epoch 1/20  Iteration 357/7380 Training loss: 6.5777 0.9784 sec/batch\n",
      "Epoch 1/20  Iteration 358/7380 Training loss: 6.5767 0.9907 sec/batch\n",
      "Epoch 1/20  Iteration 359/7380 Training loss: 6.5755 0.9800 sec/batch\n",
      "Epoch 1/20  Iteration 360/7380 Training loss: 6.5745 0.9810 sec/batch\n",
      "Epoch 1/20  Iteration 361/7380 Training loss: 6.5735 0.9792 sec/batch\n",
      "Epoch 1/20  Iteration 362/7380 Training loss: 6.5726 0.9834 sec/batch\n",
      "Epoch 1/20  Iteration 363/7380 Training loss: 6.5716 0.9848 sec/batch\n",
      "Epoch 1/20  Iteration 364/7380 Training loss: 6.5705 0.9839 sec/batch\n",
      "Epoch 1/20  Iteration 365/7380 Training loss: 6.5695 0.9775 sec/batch\n",
      "Epoch 1/20  Iteration 366/7380 Training loss: 6.5684 0.9838 sec/batch\n",
      "Epoch 1/20  Iteration 367/7380 Training loss: 6.5674 0.9860 sec/batch\n",
      "Epoch 1/20  Iteration 368/7380 Training loss: 6.5666 0.9913 sec/batch\n",
      "Epoch 1/20  Iteration 369/7380 Training loss: 6.5656 0.9840 sec/batch\n",
      "Epoch 2/20  Iteration 370/7380 Training loss: 6.9677 0.9201 sec/batch\n",
      "Epoch 2/20  Iteration 371/7380 Training loss: 6.5982 0.9923 sec/batch\n",
      "Epoch 2/20  Iteration 372/7380 Training loss: 6.4731 0.9836 sec/batch\n",
      "Epoch 2/20  Iteration 373/7380 Training loss: 6.4058 0.9876 sec/batch\n",
      "Epoch 2/20  Iteration 374/7380 Training loss: 6.3672 0.9962 sec/batch\n",
      "Epoch 2/20  Iteration 375/7380 Training loss: 6.3514 0.9855 sec/batch\n",
      "Epoch 2/20  Iteration 376/7380 Training loss: 6.3424 0.9907 sec/batch\n",
      "Epoch 2/20  Iteration 377/7380 Training loss: 6.3202 0.9810 sec/batch\n",
      "Epoch 2/20  Iteration 378/7380 Training loss: 6.3009 0.9829 sec/batch\n",
      "Epoch 2/20  Iteration 379/7380 Training loss: 6.2992 0.9960 sec/batch\n",
      "Epoch 2/20  Iteration 380/7380 Training loss: 6.2911 0.9899 sec/batch\n",
      "Epoch 2/20  Iteration 381/7380 Training loss: 6.2751 0.9935 sec/batch\n",
      "Epoch 2/20  Iteration 382/7380 Training loss: 6.2697 0.9932 sec/batch\n",
      "Epoch 2/20  Iteration 383/7380 Training loss: 6.2678 0.9903 sec/batch\n",
      "Epoch 2/20  Iteration 384/7380 Training loss: 6.2652 0.9936 sec/batch\n",
      "Epoch 2/20  Iteration 385/7380 Training loss: 6.2561 0.9834 sec/batch\n",
      "Epoch 2/20  Iteration 386/7380 Training loss: 6.2507 0.9876 sec/batch\n",
      "Epoch 2/20  Iteration 387/7380 Training loss: 6.2368 0.9878 sec/batch\n",
      "Epoch 2/20  Iteration 388/7380 Training loss: 6.2256 1.0077 sec/batch\n",
      "Epoch 2/20  Iteration 389/7380 Training loss: 6.2210 0.9943 sec/batch\n",
      "Epoch 2/20  Iteration 390/7380 Training loss: 6.2162 0.9948 sec/batch\n",
      "Epoch 2/20  Iteration 391/7380 Training loss: 6.2187 0.9813 sec/batch\n",
      "Epoch 2/20  Iteration 392/7380 Training loss: 6.2174 0.9836 sec/batch\n",
      "Epoch 2/20  Iteration 393/7380 Training loss: 6.2225 0.9866 sec/batch\n",
      "Epoch 2/20  Iteration 394/7380 Training loss: 6.2231 0.9800 sec/batch\n",
      "Epoch 2/20  Iteration 395/7380 Training loss: 6.2226 0.9824 sec/batch\n",
      "Epoch 2/20  Iteration 396/7380 Training loss: 6.2164 0.9929 sec/batch\n",
      "Epoch 2/20  Iteration 397/7380 Training loss: 6.2183 0.9872 sec/batch\n",
      "Epoch 2/20  Iteration 398/7380 Training loss: 6.2160 1.0011 sec/batch\n",
      "Epoch 2/20  Iteration 399/7380 Training loss: 6.2141 0.9876 sec/batch\n",
      "Epoch 2/20  Iteration 400/7380 Training loss: 6.2120 0.9857 sec/batch\n",
      "Validation loss: 6.08912 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 401/7380 Training loss: 6.2116 0.9932 sec/batch\n",
      "Epoch 2/20  Iteration 402/7380 Training loss: 6.2100 0.9892 sec/batch\n",
      "Epoch 2/20  Iteration 403/7380 Training loss: 6.2086 0.9882 sec/batch\n",
      "Epoch 2/20  Iteration 404/7380 Training loss: 6.2110 0.9979 sec/batch\n",
      "Epoch 2/20  Iteration 405/7380 Training loss: 6.2083 0.9966 sec/batch\n",
      "Epoch 2/20  Iteration 406/7380 Training loss: 6.2070 0.9980 sec/batch\n",
      "Epoch 2/20  Iteration 407/7380 Training loss: 6.2059 1.0045 sec/batch\n",
      "Epoch 2/20  Iteration 408/7380 Training loss: 6.2052 0.9955 sec/batch\n",
      "Epoch 2/20  Iteration 409/7380 Training loss: 6.2030 1.0018 sec/batch\n",
      "Epoch 2/20  Iteration 410/7380 Training loss: 6.2009 1.0009 sec/batch\n",
      "Epoch 2/20  Iteration 411/7380 Training loss: 6.1991 0.9970 sec/batch\n",
      "Epoch 2/20  Iteration 412/7380 Training loss: 6.1981 0.9943 sec/batch\n",
      "Epoch 2/20  Iteration 413/7380 Training loss: 6.1984 0.9968 sec/batch\n",
      "Epoch 2/20  Iteration 414/7380 Training loss: 6.1984 0.9984 sec/batch\n",
      "Epoch 2/20  Iteration 415/7380 Training loss: 6.1968 1.0040 sec/batch\n",
      "Epoch 2/20  Iteration 416/7380 Training loss: 6.1963 1.0063 sec/batch\n",
      "Epoch 2/20  Iteration 417/7380 Training loss: 6.1936 1.0012 sec/batch\n",
      "Epoch 2/20  Iteration 418/7380 Training loss: 6.1939 1.0054 sec/batch\n",
      "Epoch 2/20  Iteration 419/7380 Training loss: 6.1914 1.0117 sec/batch\n",
      "Epoch 2/20  Iteration 420/7380 Training loss: 6.1898 1.0034 sec/batch\n",
      "Epoch 2/20  Iteration 421/7380 Training loss: 6.1890 1.0028 sec/batch\n",
      "Epoch 2/20  Iteration 422/7380 Training loss: 6.1872 1.0033 sec/batch\n",
      "Epoch 2/20  Iteration 423/7380 Training loss: 6.1861 0.9985 sec/batch\n",
      "Epoch 2/20  Iteration 424/7380 Training loss: 6.1846 0.9964 sec/batch\n",
      "Epoch 2/20  Iteration 425/7380 Training loss: 6.1828 1.0055 sec/batch\n",
      "Epoch 2/20  Iteration 426/7380 Training loss: 6.1816 1.0033 sec/batch\n",
      "Epoch 2/20  Iteration 427/7380 Training loss: 6.1813 1.0112 sec/batch\n",
      "Epoch 2/20  Iteration 428/7380 Training loss: 6.1813 1.0005 sec/batch\n",
      "Epoch 2/20  Iteration 429/7380 Training loss: 6.1804 0.9932 sec/batch\n",
      "Epoch 2/20  Iteration 430/7380 Training loss: 6.1784 0.9914 sec/batch\n",
      "Epoch 2/20  Iteration 431/7380 Training loss: 6.1781 0.9974 sec/batch\n",
      "Epoch 2/20  Iteration 432/7380 Training loss: 6.1771 0.9948 sec/batch\n",
      "Epoch 2/20  Iteration 433/7380 Training loss: 6.1748 1.0060 sec/batch\n",
      "Epoch 2/20  Iteration 434/7380 Training loss: 6.1742 1.0012 sec/batch\n",
      "Epoch 2/20  Iteration 435/7380 Training loss: 6.1740 1.0069 sec/batch\n",
      "Epoch 2/20  Iteration 436/7380 Training loss: 6.1755 1.0079 sec/batch\n",
      "Epoch 2/20  Iteration 437/7380 Training loss: 6.1759 1.0059 sec/batch\n",
      "Epoch 2/20  Iteration 438/7380 Training loss: 6.1764 0.9963 sec/batch\n",
      "Epoch 2/20  Iteration 439/7380 Training loss: 6.1753 0.9932 sec/batch\n",
      "Epoch 2/20  Iteration 440/7380 Training loss: 6.1738 0.9975 sec/batch\n",
      "Epoch 2/20  Iteration 441/7380 Training loss: 6.1752 1.0052 sec/batch\n",
      "Epoch 2/20  Iteration 442/7380 Training loss: 6.1732 1.0095 sec/batch\n",
      "Epoch 2/20  Iteration 443/7380 Training loss: 6.1746 1.0020 sec/batch\n",
      "Epoch 2/20  Iteration 444/7380 Training loss: 6.1749 0.9982 sec/batch\n",
      "Epoch 2/20  Iteration 445/7380 Training loss: 6.1734 0.9992 sec/batch\n",
      "Epoch 2/20  Iteration 446/7380 Training loss: 6.1731 1.0024 sec/batch\n",
      "Epoch 2/20  Iteration 447/7380 Training loss: 6.1734 0.9997 sec/batch\n",
      "Epoch 2/20  Iteration 448/7380 Training loss: 6.1742 1.0045 sec/batch\n",
      "Epoch 2/20  Iteration 449/7380 Training loss: 6.1737 0.9933 sec/batch\n",
      "Epoch 2/20  Iteration 450/7380 Training loss: 6.1734 0.9948 sec/batch\n",
      "Validation loss: 6.05828 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 451/7380 Training loss: 6.1721 1.0149 sec/batch\n",
      "Epoch 2/20  Iteration 452/7380 Training loss: 6.1710 1.0040 sec/batch\n",
      "Epoch 2/20  Iteration 453/7380 Training loss: 6.1700 1.0084 sec/batch\n",
      "Epoch 2/20  Iteration 454/7380 Training loss: 6.1703 1.0028 sec/batch\n",
      "Epoch 2/20  Iteration 455/7380 Training loss: 6.1693 1.0019 sec/batch\n",
      "Epoch 2/20  Iteration 456/7380 Training loss: 6.1683 0.9976 sec/batch\n",
      "Epoch 2/20  Iteration 457/7380 Training loss: 6.1678 0.9964 sec/batch\n",
      "Epoch 2/20  Iteration 458/7380 Training loss: 6.1651 1.0019 sec/batch\n",
      "Epoch 2/20  Iteration 459/7380 Training loss: 6.1643 1.0075 sec/batch\n",
      "Epoch 2/20  Iteration 460/7380 Training loss: 6.1638 1.0065 sec/batch\n",
      "Epoch 2/20  Iteration 461/7380 Training loss: 6.1652 0.9993 sec/batch\n",
      "Epoch 2/20  Iteration 462/7380 Training loss: 6.1647 1.0007 sec/batch\n",
      "Epoch 2/20  Iteration 463/7380 Training loss: 6.1642 1.0067 sec/batch\n",
      "Epoch 2/20  Iteration 464/7380 Training loss: 6.1632 1.0043 sec/batch\n",
      "Epoch 2/20  Iteration 465/7380 Training loss: 6.1634 1.0102 sec/batch\n",
      "Epoch 2/20  Iteration 466/7380 Training loss: 6.1629 0.9993 sec/batch\n",
      "Epoch 2/20  Iteration 467/7380 Training loss: 6.1640 1.0117 sec/batch\n",
      "Epoch 2/20  Iteration 468/7380 Training loss: 6.1634 1.0006 sec/batch\n",
      "Epoch 2/20  Iteration 469/7380 Training loss: 6.1633 0.9987 sec/batch\n",
      "Epoch 2/20  Iteration 470/7380 Training loss: 6.1639 1.0012 sec/batch\n",
      "Epoch 2/20  Iteration 471/7380 Training loss: 6.1636 1.0062 sec/batch\n",
      "Epoch 2/20  Iteration 472/7380 Training loss: 6.1635 1.0022 sec/batch\n",
      "Epoch 2/20  Iteration 473/7380 Training loss: 6.1632 0.9977 sec/batch\n",
      "Epoch 2/20  Iteration 474/7380 Training loss: 6.1633 1.0018 sec/batch\n",
      "Epoch 2/20  Iteration 475/7380 Training loss: 6.1630 0.9989 sec/batch\n",
      "Epoch 2/20  Iteration 476/7380 Training loss: 6.1630 1.0097 sec/batch\n",
      "Epoch 2/20  Iteration 477/7380 Training loss: 6.1627 1.0108 sec/batch\n",
      "Epoch 2/20  Iteration 478/7380 Training loss: 6.1616 1.0160 sec/batch\n",
      "Epoch 2/20  Iteration 479/7380 Training loss: 6.1613 1.0016 sec/batch\n",
      "Epoch 2/20  Iteration 480/7380 Training loss: 6.1603 1.0113 sec/batch\n",
      "Epoch 2/20  Iteration 481/7380 Training loss: 6.1601 1.0037 sec/batch\n",
      "Epoch 2/20  Iteration 482/7380 Training loss: 6.1588 1.0115 sec/batch\n",
      "Epoch 2/20  Iteration 483/7380 Training loss: 6.1587 0.9990 sec/batch\n",
      "Epoch 2/20  Iteration 484/7380 Training loss: 6.1577 1.0027 sec/batch\n",
      "Epoch 2/20  Iteration 485/7380 Training loss: 6.1571 1.0096 sec/batch\n",
      "Epoch 2/20  Iteration 486/7380 Training loss: 6.1559 1.0128 sec/batch\n",
      "Epoch 2/20  Iteration 487/7380 Training loss: 6.1551 1.0165 sec/batch\n",
      "Epoch 2/20  Iteration 488/7380 Training loss: 6.1549 1.0117 sec/batch\n",
      "Epoch 2/20  Iteration 489/7380 Training loss: 6.1548 1.0026 sec/batch\n",
      "Epoch 2/20  Iteration 490/7380 Training loss: 6.1549 1.0046 sec/batch\n",
      "Epoch 2/20  Iteration 491/7380 Training loss: 6.1560 1.0081 sec/batch\n",
      "Epoch 2/20  Iteration 492/7380 Training loss: 6.1551 1.0109 sec/batch\n",
      "Epoch 2/20  Iteration 493/7380 Training loss: 6.1549 0.9975 sec/batch\n",
      "Epoch 2/20  Iteration 494/7380 Training loss: 6.1542 0.9995 sec/batch\n",
      "Epoch 2/20  Iteration 495/7380 Training loss: 6.1531 1.0137 sec/batch\n",
      "Epoch 2/20  Iteration 496/7380 Training loss: 6.1519 1.0049 sec/batch\n",
      "Epoch 2/20  Iteration 497/7380 Training loss: 6.1521 1.0048 sec/batch\n",
      "Epoch 2/20  Iteration 498/7380 Training loss: 6.1510 1.0083 sec/batch\n",
      "Epoch 2/20  Iteration 499/7380 Training loss: 6.1505 1.0015 sec/batch\n",
      "Epoch 2/20  Iteration 500/7380 Training loss: 6.1502 1.0043 sec/batch\n",
      "Validation loss: 6.01601 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 501/7380 Training loss: 6.1501 1.0097 sec/batch\n",
      "Epoch 2/20  Iteration 502/7380 Training loss: 6.1499 1.0024 sec/batch\n",
      "Epoch 2/20  Iteration 503/7380 Training loss: 6.1490 1.0124 sec/batch\n",
      "Epoch 2/20  Iteration 504/7380 Training loss: 6.1484 1.0133 sec/batch\n",
      "Epoch 2/20  Iteration 505/7380 Training loss: 6.1480 1.0179 sec/batch\n",
      "Epoch 2/20  Iteration 506/7380 Training loss: 6.1474 1.0032 sec/batch\n",
      "Epoch 2/20  Iteration 507/7380 Training loss: 6.1463 1.0087 sec/batch\n",
      "Epoch 2/20  Iteration 508/7380 Training loss: 6.1451 1.0087 sec/batch\n",
      "Epoch 2/20  Iteration 509/7380 Training loss: 6.1437 1.0041 sec/batch\n",
      "Epoch 2/20  Iteration 510/7380 Training loss: 6.1448 1.0073 sec/batch\n",
      "Epoch 2/20  Iteration 511/7380 Training loss: 6.1440 1.0167 sec/batch\n",
      "Epoch 2/20  Iteration 512/7380 Training loss: 6.1438 1.0196 sec/batch\n",
      "Epoch 2/20  Iteration 513/7380 Training loss: 6.1437 1.0029 sec/batch\n",
      "Epoch 2/20  Iteration 514/7380 Training loss: 6.1434 1.0014 sec/batch\n",
      "Epoch 2/20  Iteration 515/7380 Training loss: 6.1430 1.0048 sec/batch\n",
      "Epoch 2/20  Iteration 516/7380 Training loss: 6.1424 1.0054 sec/batch\n",
      "Epoch 2/20  Iteration 517/7380 Training loss: 6.1420 1.0042 sec/batch\n",
      "Epoch 2/20  Iteration 518/7380 Training loss: 6.1412 1.0081 sec/batch\n",
      "Epoch 2/20  Iteration 519/7380 Training loss: 6.1409 1.0066 sec/batch\n",
      "Epoch 2/20  Iteration 520/7380 Training loss: 6.1401 0.9995 sec/batch\n",
      "Epoch 2/20  Iteration 521/7380 Training loss: 6.1402 1.0032 sec/batch\n",
      "Epoch 2/20  Iteration 522/7380 Training loss: 6.1400 1.0180 sec/batch\n",
      "Epoch 2/20  Iteration 523/7380 Training loss: 6.1403 1.0046 sec/batch\n",
      "Epoch 2/20  Iteration 524/7380 Training loss: 6.1400 1.0126 sec/batch\n",
      "Epoch 2/20  Iteration 525/7380 Training loss: 6.1392 1.0109 sec/batch\n",
      "Epoch 2/20  Iteration 526/7380 Training loss: 6.1382 1.0056 sec/batch\n",
      "Epoch 2/20  Iteration 527/7380 Training loss: 6.1382 1.0072 sec/batch\n",
      "Epoch 2/20  Iteration 528/7380 Training loss: 6.1374 1.0040 sec/batch\n",
      "Epoch 2/20  Iteration 529/7380 Training loss: 6.1372 1.0153 sec/batch\n",
      "Epoch 2/20  Iteration 530/7380 Training loss: 6.1371 1.0137 sec/batch\n",
      "Epoch 2/20  Iteration 531/7380 Training loss: 6.1373 1.0125 sec/batch\n",
      "Epoch 2/20  Iteration 532/7380 Training loss: 6.1373 1.0017 sec/batch\n",
      "Epoch 2/20  Iteration 533/7380 Training loss: 6.1375 1.0029 sec/batch\n",
      "Epoch 2/20  Iteration 534/7380 Training loss: 6.1374 1.0114 sec/batch\n",
      "Epoch 2/20  Iteration 535/7380 Training loss: 6.1369 1.0082 sec/batch\n",
      "Epoch 2/20  Iteration 536/7380 Training loss: 6.1367 1.0178 sec/batch\n",
      "Epoch 2/20  Iteration 537/7380 Training loss: 6.1368 1.0102 sec/batch\n",
      "Epoch 2/20  Iteration 538/7380 Training loss: 6.1358 1.0135 sec/batch\n",
      "Epoch 2/20  Iteration 539/7380 Training loss: 6.1350 1.0058 sec/batch\n",
      "Epoch 2/20  Iteration 540/7380 Training loss: 6.1350 1.0157 sec/batch\n",
      "Epoch 2/20  Iteration 541/7380 Training loss: 6.1350 1.0094 sec/batch\n",
      "Epoch 2/20  Iteration 542/7380 Training loss: 6.1346 1.0061 sec/batch\n",
      "Epoch 2/20  Iteration 543/7380 Training loss: 6.1339 1.0058 sec/batch\n",
      "Epoch 2/20  Iteration 544/7380 Training loss: 6.1334 1.0153 sec/batch\n",
      "Epoch 2/20  Iteration 545/7380 Training loss: 6.1330 1.0066 sec/batch\n",
      "Epoch 2/20  Iteration 546/7380 Training loss: 6.1325 1.0058 sec/batch\n",
      "Epoch 2/20  Iteration 547/7380 Training loss: 6.1322 1.0088 sec/batch\n",
      "Epoch 2/20  Iteration 548/7380 Training loss: 6.1314 1.0089 sec/batch\n",
      "Epoch 2/20  Iteration 549/7380 Training loss: 6.1306 1.0080 sec/batch\n",
      "Epoch 2/20  Iteration 550/7380 Training loss: 6.1300 1.0043 sec/batch\n",
      "Validation loss: 5.97199 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 551/7380 Training loss: 6.1299 1.0233 sec/batch\n",
      "Epoch 2/20  Iteration 552/7380 Training loss: 6.1299 1.0139 sec/batch\n",
      "Epoch 2/20  Iteration 553/7380 Training loss: 6.1293 1.0088 sec/batch\n",
      "Epoch 2/20  Iteration 554/7380 Training loss: 6.1290 1.0154 sec/batch\n",
      "Epoch 2/20  Iteration 555/7380 Training loss: 6.1284 1.0071 sec/batch\n",
      "Epoch 2/20  Iteration 556/7380 Training loss: 6.1279 1.0107 sec/batch\n",
      "Epoch 2/20  Iteration 557/7380 Training loss: 6.1273 1.0198 sec/batch\n",
      "Epoch 2/20  Iteration 558/7380 Training loss: 6.1264 1.0174 sec/batch\n",
      "Epoch 2/20  Iteration 559/7380 Training loss: 6.1265 1.0169 sec/batch\n",
      "Epoch 2/20  Iteration 560/7380 Training loss: 6.1255 1.0205 sec/batch\n",
      "Epoch 2/20  Iteration 561/7380 Training loss: 6.1260 1.0105 sec/batch\n",
      "Epoch 2/20  Iteration 562/7380 Training loss: 6.1260 1.0094 sec/batch\n",
      "Epoch 2/20  Iteration 563/7380 Training loss: 6.1252 1.0174 sec/batch\n",
      "Epoch 2/20  Iteration 564/7380 Training loss: 6.1253 1.0143 sec/batch\n",
      "Epoch 2/20  Iteration 565/7380 Training loss: 6.1249 1.0084 sec/batch\n",
      "Epoch 2/20  Iteration 566/7380 Training loss: 6.1245 1.0126 sec/batch\n",
      "Epoch 2/20  Iteration 567/7380 Training loss: 6.1248 1.0099 sec/batch\n",
      "Epoch 2/20  Iteration 568/7380 Training loss: 6.1245 1.0091 sec/batch\n",
      "Epoch 2/20  Iteration 569/7380 Training loss: 6.1243 1.0058 sec/batch\n",
      "Epoch 2/20  Iteration 570/7380 Training loss: 6.1244 1.0156 sec/batch\n",
      "Epoch 2/20  Iteration 571/7380 Training loss: 6.1241 1.0080 sec/batch\n",
      "Epoch 2/20  Iteration 572/7380 Training loss: 6.1242 1.0209 sec/batch\n",
      "Epoch 2/20  Iteration 573/7380 Training loss: 6.1239 1.0189 sec/batch\n",
      "Epoch 2/20  Iteration 574/7380 Training loss: 6.1240 1.0108 sec/batch\n",
      "Epoch 2/20  Iteration 575/7380 Training loss: 6.1239 1.0073 sec/batch\n",
      "Epoch 2/20  Iteration 576/7380 Training loss: 6.1228 1.0133 sec/batch\n",
      "Epoch 2/20  Iteration 577/7380 Training loss: 6.1223 1.0097 sec/batch\n",
      "Epoch 2/20  Iteration 578/7380 Training loss: 6.1216 1.0138 sec/batch\n",
      "Epoch 2/20  Iteration 579/7380 Training loss: 6.1209 1.0100 sec/batch\n",
      "Epoch 2/20  Iteration 580/7380 Training loss: 6.1203 1.0218 sec/batch\n",
      "Epoch 2/20  Iteration 581/7380 Training loss: 6.1201 1.0095 sec/batch\n",
      "Epoch 2/20  Iteration 582/7380 Training loss: 6.1198 1.0158 sec/batch\n",
      "Epoch 2/20  Iteration 583/7380 Training loss: 6.1193 1.0080 sec/batch\n",
      "Epoch 2/20  Iteration 584/7380 Training loss: 6.1186 1.0188 sec/batch\n",
      "Epoch 2/20  Iteration 585/7380 Training loss: 6.1177 1.0091 sec/batch\n",
      "Epoch 2/20  Iteration 586/7380 Training loss: 6.1170 1.0114 sec/batch\n",
      "Epoch 2/20  Iteration 587/7380 Training loss: 6.1167 1.0183 sec/batch\n",
      "Epoch 2/20  Iteration 588/7380 Training loss: 6.1166 1.0144 sec/batch\n",
      "Epoch 2/20  Iteration 589/7380 Training loss: 6.1164 1.0134 sec/batch\n",
      "Epoch 2/20  Iteration 590/7380 Training loss: 6.1158 1.0191 sec/batch\n",
      "Epoch 2/20  Iteration 591/7380 Training loss: 6.1159 1.0205 sec/batch\n",
      "Epoch 2/20  Iteration 592/7380 Training loss: 6.1156 1.0091 sec/batch\n",
      "Epoch 2/20  Iteration 593/7380 Training loss: 6.1149 1.0167 sec/batch\n",
      "Epoch 2/20  Iteration 594/7380 Training loss: 6.1146 1.0134 sec/batch\n",
      "Epoch 2/20  Iteration 595/7380 Training loss: 6.1141 1.0207 sec/batch\n",
      "Epoch 2/20  Iteration 596/7380 Training loss: 6.1137 1.0124 sec/batch\n",
      "Epoch 2/20  Iteration 597/7380 Training loss: 6.1129 1.0184 sec/batch\n",
      "Epoch 2/20  Iteration 598/7380 Training loss: 6.1126 1.0139 sec/batch\n",
      "Epoch 2/20  Iteration 599/7380 Training loss: 6.1126 1.0159 sec/batch\n",
      "Epoch 2/20  Iteration 600/7380 Training loss: 6.1118 1.0196 sec/batch\n",
      "Validation loss: 5.93202 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 601/7380 Training loss: 6.1112 1.0211 sec/batch\n",
      "Epoch 2/20  Iteration 602/7380 Training loss: 6.1104 1.0149 sec/batch\n",
      "Epoch 2/20  Iteration 603/7380 Training loss: 6.1096 1.0198 sec/batch\n",
      "Epoch 2/20  Iteration 604/7380 Training loss: 6.1092 1.0148 sec/batch\n",
      "Epoch 2/20  Iteration 605/7380 Training loss: 6.1084 1.0097 sec/batch\n",
      "Epoch 2/20  Iteration 606/7380 Training loss: 6.1077 1.0091 sec/batch\n",
      "Epoch 2/20  Iteration 607/7380 Training loss: 6.1068 1.0150 sec/batch\n",
      "Epoch 2/20  Iteration 608/7380 Training loss: 6.1055 1.0230 sec/batch\n",
      "Epoch 2/20  Iteration 609/7380 Training loss: 6.1046 1.0168 sec/batch\n",
      "Epoch 2/20  Iteration 610/7380 Training loss: 6.1044 1.0162 sec/batch\n",
      "Epoch 2/20  Iteration 611/7380 Training loss: 6.1046 1.0180 sec/batch\n",
      "Epoch 2/20  Iteration 612/7380 Training loss: 6.1039 1.0074 sec/batch\n",
      "Epoch 2/20  Iteration 613/7380 Training loss: 6.1036 1.0186 sec/batch\n",
      "Epoch 2/20  Iteration 614/7380 Training loss: 6.1035 1.0217 sec/batch\n",
      "Epoch 2/20  Iteration 615/7380 Training loss: 6.1035 1.0126 sec/batch\n",
      "Epoch 2/20  Iteration 616/7380 Training loss: 6.1031 1.0225 sec/batch\n",
      "Epoch 2/20  Iteration 617/7380 Training loss: 6.1028 1.0126 sec/batch\n",
      "Epoch 2/20  Iteration 618/7380 Training loss: 6.1023 1.0163 sec/batch\n",
      "Epoch 2/20  Iteration 619/7380 Training loss: 6.1021 1.0222 sec/batch\n",
      "Epoch 2/20  Iteration 620/7380 Training loss: 6.1013 1.0130 sec/batch\n",
      "Epoch 2/20  Iteration 621/7380 Training loss: 6.1009 1.0078 sec/batch\n",
      "Epoch 2/20  Iteration 622/7380 Training loss: 6.1006 1.0116 sec/batch\n",
      "Epoch 2/20  Iteration 623/7380 Training loss: 6.0998 1.0194 sec/batch\n",
      "Epoch 2/20  Iteration 624/7380 Training loss: 6.0998 1.0192 sec/batch\n",
      "Epoch 2/20  Iteration 625/7380 Training loss: 6.0994 1.0180 sec/batch\n",
      "Epoch 2/20  Iteration 626/7380 Training loss: 6.0992 1.0116 sec/batch\n",
      "Epoch 2/20  Iteration 627/7380 Training loss: 6.0985 1.0102 sec/batch\n",
      "Epoch 2/20  Iteration 628/7380 Training loss: 6.0986 1.0164 sec/batch\n",
      "Epoch 2/20  Iteration 629/7380 Training loss: 6.0980 1.0130 sec/batch\n",
      "Epoch 2/20  Iteration 630/7380 Training loss: 6.0978 1.0160 sec/batch\n",
      "Epoch 2/20  Iteration 631/7380 Training loss: 6.0975 1.0116 sec/batch\n",
      "Epoch 2/20  Iteration 632/7380 Training loss: 6.0975 1.0126 sec/batch\n",
      "Epoch 2/20  Iteration 633/7380 Training loss: 6.0969 1.0192 sec/batch\n",
      "Epoch 2/20  Iteration 634/7380 Training loss: 6.0965 1.0131 sec/batch\n",
      "Epoch 2/20  Iteration 635/7380 Training loss: 6.0958 1.0112 sec/batch\n",
      "Epoch 2/20  Iteration 636/7380 Training loss: 6.0957 1.0130 sec/batch\n",
      "Epoch 2/20  Iteration 637/7380 Training loss: 6.0953 1.0131 sec/batch\n",
      "Epoch 2/20  Iteration 638/7380 Training loss: 6.0946 1.0155 sec/batch\n",
      "Epoch 2/20  Iteration 639/7380 Training loss: 6.0945 1.0127 sec/batch\n",
      "Epoch 2/20  Iteration 640/7380 Training loss: 6.0942 1.0145 sec/batch\n",
      "Epoch 2/20  Iteration 641/7380 Training loss: 6.0933 1.0156 sec/batch\n",
      "Epoch 2/20  Iteration 642/7380 Training loss: 6.0934 1.0105 sec/batch\n",
      "Epoch 2/20  Iteration 643/7380 Training loss: 6.0932 1.0092 sec/batch\n",
      "Epoch 2/20  Iteration 644/7380 Training loss: 6.0928 1.0109 sec/batch\n",
      "Epoch 2/20  Iteration 645/7380 Training loss: 6.0924 1.0264 sec/batch\n",
      "Epoch 2/20  Iteration 646/7380 Training loss: 6.0925 1.0151 sec/batch\n",
      "Epoch 2/20  Iteration 647/7380 Training loss: 6.0921 1.0223 sec/batch\n",
      "Epoch 2/20  Iteration 648/7380 Training loss: 6.0918 1.0130 sec/batch\n",
      "Epoch 2/20  Iteration 649/7380 Training loss: 6.0913 1.0127 sec/batch\n",
      "Epoch 2/20  Iteration 650/7380 Training loss: 6.0910 1.0099 sec/batch\n",
      "Validation loss: 5.89575 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 651/7380 Training loss: 6.0908 1.0166 sec/batch\n",
      "Epoch 2/20  Iteration 652/7380 Training loss: 6.0907 1.0264 sec/batch\n",
      "Epoch 2/20  Iteration 653/7380 Training loss: 6.0905 1.0298 sec/batch\n",
      "Epoch 2/20  Iteration 654/7380 Training loss: 6.0905 1.0185 sec/batch\n",
      "Epoch 2/20  Iteration 655/7380 Training loss: 6.0899 1.0168 sec/batch\n",
      "Epoch 2/20  Iteration 656/7380 Training loss: 6.0894 1.0230 sec/batch\n",
      "Epoch 2/20  Iteration 657/7380 Training loss: 6.0890 1.0142 sec/batch\n",
      "Epoch 2/20  Iteration 658/7380 Training loss: 6.0883 1.0244 sec/batch\n",
      "Epoch 2/20  Iteration 659/7380 Training loss: 6.0878 1.0192 sec/batch\n",
      "Epoch 2/20  Iteration 660/7380 Training loss: 6.0871 1.0157 sec/batch\n",
      "Epoch 2/20  Iteration 661/7380 Training loss: 6.0867 1.0187 sec/batch\n",
      "Epoch 2/20  Iteration 662/7380 Training loss: 6.0864 1.0252 sec/batch\n",
      "Epoch 2/20  Iteration 663/7380 Training loss: 6.0861 1.0237 sec/batch\n",
      "Epoch 2/20  Iteration 664/7380 Training loss: 6.0860 1.0258 sec/batch\n",
      "Epoch 2/20  Iteration 665/7380 Training loss: 6.0858 1.0277 sec/batch\n",
      "Epoch 2/20  Iteration 666/7380 Training loss: 6.0852 1.0380 sec/batch\n",
      "Epoch 2/20  Iteration 667/7380 Training loss: 6.0849 1.0152 sec/batch\n",
      "Epoch 2/20  Iteration 668/7380 Training loss: 6.0848 1.0159 sec/batch\n",
      "Epoch 2/20  Iteration 669/7380 Training loss: 6.0849 1.0262 sec/batch\n",
      "Epoch 2/20  Iteration 670/7380 Training loss: 6.0847 1.0288 sec/batch\n",
      "Epoch 2/20  Iteration 671/7380 Training loss: 6.0845 1.0135 sec/batch\n",
      "Epoch 2/20  Iteration 672/7380 Training loss: 6.0842 1.0148 sec/batch\n",
      "Epoch 2/20  Iteration 673/7380 Training loss: 6.0844 1.0094 sec/batch\n",
      "Epoch 2/20  Iteration 674/7380 Training loss: 6.0844 1.0129 sec/batch\n",
      "Epoch 2/20  Iteration 675/7380 Training loss: 6.0840 1.0100 sec/batch\n",
      "Epoch 2/20  Iteration 676/7380 Training loss: 6.0834 1.0213 sec/batch\n",
      "Epoch 2/20  Iteration 677/7380 Training loss: 6.0828 1.0157 sec/batch\n",
      "Epoch 2/20  Iteration 678/7380 Training loss: 6.0824 1.0147 sec/batch\n",
      "Epoch 2/20  Iteration 679/7380 Training loss: 6.0816 1.0187 sec/batch\n",
      "Epoch 2/20  Iteration 680/7380 Training loss: 6.0813 1.0263 sec/batch\n",
      "Epoch 2/20  Iteration 681/7380 Training loss: 6.0807 1.0169 sec/batch\n",
      "Epoch 2/20  Iteration 682/7380 Training loss: 6.0801 1.0207 sec/batch\n",
      "Epoch 2/20  Iteration 683/7380 Training loss: 6.0798 1.0201 sec/batch\n",
      "Epoch 2/20  Iteration 684/7380 Training loss: 6.0797 1.0227 sec/batch\n",
      "Epoch 2/20  Iteration 685/7380 Training loss: 6.0796 1.0173 sec/batch\n",
      "Epoch 2/20  Iteration 686/7380 Training loss: 6.0795 1.0213 sec/batch\n",
      "Epoch 2/20  Iteration 687/7380 Training loss: 6.0794 1.0225 sec/batch\n",
      "Epoch 2/20  Iteration 688/7380 Training loss: 6.0796 1.0174 sec/batch\n",
      "Epoch 2/20  Iteration 689/7380 Training loss: 6.0795 1.0301 sec/batch\n",
      "Epoch 2/20  Iteration 690/7380 Training loss: 6.0791 1.0184 sec/batch\n",
      "Epoch 2/20  Iteration 691/7380 Training loss: 6.0788 1.0108 sec/batch\n",
      "Epoch 2/20  Iteration 692/7380 Training loss: 6.0785 1.0152 sec/batch\n",
      "Epoch 2/20  Iteration 693/7380 Training loss: 6.0781 1.0206 sec/batch\n",
      "Epoch 2/20  Iteration 694/7380 Training loss: 6.0779 1.0165 sec/batch\n",
      "Epoch 2/20  Iteration 695/7380 Training loss: 6.0780 1.0166 sec/batch\n",
      "Epoch 2/20  Iteration 696/7380 Training loss: 6.0775 1.0159 sec/batch\n",
      "Epoch 2/20  Iteration 697/7380 Training loss: 6.0767 1.0124 sec/batch\n",
      "Epoch 2/20  Iteration 698/7380 Training loss: 6.0762 1.0217 sec/batch\n",
      "Epoch 2/20  Iteration 699/7380 Training loss: 6.0758 1.0118 sec/batch\n",
      "Epoch 2/20  Iteration 700/7380 Training loss: 6.0752 1.0356 sec/batch\n",
      "Validation loss: 5.86097 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 701/7380 Training loss: 6.0750 1.0168 sec/batch\n",
      "Epoch 2/20  Iteration 702/7380 Training loss: 6.0743 1.0302 sec/batch\n",
      "Epoch 2/20  Iteration 703/7380 Training loss: 6.0741 1.0135 sec/batch\n",
      "Epoch 2/20  Iteration 704/7380 Training loss: 6.0737 1.0186 sec/batch\n",
      "Epoch 2/20  Iteration 705/7380 Training loss: 6.0735 1.0227 sec/batch\n",
      "Epoch 2/20  Iteration 706/7380 Training loss: 6.0735 1.0211 sec/batch\n",
      "Epoch 2/20  Iteration 707/7380 Training loss: 6.0729 1.0151 sec/batch\n",
      "Epoch 2/20  Iteration 708/7380 Training loss: 6.0724 1.0180 sec/batch\n",
      "Epoch 2/20  Iteration 709/7380 Training loss: 6.0717 1.0173 sec/batch\n",
      "Epoch 2/20  Iteration 710/7380 Training loss: 6.0711 1.0261 sec/batch\n",
      "Epoch 2/20  Iteration 711/7380 Training loss: 6.0705 1.0163 sec/batch\n",
      "Epoch 2/20  Iteration 712/7380 Training loss: 6.0702 1.0335 sec/batch\n",
      "Epoch 2/20  Iteration 713/7380 Training loss: 6.0698 1.0208 sec/batch\n",
      "Epoch 2/20  Iteration 714/7380 Training loss: 6.0694 1.0218 sec/batch\n",
      "Epoch 2/20  Iteration 715/7380 Training loss: 6.0690 1.0188 sec/batch\n",
      "Epoch 2/20  Iteration 716/7380 Training loss: 6.0688 1.0156 sec/batch\n",
      "Epoch 2/20  Iteration 717/7380 Training loss: 6.0686 1.0133 sec/batch\n",
      "Epoch 2/20  Iteration 718/7380 Training loss: 6.0682 1.0157 sec/batch\n",
      "Epoch 2/20  Iteration 719/7380 Training loss: 6.0678 1.0276 sec/batch\n",
      "Epoch 2/20  Iteration 720/7380 Training loss: 6.0680 1.0234 sec/batch\n",
      "Epoch 2/20  Iteration 721/7380 Training loss: 6.0674 1.0201 sec/batch\n",
      "Epoch 2/20  Iteration 722/7380 Training loss: 6.0670 1.0195 sec/batch\n",
      "Epoch 2/20  Iteration 723/7380 Training loss: 6.0665 1.0153 sec/batch\n",
      "Epoch 2/20  Iteration 724/7380 Training loss: 6.0660 1.0169 sec/batch\n",
      "Epoch 2/20  Iteration 725/7380 Training loss: 6.0656 1.0200 sec/batch\n",
      "Epoch 2/20  Iteration 726/7380 Training loss: 6.0655 1.0135 sec/batch\n",
      "Epoch 2/20  Iteration 727/7380 Training loss: 6.0650 1.0160 sec/batch\n",
      "Epoch 2/20  Iteration 728/7380 Training loss: 6.0643 1.0238 sec/batch\n",
      "Epoch 2/20  Iteration 729/7380 Training loss: 6.0638 1.0137 sec/batch\n",
      "Epoch 2/20  Iteration 730/7380 Training loss: 6.0634 1.0175 sec/batch\n",
      "Epoch 2/20  Iteration 731/7380 Training loss: 6.0631 1.0160 sec/batch\n",
      "Epoch 2/20  Iteration 732/7380 Training loss: 6.0626 1.0204 sec/batch\n",
      "Epoch 2/20  Iteration 733/7380 Training loss: 6.0620 1.0206 sec/batch\n",
      "Epoch 2/20  Iteration 734/7380 Training loss: 6.0616 1.0157 sec/batch\n",
      "Epoch 2/20  Iteration 735/7380 Training loss: 6.0610 1.0149 sec/batch\n",
      "Epoch 2/20  Iteration 736/7380 Training loss: 6.0605 1.0259 sec/batch\n",
      "Epoch 2/20  Iteration 737/7380 Training loss: 6.0603 1.0157 sec/batch\n",
      "Epoch 2/20  Iteration 738/7380 Training loss: 6.0597 1.0162 sec/batch\n",
      "Epoch 3/20  Iteration 739/7380 Training loss: 6.2901 0.9918 sec/batch\n",
      "Epoch 3/20  Iteration 740/7380 Training loss: 6.1371 1.0196 sec/batch\n",
      "Epoch 3/20  Iteration 741/7380 Training loss: 6.0722 1.0255 sec/batch\n",
      "Epoch 3/20  Iteration 742/7380 Training loss: 6.0260 1.0206 sec/batch\n",
      "Epoch 3/20  Iteration 743/7380 Training loss: 6.0013 1.0292 sec/batch\n",
      "Epoch 3/20  Iteration 744/7380 Training loss: 5.9930 1.0143 sec/batch\n",
      "Epoch 3/20  Iteration 745/7380 Training loss: 5.9969 1.0177 sec/batch\n",
      "Epoch 3/20  Iteration 746/7380 Training loss: 5.9805 1.0131 sec/batch\n",
      "Epoch 3/20  Iteration 747/7380 Training loss: 5.9714 1.0146 sec/batch\n",
      "Epoch 3/20  Iteration 748/7380 Training loss: 5.9796 1.0219 sec/batch\n",
      "Epoch 3/20  Iteration 749/7380 Training loss: 5.9732 1.0187 sec/batch\n",
      "Epoch 3/20  Iteration 750/7380 Training loss: 5.9574 1.0203 sec/batch\n",
      "Validation loss: 5.81842 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 751/7380 Training loss: 5.9546 1.0237 sec/batch\n",
      "Epoch 3/20  Iteration 752/7380 Training loss: 5.9577 1.0193 sec/batch\n",
      "Epoch 3/20  Iteration 753/7380 Training loss: 5.9582 1.0244 sec/batch\n",
      "Epoch 3/20  Iteration 754/7380 Training loss: 5.9523 1.0183 sec/batch\n",
      "Epoch 3/20  Iteration 755/7380 Training loss: 5.9459 1.0178 sec/batch\n",
      "Epoch 3/20  Iteration 756/7380 Training loss: 5.9312 1.0341 sec/batch\n",
      "Epoch 3/20  Iteration 757/7380 Training loss: 5.9226 1.0210 sec/batch\n",
      "Epoch 3/20  Iteration 758/7380 Training loss: 5.9194 1.0285 sec/batch\n",
      "Epoch 3/20  Iteration 759/7380 Training loss: 5.9143 1.0315 sec/batch\n",
      "Epoch 3/20  Iteration 760/7380 Training loss: 5.9183 1.0202 sec/batch\n",
      "Epoch 3/20  Iteration 761/7380 Training loss: 5.9176 1.0178 sec/batch\n",
      "Epoch 3/20  Iteration 762/7380 Training loss: 5.9226 1.0185 sec/batch\n",
      "Epoch 3/20  Iteration 763/7380 Training loss: 5.9253 1.0244 sec/batch\n",
      "Epoch 3/20  Iteration 764/7380 Training loss: 5.9260 1.0223 sec/batch\n",
      "Epoch 3/20  Iteration 765/7380 Training loss: 5.9202 1.0304 sec/batch\n",
      "Epoch 3/20  Iteration 766/7380 Training loss: 5.9238 1.0189 sec/batch\n",
      "Epoch 3/20  Iteration 767/7380 Training loss: 5.9229 1.0192 sec/batch\n",
      "Epoch 3/20  Iteration 768/7380 Training loss: 5.9235 1.0336 sec/batch\n",
      "Epoch 3/20  Iteration 769/7380 Training loss: 5.9220 1.0161 sec/batch\n",
      "Epoch 3/20  Iteration 770/7380 Training loss: 5.9210 1.0258 sec/batch\n",
      "Epoch 3/20  Iteration 771/7380 Training loss: 5.9205 1.0161 sec/batch\n",
      "Epoch 3/20  Iteration 772/7380 Training loss: 5.9202 1.0278 sec/batch\n",
      "Epoch 3/20  Iteration 773/7380 Training loss: 5.9237 1.0231 sec/batch\n",
      "Epoch 3/20  Iteration 774/7380 Training loss: 5.9204 1.0337 sec/batch\n",
      "Epoch 3/20  Iteration 775/7380 Training loss: 5.9195 1.0288 sec/batch\n",
      "Epoch 3/20  Iteration 776/7380 Training loss: 5.9189 1.0247 sec/batch\n",
      "Epoch 3/20  Iteration 777/7380 Training loss: 5.9199 1.0222 sec/batch\n",
      "Epoch 3/20  Iteration 778/7380 Training loss: 5.9188 1.0219 sec/batch\n",
      "Epoch 3/20  Iteration 779/7380 Training loss: 5.9176 1.0273 sec/batch\n",
      "Epoch 3/20  Iteration 780/7380 Training loss: 5.9158 1.0178 sec/batch\n",
      "Epoch 3/20  Iteration 781/7380 Training loss: 5.9151 1.0254 sec/batch\n",
      "Epoch 3/20  Iteration 782/7380 Training loss: 5.9165 1.0183 sec/batch\n",
      "Epoch 3/20  Iteration 783/7380 Training loss: 5.9169 1.0171 sec/batch\n",
      "Epoch 3/20  Iteration 784/7380 Training loss: 5.9155 1.0174 sec/batch\n",
      "Epoch 3/20  Iteration 785/7380 Training loss: 5.9152 1.0178 sec/batch\n",
      "Epoch 3/20  Iteration 786/7380 Training loss: 5.9119 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 787/7380 Training loss: 5.9118 1.0372 sec/batch\n",
      "Epoch 3/20  Iteration 788/7380 Training loss: 5.9104 1.0336 sec/batch\n",
      "Epoch 3/20  Iteration 789/7380 Training loss: 5.9089 1.0288 sec/batch\n",
      "Epoch 3/20  Iteration 790/7380 Training loss: 5.9081 1.0262 sec/batch\n",
      "Epoch 3/20  Iteration 791/7380 Training loss: 5.9062 1.0185 sec/batch\n",
      "Epoch 3/20  Iteration 792/7380 Training loss: 5.9054 1.0247 sec/batch\n",
      "Epoch 3/20  Iteration 793/7380 Training loss: 5.9042 1.0292 sec/batch\n",
      "Epoch 3/20  Iteration 794/7380 Training loss: 5.9022 1.0256 sec/batch\n",
      "Epoch 3/20  Iteration 795/7380 Training loss: 5.9010 1.0377 sec/batch\n",
      "Epoch 3/20  Iteration 796/7380 Training loss: 5.9006 1.0282 sec/batch\n",
      "Epoch 3/20  Iteration 797/7380 Training loss: 5.9003 1.0233 sec/batch\n",
      "Epoch 3/20  Iteration 798/7380 Training loss: 5.9002 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 799/7380 Training loss: 5.8982 1.0228 sec/batch\n",
      "Epoch 3/20  Iteration 800/7380 Training loss: 5.8987 1.0274 sec/batch\n",
      "Validation loss: 5.79203 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 801/7380 Training loss: 5.8974 1.0250 sec/batch\n",
      "Epoch 3/20  Iteration 802/7380 Training loss: 5.8953 1.0251 sec/batch\n",
      "Epoch 3/20  Iteration 803/7380 Training loss: 5.8945 1.0233 sec/batch\n",
      "Epoch 3/20  Iteration 804/7380 Training loss: 5.8948 1.0299 sec/batch\n",
      "Epoch 3/20  Iteration 805/7380 Training loss: 5.8960 1.0213 sec/batch\n",
      "Epoch 3/20  Iteration 806/7380 Training loss: 5.8964 1.0256 sec/batch\n",
      "Epoch 3/20  Iteration 807/7380 Training loss: 5.8972 1.0193 sec/batch\n",
      "Epoch 3/20  Iteration 808/7380 Training loss: 5.8958 1.0218 sec/batch\n",
      "Epoch 3/20  Iteration 809/7380 Training loss: 5.8942 1.0195 sec/batch\n",
      "Epoch 3/20  Iteration 810/7380 Training loss: 5.8956 1.0338 sec/batch\n",
      "Epoch 3/20  Iteration 811/7380 Training loss: 5.8933 1.0317 sec/batch\n",
      "Epoch 3/20  Iteration 812/7380 Training loss: 5.8948 1.0180 sec/batch\n",
      "Epoch 3/20  Iteration 813/7380 Training loss: 5.8952 1.0290 sec/batch\n",
      "Epoch 3/20  Iteration 814/7380 Training loss: 5.8938 1.0299 sec/batch\n",
      "Epoch 3/20  Iteration 815/7380 Training loss: 5.8935 1.0241 sec/batch\n",
      "Epoch 3/20  Iteration 816/7380 Training loss: 5.8935 1.0266 sec/batch\n",
      "Epoch 3/20  Iteration 817/7380 Training loss: 5.8945 1.0335 sec/batch\n",
      "Epoch 3/20  Iteration 818/7380 Training loss: 5.8945 1.0226 sec/batch\n",
      "Epoch 3/20  Iteration 819/7380 Training loss: 5.8941 1.0305 sec/batch\n",
      "Epoch 3/20  Iteration 820/7380 Training loss: 5.8930 1.0209 sec/batch\n",
      "Epoch 3/20  Iteration 821/7380 Training loss: 5.8921 1.0216 sec/batch\n",
      "Epoch 3/20  Iteration 822/7380 Training loss: 5.8912 1.0247 sec/batch\n",
      "Epoch 3/20  Iteration 823/7380 Training loss: 5.8916 1.0392 sec/batch\n",
      "Epoch 3/20  Iteration 824/7380 Training loss: 5.8909 1.0278 sec/batch\n",
      "Epoch 3/20  Iteration 825/7380 Training loss: 5.8902 1.0254 sec/batch\n",
      "Epoch 3/20  Iteration 826/7380 Training loss: 5.8897 1.0227 sec/batch\n",
      "Epoch 3/20  Iteration 827/7380 Training loss: 5.8874 1.0226 sec/batch\n",
      "Epoch 3/20  Iteration 828/7380 Training loss: 5.8869 1.0213 sec/batch\n",
      "Epoch 3/20  Iteration 829/7380 Training loss: 5.8866 1.0236 sec/batch\n",
      "Epoch 3/20  Iteration 830/7380 Training loss: 5.8882 1.0340 sec/batch\n",
      "Epoch 3/20  Iteration 831/7380 Training loss: 5.8877 1.0244 sec/batch\n",
      "Epoch 3/20  Iteration 832/7380 Training loss: 5.8875 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 833/7380 Training loss: 5.8871 1.0222 sec/batch\n",
      "Epoch 3/20  Iteration 834/7380 Training loss: 5.8871 1.0346 sec/batch\n",
      "Epoch 3/20  Iteration 835/7380 Training loss: 5.8867 1.0249 sec/batch\n",
      "Epoch 3/20  Iteration 836/7380 Training loss: 5.8880 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 837/7380 Training loss: 5.8874 1.0315 sec/batch\n",
      "Epoch 3/20  Iteration 838/7380 Training loss: 5.8873 1.0326 sec/batch\n",
      "Epoch 3/20  Iteration 839/7380 Training loss: 5.8888 1.0279 sec/batch\n",
      "Epoch 3/20  Iteration 840/7380 Training loss: 5.8886 1.0298 sec/batch\n",
      "Epoch 3/20  Iteration 841/7380 Training loss: 5.8887 1.0231 sec/batch\n",
      "Epoch 3/20  Iteration 842/7380 Training loss: 5.8882 1.0310 sec/batch\n",
      "Epoch 3/20  Iteration 843/7380 Training loss: 5.8888 1.0228 sec/batch\n",
      "Epoch 3/20  Iteration 844/7380 Training loss: 5.8888 1.0238 sec/batch\n",
      "Epoch 3/20  Iteration 845/7380 Training loss: 5.8891 1.0315 sec/batch\n",
      "Epoch 3/20  Iteration 846/7380 Training loss: 5.8885 1.0241 sec/batch\n",
      "Epoch 3/20  Iteration 847/7380 Training loss: 5.8870 1.0275 sec/batch\n",
      "Epoch 3/20  Iteration 848/7380 Training loss: 5.8864 1.0299 sec/batch\n",
      "Epoch 3/20  Iteration 849/7380 Training loss: 5.8857 1.0310 sec/batch\n",
      "Epoch 3/20  Iteration 850/7380 Training loss: 5.8858 1.0383 sec/batch\n",
      "Validation loss: 5.76621 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 851/7380 Training loss: 5.8849 1.0288 sec/batch\n",
      "Epoch 3/20  Iteration 852/7380 Training loss: 5.8849 1.0294 sec/batch\n",
      "Epoch 3/20  Iteration 853/7380 Training loss: 5.8843 1.0316 sec/batch\n",
      "Epoch 3/20  Iteration 854/7380 Training loss: 5.8835 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 855/7380 Training loss: 5.8823 1.0230 sec/batch\n",
      "Epoch 3/20  Iteration 856/7380 Training loss: 5.8815 1.0349 sec/batch\n",
      "Epoch 3/20  Iteration 857/7380 Training loss: 5.8814 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 858/7380 Training loss: 5.8812 1.0251 sec/batch\n",
      "Epoch 3/20  Iteration 859/7380 Training loss: 5.8814 1.0303 sec/batch\n",
      "Epoch 3/20  Iteration 860/7380 Training loss: 5.8827 1.0253 sec/batch\n",
      "Epoch 3/20  Iteration 861/7380 Training loss: 5.8821 1.0232 sec/batch\n",
      "Epoch 3/20  Iteration 862/7380 Training loss: 5.8822 1.0228 sec/batch\n",
      "Epoch 3/20  Iteration 863/7380 Training loss: 5.8817 1.0318 sec/batch\n",
      "Epoch 3/20  Iteration 864/7380 Training loss: 5.8805 1.0295 sec/batch\n",
      "Epoch 3/20  Iteration 865/7380 Training loss: 5.8793 1.0404 sec/batch\n",
      "Epoch 3/20  Iteration 866/7380 Training loss: 5.8794 1.0238 sec/batch\n",
      "Epoch 3/20  Iteration 867/7380 Training loss: 5.8786 1.0371 sec/batch\n",
      "Epoch 3/20  Iteration 868/7380 Training loss: 5.8786 1.0243 sec/batch\n",
      "Epoch 3/20  Iteration 869/7380 Training loss: 5.8784 1.0283 sec/batch\n",
      "Epoch 3/20  Iteration 870/7380 Training loss: 5.8786 1.0321 sec/batch\n",
      "Epoch 3/20  Iteration 871/7380 Training loss: 5.8786 1.0321 sec/batch\n",
      "Epoch 3/20  Iteration 872/7380 Training loss: 5.8779 1.0229 sec/batch\n",
      "Epoch 3/20  Iteration 873/7380 Training loss: 5.8776 1.0391 sec/batch\n",
      "Epoch 3/20  Iteration 874/7380 Training loss: 5.8773 1.0286 sec/batch\n",
      "Epoch 3/20  Iteration 875/7380 Training loss: 5.8767 1.0249 sec/batch\n",
      "Epoch 3/20  Iteration 876/7380 Training loss: 5.8762 1.0292 sec/batch\n",
      "Epoch 3/20  Iteration 877/7380 Training loss: 5.8752 1.0257 sec/batch\n",
      "Epoch 3/20  Iteration 878/7380 Training loss: 5.8741 1.0268 sec/batch\n",
      "Epoch 3/20  Iteration 879/7380 Training loss: 5.8752 1.0346 sec/batch\n",
      "Epoch 3/20  Iteration 880/7380 Training loss: 5.8749 1.0218 sec/batch\n",
      "Epoch 3/20  Iteration 881/7380 Training loss: 5.8750 1.0219 sec/batch\n",
      "Epoch 3/20  Iteration 882/7380 Training loss: 5.8748 1.0306 sec/batch\n",
      "Epoch 3/20  Iteration 883/7380 Training loss: 5.8746 1.0240 sec/batch\n",
      "Epoch 3/20  Iteration 884/7380 Training loss: 5.8744 1.0341 sec/batch\n",
      "Epoch 3/20  Iteration 885/7380 Training loss: 5.8742 1.0283 sec/batch\n",
      "Epoch 3/20  Iteration 886/7380 Training loss: 5.8739 1.0363 sec/batch\n",
      "Epoch 3/20  Iteration 887/7380 Training loss: 5.8733 1.0323 sec/batch\n",
      "Epoch 3/20  Iteration 888/7380 Training loss: 5.8734 1.0230 sec/batch\n",
      "Epoch 3/20  Iteration 889/7380 Training loss: 5.8727 1.0329 sec/batch\n",
      "Epoch 3/20  Iteration 890/7380 Training loss: 5.8730 1.0237 sec/batch\n",
      "Epoch 3/20  Iteration 891/7380 Training loss: 5.8730 1.0210 sec/batch\n",
      "Epoch 3/20  Iteration 892/7380 Training loss: 5.8734 1.0278 sec/batch\n",
      "Epoch 3/20  Iteration 893/7380 Training loss: 5.8732 1.0240 sec/batch\n",
      "Epoch 3/20  Iteration 894/7380 Training loss: 5.8727 1.0363 sec/batch\n",
      "Epoch 3/20  Iteration 895/7380 Training loss: 5.8717 1.0275 sec/batch\n",
      "Epoch 3/20  Iteration 896/7380 Training loss: 5.8719 1.0351 sec/batch\n",
      "Epoch 3/20  Iteration 897/7380 Training loss: 5.8714 1.0239 sec/batch\n",
      "Epoch 3/20  Iteration 898/7380 Training loss: 5.8713 1.0239 sec/batch\n",
      "Epoch 3/20  Iteration 899/7380 Training loss: 5.8719 1.0236 sec/batch\n",
      "Epoch 3/20  Iteration 900/7380 Training loss: 5.8720 1.0290 sec/batch\n",
      "Validation loss: 5.74354 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 901/7380 Training loss: 5.8723 1.0324 sec/batch\n",
      "Epoch 3/20  Iteration 902/7380 Training loss: 5.8727 1.0266 sec/batch\n",
      "Epoch 3/20  Iteration 903/7380 Training loss: 5.8728 1.0291 sec/batch\n",
      "Epoch 3/20  Iteration 904/7380 Training loss: 5.8728 1.0325 sec/batch\n",
      "Epoch 3/20  Iteration 905/7380 Training loss: 5.8725 1.0264 sec/batch\n",
      "Epoch 3/20  Iteration 906/7380 Training loss: 5.8726 1.0226 sec/batch\n",
      "Epoch 3/20  Iteration 907/7380 Training loss: 5.8717 1.0245 sec/batch\n",
      "Epoch 3/20  Iteration 908/7380 Training loss: 5.8711 1.0304 sec/batch\n",
      "Epoch 3/20  Iteration 909/7380 Training loss: 5.8711 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 910/7380 Training loss: 5.8714 1.0275 sec/batch\n",
      "Epoch 3/20  Iteration 911/7380 Training loss: 5.8713 1.0208 sec/batch\n",
      "Epoch 3/20  Iteration 912/7380 Training loss: 5.8707 1.0212 sec/batch\n",
      "Epoch 3/20  Iteration 913/7380 Training loss: 5.8702 1.0242 sec/batch\n",
      "Epoch 3/20  Iteration 914/7380 Training loss: 5.8697 1.0279 sec/batch\n",
      "Epoch 3/20  Iteration 915/7380 Training loss: 5.8694 1.0367 sec/batch\n",
      "Epoch 3/20  Iteration 916/7380 Training loss: 5.8693 1.0342 sec/batch\n",
      "Epoch 3/20  Iteration 917/7380 Training loss: 5.8686 1.0306 sec/batch\n",
      "Epoch 3/20  Iteration 918/7380 Training loss: 5.8679 1.0261 sec/batch\n",
      "Epoch 3/20  Iteration 919/7380 Training loss: 5.8675 1.0244 sec/batch\n",
      "Epoch 3/20  Iteration 920/7380 Training loss: 5.8674 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 921/7380 Training loss: 5.8678 1.0265 sec/batch\n",
      "Epoch 3/20  Iteration 922/7380 Training loss: 5.8672 1.0216 sec/batch\n",
      "Epoch 3/20  Iteration 923/7380 Training loss: 5.8670 1.0396 sec/batch\n",
      "Epoch 3/20  Iteration 924/7380 Training loss: 5.8668 1.0312 sec/batch\n",
      "Epoch 3/20  Iteration 925/7380 Training loss: 5.8664 1.0386 sec/batch\n",
      "Epoch 3/20  Iteration 926/7380 Training loss: 5.8660 1.0273 sec/batch\n",
      "Epoch 3/20  Iteration 927/7380 Training loss: 5.8652 1.0278 sec/batch\n",
      "Epoch 3/20  Iteration 928/7380 Training loss: 5.8653 1.0234 sec/batch\n",
      "Epoch 3/20  Iteration 929/7380 Training loss: 5.8642 1.0326 sec/batch\n",
      "Epoch 3/20  Iteration 930/7380 Training loss: 5.8648 1.0329 sec/batch\n",
      "Epoch 3/20  Iteration 931/7380 Training loss: 5.8650 1.0234 sec/batch\n",
      "Epoch 3/20  Iteration 932/7380 Training loss: 5.8642 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 933/7380 Training loss: 5.8644 1.0255 sec/batch\n",
      "Epoch 3/20  Iteration 934/7380 Training loss: 5.8643 1.0224 sec/batch\n",
      "Epoch 3/20  Iteration 935/7380 Training loss: 5.8642 1.0346 sec/batch\n",
      "Epoch 3/20  Iteration 936/7380 Training loss: 5.8646 1.0362 sec/batch\n",
      "Epoch 3/20  Iteration 937/7380 Training loss: 5.8643 1.0240 sec/batch\n",
      "Epoch 3/20  Iteration 938/7380 Training loss: 5.8644 1.0234 sec/batch\n",
      "Epoch 3/20  Iteration 939/7380 Training loss: 5.8647 1.0224 sec/batch\n",
      "Epoch 3/20  Iteration 940/7380 Training loss: 5.8647 1.0221 sec/batch\n",
      "Epoch 3/20  Iteration 941/7380 Training loss: 5.8650 1.0326 sec/batch\n",
      "Epoch 3/20  Iteration 942/7380 Training loss: 5.8649 1.0318 sec/batch\n",
      "Epoch 3/20  Iteration 943/7380 Training loss: 5.8650 1.0237 sec/batch\n",
      "Epoch 3/20  Iteration 944/7380 Training loss: 5.8651 1.0250 sec/batch\n",
      "Epoch 3/20  Iteration 945/7380 Training loss: 5.8640 1.0244 sec/batch\n",
      "Epoch 3/20  Iteration 946/7380 Training loss: 5.8635 1.0330 sec/batch\n",
      "Epoch 3/20  Iteration 947/7380 Training loss: 5.8630 1.0266 sec/batch\n",
      "Epoch 3/20  Iteration 948/7380 Training loss: 5.8624 1.0239 sec/batch\n",
      "Epoch 3/20  Iteration 949/7380 Training loss: 5.8621 1.0200 sec/batch\n",
      "Epoch 3/20  Iteration 950/7380 Training loss: 5.8618 1.0188 sec/batch\n",
      "Validation loss: 5.7184 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 951/7380 Training loss: 5.8615 1.0412 sec/batch\n",
      "Epoch 3/20  Iteration 952/7380 Training loss: 5.8612 1.0442 sec/batch\n",
      "Epoch 3/20  Iteration 953/7380 Training loss: 5.8608 1.0270 sec/batch\n",
      "Epoch 3/20  Iteration 954/7380 Training loss: 5.8601 1.0233 sec/batch\n",
      "Epoch 3/20  Iteration 955/7380 Training loss: 5.8593 1.0465 sec/batch\n",
      "Epoch 3/20  Iteration 956/7380 Training loss: 5.8591 1.0260 sec/batch\n",
      "Epoch 3/20  Iteration 957/7380 Training loss: 5.8594 1.0260 sec/batch\n",
      "Epoch 3/20  Iteration 958/7380 Training loss: 5.8594 1.0258 sec/batch\n",
      "Epoch 3/20  Iteration 959/7380 Training loss: 5.8589 1.0226 sec/batch\n",
      "Epoch 3/20  Iteration 960/7380 Training loss: 5.8592 1.0214 sec/batch\n",
      "Epoch 3/20  Iteration 961/7380 Training loss: 5.8591 1.0291 sec/batch\n",
      "Epoch 3/20  Iteration 962/7380 Training loss: 5.8586 1.0243 sec/batch\n",
      "Epoch 3/20  Iteration 963/7380 Training loss: 5.8585 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 964/7380 Training loss: 5.8581 1.0263 sec/batch\n",
      "Epoch 3/20  Iteration 965/7380 Training loss: 5.8578 1.0314 sec/batch\n",
      "Epoch 3/20  Iteration 966/7380 Training loss: 5.8573 1.0340 sec/batch\n",
      "Epoch 3/20  Iteration 967/7380 Training loss: 5.8573 1.0357 sec/batch\n",
      "Epoch 3/20  Iteration 968/7380 Training loss: 5.8575 1.0217 sec/batch\n",
      "Epoch 3/20  Iteration 969/7380 Training loss: 5.8570 1.0201 sec/batch\n",
      "Epoch 3/20  Iteration 970/7380 Training loss: 5.8564 1.0344 sec/batch\n",
      "Epoch 3/20  Iteration 971/7380 Training loss: 5.8558 1.0210 sec/batch\n",
      "Epoch 3/20  Iteration 972/7380 Training loss: 5.8551 1.0289 sec/batch\n",
      "Epoch 3/20  Iteration 973/7380 Training loss: 5.8548 1.0269 sec/batch\n",
      "Epoch 3/20  Iteration 974/7380 Training loss: 5.8543 1.0243 sec/batch\n",
      "Epoch 3/20  Iteration 975/7380 Training loss: 5.8537 1.0270 sec/batch\n",
      "Epoch 3/20  Iteration 976/7380 Training loss: 5.8529 1.0239 sec/batch\n",
      "Epoch 3/20  Iteration 977/7380 Training loss: 5.8517 1.0378 sec/batch\n",
      "Epoch 3/20  Iteration 978/7380 Training loss: 5.8509 1.0305 sec/batch\n",
      "Epoch 3/20  Iteration 979/7380 Training loss: 5.8509 1.0248 sec/batch\n",
      "Epoch 3/20  Iteration 980/7380 Training loss: 5.8513 1.0369 sec/batch\n",
      "Epoch 3/20  Iteration 981/7380 Training loss: 5.8508 1.0358 sec/batch\n",
      "Epoch 3/20  Iteration 982/7380 Training loss: 5.8507 1.0272 sec/batch\n",
      "Epoch 3/20  Iteration 983/7380 Training loss: 5.8506 1.0383 sec/batch\n",
      "Epoch 3/20  Iteration 984/7380 Training loss: 5.8507 1.0398 sec/batch\n",
      "Epoch 3/20  Iteration 985/7380 Training loss: 5.8504 1.0344 sec/batch\n",
      "Epoch 3/20  Iteration 986/7380 Training loss: 5.8502 1.0379 sec/batch\n",
      "Epoch 3/20  Iteration 987/7380 Training loss: 5.8498 1.0281 sec/batch\n",
      "Epoch 3/20  Iteration 988/7380 Training loss: 5.8497 1.0251 sec/batch\n",
      "Epoch 3/20  Iteration 989/7380 Training loss: 5.8490 1.0319 sec/batch\n",
      "Epoch 3/20  Iteration 990/7380 Training loss: 5.8488 1.0265 sec/batch\n",
      "Epoch 3/20  Iteration 991/7380 Training loss: 5.8487 1.0243 sec/batch\n",
      "Epoch 3/20  Iteration 992/7380 Training loss: 5.8479 1.0236 sec/batch\n",
      "Epoch 3/20  Iteration 993/7380 Training loss: 5.8479 1.0230 sec/batch\n",
      "Epoch 3/20  Iteration 994/7380 Training loss: 5.8478 1.0302 sec/batch\n",
      "Epoch 3/20  Iteration 995/7380 Training loss: 5.8478 1.0253 sec/batch\n",
      "Epoch 3/20  Iteration 996/7380 Training loss: 5.8473 1.0345 sec/batch\n",
      "Epoch 3/20  Iteration 997/7380 Training loss: 5.8475 1.0241 sec/batch\n",
      "Epoch 3/20  Iteration 998/7380 Training loss: 5.8470 1.0290 sec/batch\n",
      "Epoch 3/20  Iteration 999/7380 Training loss: 5.8467 1.0306 sec/batch\n",
      "Epoch 3/20  Iteration 1000/7380 Training loss: 5.8467 1.0236 sec/batch\n",
      "Validation loss: 5.69554 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 1001/7380 Training loss: 5.8470 1.0300 sec/batch\n",
      "Epoch 3/20  Iteration 1002/7380 Training loss: 5.8465 1.0359 sec/batch\n",
      "Epoch 3/20  Iteration 1003/7380 Training loss: 5.8462 1.0249 sec/batch\n",
      "Epoch 3/20  Iteration 1004/7380 Training loss: 5.8456 1.0252 sec/batch\n",
      "Epoch 3/20  Iteration 1005/7380 Training loss: 5.8456 1.0226 sec/batch\n",
      "Epoch 3/20  Iteration 1006/7380 Training loss: 5.8455 1.0313 sec/batch\n",
      "Epoch 3/20  Iteration 1007/7380 Training loss: 5.8448 1.0217 sec/batch\n",
      "Epoch 3/20  Iteration 1008/7380 Training loss: 5.8447 1.0225 sec/batch\n",
      "Epoch 3/20  Iteration 1009/7380 Training loss: 5.8447 1.0233 sec/batch\n",
      "Epoch 3/20  Iteration 1010/7380 Training loss: 5.8438 1.0368 sec/batch\n",
      "Epoch 3/20  Iteration 1011/7380 Training loss: 5.8440 1.0226 sec/batch\n",
      "Epoch 3/20  Iteration 1012/7380 Training loss: 5.8439 1.0222 sec/batch\n",
      "Epoch 3/20  Iteration 1013/7380 Training loss: 5.8435 1.0234 sec/batch\n",
      "Epoch 3/20  Iteration 1014/7380 Training loss: 5.8432 1.0249 sec/batch\n",
      "Epoch 3/20  Iteration 1015/7380 Training loss: 5.8436 1.0419 sec/batch\n",
      "Epoch 3/20  Iteration 1016/7380 Training loss: 5.8435 1.0420 sec/batch\n",
      "Epoch 3/20  Iteration 1017/7380 Training loss: 5.8433 1.0241 sec/batch\n",
      "Epoch 3/20  Iteration 1018/7380 Training loss: 5.8429 1.0317 sec/batch\n",
      "Epoch 3/20  Iteration 1019/7380 Training loss: 5.8428 1.0319 sec/batch\n",
      "Epoch 3/20  Iteration 1020/7380 Training loss: 5.8426 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 1021/7380 Training loss: 5.8428 1.0215 sec/batch\n",
      "Epoch 3/20  Iteration 1022/7380 Training loss: 5.8427 1.0251 sec/batch\n",
      "Epoch 3/20  Iteration 1023/7380 Training loss: 5.8428 1.0364 sec/batch\n",
      "Epoch 3/20  Iteration 1024/7380 Training loss: 5.8423 1.0369 sec/batch\n",
      "Epoch 3/20  Iteration 1025/7380 Training loss: 5.8420 1.0281 sec/batch\n",
      "Epoch 3/20  Iteration 1026/7380 Training loss: 5.8415 1.0322 sec/batch\n",
      "Epoch 3/20  Iteration 1027/7380 Training loss: 5.8411 1.0353 sec/batch\n",
      "Epoch 3/20  Iteration 1028/7380 Training loss: 5.8407 1.0268 sec/batch\n",
      "Epoch 3/20  Iteration 1029/7380 Training loss: 5.8402 1.0368 sec/batch\n",
      "Epoch 3/20  Iteration 1030/7380 Training loss: 5.8401 1.0242 sec/batch\n",
      "Epoch 3/20  Iteration 1031/7380 Training loss: 5.8399 1.0263 sec/batch\n",
      "Epoch 3/20  Iteration 1032/7380 Training loss: 5.8396 1.0321 sec/batch\n",
      "Epoch 3/20  Iteration 1033/7380 Training loss: 5.8396 1.0367 sec/batch\n",
      "Epoch 3/20  Iteration 1034/7380 Training loss: 5.8394 1.0261 sec/batch\n",
      "Epoch 3/20  Iteration 1035/7380 Training loss: 5.8389 1.0230 sec/batch\n",
      "Epoch 3/20  Iteration 1036/7380 Training loss: 5.8386 1.0325 sec/batch\n",
      "Epoch 3/20  Iteration 1037/7380 Training loss: 5.8386 1.0245 sec/batch\n",
      "Epoch 3/20  Iteration 1038/7380 Training loss: 5.8389 1.0308 sec/batch\n",
      "Epoch 3/20  Iteration 1039/7380 Training loss: 5.8387 1.0228 sec/batch\n",
      "Epoch 3/20  Iteration 1040/7380 Training loss: 5.8388 1.0315 sec/batch\n",
      "Epoch 3/20  Iteration 1041/7380 Training loss: 5.8385 1.0424 sec/batch\n",
      "Epoch 3/20  Iteration 1042/7380 Training loss: 5.8388 1.0263 sec/batch\n",
      "Epoch 3/20  Iteration 1043/7380 Training loss: 5.8390 1.0391 sec/batch\n",
      "Epoch 3/20  Iteration 1044/7380 Training loss: 5.8388 1.0243 sec/batch\n",
      "Epoch 3/20  Iteration 1045/7380 Training loss: 5.8383 1.0208 sec/batch\n",
      "Epoch 3/20  Iteration 1046/7380 Training loss: 5.8379 1.0214 sec/batch\n",
      "Epoch 3/20  Iteration 1047/7380 Training loss: 5.8375 1.0218 sec/batch\n",
      "Epoch 3/20  Iteration 1048/7380 Training loss: 5.8368 1.0247 sec/batch\n",
      "Epoch 3/20  Iteration 1049/7380 Training loss: 5.8366 1.0403 sec/batch\n",
      "Epoch 3/20  Iteration 1050/7380 Training loss: 5.8361 1.0196 sec/batch\n",
      "Validation loss: 5.67557 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 1051/7380 Training loss: 5.8359 1.0276 sec/batch\n",
      "Epoch 3/20  Iteration 1052/7380 Training loss: 5.8358 1.0432 sec/batch\n",
      "Epoch 3/20  Iteration 1053/7380 Training loss: 5.8357 1.0329 sec/batch\n",
      "Epoch 3/20  Iteration 1054/7380 Training loss: 5.8357 1.0238 sec/batch\n",
      "Epoch 3/20  Iteration 1055/7380 Training loss: 5.8356 1.0239 sec/batch\n",
      "Epoch 3/20  Iteration 1056/7380 Training loss: 5.8355 1.0378 sec/batch\n",
      "Epoch 3/20  Iteration 1057/7380 Training loss: 5.8359 1.0433 sec/batch\n",
      "Epoch 3/20  Iteration 1058/7380 Training loss: 5.8359 1.0206 sec/batch\n",
      "Epoch 3/20  Iteration 1059/7380 Training loss: 5.8356 1.0369 sec/batch\n",
      "Epoch 3/20  Iteration 1060/7380 Training loss: 5.8353 1.0294 sec/batch\n",
      "Epoch 3/20  Iteration 1061/7380 Training loss: 5.8351 1.0228 sec/batch\n",
      "Epoch 3/20  Iteration 1062/7380 Training loss: 5.8347 1.0388 sec/batch\n",
      "Epoch 3/20  Iteration 1063/7380 Training loss: 5.8347 1.0295 sec/batch\n",
      "Epoch 3/20  Iteration 1064/7380 Training loss: 5.8349 1.0260 sec/batch\n",
      "Epoch 3/20  Iteration 1065/7380 Training loss: 5.8344 1.0341 sec/batch\n",
      "Epoch 3/20  Iteration 1066/7380 Training loss: 5.8338 1.0263 sec/batch\n",
      "Epoch 3/20  Iteration 1067/7380 Training loss: 5.8335 1.0306 sec/batch\n",
      "Epoch 3/20  Iteration 1068/7380 Training loss: 5.8332 1.0240 sec/batch\n",
      "Epoch 3/20  Iteration 1069/7380 Training loss: 5.8327 1.0240 sec/batch\n",
      "Epoch 3/20  Iteration 1070/7380 Training loss: 5.8326 1.0207 sec/batch\n",
      "Epoch 3/20  Iteration 1071/7380 Training loss: 5.8320 1.0315 sec/batch\n",
      "Epoch 3/20  Iteration 1072/7380 Training loss: 5.8319 1.0222 sec/batch\n",
      "Epoch 3/20  Iteration 1073/7380 Training loss: 5.8317 1.0266 sec/batch\n",
      "Epoch 3/20  Iteration 1074/7380 Training loss: 5.8317 1.0407 sec/batch\n",
      "Epoch 3/20  Iteration 1075/7380 Training loss: 5.8317 1.0378 sec/batch\n",
      "Epoch 3/20  Iteration 1076/7380 Training loss: 5.8314 1.0235 sec/batch\n",
      "Epoch 3/20  Iteration 1077/7380 Training loss: 5.8312 1.0251 sec/batch\n",
      "Epoch 3/20  Iteration 1078/7380 Training loss: 5.8305 1.0212 sec/batch\n",
      "Epoch 3/20  Iteration 1079/7380 Training loss: 5.8300 1.0230 sec/batch\n",
      "Epoch 3/20  Iteration 1080/7380 Training loss: 5.8296 1.0292 sec/batch\n",
      "Epoch 3/20  Iteration 1081/7380 Training loss: 5.8294 1.0328 sec/batch\n",
      "Epoch 3/20  Iteration 1082/7380 Training loss: 5.8291 1.0243 sec/batch\n",
      "Epoch 3/20  Iteration 1083/7380 Training loss: 5.8288 1.0424 sec/batch\n",
      "Epoch 3/20  Iteration 1084/7380 Training loss: 5.8285 1.0341 sec/batch\n",
      "Epoch 3/20  Iteration 1085/7380 Training loss: 5.8286 1.0223 sec/batch\n",
      "Epoch 3/20  Iteration 1086/7380 Training loss: 5.8285 1.0255 sec/batch\n",
      "Epoch 3/20  Iteration 1087/7380 Training loss: 5.8282 1.0334 sec/batch\n",
      "Epoch 3/20  Iteration 1088/7380 Training loss: 5.8279 1.0278 sec/batch\n",
      "Epoch 3/20  Iteration 1089/7380 Training loss: 5.8283 1.0274 sec/batch\n",
      "Epoch 3/20  Iteration 1090/7380 Training loss: 5.8278 1.0513 sec/batch\n",
      "Epoch 3/20  Iteration 1091/7380 Training loss: 5.8276 1.0325 sec/batch\n",
      "Epoch 3/20  Iteration 1092/7380 Training loss: 5.8272 1.0355 sec/batch\n",
      "Epoch 3/20  Iteration 1093/7380 Training loss: 5.8268 1.0338 sec/batch\n",
      "Epoch 3/20  Iteration 1094/7380 Training loss: 5.8265 1.0239 sec/batch\n",
      "Epoch 3/20  Iteration 1095/7380 Training loss: 5.8265 1.0248 sec/batch\n",
      "Epoch 3/20  Iteration 1096/7380 Training loss: 5.8261 1.0280 sec/batch\n",
      "Epoch 3/20  Iteration 1097/7380 Training loss: 5.8256 1.0220 sec/batch\n",
      "Epoch 3/20  Iteration 1098/7380 Training loss: 5.8252 1.0264 sec/batch\n",
      "Epoch 3/20  Iteration 1099/7380 Training loss: 5.8250 1.0351 sec/batch\n",
      "Epoch 3/20  Iteration 1100/7380 Training loss: 5.8249 1.0275 sec/batch\n",
      "Validation loss: 5.65743 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 1101/7380 Training loss: 5.8247 1.0299 sec/batch\n",
      "Epoch 3/20  Iteration 1102/7380 Training loss: 5.8243 1.0351 sec/batch\n",
      "Epoch 3/20  Iteration 1103/7380 Training loss: 5.8238 1.0282 sec/batch\n",
      "Epoch 3/20  Iteration 1104/7380 Training loss: 5.8234 1.0220 sec/batch\n",
      "Epoch 3/20  Iteration 1105/7380 Training loss: 5.8230 1.0248 sec/batch\n",
      "Epoch 3/20  Iteration 1106/7380 Training loss: 5.8229 1.0413 sec/batch\n",
      "Epoch 3/20  Iteration 1107/7380 Training loss: 5.8225 1.0354 sec/batch\n",
      "Epoch 4/20  Iteration 1108/7380 Training loss: 6.0053 1.0044 sec/batch\n",
      "Epoch 4/20  Iteration 1109/7380 Training loss: 5.8991 1.0277 sec/batch\n",
      "Epoch 4/20  Iteration 1110/7380 Training loss: 5.8424 1.0362 sec/batch\n",
      "Epoch 4/20  Iteration 1111/7380 Training loss: 5.8058 1.0367 sec/batch\n",
      "Epoch 4/20  Iteration 1112/7380 Training loss: 5.7854 1.0307 sec/batch\n",
      "Epoch 4/20  Iteration 1113/7380 Training loss: 5.7891 1.0298 sec/batch\n",
      "Epoch 4/20  Iteration 1114/7380 Training loss: 5.7948 1.0331 sec/batch\n",
      "Epoch 4/20  Iteration 1115/7380 Training loss: 5.7793 1.0281 sec/batch\n",
      "Epoch 4/20  Iteration 1116/7380 Training loss: 5.7718 1.0257 sec/batch\n",
      "Epoch 4/20  Iteration 1117/7380 Training loss: 5.7792 1.0250 sec/batch\n",
      "Epoch 4/20  Iteration 1118/7380 Training loss: 5.7725 1.0277 sec/batch\n",
      "Epoch 4/20  Iteration 1119/7380 Training loss: 5.7596 1.0458 sec/batch\n",
      "Epoch 4/20  Iteration 1120/7380 Training loss: 5.7586 1.0380 sec/batch\n",
      "Epoch 4/20  Iteration 1121/7380 Training loss: 5.7607 1.0328 sec/batch\n",
      "Epoch 4/20  Iteration 1122/7380 Training loss: 5.7637 1.0234 sec/batch\n",
      "Epoch 4/20  Iteration 1123/7380 Training loss: 5.7586 1.0233 sec/batch\n",
      "Epoch 4/20  Iteration 1124/7380 Training loss: 5.7542 1.0248 sec/batch\n",
      "Epoch 4/20  Iteration 1125/7380 Training loss: 5.7415 1.0231 sec/batch\n",
      "Epoch 4/20  Iteration 1126/7380 Training loss: 5.7320 1.0267 sec/batch\n",
      "Epoch 4/20  Iteration 1127/7380 Training loss: 5.7282 1.0260 sec/batch\n",
      "Epoch 4/20  Iteration 1128/7380 Training loss: 5.7222 1.0228 sec/batch\n",
      "Epoch 4/20  Iteration 1129/7380 Training loss: 5.7275 1.0356 sec/batch\n",
      "Epoch 4/20  Iteration 1130/7380 Training loss: 5.7275 1.0284 sec/batch\n",
      "Epoch 4/20  Iteration 1131/7380 Training loss: 5.7345 1.0299 sec/batch\n",
      "Epoch 4/20  Iteration 1132/7380 Training loss: 5.7372 1.0379 sec/batch\n",
      "Epoch 4/20  Iteration 1133/7380 Training loss: 5.7380 1.0319 sec/batch\n",
      "Epoch 4/20  Iteration 1134/7380 Training loss: 5.7320 1.0211 sec/batch\n",
      "Epoch 4/20  Iteration 1135/7380 Training loss: 5.7353 1.0247 sec/batch\n",
      "Epoch 4/20  Iteration 1136/7380 Training loss: 5.7329 1.0267 sec/batch\n",
      "Epoch 4/20  Iteration 1137/7380 Training loss: 5.7327 1.0318 sec/batch\n",
      "Epoch 4/20  Iteration 1138/7380 Training loss: 5.7314 1.0272 sec/batch\n",
      "Epoch 4/20  Iteration 1139/7380 Training loss: 5.7309 1.0392 sec/batch\n",
      "Epoch 4/20  Iteration 1140/7380 Training loss: 5.7312 1.0399 sec/batch\n",
      "Epoch 4/20  Iteration 1141/7380 Training loss: 5.7308 1.0237 sec/batch\n",
      "Epoch 4/20  Iteration 1142/7380 Training loss: 5.7338 1.0318 sec/batch\n",
      "Epoch 4/20  Iteration 1143/7380 Training loss: 5.7321 1.0465 sec/batch\n",
      "Epoch 4/20  Iteration 1144/7380 Training loss: 5.7307 1.0237 sec/batch\n",
      "Epoch 4/20  Iteration 1145/7380 Training loss: 5.7295 1.0241 sec/batch\n",
      "Epoch 4/20  Iteration 1146/7380 Training loss: 5.7312 1.0400 sec/batch\n",
      "Epoch 4/20  Iteration 1147/7380 Training loss: 5.7296 1.0265 sec/batch\n",
      "Epoch 4/20  Iteration 1148/7380 Training loss: 5.7288 1.0250 sec/batch\n",
      "Epoch 4/20  Iteration 1149/7380 Training loss: 5.7272 1.0362 sec/batch\n",
      "Epoch 4/20  Iteration 1150/7380 Training loss: 5.7269 1.0243 sec/batch\n",
      "Validation loss: 5.63257 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1151/7380 Training loss: 5.7291 1.0281 sec/batch\n",
      "Epoch 4/20  Iteration 1152/7380 Training loss: 5.7294 1.0276 sec/batch\n",
      "Epoch 4/20  Iteration 1153/7380 Training loss: 5.7276 1.0351 sec/batch\n",
      "Epoch 4/20  Iteration 1154/7380 Training loss: 5.7281 1.0258 sec/batch\n",
      "Epoch 4/20  Iteration 1155/7380 Training loss: 5.7255 1.0344 sec/batch\n",
      "Epoch 4/20  Iteration 1156/7380 Training loss: 5.7255 1.0312 sec/batch\n",
      "Epoch 4/20  Iteration 1157/7380 Training loss: 5.7246 1.0290 sec/batch\n",
      "Epoch 4/20  Iteration 1158/7380 Training loss: 5.7234 1.0351 sec/batch\n",
      "Epoch 4/20  Iteration 1159/7380 Training loss: 5.7224 1.0273 sec/batch\n",
      "Epoch 4/20  Iteration 1160/7380 Training loss: 5.7212 1.0391 sec/batch\n",
      "Epoch 4/20  Iteration 1161/7380 Training loss: 5.7206 1.0265 sec/batch\n",
      "Epoch 4/20  Iteration 1162/7380 Training loss: 5.7197 1.0230 sec/batch\n",
      "Epoch 4/20  Iteration 1163/7380 Training loss: 5.7176 1.0347 sec/batch\n",
      "Epoch 4/20  Iteration 1164/7380 Training loss: 5.7167 1.0312 sec/batch\n",
      "Epoch 4/20  Iteration 1165/7380 Training loss: 5.7166 1.0301 sec/batch\n",
      "Epoch 4/20  Iteration 1166/7380 Training loss: 5.7161 1.0370 sec/batch\n",
      "Epoch 4/20  Iteration 1167/7380 Training loss: 5.7167 1.0246 sec/batch\n",
      "Epoch 4/20  Iteration 1168/7380 Training loss: 5.7139 1.0270 sec/batch\n",
      "Epoch 4/20  Iteration 1169/7380 Training loss: 5.7146 1.0512 sec/batch\n",
      "Epoch 4/20  Iteration 1170/7380 Training loss: 5.7131 1.0247 sec/batch\n",
      "Epoch 4/20  Iteration 1171/7380 Training loss: 5.7112 1.0357 sec/batch\n",
      "Epoch 4/20  Iteration 1172/7380 Training loss: 5.7112 1.0318 sec/batch\n",
      "Epoch 4/20  Iteration 1173/7380 Training loss: 5.7111 1.0283 sec/batch\n",
      "Epoch 4/20  Iteration 1174/7380 Training loss: 5.7123 1.0278 sec/batch\n",
      "Epoch 4/20  Iteration 1175/7380 Training loss: 5.7133 1.0277 sec/batch\n",
      "Epoch 4/20  Iteration 1176/7380 Training loss: 5.7145 1.0264 sec/batch\n",
      "Epoch 4/20  Iteration 1177/7380 Training loss: 5.7132 1.0243 sec/batch\n",
      "Epoch 4/20  Iteration 1178/7380 Training loss: 5.7119 1.0237 sec/batch\n",
      "Epoch 4/20  Iteration 1179/7380 Training loss: 5.7133 1.0261 sec/batch\n",
      "Epoch 4/20  Iteration 1180/7380 Training loss: 5.7113 1.0221 sec/batch\n",
      "Epoch 4/20  Iteration 1181/7380 Training loss: 5.7123 1.0276 sec/batch\n",
      "Epoch 4/20  Iteration 1182/7380 Training loss: 5.7128 1.0329 sec/batch\n",
      "Epoch 4/20  Iteration 1183/7380 Training loss: 5.7116 1.0570 sec/batch\n",
      "Epoch 4/20  Iteration 1184/7380 Training loss: 5.7116 1.0395 sec/batch\n",
      "Epoch 4/20  Iteration 1185/7380 Training loss: 5.7117 1.0236 sec/batch\n",
      "Epoch 4/20  Iteration 1186/7380 Training loss: 5.7128 1.0281 sec/batch\n",
      "Epoch 4/20  Iteration 1187/7380 Training loss: 5.7127 1.0216 sec/batch\n",
      "Epoch 4/20  Iteration 1188/7380 Training loss: 5.7122 1.0227 sec/batch\n",
      "Epoch 4/20  Iteration 1189/7380 Training loss: 5.7113 1.0395 sec/batch\n",
      "Epoch 4/20  Iteration 1190/7380 Training loss: 5.7098 1.0381 sec/batch\n",
      "Epoch 4/20  Iteration 1191/7380 Training loss: 5.7085 1.0268 sec/batch\n",
      "Epoch 4/20  Iteration 1192/7380 Training loss: 5.7088 1.0262 sec/batch\n",
      "Epoch 4/20  Iteration 1193/7380 Training loss: 5.7082 1.0332 sec/batch\n",
      "Epoch 4/20  Iteration 1194/7380 Training loss: 5.7073 1.0408 sec/batch\n",
      "Epoch 4/20  Iteration 1195/7380 Training loss: 5.7069 1.0283 sec/batch\n",
      "Epoch 4/20  Iteration 1196/7380 Training loss: 5.7044 1.0349 sec/batch\n",
      "Epoch 4/20  Iteration 1197/7380 Training loss: 5.7040 1.0239 sec/batch\n",
      "Epoch 4/20  Iteration 1198/7380 Training loss: 5.7037 1.0324 sec/batch\n",
      "Epoch 4/20  Iteration 1199/7380 Training loss: 5.7051 1.0261 sec/batch\n",
      "Epoch 4/20  Iteration 1200/7380 Training loss: 5.7047 1.0302 sec/batch\n",
      "Validation loss: 5.61042 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1201/7380 Training loss: 5.7046 1.0366 sec/batch\n",
      "Epoch 4/20  Iteration 1202/7380 Training loss: 5.7044 1.0407 sec/batch\n",
      "Epoch 4/20  Iteration 1203/7380 Training loss: 5.7048 1.0365 sec/batch\n",
      "Epoch 4/20  Iteration 1204/7380 Training loss: 5.7046 1.0464 sec/batch\n",
      "Epoch 4/20  Iteration 1205/7380 Training loss: 5.7061 1.0257 sec/batch\n",
      "Epoch 4/20  Iteration 1206/7380 Training loss: 5.7057 1.0218 sec/batch\n",
      "Epoch 4/20  Iteration 1207/7380 Training loss: 5.7057 1.0327 sec/batch\n",
      "Epoch 4/20  Iteration 1208/7380 Training loss: 5.7072 1.0326 sec/batch\n",
      "Epoch 4/20  Iteration 1209/7380 Training loss: 5.7070 1.0271 sec/batch\n",
      "Epoch 4/20  Iteration 1210/7380 Training loss: 5.7073 1.0312 sec/batch\n",
      "Epoch 4/20  Iteration 1211/7380 Training loss: 5.7070 1.0325 sec/batch\n",
      "Epoch 4/20  Iteration 1212/7380 Training loss: 5.7075 1.0283 sec/batch\n",
      "Epoch 4/20  Iteration 1213/7380 Training loss: 5.7075 1.0333 sec/batch\n",
      "Epoch 4/20  Iteration 1214/7380 Training loss: 5.7076 1.0284 sec/batch\n",
      "Epoch 4/20  Iteration 1215/7380 Training loss: 5.7072 1.0338 sec/batch\n",
      "Epoch 4/20  Iteration 1216/7380 Training loss: 5.7057 1.0417 sec/batch\n",
      "Epoch 4/20  Iteration 1217/7380 Training loss: 5.7053 1.0251 sec/batch\n",
      "Epoch 4/20  Iteration 1218/7380 Training loss: 5.7047 1.0218 sec/batch\n",
      "Epoch 4/20  Iteration 1219/7380 Training loss: 5.7047 1.0301 sec/batch\n",
      "Epoch 4/20  Iteration 1220/7380 Training loss: 5.7038 1.0402 sec/batch\n",
      "Epoch 4/20  Iteration 1221/7380 Training loss: 5.7041 1.0347 sec/batch\n",
      "Epoch 4/20  Iteration 1222/7380 Training loss: 5.7037 1.0295 sec/batch\n",
      "Epoch 4/20  Iteration 1223/7380 Training loss: 5.7032 1.0456 sec/batch\n",
      "Epoch 4/20  Iteration 1224/7380 Training loss: 5.7024 1.0278 sec/batch\n",
      "Epoch 4/20  Iteration 1225/7380 Training loss: 5.7018 1.0317 sec/batch\n",
      "Epoch 4/20  Iteration 1226/7380 Training loss: 5.7018 1.0291 sec/batch\n",
      "Epoch 4/20  Iteration 1227/7380 Training loss: 5.7016 1.0278 sec/batch\n",
      "Epoch 4/20  Iteration 1228/7380 Training loss: 5.7019 1.0258 sec/batch\n",
      "Epoch 4/20  Iteration 1229/7380 Training loss: 5.7033 1.0362 sec/batch\n",
      "Epoch 4/20  Iteration 1230/7380 Training loss: 5.7027 1.0237 sec/batch\n",
      "Epoch 4/20  Iteration 1231/7380 Training loss: 5.7026 1.0356 sec/batch\n",
      "Epoch 4/20  Iteration 1232/7380 Training loss: 5.7020 1.0267 sec/batch\n",
      "Epoch 4/20  Iteration 1233/7380 Training loss: 5.7010 1.0272 sec/batch\n",
      "Epoch 4/20  Iteration 1234/7380 Training loss: 5.6998 1.0311 sec/batch\n",
      "Epoch 4/20  Iteration 1235/7380 Training loss: 5.7000 1.0269 sec/batch\n",
      "Epoch 4/20  Iteration 1236/7380 Training loss: 5.6994 1.0357 sec/batch\n",
      "Epoch 4/20  Iteration 1237/7380 Training loss: 5.6992 1.0272 sec/batch\n",
      "Epoch 4/20  Iteration 1238/7380 Training loss: 5.6991 1.0300 sec/batch\n",
      "Epoch 4/20  Iteration 1239/7380 Training loss: 5.6991 1.0389 sec/batch\n",
      "Epoch 4/20  Iteration 1240/7380 Training loss: 5.6992 1.0372 sec/batch\n",
      "Epoch 4/20  Iteration 1241/7380 Training loss: 5.6983 1.0245 sec/batch\n",
      "Epoch 4/20  Iteration 1242/7380 Training loss: 5.6977 1.0398 sec/batch\n",
      "Epoch 4/20  Iteration 1243/7380 Training loss: 5.6976 1.0305 sec/batch\n",
      "Epoch 4/20  Iteration 1244/7380 Training loss: 5.6974 1.0393 sec/batch\n",
      "Epoch 4/20  Iteration 1245/7380 Training loss: 5.6967 1.0258 sec/batch\n",
      "Epoch 4/20  Iteration 1246/7380 Training loss: 5.6958 1.0349 sec/batch\n",
      "Epoch 4/20  Iteration 1247/7380 Training loss: 5.6946 1.0347 sec/batch\n",
      "Epoch 4/20  Iteration 1248/7380 Training loss: 5.6958 1.0389 sec/batch\n",
      "Epoch 4/20  Iteration 1249/7380 Training loss: 5.6954 1.0521 sec/batch\n",
      "Epoch 4/20  Iteration 1250/7380 Training loss: 5.6954 1.0270 sec/batch\n",
      "Validation loss: 5.58137 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1251/7380 Training loss: 5.6954 1.0315 sec/batch\n",
      "Epoch 4/20  Iteration 1252/7380 Training loss: 5.6952 1.0383 sec/batch\n",
      "Epoch 4/20  Iteration 1253/7380 Training loss: 5.6950 1.0344 sec/batch\n",
      "Epoch 4/20  Iteration 1254/7380 Training loss: 5.6949 1.0441 sec/batch\n",
      "Epoch 4/20  Iteration 1255/7380 Training loss: 5.6945 1.0280 sec/batch\n",
      "Epoch 4/20  Iteration 1256/7380 Training loss: 5.6936 1.0313 sec/batch\n",
      "Epoch 4/20  Iteration 1257/7380 Training loss: 5.6938 1.0397 sec/batch\n",
      "Epoch 4/20  Iteration 1258/7380 Training loss: 5.6932 1.0255 sec/batch\n",
      "Epoch 4/20  Iteration 1259/7380 Training loss: 5.6935 1.0380 sec/batch\n",
      "Epoch 4/20  Iteration 1260/7380 Training loss: 5.6935 1.0241 sec/batch\n",
      "Epoch 4/20  Iteration 1261/7380 Training loss: 5.6939 1.0425 sec/batch\n",
      "Epoch 4/20  Iteration 1262/7380 Training loss: 5.6936 1.0260 sec/batch\n",
      "Epoch 4/20  Iteration 1263/7380 Training loss: 5.6930 1.0270 sec/batch\n",
      "Epoch 4/20  Iteration 1264/7380 Training loss: 5.6920 1.0316 sec/batch\n",
      "Epoch 4/20  Iteration 1265/7380 Training loss: 5.6922 1.0299 sec/batch\n",
      "Epoch 4/20  Iteration 1266/7380 Training loss: 5.6917 1.0270 sec/batch\n",
      "Epoch 4/20  Iteration 1267/7380 Training loss: 5.6916 1.0276 sec/batch\n",
      "Epoch 4/20  Iteration 1268/7380 Training loss: 5.6919 1.0265 sec/batch\n",
      "Epoch 4/20  Iteration 1269/7380 Training loss: 5.6920 1.0270 sec/batch\n",
      "Epoch 4/20  Iteration 1270/7380 Training loss: 5.6922 1.0266 sec/batch\n",
      "Epoch 4/20  Iteration 1271/7380 Training loss: 5.6925 1.0416 sec/batch\n",
      "Epoch 4/20  Iteration 1272/7380 Training loss: 5.6926 1.0253 sec/batch\n",
      "Epoch 4/20  Iteration 1273/7380 Training loss: 5.6926 1.0297 sec/batch\n",
      "Epoch 4/20  Iteration 1274/7380 Training loss: 5.6925 1.0260 sec/batch\n",
      "Epoch 4/20  Iteration 1275/7380 Training loss: 5.6928 1.0356 sec/batch\n",
      "Epoch 4/20  Iteration 1276/7380 Training loss: 5.6921 1.0474 sec/batch\n",
      "Epoch 4/20  Iteration 1277/7380 Training loss: 5.6915 1.0277 sec/batch\n",
      "Epoch 4/20  Iteration 1278/7380 Training loss: 5.6914 1.0364 sec/batch\n",
      "Epoch 4/20  Iteration 1279/7380 Training loss: 5.6917 1.0272 sec/batch\n",
      "Epoch 4/20  Iteration 1280/7380 Training loss: 5.6917 1.0294 sec/batch\n",
      "Epoch 4/20  Iteration 1281/7380 Training loss: 5.6913 1.0357 sec/batch\n",
      "Epoch 4/20  Iteration 1282/7380 Training loss: 5.6909 1.0332 sec/batch\n",
      "Epoch 4/20  Iteration 1283/7380 Training loss: 5.6906 1.0355 sec/batch\n",
      "Epoch 4/20  Iteration 1284/7380 Training loss: 5.6902 1.0309 sec/batch\n",
      "Epoch 4/20  Iteration 1285/7380 Training loss: 5.6903 1.0297 sec/batch\n",
      "Epoch 4/20  Iteration 1286/7380 Training loss: 5.6895 1.0337 sec/batch\n",
      "Epoch 4/20  Iteration 1287/7380 Training loss: 5.6887 1.0310 sec/batch\n",
      "Epoch 4/20  Iteration 1288/7380 Training loss: 5.6884 1.0360 sec/batch\n",
      "Epoch 4/20  Iteration 1289/7380 Training loss: 5.6883 1.0470 sec/batch\n",
      "Epoch 4/20  Iteration 1290/7380 Training loss: 5.6888 1.0406 sec/batch\n",
      "Epoch 4/20  Iteration 1291/7380 Training loss: 5.6884 1.0302 sec/batch\n",
      "Epoch 4/20  Iteration 1292/7380 Training loss: 5.6881 1.0285 sec/batch\n",
      "Epoch 4/20  Iteration 1293/7380 Training loss: 5.6878 1.0274 sec/batch\n",
      "Epoch 4/20  Iteration 1294/7380 Training loss: 5.6873 1.0334 sec/batch\n",
      "Epoch 4/20  Iteration 1295/7380 Training loss: 5.6870 1.0315 sec/batch\n",
      "Epoch 4/20  Iteration 1296/7380 Training loss: 5.6864 1.0368 sec/batch\n",
      "Epoch 4/20  Iteration 1297/7380 Training loss: 5.6864 1.0309 sec/batch\n",
      "Epoch 4/20  Iteration 1298/7380 Training loss: 5.6855 1.0323 sec/batch\n",
      "Epoch 4/20  Iteration 1299/7380 Training loss: 5.6860 1.0264 sec/batch\n",
      "Epoch 4/20  Iteration 1300/7380 Training loss: 5.6863 1.0287 sec/batch\n",
      "Validation loss: 5.56075 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1301/7380 Training loss: 5.6858 1.0345 sec/batch\n",
      "Epoch 4/20  Iteration 1302/7380 Training loss: 5.6859 1.0287 sec/batch\n",
      "Epoch 4/20  Iteration 1303/7380 Training loss: 5.6858 1.0272 sec/batch\n",
      "Epoch 4/20  Iteration 1304/7380 Training loss: 5.6856 1.0261 sec/batch\n",
      "Epoch 4/20  Iteration 1305/7380 Training loss: 5.6861 1.0293 sec/batch\n",
      "Epoch 4/20  Iteration 1306/7380 Training loss: 5.6859 1.0323 sec/batch\n",
      "Epoch 4/20  Iteration 1307/7380 Training loss: 5.6859 1.0243 sec/batch\n",
      "Epoch 4/20  Iteration 1308/7380 Training loss: 5.6864 1.0462 sec/batch\n",
      "Epoch 4/20  Iteration 1309/7380 Training loss: 5.6865 1.0313 sec/batch\n",
      "Epoch 4/20  Iteration 1310/7380 Training loss: 5.6867 1.0231 sec/batch\n",
      "Epoch 4/20  Iteration 1311/7380 Training loss: 5.6866 1.0430 sec/batch\n",
      "Epoch 4/20  Iteration 1312/7380 Training loss: 5.6867 1.0368 sec/batch\n",
      "Epoch 4/20  Iteration 1313/7380 Training loss: 5.6869 1.0256 sec/batch\n",
      "Epoch 4/20  Iteration 1314/7380 Training loss: 5.6859 1.0345 sec/batch\n",
      "Epoch 4/20  Iteration 1315/7380 Training loss: 5.6854 1.0376 sec/batch\n",
      "Epoch 4/20  Iteration 1316/7380 Training loss: 5.6847 1.0507 sec/batch\n",
      "Epoch 4/20  Iteration 1317/7380 Training loss: 5.6842 1.0393 sec/batch\n",
      "Epoch 4/20  Iteration 1318/7380 Training loss: 5.6840 1.0285 sec/batch\n",
      "Epoch 4/20  Iteration 1319/7380 Training loss: 5.6837 1.0374 sec/batch\n",
      "Epoch 4/20  Iteration 1320/7380 Training loss: 5.6836 1.0347 sec/batch\n",
      "Epoch 4/20  Iteration 1321/7380 Training loss: 5.6831 1.0481 sec/batch\n",
      "Epoch 4/20  Iteration 1322/7380 Training loss: 5.6827 1.0288 sec/batch\n",
      "Epoch 4/20  Iteration 1323/7380 Training loss: 5.6821 1.0292 sec/batch\n",
      "Epoch 4/20  Iteration 1324/7380 Training loss: 5.6814 1.0455 sec/batch\n",
      "Epoch 4/20  Iteration 1325/7380 Training loss: 5.6811 1.0435 sec/batch\n",
      "Epoch 4/20  Iteration 1326/7380 Training loss: 5.6814 1.0346 sec/batch\n",
      "Epoch 4/20  Iteration 1327/7380 Training loss: 5.6815 1.0317 sec/batch\n",
      "Epoch 4/20  Iteration 1328/7380 Training loss: 5.6810 1.0314 sec/batch\n",
      "Epoch 4/20  Iteration 1329/7380 Training loss: 5.6814 1.0360 sec/batch\n",
      "Epoch 4/20  Iteration 1330/7380 Training loss: 5.6812 1.0321 sec/batch\n",
      "Epoch 4/20  Iteration 1331/7380 Training loss: 5.6807 1.0278 sec/batch\n",
      "Epoch 4/20  Iteration 1332/7380 Training loss: 5.6807 1.0306 sec/batch\n",
      "Epoch 4/20  Iteration 1333/7380 Training loss: 5.6802 1.0298 sec/batch\n",
      "Epoch 4/20  Iteration 1334/7380 Training loss: 5.6798 1.0408 sec/batch\n",
      "Epoch 4/20  Iteration 1335/7380 Training loss: 5.6791 1.0241 sec/batch\n",
      "Epoch 4/20  Iteration 1336/7380 Training loss: 5.6790 1.0394 sec/batch\n",
      "Epoch 4/20  Iteration 1337/7380 Training loss: 5.6792 1.0380 sec/batch\n",
      "Epoch 4/20  Iteration 1338/7380 Training loss: 5.6787 1.0462 sec/batch\n",
      "Epoch 4/20  Iteration 1339/7380 Training loss: 5.6782 1.0283 sec/batch\n",
      "Epoch 4/20  Iteration 1340/7380 Training loss: 5.6776 1.0358 sec/batch\n",
      "Epoch 4/20  Iteration 1341/7380 Training loss: 5.6768 1.0249 sec/batch\n",
      "Epoch 4/20  Iteration 1342/7380 Training loss: 5.6764 1.0345 sec/batch\n",
      "Epoch 4/20  Iteration 1343/7380 Training loss: 5.6759 1.0274 sec/batch\n",
      "Epoch 4/20  Iteration 1344/7380 Training loss: 5.6754 1.0299 sec/batch\n",
      "Epoch 4/20  Iteration 1345/7380 Training loss: 5.6746 1.0364 sec/batch\n",
      "Epoch 4/20  Iteration 1346/7380 Training loss: 5.6734 1.0288 sec/batch\n",
      "Epoch 4/20  Iteration 1347/7380 Training loss: 5.6726 1.0410 sec/batch\n",
      "Epoch 4/20  Iteration 1348/7380 Training loss: 5.6726 1.0323 sec/batch\n",
      "Epoch 4/20  Iteration 1349/7380 Training loss: 5.6731 1.0369 sec/batch\n",
      "Epoch 4/20  Iteration 1350/7380 Training loss: 5.6726 1.0287 sec/batch\n",
      "Validation loss: 5.53761 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1351/7380 Training loss: 5.6727 1.0383 sec/batch\n",
      "Epoch 4/20  Iteration 1352/7380 Training loss: 5.6728 1.0341 sec/batch\n",
      "Epoch 4/20  Iteration 1353/7380 Training loss: 5.6729 1.0444 sec/batch\n",
      "Epoch 4/20  Iteration 1354/7380 Training loss: 5.6728 1.0324 sec/batch\n",
      "Epoch 4/20  Iteration 1355/7380 Training loss: 5.6727 1.0456 sec/batch\n",
      "Epoch 4/20  Iteration 1356/7380 Training loss: 5.6723 1.0292 sec/batch\n",
      "Epoch 4/20  Iteration 1357/7380 Training loss: 5.6722 1.0393 sec/batch\n",
      "Epoch 4/20  Iteration 1358/7380 Training loss: 5.6716 1.0291 sec/batch\n",
      "Epoch 4/20  Iteration 1359/7380 Training loss: 5.6714 1.0546 sec/batch\n",
      "Epoch 4/20  Iteration 1360/7380 Training loss: 5.6712 1.0296 sec/batch\n",
      "Epoch 4/20  Iteration 1361/7380 Training loss: 5.6706 1.0414 sec/batch\n",
      "Epoch 4/20  Iteration 1362/7380 Training loss: 5.6707 1.0320 sec/batch\n",
      "Epoch 4/20  Iteration 1363/7380 Training loss: 5.6706 1.0355 sec/batch\n",
      "Epoch 4/20  Iteration 1364/7380 Training loss: 5.6706 1.0433 sec/batch\n",
      "Epoch 4/20  Iteration 1365/7380 Training loss: 5.6701 1.0251 sec/batch\n",
      "Epoch 4/20  Iteration 1366/7380 Training loss: 5.6704 1.0263 sec/batch\n",
      "Epoch 4/20  Iteration 1367/7380 Training loss: 5.6698 1.0396 sec/batch\n",
      "Epoch 4/20  Iteration 1368/7380 Training loss: 5.6696 1.0388 sec/batch\n",
      "Epoch 4/20  Iteration 1369/7380 Training loss: 5.6695 1.0371 sec/batch\n",
      "Epoch 4/20  Iteration 1370/7380 Training loss: 5.6697 1.0291 sec/batch\n",
      "Epoch 4/20  Iteration 1371/7380 Training loss: 5.6693 1.0255 sec/batch\n",
      "Epoch 4/20  Iteration 1372/7380 Training loss: 5.6688 1.0279 sec/batch\n",
      "Epoch 4/20  Iteration 1373/7380 Training loss: 5.6685 1.0345 sec/batch\n",
      "Epoch 4/20  Iteration 1374/7380 Training loss: 5.6685 1.0249 sec/batch\n",
      "Epoch 4/20  Iteration 1375/7380 Training loss: 5.6683 1.0358 sec/batch\n",
      "Epoch 4/20  Iteration 1376/7380 Training loss: 5.6675 1.0263 sec/batch\n",
      "Epoch 4/20  Iteration 1377/7380 Training loss: 5.6676 1.0369 sec/batch\n",
      "Epoch 4/20  Iteration 1378/7380 Training loss: 5.6675 1.0374 sec/batch\n",
      "Epoch 4/20  Iteration 1379/7380 Training loss: 5.6667 1.0433 sec/batch\n",
      "Epoch 4/20  Iteration 1380/7380 Training loss: 5.6667 1.0308 sec/batch\n",
      "Epoch 4/20  Iteration 1381/7380 Training loss: 5.6667 1.0461 sec/batch\n",
      "Epoch 4/20  Iteration 1382/7380 Training loss: 5.6664 1.0342 sec/batch\n",
      "Epoch 4/20  Iteration 1383/7380 Training loss: 5.6660 1.0271 sec/batch\n",
      "Epoch 4/20  Iteration 1384/7380 Training loss: 5.6665 1.0374 sec/batch\n",
      "Epoch 4/20  Iteration 1385/7380 Training loss: 5.6664 1.0234 sec/batch\n",
      "Epoch 4/20  Iteration 1386/7380 Training loss: 5.6664 1.0292 sec/batch\n",
      "Epoch 4/20  Iteration 1387/7380 Training loss: 5.6659 1.0273 sec/batch\n",
      "Epoch 4/20  Iteration 1388/7380 Training loss: 5.6658 1.0354 sec/batch\n",
      "Epoch 4/20  Iteration 1389/7380 Training loss: 5.6657 1.0262 sec/batch\n",
      "Epoch 4/20  Iteration 1390/7380 Training loss: 5.6659 1.0293 sec/batch\n",
      "Epoch 4/20  Iteration 1391/7380 Training loss: 5.6658 1.0289 sec/batch\n",
      "Epoch 4/20  Iteration 1392/7380 Training loss: 5.6659 1.0270 sec/batch\n",
      "Epoch 4/20  Iteration 1393/7380 Training loss: 5.6654 1.0301 sec/batch\n",
      "Epoch 4/20  Iteration 1394/7380 Training loss: 5.6649 1.0315 sec/batch\n",
      "Epoch 4/20  Iteration 1395/7380 Training loss: 5.6647 1.0263 sec/batch\n",
      "Epoch 4/20  Iteration 1396/7380 Training loss: 5.6643 1.0297 sec/batch\n",
      "Epoch 4/20  Iteration 1397/7380 Training loss: 5.6640 1.0299 sec/batch\n",
      "Epoch 4/20  Iteration 1398/7380 Training loss: 5.6635 1.0310 sec/batch\n",
      "Epoch 4/20  Iteration 1399/7380 Training loss: 5.6633 1.0268 sec/batch\n",
      "Epoch 4/20  Iteration 1400/7380 Training loss: 5.6631 1.0469 sec/batch\n",
      "Validation loss: 5.51852 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1401/7380 Training loss: 5.6631 1.0331 sec/batch\n",
      "Epoch 4/20  Iteration 1402/7380 Training loss: 5.6632 1.0342 sec/batch\n",
      "Epoch 4/20  Iteration 1403/7380 Training loss: 5.6630 1.0296 sec/batch\n",
      "Epoch 4/20  Iteration 1404/7380 Training loss: 5.6626 1.0414 sec/batch\n",
      "Epoch 4/20  Iteration 1405/7380 Training loss: 5.6624 1.0383 sec/batch\n",
      "Epoch 4/20  Iteration 1406/7380 Training loss: 5.6624 1.0296 sec/batch\n",
      "Epoch 4/20  Iteration 1407/7380 Training loss: 5.6628 1.0258 sec/batch\n",
      "Epoch 4/20  Iteration 1408/7380 Training loss: 5.6626 1.0252 sec/batch\n",
      "Epoch 4/20  Iteration 1409/7380 Training loss: 5.6627 1.0395 sec/batch\n",
      "Epoch 4/20  Iteration 1410/7380 Training loss: 5.6625 1.0291 sec/batch\n",
      "Epoch 4/20  Iteration 1411/7380 Training loss: 5.6629 1.0285 sec/batch\n",
      "Epoch 4/20  Iteration 1412/7380 Training loss: 5.6630 1.0311 sec/batch\n",
      "Epoch 4/20  Iteration 1413/7380 Training loss: 5.6627 1.0268 sec/batch\n",
      "Epoch 4/20  Iteration 1414/7380 Training loss: 5.6621 1.0263 sec/batch\n",
      "Epoch 4/20  Iteration 1415/7380 Training loss: 5.6616 1.0283 sec/batch\n",
      "Epoch 4/20  Iteration 1416/7380 Training loss: 5.6613 1.0311 sec/batch\n",
      "Epoch 4/20  Iteration 1417/7380 Training loss: 5.6606 1.0268 sec/batch\n",
      "Epoch 4/20  Iteration 1418/7380 Training loss: 5.6604 1.0254 sec/batch\n",
      "Epoch 4/20  Iteration 1419/7380 Training loss: 5.6597 1.0364 sec/batch\n",
      "Epoch 4/20  Iteration 1420/7380 Training loss: 5.6594 1.0399 sec/batch\n",
      "Epoch 4/20  Iteration 1421/7380 Training loss: 5.6592 1.0386 sec/batch\n",
      "Epoch 4/20  Iteration 1422/7380 Training loss: 5.6591 1.0363 sec/batch\n",
      "Epoch 4/20  Iteration 1423/7380 Training loss: 5.6592 1.0415 sec/batch\n",
      "Epoch 4/20  Iteration 1424/7380 Training loss: 5.6593 1.0337 sec/batch\n",
      "Epoch 4/20  Iteration 1425/7380 Training loss: 5.6592 1.0282 sec/batch\n",
      "Epoch 4/20  Iteration 1426/7380 Training loss: 5.6596 1.0280 sec/batch\n",
      "Epoch 4/20  Iteration 1427/7380 Training loss: 5.6597 1.0318 sec/batch\n",
      "Epoch 4/20  Iteration 1428/7380 Training loss: 5.6592 1.0267 sec/batch\n",
      "Epoch 4/20  Iteration 1429/7380 Training loss: 5.6589 1.0294 sec/batch\n",
      "Epoch 4/20  Iteration 1430/7380 Training loss: 5.6587 1.0226 sec/batch\n",
      "Epoch 4/20  Iteration 1431/7380 Training loss: 5.6584 1.0227 sec/batch\n",
      "Epoch 4/20  Iteration 1432/7380 Training loss: 5.6584 1.0309 sec/batch\n",
      "Epoch 4/20  Iteration 1433/7380 Training loss: 5.6585 1.0250 sec/batch\n",
      "Epoch 4/20  Iteration 1434/7380 Training loss: 5.6580 1.0257 sec/batch\n",
      "Epoch 4/20  Iteration 1435/7380 Training loss: 5.6574 1.0317 sec/batch\n",
      "Epoch 4/20  Iteration 1436/7380 Training loss: 5.6571 1.0392 sec/batch\n",
      "Epoch 4/20  Iteration 1437/7380 Training loss: 5.6566 1.0366 sec/batch\n",
      "Epoch 4/20  Iteration 1438/7380 Training loss: 5.6561 1.0375 sec/batch\n",
      "Epoch 4/20  Iteration 1439/7380 Training loss: 5.6561 1.0252 sec/batch\n",
      "Epoch 4/20  Iteration 1440/7380 Training loss: 5.6556 1.0468 sec/batch\n",
      "Epoch 4/20  Iteration 1441/7380 Training loss: 5.6555 1.0349 sec/batch\n",
      "Epoch 4/20  Iteration 1442/7380 Training loss: 5.6554 1.0256 sec/batch\n",
      "Epoch 4/20  Iteration 1443/7380 Training loss: 5.6554 1.0298 sec/batch\n",
      "Epoch 4/20  Iteration 1444/7380 Training loss: 5.6554 1.0367 sec/batch\n",
      "Epoch 4/20  Iteration 1445/7380 Training loss: 5.6551 1.0290 sec/batch\n",
      "Epoch 4/20  Iteration 1446/7380 Training loss: 5.6549 1.0249 sec/batch\n",
      "Epoch 4/20  Iteration 1447/7380 Training loss: 5.6543 1.0340 sec/batch\n",
      "Epoch 4/20  Iteration 1448/7380 Training loss: 5.6537 1.0308 sec/batch\n",
      "Epoch 4/20  Iteration 1449/7380 Training loss: 5.6533 1.0490 sec/batch\n",
      "Epoch 4/20  Iteration 1450/7380 Training loss: 5.6531 1.0434 sec/batch\n",
      "Validation loss: 5.49467 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1451/7380 Training loss: 5.6529 1.0385 sec/batch\n",
      "Epoch 4/20  Iteration 1452/7380 Training loss: 5.6526 1.0313 sec/batch\n",
      "Epoch 4/20  Iteration 1453/7380 Training loss: 5.6524 1.0374 sec/batch\n",
      "Epoch 4/20  Iteration 1454/7380 Training loss: 5.6524 1.0288 sec/batch\n",
      "Epoch 4/20  Iteration 1455/7380 Training loss: 5.6523 1.0302 sec/batch\n",
      "Epoch 4/20  Iteration 1456/7380 Training loss: 5.6520 1.0399 sec/batch\n",
      "Epoch 4/20  Iteration 1457/7380 Training loss: 5.6517 1.0283 sec/batch\n",
      "Epoch 4/20  Iteration 1458/7380 Training loss: 5.6521 1.0281 sec/batch\n",
      "Epoch 4/20  Iteration 1459/7380 Training loss: 5.6516 1.0316 sec/batch\n",
      "Epoch 4/20  Iteration 1460/7380 Training loss: 5.6514 1.0560 sec/batch\n",
      "Epoch 4/20  Iteration 1461/7380 Training loss: 5.6509 1.0306 sec/batch\n",
      "Epoch 4/20  Iteration 1462/7380 Training loss: 5.6505 1.0271 sec/batch\n",
      "Epoch 4/20  Iteration 1463/7380 Training loss: 5.6501 1.0331 sec/batch\n",
      "Epoch 4/20  Iteration 1464/7380 Training loss: 5.6501 1.0336 sec/batch\n",
      "Epoch 4/20  Iteration 1465/7380 Training loss: 5.6498 1.0312 sec/batch\n",
      "Epoch 4/20  Iteration 1466/7380 Training loss: 5.6493 1.0356 sec/batch\n",
      "Epoch 4/20  Iteration 1467/7380 Training loss: 5.6489 1.0354 sec/batch\n",
      "Epoch 4/20  Iteration 1468/7380 Training loss: 5.6487 1.0270 sec/batch\n",
      "Epoch 4/20  Iteration 1469/7380 Training loss: 5.6486 1.0295 sec/batch\n",
      "Epoch 4/20  Iteration 1470/7380 Training loss: 5.6483 1.0386 sec/batch\n",
      "Epoch 4/20  Iteration 1471/7380 Training loss: 5.6479 1.0282 sec/batch\n",
      "Epoch 4/20  Iteration 1472/7380 Training loss: 5.6475 1.0494 sec/batch\n",
      "Epoch 4/20  Iteration 1473/7380 Training loss: 5.6471 1.0328 sec/batch\n",
      "Epoch 4/20  Iteration 1474/7380 Training loss: 5.6467 1.0266 sec/batch\n",
      "Epoch 4/20  Iteration 1475/7380 Training loss: 5.6465 1.0497 sec/batch\n",
      "Epoch 4/20  Iteration 1476/7380 Training loss: 5.6461 1.0512 sec/batch\n",
      "Epoch 5/20  Iteration 1477/7380 Training loss: 5.7980 1.0095 sec/batch\n",
      "Epoch 5/20  Iteration 1478/7380 Training loss: 5.7159 1.0309 sec/batch\n",
      "Epoch 5/20  Iteration 1479/7380 Training loss: 5.6585 1.0370 sec/batch\n",
      "Epoch 5/20  Iteration 1480/7380 Training loss: 5.6294 1.0384 sec/batch\n",
      "Epoch 5/20  Iteration 1481/7380 Training loss: 5.6096 1.0483 sec/batch\n",
      "Epoch 5/20  Iteration 1482/7380 Training loss: 5.6139 1.0283 sec/batch\n",
      "Epoch 5/20  Iteration 1483/7380 Training loss: 5.6245 1.0356 sec/batch\n",
      "Epoch 5/20  Iteration 1484/7380 Training loss: 5.6077 1.0265 sec/batch\n",
      "Epoch 5/20  Iteration 1485/7380 Training loss: 5.5980 1.0418 sec/batch\n",
      "Epoch 5/20  Iteration 1486/7380 Training loss: 5.6054 1.0227 sec/batch\n",
      "Epoch 5/20  Iteration 1487/7380 Training loss: 5.6009 1.0263 sec/batch\n",
      "Epoch 5/20  Iteration 1488/7380 Training loss: 5.5875 1.0309 sec/batch\n",
      "Epoch 5/20  Iteration 1489/7380 Training loss: 5.5842 1.0379 sec/batch\n",
      "Epoch 5/20  Iteration 1490/7380 Training loss: 5.5879 1.0275 sec/batch\n",
      "Epoch 5/20  Iteration 1491/7380 Training loss: 5.5915 1.0235 sec/batch\n",
      "Epoch 5/20  Iteration 1492/7380 Training loss: 5.5876 1.0233 sec/batch\n",
      "Epoch 5/20  Iteration 1493/7380 Training loss: 5.5820 1.0346 sec/batch\n",
      "Epoch 5/20  Iteration 1494/7380 Training loss: 5.5682 1.0305 sec/batch\n",
      "Epoch 5/20  Iteration 1495/7380 Training loss: 5.5594 1.0577 sec/batch\n",
      "Epoch 5/20  Iteration 1496/7380 Training loss: 5.5571 1.0346 sec/batch\n",
      "Epoch 5/20  Iteration 1497/7380 Training loss: 5.5524 1.0314 sec/batch\n",
      "Epoch 5/20  Iteration 1498/7380 Training loss: 5.5585 1.0374 sec/batch\n",
      "Epoch 5/20  Iteration 1499/7380 Training loss: 5.5577 1.0278 sec/batch\n",
      "Epoch 5/20  Iteration 1500/7380 Training loss: 5.5644 1.0486 sec/batch\n",
      "Validation loss: 5.4871 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1501/7380 Training loss: 5.5688 1.0343 sec/batch\n",
      "Epoch 5/20  Iteration 1502/7380 Training loss: 5.5700 1.0344 sec/batch\n",
      "Epoch 5/20  Iteration 1503/7380 Training loss: 5.5639 1.0360 sec/batch\n",
      "Epoch 5/20  Iteration 1504/7380 Training loss: 5.5684 1.0746 sec/batch\n",
      "Epoch 5/20  Iteration 1505/7380 Training loss: 5.5679 1.0344 sec/batch\n",
      "Epoch 5/20  Iteration 1506/7380 Training loss: 5.5681 1.0293 sec/batch\n",
      "Epoch 5/20  Iteration 1507/7380 Training loss: 5.5660 1.0342 sec/batch\n",
      "Epoch 5/20  Iteration 1508/7380 Training loss: 5.5663 1.0279 sec/batch\n",
      "Epoch 5/20  Iteration 1509/7380 Training loss: 5.5659 1.0286 sec/batch\n",
      "Epoch 5/20  Iteration 1510/7380 Training loss: 5.5664 1.0277 sec/batch\n",
      "Epoch 5/20  Iteration 1511/7380 Training loss: 5.5696 1.0295 sec/batch\n",
      "Epoch 5/20  Iteration 1512/7380 Training loss: 5.5675 1.0335 sec/batch\n",
      "Epoch 5/20  Iteration 1513/7380 Training loss: 5.5674 1.0367 sec/batch\n",
      "Epoch 5/20  Iteration 1514/7380 Training loss: 5.5668 1.0386 sec/batch\n",
      "Epoch 5/20  Iteration 1515/7380 Training loss: 5.5684 1.0266 sec/batch\n",
      "Epoch 5/20  Iteration 1516/7380 Training loss: 5.5671 1.0331 sec/batch\n",
      "Epoch 5/20  Iteration 1517/7380 Training loss: 5.5668 1.0536 sec/batch\n",
      "Epoch 5/20  Iteration 1518/7380 Training loss: 5.5644 1.0311 sec/batch\n",
      "Epoch 5/20  Iteration 1519/7380 Training loss: 5.5642 1.0293 sec/batch\n",
      "Epoch 5/20  Iteration 1520/7380 Training loss: 5.5663 1.0312 sec/batch\n",
      "Epoch 5/20  Iteration 1521/7380 Training loss: 5.5668 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1522/7380 Training loss: 5.5647 1.0370 sec/batch\n",
      "Epoch 5/20  Iteration 1523/7380 Training loss: 5.5655 1.0472 sec/batch\n",
      "Epoch 5/20  Iteration 1524/7380 Training loss: 5.5625 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1525/7380 Training loss: 5.5634 1.0350 sec/batch\n",
      "Epoch 5/20  Iteration 1526/7380 Training loss: 5.5617 1.0279 sec/batch\n",
      "Epoch 5/20  Iteration 1527/7380 Training loss: 5.5605 1.0294 sec/batch\n",
      "Epoch 5/20  Iteration 1528/7380 Training loss: 5.5600 1.0326 sec/batch\n",
      "Epoch 5/20  Iteration 1529/7380 Training loss: 5.5583 1.0440 sec/batch\n",
      "Epoch 5/20  Iteration 1530/7380 Training loss: 5.5574 1.0365 sec/batch\n",
      "Epoch 5/20  Iteration 1531/7380 Training loss: 5.5562 1.0342 sec/batch\n",
      "Epoch 5/20  Iteration 1532/7380 Training loss: 5.5543 1.0293 sec/batch\n",
      "Epoch 5/20  Iteration 1533/7380 Training loss: 5.5532 1.0397 sec/batch\n",
      "Epoch 5/20  Iteration 1534/7380 Training loss: 5.5538 1.0317 sec/batch\n",
      "Epoch 5/20  Iteration 1535/7380 Training loss: 5.5531 1.0328 sec/batch\n",
      "Epoch 5/20  Iteration 1536/7380 Training loss: 5.5537 1.0441 sec/batch\n",
      "Epoch 5/20  Iteration 1537/7380 Training loss: 5.5521 1.0315 sec/batch\n",
      "Epoch 5/20  Iteration 1538/7380 Training loss: 5.5527 1.0287 sec/batch\n",
      "Epoch 5/20  Iteration 1539/7380 Training loss: 5.5510 1.0340 sec/batch\n",
      "Epoch 5/20  Iteration 1540/7380 Training loss: 5.5496 1.0312 sec/batch\n",
      "Epoch 5/20  Iteration 1541/7380 Training loss: 5.5498 1.0318 sec/batch\n",
      "Epoch 5/20  Iteration 1542/7380 Training loss: 5.5494 1.0363 sec/batch\n",
      "Epoch 5/20  Iteration 1543/7380 Training loss: 5.5499 1.0316 sec/batch\n",
      "Epoch 5/20  Iteration 1544/7380 Training loss: 5.5510 1.0305 sec/batch\n",
      "Epoch 5/20  Iteration 1545/7380 Training loss: 5.5522 1.0306 sec/batch\n",
      "Epoch 5/20  Iteration 1546/7380 Training loss: 5.5513 1.0284 sec/batch\n",
      "Epoch 5/20  Iteration 1547/7380 Training loss: 5.5495 1.0378 sec/batch\n",
      "Epoch 5/20  Iteration 1548/7380 Training loss: 5.5507 1.0375 sec/batch\n",
      "Epoch 5/20  Iteration 1549/7380 Training loss: 5.5485 1.0329 sec/batch\n",
      "Epoch 5/20  Iteration 1550/7380 Training loss: 5.5498 1.0382 sec/batch\n",
      "Validation loss: 5.45917 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1551/7380 Training loss: 5.5512 1.0369 sec/batch\n",
      "Epoch 5/20  Iteration 1552/7380 Training loss: 5.5503 1.0468 sec/batch\n",
      "Epoch 5/20  Iteration 1553/7380 Training loss: 5.5502 1.0357 sec/batch\n",
      "Epoch 5/20  Iteration 1554/7380 Training loss: 5.5500 1.0435 sec/batch\n",
      "Epoch 5/20  Iteration 1555/7380 Training loss: 5.5511 1.0289 sec/batch\n",
      "Epoch 5/20  Iteration 1556/7380 Training loss: 5.5513 1.0398 sec/batch\n",
      "Epoch 5/20  Iteration 1557/7380 Training loss: 5.5509 1.0396 sec/batch\n",
      "Epoch 5/20  Iteration 1558/7380 Training loss: 5.5502 1.0285 sec/batch\n",
      "Epoch 5/20  Iteration 1559/7380 Training loss: 5.5488 1.0312 sec/batch\n",
      "Epoch 5/20  Iteration 1560/7380 Training loss: 5.5476 1.0304 sec/batch\n",
      "Epoch 5/20  Iteration 1561/7380 Training loss: 5.5481 1.0305 sec/batch\n",
      "Epoch 5/20  Iteration 1562/7380 Training loss: 5.5474 1.0382 sec/batch\n",
      "Epoch 5/20  Iteration 1563/7380 Training loss: 5.5469 1.0289 sec/batch\n",
      "Epoch 5/20  Iteration 1564/7380 Training loss: 5.5463 1.0482 sec/batch\n",
      "Epoch 5/20  Iteration 1565/7380 Training loss: 5.5441 1.0368 sec/batch\n",
      "Epoch 5/20  Iteration 1566/7380 Training loss: 5.5440 1.0414 sec/batch\n",
      "Epoch 5/20  Iteration 1567/7380 Training loss: 5.5439 1.0370 sec/batch\n",
      "Epoch 5/20  Iteration 1568/7380 Training loss: 5.5457 1.0306 sec/batch\n",
      "Epoch 5/20  Iteration 1569/7380 Training loss: 5.5452 1.0308 sec/batch\n",
      "Epoch 5/20  Iteration 1570/7380 Training loss: 5.5452 1.0416 sec/batch\n",
      "Epoch 5/20  Iteration 1571/7380 Training loss: 5.5449 1.0386 sec/batch\n",
      "Epoch 5/20  Iteration 1572/7380 Training loss: 5.5454 1.0335 sec/batch\n",
      "Epoch 5/20  Iteration 1573/7380 Training loss: 5.5449 1.0578 sec/batch\n",
      "Epoch 5/20  Iteration 1574/7380 Training loss: 5.5461 1.0307 sec/batch\n",
      "Epoch 5/20  Iteration 1575/7380 Training loss: 5.5456 1.0288 sec/batch\n",
      "Epoch 5/20  Iteration 1576/7380 Training loss: 5.5452 1.0403 sec/batch\n",
      "Epoch 5/20  Iteration 1577/7380 Training loss: 5.5466 1.0428 sec/batch\n",
      "Epoch 5/20  Iteration 1578/7380 Training loss: 5.5468 1.0327 sec/batch\n",
      "Epoch 5/20  Iteration 1579/7380 Training loss: 5.5473 1.0274 sec/batch\n",
      "Epoch 5/20  Iteration 1580/7380 Training loss: 5.5468 1.0382 sec/batch\n",
      "Epoch 5/20  Iteration 1581/7380 Training loss: 5.5472 1.0308 sec/batch\n",
      "Epoch 5/20  Iteration 1582/7380 Training loss: 5.5471 1.0307 sec/batch\n",
      "Epoch 5/20  Iteration 1583/7380 Training loss: 5.5469 1.0364 sec/batch\n",
      "Epoch 5/20  Iteration 1584/7380 Training loss: 5.5463 1.0297 sec/batch\n",
      "Epoch 5/20  Iteration 1585/7380 Training loss: 5.5448 1.0259 sec/batch\n",
      "Epoch 5/20  Iteration 1586/7380 Training loss: 5.5447 1.0385 sec/batch\n",
      "Epoch 5/20  Iteration 1587/7380 Training loss: 5.5445 1.0389 sec/batch\n",
      "Epoch 5/20  Iteration 1588/7380 Training loss: 5.5447 1.0429 sec/batch\n",
      "Epoch 5/20  Iteration 1589/7380 Training loss: 5.5438 1.0487 sec/batch\n",
      "Epoch 5/20  Iteration 1590/7380 Training loss: 5.5441 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1591/7380 Training loss: 5.5439 1.0333 sec/batch\n",
      "Epoch 5/20  Iteration 1592/7380 Training loss: 5.5434 1.0298 sec/batch\n",
      "Epoch 5/20  Iteration 1593/7380 Training loss: 5.5424 1.0326 sec/batch\n",
      "Epoch 5/20  Iteration 1594/7380 Training loss: 5.5417 1.0308 sec/batch\n",
      "Epoch 5/20  Iteration 1595/7380 Training loss: 5.5417 1.0524 sec/batch\n",
      "Epoch 5/20  Iteration 1596/7380 Training loss: 5.5415 1.0514 sec/batch\n",
      "Epoch 5/20  Iteration 1597/7380 Training loss: 5.5418 1.0336 sec/batch\n",
      "Epoch 5/20  Iteration 1598/7380 Training loss: 5.5431 1.0316 sec/batch\n",
      "Epoch 5/20  Iteration 1599/7380 Training loss: 5.5426 1.0330 sec/batch\n",
      "Epoch 5/20  Iteration 1600/7380 Training loss: 5.5426 1.0326 sec/batch\n",
      "Validation loss: 5.44523 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1601/7380 Training loss: 5.5428 1.0407 sec/batch\n",
      "Epoch 5/20  Iteration 1602/7380 Training loss: 5.5419 1.0388 sec/batch\n",
      "Epoch 5/20  Iteration 1603/7380 Training loss: 5.5407 1.0337 sec/batch\n",
      "Epoch 5/20  Iteration 1604/7380 Training loss: 5.5410 1.0323 sec/batch\n",
      "Epoch 5/20  Iteration 1605/7380 Training loss: 5.5402 1.0322 sec/batch\n",
      "Epoch 5/20  Iteration 1606/7380 Training loss: 5.5402 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1607/7380 Training loss: 5.5402 1.0364 sec/batch\n",
      "Epoch 5/20  Iteration 1608/7380 Training loss: 5.5404 1.0294 sec/batch\n",
      "Epoch 5/20  Iteration 1609/7380 Training loss: 5.5406 1.0327 sec/batch\n",
      "Epoch 5/20  Iteration 1610/7380 Training loss: 5.5396 1.0284 sec/batch\n",
      "Epoch 5/20  Iteration 1611/7380 Training loss: 5.5391 1.0507 sec/batch\n",
      "Epoch 5/20  Iteration 1612/7380 Training loss: 5.5392 1.0320 sec/batch\n",
      "Epoch 5/20  Iteration 1613/7380 Training loss: 5.5388 1.0324 sec/batch\n",
      "Epoch 5/20  Iteration 1614/7380 Training loss: 5.5385 1.0268 sec/batch\n",
      "Epoch 5/20  Iteration 1615/7380 Training loss: 5.5378 1.0513 sec/batch\n",
      "Epoch 5/20  Iteration 1616/7380 Training loss: 5.5367 1.0304 sec/batch\n",
      "Epoch 5/20  Iteration 1617/7380 Training loss: 5.5380 1.0403 sec/batch\n",
      "Epoch 5/20  Iteration 1618/7380 Training loss: 5.5376 1.0408 sec/batch\n",
      "Epoch 5/20  Iteration 1619/7380 Training loss: 5.5379 1.0290 sec/batch\n",
      "Epoch 5/20  Iteration 1620/7380 Training loss: 5.5379 1.0537 sec/batch\n",
      "Epoch 5/20  Iteration 1621/7380 Training loss: 5.5378 1.0500 sec/batch\n",
      "Epoch 5/20  Iteration 1622/7380 Training loss: 5.5375 1.0431 sec/batch\n",
      "Epoch 5/20  Iteration 1623/7380 Training loss: 5.5375 1.0377 sec/batch\n",
      "Epoch 5/20  Iteration 1624/7380 Training loss: 5.5373 1.0311 sec/batch\n",
      "Epoch 5/20  Iteration 1625/7380 Training loss: 5.5365 1.0325 sec/batch\n",
      "Epoch 5/20  Iteration 1626/7380 Training loss: 5.5369 1.0321 sec/batch\n",
      "Epoch 5/20  Iteration 1627/7380 Training loss: 5.5362 1.0333 sec/batch\n",
      "Epoch 5/20  Iteration 1628/7380 Training loss: 5.5365 1.0266 sec/batch\n",
      "Epoch 5/20  Iteration 1629/7380 Training loss: 5.5365 1.0294 sec/batch\n",
      "Epoch 5/20  Iteration 1630/7380 Training loss: 5.5367 1.0337 sec/batch\n",
      "Epoch 5/20  Iteration 1631/7380 Training loss: 5.5365 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1632/7380 Training loss: 5.5359 1.0390 sec/batch\n",
      "Epoch 5/20  Iteration 1633/7380 Training loss: 5.5349 1.0353 sec/batch\n",
      "Epoch 5/20  Iteration 1634/7380 Training loss: 5.5355 1.0366 sec/batch\n",
      "Epoch 5/20  Iteration 1635/7380 Training loss: 5.5351 1.0324 sec/batch\n",
      "Epoch 5/20  Iteration 1636/7380 Training loss: 5.5349 1.0428 sec/batch\n",
      "Epoch 5/20  Iteration 1637/7380 Training loss: 5.5352 1.0479 sec/batch\n",
      "Epoch 5/20  Iteration 1638/7380 Training loss: 5.5355 1.0294 sec/batch\n",
      "Epoch 5/20  Iteration 1639/7380 Training loss: 5.5358 1.0324 sec/batch\n",
      "Epoch 5/20  Iteration 1640/7380 Training loss: 5.5362 1.0289 sec/batch\n",
      "Epoch 5/20  Iteration 1641/7380 Training loss: 5.5362 1.0278 sec/batch\n",
      "Epoch 5/20  Iteration 1642/7380 Training loss: 5.5364 1.0332 sec/batch\n",
      "Epoch 5/20  Iteration 1643/7380 Training loss: 5.5363 1.0320 sec/batch\n",
      "Epoch 5/20  Iteration 1644/7380 Training loss: 5.5366 1.0357 sec/batch\n",
      "Epoch 5/20  Iteration 1645/7380 Training loss: 5.5359 1.0270 sec/batch\n",
      "Epoch 5/20  Iteration 1646/7380 Training loss: 5.5355 1.0376 sec/batch\n",
      "Epoch 5/20  Iteration 1647/7380 Training loss: 5.5356 1.0254 sec/batch\n",
      "Epoch 5/20  Iteration 1648/7380 Training loss: 5.5361 1.0412 sec/batch\n",
      "Epoch 5/20  Iteration 1649/7380 Training loss: 5.5361 1.0617 sec/batch\n",
      "Epoch 5/20  Iteration 1650/7380 Training loss: 5.5358 1.0298 sec/batch\n",
      "Validation loss: 5.4245 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1651/7380 Training loss: 5.5356 1.0366 sec/batch\n",
      "Epoch 5/20  Iteration 1652/7380 Training loss: 5.5354 1.0599 sec/batch\n",
      "Epoch 5/20  Iteration 1653/7380 Training loss: 5.5352 1.0315 sec/batch\n",
      "Epoch 5/20  Iteration 1654/7380 Training loss: 5.5354 1.0436 sec/batch\n",
      "Epoch 5/20  Iteration 1655/7380 Training loss: 5.5348 1.0323 sec/batch\n",
      "Epoch 5/20  Iteration 1656/7380 Training loss: 5.5342 1.0322 sec/batch\n",
      "Epoch 5/20  Iteration 1657/7380 Training loss: 5.5338 1.0376 sec/batch\n",
      "Epoch 5/20  Iteration 1658/7380 Training loss: 5.5339 1.0333 sec/batch\n",
      "Epoch 5/20  Iteration 1659/7380 Training loss: 5.5346 1.0291 sec/batch\n",
      "Epoch 5/20  Iteration 1660/7380 Training loss: 5.5342 1.0286 sec/batch\n",
      "Epoch 5/20  Iteration 1661/7380 Training loss: 5.5340 1.0318 sec/batch\n",
      "Epoch 5/20  Iteration 1662/7380 Training loss: 5.5338 1.0337 sec/batch\n",
      "Epoch 5/20  Iteration 1663/7380 Training loss: 5.5333 1.0292 sec/batch\n",
      "Epoch 5/20  Iteration 1664/7380 Training loss: 5.5328 1.0370 sec/batch\n",
      "Epoch 5/20  Iteration 1665/7380 Training loss: 5.5322 1.0305 sec/batch\n",
      "Epoch 5/20  Iteration 1666/7380 Training loss: 5.5322 1.0337 sec/batch\n",
      "Epoch 5/20  Iteration 1667/7380 Training loss: 5.5313 1.0392 sec/batch\n",
      "Epoch 5/20  Iteration 1668/7380 Training loss: 5.5319 1.0284 sec/batch\n",
      "Epoch 5/20  Iteration 1669/7380 Training loss: 5.5323 1.0436 sec/batch\n",
      "Epoch 5/20  Iteration 1670/7380 Training loss: 5.5315 1.0320 sec/batch\n",
      "Epoch 5/20  Iteration 1671/7380 Training loss: 5.5319 1.0325 sec/batch\n",
      "Epoch 5/20  Iteration 1672/7380 Training loss: 5.5320 1.0389 sec/batch\n",
      "Epoch 5/20  Iteration 1673/7380 Training loss: 5.5320 1.0341 sec/batch\n",
      "Epoch 5/20  Iteration 1674/7380 Training loss: 5.5324 1.0334 sec/batch\n",
      "Epoch 5/20  Iteration 1675/7380 Training loss: 5.5323 1.0313 sec/batch\n",
      "Epoch 5/20  Iteration 1676/7380 Training loss: 5.5322 1.0402 sec/batch\n",
      "Epoch 5/20  Iteration 1677/7380 Training loss: 5.5327 1.0258 sec/batch\n",
      "Epoch 5/20  Iteration 1678/7380 Training loss: 5.5328 1.0376 sec/batch\n",
      "Epoch 5/20  Iteration 1679/7380 Training loss: 5.5332 1.0312 sec/batch\n",
      "Epoch 5/20  Iteration 1680/7380 Training loss: 5.5330 1.0348 sec/batch\n",
      "Epoch 5/20  Iteration 1681/7380 Training loss: 5.5333 1.0327 sec/batch\n",
      "Epoch 5/20  Iteration 1682/7380 Training loss: 5.5335 1.0376 sec/batch\n",
      "Epoch 5/20  Iteration 1683/7380 Training loss: 5.5326 1.0370 sec/batch\n",
      "Epoch 5/20  Iteration 1684/7380 Training loss: 5.5322 1.0390 sec/batch\n",
      "Epoch 5/20  Iteration 1685/7380 Training loss: 5.5316 1.0416 sec/batch\n",
      "Epoch 5/20  Iteration 1686/7380 Training loss: 5.5310 1.0384 sec/batch\n",
      "Epoch 5/20  Iteration 1687/7380 Training loss: 5.5309 1.0318 sec/batch\n",
      "Epoch 5/20  Iteration 1688/7380 Training loss: 5.5306 1.0251 sec/batch\n",
      "Epoch 5/20  Iteration 1689/7380 Training loss: 5.5305 1.0278 sec/batch\n",
      "Epoch 5/20  Iteration 1690/7380 Training loss: 5.5302 1.0311 sec/batch\n",
      "Epoch 5/20  Iteration 1691/7380 Training loss: 5.5297 1.0315 sec/batch\n",
      "Epoch 5/20  Iteration 1692/7380 Training loss: 5.5291 1.0390 sec/batch\n",
      "Epoch 5/20  Iteration 1693/7380 Training loss: 5.5283 1.0277 sec/batch\n",
      "Epoch 5/20  Iteration 1694/7380 Training loss: 5.5282 1.0361 sec/batch\n",
      "Epoch 5/20  Iteration 1695/7380 Training loss: 5.5285 1.0324 sec/batch\n",
      "Epoch 5/20  Iteration 1696/7380 Training loss: 5.5287 1.0428 sec/batch\n",
      "Epoch 5/20  Iteration 1697/7380 Training loss: 5.5282 1.0335 sec/batch\n",
      "Epoch 5/20  Iteration 1698/7380 Training loss: 5.5286 1.0313 sec/batch\n",
      "Epoch 5/20  Iteration 1699/7380 Training loss: 5.5285 1.0330 sec/batch\n",
      "Epoch 5/20  Iteration 1700/7380 Training loss: 5.5281 1.0295 sec/batch\n",
      "Validation loss: 5.41023 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1701/7380 Training loss: 5.5283 1.0431 sec/batch\n",
      "Epoch 5/20  Iteration 1702/7380 Training loss: 5.5279 1.0334 sec/batch\n",
      "Epoch 5/20  Iteration 1703/7380 Training loss: 5.5277 1.0301 sec/batch\n",
      "Epoch 5/20  Iteration 1704/7380 Training loss: 5.5271 1.0403 sec/batch\n",
      "Epoch 5/20  Iteration 1705/7380 Training loss: 5.5270 1.0296 sec/batch\n",
      "Epoch 5/20  Iteration 1706/7380 Training loss: 5.5274 1.0539 sec/batch\n",
      "Epoch 5/20  Iteration 1707/7380 Training loss: 5.5269 1.0411 sec/batch\n",
      "Epoch 5/20  Iteration 1708/7380 Training loss: 5.5264 1.0318 sec/batch\n",
      "Epoch 5/20  Iteration 1709/7380 Training loss: 5.5261 1.0324 sec/batch\n",
      "Epoch 5/20  Iteration 1710/7380 Training loss: 5.5256 1.0304 sec/batch\n",
      "Epoch 5/20  Iteration 1711/7380 Training loss: 5.5253 1.0287 sec/batch\n",
      "Epoch 5/20  Iteration 1712/7380 Training loss: 5.5247 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1713/7380 Training loss: 5.5244 1.0367 sec/batch\n",
      "Epoch 5/20  Iteration 1714/7380 Training loss: 5.5236 1.0290 sec/batch\n",
      "Epoch 5/20  Iteration 1715/7380 Training loss: 5.5225 1.0347 sec/batch\n",
      "Epoch 5/20  Iteration 1716/7380 Training loss: 5.5217 1.0351 sec/batch\n",
      "Epoch 5/20  Iteration 1717/7380 Training loss: 5.5218 1.0625 sec/batch\n",
      "Epoch 5/20  Iteration 1718/7380 Training loss: 5.5223 1.0322 sec/batch\n",
      "Epoch 5/20  Iteration 1719/7380 Training loss: 5.5218 1.0312 sec/batch\n",
      "Epoch 5/20  Iteration 1720/7380 Training loss: 5.5219 1.0319 sec/batch\n",
      "Epoch 5/20  Iteration 1721/7380 Training loss: 5.5219 1.0581 sec/batch\n",
      "Epoch 5/20  Iteration 1722/7380 Training loss: 5.5221 1.0420 sec/batch\n",
      "Epoch 5/20  Iteration 1723/7380 Training loss: 5.5219 1.0343 sec/batch\n",
      "Epoch 5/20  Iteration 1724/7380 Training loss: 5.5218 1.0422 sec/batch\n",
      "Epoch 5/20  Iteration 1725/7380 Training loss: 5.5217 1.0345 sec/batch\n",
      "Epoch 5/20  Iteration 1726/7380 Training loss: 5.5216 1.0361 sec/batch\n",
      "Epoch 5/20  Iteration 1727/7380 Training loss: 5.5210 1.0330 sec/batch\n",
      "Epoch 5/20  Iteration 1728/7380 Training loss: 5.5207 1.0341 sec/batch\n",
      "Epoch 5/20  Iteration 1729/7380 Training loss: 5.5206 1.0337 sec/batch\n",
      "Epoch 5/20  Iteration 1730/7380 Training loss: 5.5197 1.0335 sec/batch\n",
      "Epoch 5/20  Iteration 1731/7380 Training loss: 5.5197 1.0359 sec/batch\n",
      "Epoch 5/20  Iteration 1732/7380 Training loss: 5.5197 1.0286 sec/batch\n",
      "Epoch 5/20  Iteration 1733/7380 Training loss: 5.5198 1.0852 sec/batch\n",
      "Epoch 5/20  Iteration 1734/7380 Training loss: 5.5195 1.0338 sec/batch\n",
      "Epoch 5/20  Iteration 1735/7380 Training loss: 5.5197 1.0342 sec/batch\n",
      "Epoch 5/20  Iteration 1736/7380 Training loss: 5.5194 1.0301 sec/batch\n",
      "Epoch 5/20  Iteration 1737/7380 Training loss: 5.5192 1.0413 sec/batch\n",
      "Epoch 5/20  Iteration 1738/7380 Training loss: 5.5192 1.0310 sec/batch\n",
      "Epoch 5/20  Iteration 1739/7380 Training loss: 5.5191 1.0395 sec/batch\n",
      "Epoch 5/20  Iteration 1740/7380 Training loss: 5.5188 1.0317 sec/batch\n",
      "Epoch 5/20  Iteration 1741/7380 Training loss: 5.5185 1.0323 sec/batch\n",
      "Epoch 5/20  Iteration 1742/7380 Training loss: 5.5182 1.0345 sec/batch\n",
      "Epoch 5/20  Iteration 1743/7380 Training loss: 5.5183 1.0326 sec/batch\n",
      "Epoch 5/20  Iteration 1744/7380 Training loss: 5.5184 1.0451 sec/batch\n",
      "Epoch 5/20  Iteration 1745/7380 Training loss: 5.5178 1.0313 sec/batch\n",
      "Epoch 5/20  Iteration 1746/7380 Training loss: 5.5179 1.0327 sec/batch\n",
      "Epoch 5/20  Iteration 1747/7380 Training loss: 5.5180 1.0298 sec/batch\n",
      "Epoch 5/20  Iteration 1748/7380 Training loss: 5.5172 1.0366 sec/batch\n",
      "Epoch 5/20  Iteration 1749/7380 Training loss: 5.5173 1.0352 sec/batch\n",
      "Epoch 5/20  Iteration 1750/7380 Training loss: 5.5173 1.0366 sec/batch\n",
      "Validation loss: 5.39156 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1751/7380 Training loss: 5.5173 1.0504 sec/batch\n",
      "Epoch 5/20  Iteration 1752/7380 Training loss: 5.5169 1.0486 sec/batch\n",
      "Epoch 5/20  Iteration 1753/7380 Training loss: 5.5174 1.0440 sec/batch\n",
      "Epoch 5/20  Iteration 1754/7380 Training loss: 5.5173 1.0410 sec/batch\n",
      "Epoch 5/20  Iteration 1755/7380 Training loss: 5.5172 1.0329 sec/batch\n",
      "Epoch 5/20  Iteration 1756/7380 Training loss: 5.5169 1.0554 sec/batch\n",
      "Epoch 5/20  Iteration 1757/7380 Training loss: 5.5169 1.0337 sec/batch\n",
      "Epoch 5/20  Iteration 1758/7380 Training loss: 5.5168 1.0343 sec/batch\n",
      "Epoch 5/20  Iteration 1759/7380 Training loss: 5.5169 1.0471 sec/batch\n",
      "Epoch 5/20  Iteration 1760/7380 Training loss: 5.5168 1.0367 sec/batch\n",
      "Epoch 5/20  Iteration 1761/7380 Training loss: 5.5170 1.0405 sec/batch\n",
      "Epoch 5/20  Iteration 1762/7380 Training loss: 5.5166 1.0445 sec/batch\n",
      "Epoch 5/20  Iteration 1763/7380 Training loss: 5.5160 1.0392 sec/batch\n",
      "Epoch 5/20  Iteration 1764/7380 Training loss: 5.5157 1.0414 sec/batch\n",
      "Epoch 5/20  Iteration 1765/7380 Training loss: 5.5152 1.0346 sec/batch\n",
      "Epoch 5/20  Iteration 1766/7380 Training loss: 5.5148 1.0491 sec/batch\n",
      "Epoch 5/20  Iteration 1767/7380 Training loss: 5.5143 1.0307 sec/batch\n",
      "Epoch 5/20  Iteration 1768/7380 Training loss: 5.5141 1.0338 sec/batch\n",
      "Epoch 5/20  Iteration 1769/7380 Training loss: 5.5139 1.0328 sec/batch\n",
      "Epoch 5/20  Iteration 1770/7380 Training loss: 5.5137 1.0546 sec/batch\n",
      "Epoch 5/20  Iteration 1771/7380 Training loss: 5.5139 1.0413 sec/batch\n",
      "Epoch 5/20  Iteration 1772/7380 Training loss: 5.5138 1.0594 sec/batch\n",
      "Epoch 5/20  Iteration 1773/7380 Training loss: 5.5134 1.0383 sec/batch\n",
      "Epoch 5/20  Iteration 1774/7380 Training loss: 5.5132 1.0339 sec/batch\n",
      "Epoch 5/20  Iteration 1775/7380 Training loss: 5.5132 1.0432 sec/batch\n",
      "Epoch 5/20  Iteration 1776/7380 Training loss: 5.5136 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1777/7380 Training loss: 5.5134 1.0267 sec/batch\n",
      "Epoch 5/20  Iteration 1778/7380 Training loss: 5.5136 1.0363 sec/batch\n",
      "Epoch 5/20  Iteration 1779/7380 Training loss: 5.5134 1.0389 sec/batch\n",
      "Epoch 5/20  Iteration 1780/7380 Training loss: 5.5139 1.0353 sec/batch\n",
      "Epoch 5/20  Iteration 1781/7380 Training loss: 5.5142 1.0341 sec/batch\n",
      "Epoch 5/20  Iteration 1782/7380 Training loss: 5.5140 1.0310 sec/batch\n",
      "Epoch 5/20  Iteration 1783/7380 Training loss: 5.5135 1.0332 sec/batch\n",
      "Epoch 5/20  Iteration 1784/7380 Training loss: 5.5131 1.0314 sec/batch\n",
      "Epoch 5/20  Iteration 1785/7380 Training loss: 5.5129 1.0320 sec/batch\n",
      "Epoch 5/20  Iteration 1786/7380 Training loss: 5.5124 1.0453 sec/batch\n",
      "Epoch 5/20  Iteration 1787/7380 Training loss: 5.5122 1.0441 sec/batch\n",
      "Epoch 5/20  Iteration 1788/7380 Training loss: 5.5117 1.0351 sec/batch\n",
      "Epoch 5/20  Iteration 1789/7380 Training loss: 5.5115 1.0321 sec/batch\n",
      "Epoch 5/20  Iteration 1790/7380 Training loss: 5.5114 1.0339 sec/batch\n",
      "Epoch 5/20  Iteration 1791/7380 Training loss: 5.5113 1.0594 sec/batch\n",
      "Epoch 5/20  Iteration 1792/7380 Training loss: 5.5113 1.0427 sec/batch\n",
      "Epoch 5/20  Iteration 1793/7380 Training loss: 5.5115 1.0334 sec/batch\n",
      "Epoch 5/20  Iteration 1794/7380 Training loss: 5.5116 1.0412 sec/batch\n",
      "Epoch 5/20  Iteration 1795/7380 Training loss: 5.5121 1.0297 sec/batch\n",
      "Epoch 5/20  Iteration 1796/7380 Training loss: 5.5122 1.0295 sec/batch\n",
      "Epoch 5/20  Iteration 1797/7380 Training loss: 5.5118 1.0305 sec/batch\n",
      "Epoch 5/20  Iteration 1798/7380 Training loss: 5.5115 1.0533 sec/batch\n",
      "Epoch 5/20  Iteration 1799/7380 Training loss: 5.5113 1.0324 sec/batch\n",
      "Epoch 5/20  Iteration 1800/7380 Training loss: 5.5111 1.0310 sec/batch\n",
      "Validation loss: 5.38027 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1801/7380 Training loss: 5.5112 1.0357 sec/batch\n",
      "Epoch 5/20  Iteration 1802/7380 Training loss: 5.5116 1.0316 sec/batch\n",
      "Epoch 5/20  Iteration 1803/7380 Training loss: 5.5112 1.0297 sec/batch\n",
      "Epoch 5/20  Iteration 1804/7380 Training loss: 5.5107 1.0449 sec/batch\n",
      "Epoch 5/20  Iteration 1805/7380 Training loss: 5.5105 1.0528 sec/batch\n",
      "Epoch 5/20  Iteration 1806/7380 Training loss: 5.5100 1.0328 sec/batch\n",
      "Epoch 5/20  Iteration 1807/7380 Training loss: 5.5097 1.0278 sec/batch\n",
      "Epoch 5/20  Iteration 1808/7380 Training loss: 5.5097 1.0325 sec/batch\n",
      "Epoch 5/20  Iteration 1809/7380 Training loss: 5.5092 1.0504 sec/batch\n",
      "Epoch 5/20  Iteration 1810/7380 Training loss: 5.5093 1.0395 sec/batch\n",
      "Epoch 5/20  Iteration 1811/7380 Training loss: 5.5091 1.0549 sec/batch\n",
      "Epoch 5/20  Iteration 1812/7380 Training loss: 5.5092 1.0306 sec/batch\n",
      "Epoch 5/20  Iteration 1813/7380 Training loss: 5.5093 1.0420 sec/batch\n",
      "Epoch 5/20  Iteration 1814/7380 Training loss: 5.5091 1.0331 sec/batch\n",
      "Epoch 5/20  Iteration 1815/7380 Training loss: 5.5090 1.0342 sec/batch\n",
      "Epoch 5/20  Iteration 1816/7380 Training loss: 5.5084 1.0418 sec/batch\n",
      "Epoch 5/20  Iteration 1817/7380 Training loss: 5.5078 1.0327 sec/batch\n",
      "Epoch 5/20  Iteration 1818/7380 Training loss: 5.5075 1.0610 sec/batch\n",
      "Epoch 5/20  Iteration 1819/7380 Training loss: 5.5074 1.0430 sec/batch\n",
      "Epoch 5/20  Iteration 1820/7380 Training loss: 5.5072 1.0414 sec/batch\n",
      "Epoch 5/20  Iteration 1821/7380 Training loss: 5.5071 1.0344 sec/batch\n",
      "Epoch 5/20  Iteration 1822/7380 Training loss: 5.5069 1.0439 sec/batch\n",
      "Epoch 5/20  Iteration 1823/7380 Training loss: 5.5070 1.0324 sec/batch\n",
      "Epoch 5/20  Iteration 1824/7380 Training loss: 5.5070 1.0290 sec/batch\n",
      "Epoch 5/20  Iteration 1825/7380 Training loss: 5.5067 1.0307 sec/batch\n",
      "Epoch 5/20  Iteration 1826/7380 Training loss: 5.5066 1.0326 sec/batch\n",
      "Epoch 5/20  Iteration 1827/7380 Training loss: 5.5069 1.0382 sec/batch\n",
      "Epoch 5/20  Iteration 1828/7380 Training loss: 5.5066 1.0331 sec/batch\n",
      "Epoch 5/20  Iteration 1829/7380 Training loss: 5.5064 1.0344 sec/batch\n",
      "Epoch 5/20  Iteration 1830/7380 Training loss: 5.5059 1.0579 sec/batch\n",
      "Epoch 5/20  Iteration 1831/7380 Training loss: 5.5056 1.0299 sec/batch\n",
      "Epoch 5/20  Iteration 1832/7380 Training loss: 5.5053 1.0339 sec/batch\n",
      "Epoch 5/20  Iteration 1833/7380 Training loss: 5.5054 1.0317 sec/batch\n",
      "Epoch 5/20  Iteration 1834/7380 Training loss: 5.5051 1.0315 sec/batch\n",
      "Epoch 5/20  Iteration 1835/7380 Training loss: 5.5046 1.0345 sec/batch\n",
      "Epoch 5/20  Iteration 1836/7380 Training loss: 5.5043 1.0348 sec/batch\n",
      "Epoch 5/20  Iteration 1837/7380 Training loss: 5.5042 1.0356 sec/batch\n",
      "Epoch 5/20  Iteration 1838/7380 Training loss: 5.5042 1.0275 sec/batch\n",
      "Epoch 5/20  Iteration 1839/7380 Training loss: 5.5039 1.0332 sec/batch\n",
      "Epoch 5/20  Iteration 1840/7380 Training loss: 5.5035 1.0493 sec/batch\n",
      "Epoch 5/20  Iteration 1841/7380 Training loss: 5.5032 1.0349 sec/batch\n",
      "Epoch 5/20  Iteration 1842/7380 Training loss: 5.5029 1.0350 sec/batch\n",
      "Epoch 5/20  Iteration 1843/7380 Training loss: 5.5025 1.0397 sec/batch\n",
      "Epoch 5/20  Iteration 1844/7380 Training loss: 5.5025 1.0433 sec/batch\n",
      "Epoch 5/20  Iteration 1845/7380 Training loss: 5.5023 1.0352 sec/batch\n",
      "Epoch 6/20  Iteration 1846/7380 Training loss: 5.6160 1.0261 sec/batch\n",
      "Epoch 6/20  Iteration 1847/7380 Training loss: 5.5551 1.0341 sec/batch\n",
      "Epoch 6/20  Iteration 1848/7380 Training loss: 5.4932 1.0438 sec/batch\n",
      "Epoch 6/20  Iteration 1849/7380 Training loss: 5.4729 1.0451 sec/batch\n",
      "Epoch 6/20  Iteration 1850/7380 Training loss: 5.4600 1.0330 sec/batch\n",
      "Validation loss: 5.37669 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1851/7380 Training loss: 5.4777 1.0385 sec/batch\n",
      "Epoch 6/20  Iteration 1852/7380 Training loss: 5.4893 1.0615 sec/batch\n",
      "Epoch 6/20  Iteration 1853/7380 Training loss: 5.4759 1.0333 sec/batch\n",
      "Epoch 6/20  Iteration 1854/7380 Training loss: 5.4702 1.0373 sec/batch\n",
      "Epoch 6/20  Iteration 1855/7380 Training loss: 5.4799 1.0284 sec/batch\n",
      "Epoch 6/20  Iteration 1856/7380 Training loss: 5.4764 1.0314 sec/batch\n",
      "Epoch 6/20  Iteration 1857/7380 Training loss: 5.4619 1.0520 sec/batch\n",
      "Epoch 6/20  Iteration 1858/7380 Training loss: 5.4583 1.0316 sec/batch\n",
      "Epoch 6/20  Iteration 1859/7380 Training loss: 5.4643 1.0386 sec/batch\n",
      "Epoch 6/20  Iteration 1860/7380 Training loss: 5.4684 1.0334 sec/batch\n",
      "Epoch 6/20  Iteration 1861/7380 Training loss: 5.4646 1.0271 sec/batch\n",
      "Epoch 6/20  Iteration 1862/7380 Training loss: 5.4597 1.0370 sec/batch\n",
      "Epoch 6/20  Iteration 1863/7380 Training loss: 5.4455 1.0425 sec/batch\n",
      "Epoch 6/20  Iteration 1864/7380 Training loss: 5.4380 1.0374 sec/batch\n",
      "Epoch 6/20  Iteration 1865/7380 Training loss: 5.4361 1.0384 sec/batch\n",
      "Epoch 6/20  Iteration 1866/7380 Training loss: 5.4305 1.0319 sec/batch\n",
      "Epoch 6/20  Iteration 1867/7380 Training loss: 5.4373 1.0458 sec/batch\n",
      "Epoch 6/20  Iteration 1868/7380 Training loss: 5.4382 1.0362 sec/batch\n",
      "Epoch 6/20  Iteration 1869/7380 Training loss: 5.4460 1.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1870/7380 Training loss: 5.4480 1.0555 sec/batch\n",
      "Epoch 6/20  Iteration 1871/7380 Training loss: 5.4487 1.0380 sec/batch\n",
      "Epoch 6/20  Iteration 1872/7380 Training loss: 5.4429 1.0414 sec/batch\n",
      "Epoch 6/20  Iteration 1873/7380 Training loss: 5.4464 1.0327 sec/batch\n",
      "Epoch 6/20  Iteration 1874/7380 Training loss: 5.4455 1.0338 sec/batch\n",
      "Epoch 6/20  Iteration 1875/7380 Training loss: 5.4457 1.0397 sec/batch\n",
      "Epoch 6/20  Iteration 1876/7380 Training loss: 5.4441 1.0544 sec/batch\n",
      "Epoch 6/20  Iteration 1877/7380 Training loss: 5.4443 1.0386 sec/batch\n",
      "Epoch 6/20  Iteration 1878/7380 Training loss: 5.4442 1.0327 sec/batch\n",
      "Epoch 6/20  Iteration 1879/7380 Training loss: 5.4445 1.0525 sec/batch\n",
      "Epoch 6/20  Iteration 1880/7380 Training loss: 5.4478 1.0570 sec/batch\n",
      "Epoch 6/20  Iteration 1881/7380 Training loss: 5.4467 1.0589 sec/batch\n",
      "Epoch 6/20  Iteration 1882/7380 Training loss: 5.4459 1.0366 sec/batch\n",
      "Epoch 6/20  Iteration 1883/7380 Training loss: 5.4454 1.0349 sec/batch\n",
      "Epoch 6/20  Iteration 1884/7380 Training loss: 5.4476 1.2270 sec/batch\n",
      "Epoch 6/20  Iteration 1885/7380 Training loss: 5.4469 1.0630 sec/batch\n",
      "Epoch 6/20  Iteration 1886/7380 Training loss: 5.4465 1.0419 sec/batch\n",
      "Epoch 6/20  Iteration 1887/7380 Training loss: 5.4450 1.0448 sec/batch\n",
      "Epoch 6/20  Iteration 1888/7380 Training loss: 5.4452 1.0358 sec/batch\n",
      "Epoch 6/20  Iteration 1889/7380 Training loss: 5.4467 1.0343 sec/batch\n",
      "Epoch 6/20  Iteration 1890/7380 Training loss: 5.4471 1.0321 sec/batch\n",
      "Epoch 6/20  Iteration 1891/7380 Training loss: 5.4455 1.0422 sec/batch\n",
      "Epoch 6/20  Iteration 1892/7380 Training loss: 5.4461 1.0342 sec/batch\n",
      "Epoch 6/20  Iteration 1893/7380 Training loss: 5.4439 1.0407 sec/batch\n",
      "Epoch 6/20  Iteration 1894/7380 Training loss: 5.4446 1.0361 sec/batch\n",
      "Epoch 6/20  Iteration 1895/7380 Training loss: 5.4434 1.0349 sec/batch\n",
      "Epoch 6/20  Iteration 1896/7380 Training loss: 5.4416 1.0350 sec/batch\n",
      "Epoch 6/20  Iteration 1897/7380 Training loss: 5.4411 1.0345 sec/batch\n",
      "Epoch 6/20  Iteration 1898/7380 Training loss: 5.4399 1.0368 sec/batch\n",
      "Epoch 6/20  Iteration 1899/7380 Training loss: 5.4397 1.0358 sec/batch\n",
      "Epoch 6/20  Iteration 1900/7380 Training loss: 5.4384 1.0314 sec/batch\n",
      "Validation loss: 5.35668 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1901/7380 Training loss: 5.4371 1.0488 sec/batch\n",
      "Epoch 6/20  Iteration 1902/7380 Training loss: 5.4365 1.0451 sec/batch\n",
      "Epoch 6/20  Iteration 1903/7380 Training loss: 5.4367 1.0300 sec/batch\n",
      "Epoch 6/20  Iteration 1904/7380 Training loss: 5.4358 1.0360 sec/batch\n",
      "Epoch 6/20  Iteration 1905/7380 Training loss: 5.4363 1.0389 sec/batch\n",
      "Epoch 6/20  Iteration 1906/7380 Training loss: 5.4340 1.0359 sec/batch\n",
      "Epoch 6/20  Iteration 1907/7380 Training loss: 5.4352 1.0420 sec/batch\n",
      "Epoch 6/20  Iteration 1908/7380 Training loss: 5.4340 1.0375 sec/batch\n",
      "Epoch 6/20  Iteration 1909/7380 Training loss: 5.4324 1.0324 sec/batch\n",
      "Epoch 6/20  Iteration 1910/7380 Training loss: 5.4326 1.0523 sec/batch\n",
      "Epoch 6/20  Iteration 1911/7380 Training loss: 5.4322 1.0391 sec/batch\n",
      "Epoch 6/20  Iteration 1912/7380 Training loss: 5.4328 1.0440 sec/batch\n",
      "Epoch 6/20  Iteration 1913/7380 Training loss: 5.4339 1.0422 sec/batch\n",
      "Epoch 6/20  Iteration 1914/7380 Training loss: 5.4352 1.0413 sec/batch\n",
      "Epoch 6/20  Iteration 1915/7380 Training loss: 5.4341 1.0349 sec/batch\n",
      "Epoch 6/20  Iteration 1916/7380 Training loss: 5.4329 1.0412 sec/batch\n",
      "Epoch 6/20  Iteration 1917/7380 Training loss: 5.4334 1.0326 sec/batch\n",
      "Epoch 6/20  Iteration 1918/7380 Training loss: 5.4319 1.0457 sec/batch\n",
      "Epoch 6/20  Iteration 1919/7380 Training loss: 5.4332 1.0347 sec/batch\n",
      "Epoch 6/20  Iteration 1920/7380 Training loss: 5.4341 1.0344 sec/batch\n",
      "Epoch 6/20  Iteration 1921/7380 Training loss: 5.4333 1.0374 sec/batch\n",
      "Epoch 6/20  Iteration 1922/7380 Training loss: 5.4333 1.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1923/7380 Training loss: 5.4332 1.0394 sec/batch\n",
      "Epoch 6/20  Iteration 1924/7380 Training loss: 5.4340 1.0346 sec/batch\n",
      "Epoch 6/20  Iteration 1925/7380 Training loss: 5.4341 1.0344 sec/batch\n",
      "Epoch 6/20  Iteration 1926/7380 Training loss: 5.4336 1.0407 sec/batch\n",
      "Epoch 6/20  Iteration 1927/7380 Training loss: 5.4327 1.0627 sec/batch\n",
      "Epoch 6/20  Iteration 1928/7380 Training loss: 5.4315 1.0346 sec/batch\n",
      "Epoch 6/20  Iteration 1929/7380 Training loss: 5.4306 1.0350 sec/batch\n",
      "Epoch 6/20  Iteration 1930/7380 Training loss: 5.4307 1.0315 sec/batch\n",
      "Epoch 6/20  Iteration 1931/7380 Training loss: 5.4302 1.0319 sec/batch\n",
      "Epoch 6/20  Iteration 1932/7380 Training loss: 5.4303 1.0386 sec/batch\n",
      "Epoch 6/20  Iteration 1933/7380 Training loss: 5.4300 1.0339 sec/batch\n",
      "Epoch 6/20  Iteration 1934/7380 Training loss: 5.4278 1.0333 sec/batch\n",
      "Epoch 6/20  Iteration 1935/7380 Training loss: 5.4274 1.0394 sec/batch\n",
      "Epoch 6/20  Iteration 1936/7380 Training loss: 5.4276 1.0352 sec/batch\n",
      "Epoch 6/20  Iteration 1937/7380 Training loss: 5.4292 1.0341 sec/batch\n",
      "Epoch 6/20  Iteration 1938/7380 Training loss: 5.4289 1.0460 sec/batch\n",
      "Epoch 6/20  Iteration 1939/7380 Training loss: 5.4286 1.0329 sec/batch\n",
      "Epoch 6/20  Iteration 1940/7380 Training loss: 5.4280 1.0341 sec/batch\n",
      "Epoch 6/20  Iteration 1941/7380 Training loss: 5.4282 1.0423 sec/batch\n",
      "Epoch 6/20  Iteration 1942/7380 Training loss: 5.4280 1.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1943/7380 Training loss: 5.4294 1.0362 sec/batch\n",
      "Epoch 6/20  Iteration 1944/7380 Training loss: 5.4291 1.0460 sec/batch\n",
      "Epoch 6/20  Iteration 1945/7380 Training loss: 5.4292 1.0363 sec/batch\n",
      "Epoch 6/20  Iteration 1946/7380 Training loss: 5.4305 1.0341 sec/batch\n",
      "Epoch 6/20  Iteration 1947/7380 Training loss: 5.4307 1.0499 sec/batch\n",
      "Epoch 6/20  Iteration 1948/7380 Training loss: 5.4308 1.0372 sec/batch\n",
      "Epoch 6/20  Iteration 1949/7380 Training loss: 5.4304 1.0385 sec/batch\n",
      "Epoch 6/20  Iteration 1950/7380 Training loss: 5.4307 1.0356 sec/batch\n",
      "Validation loss: 5.34174 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1951/7380 Training loss: 5.4313 1.0475 sec/batch\n",
      "Epoch 6/20  Iteration 1952/7380 Training loss: 5.4316 1.0329 sec/batch\n",
      "Epoch 6/20  Iteration 1953/7380 Training loss: 5.4314 1.0453 sec/batch\n",
      "Epoch 6/20  Iteration 1954/7380 Training loss: 5.4299 1.0342 sec/batch\n",
      "Epoch 6/20  Iteration 1955/7380 Training loss: 5.4294 1.0401 sec/batch\n",
      "Epoch 6/20  Iteration 1956/7380 Training loss: 5.4293 1.0343 sec/batch\n",
      "Epoch 6/20  Iteration 1957/7380 Training loss: 5.4295 1.0455 sec/batch\n",
      "Epoch 6/20  Iteration 1958/7380 Training loss: 5.4283 1.0492 sec/batch\n",
      "Epoch 6/20  Iteration 1959/7380 Training loss: 5.4290 1.0332 sec/batch\n",
      "Epoch 6/20  Iteration 1960/7380 Training loss: 5.4287 1.0351 sec/batch\n",
      "Epoch 6/20  Iteration 1961/7380 Training loss: 5.4282 1.0441 sec/batch\n",
      "Epoch 6/20  Iteration 1962/7380 Training loss: 5.4275 1.0652 sec/batch\n",
      "Epoch 6/20  Iteration 1963/7380 Training loss: 5.4267 1.0347 sec/batch\n",
      "Epoch 6/20  Iteration 1964/7380 Training loss: 5.4268 1.0378 sec/batch\n",
      "Epoch 6/20  Iteration 1965/7380 Training loss: 5.4266 1.0443 sec/batch\n",
      "Epoch 6/20  Iteration 1966/7380 Training loss: 5.4269 1.0341 sec/batch\n",
      "Epoch 6/20  Iteration 1967/7380 Training loss: 5.4284 1.0543 sec/batch\n",
      "Epoch 6/20  Iteration 1968/7380 Training loss: 5.4279 1.0503 sec/batch\n",
      "Epoch 6/20  Iteration 1969/7380 Training loss: 5.4277 1.0371 sec/batch\n",
      "Epoch 6/20  Iteration 1970/7380 Training loss: 5.4273 1.0509 sec/batch\n",
      "Epoch 6/20  Iteration 1971/7380 Training loss: 5.4262 1.0411 sec/batch\n",
      "Epoch 6/20  Iteration 1972/7380 Training loss: 5.4254 1.0399 sec/batch\n",
      "Epoch 6/20  Iteration 1973/7380 Training loss: 5.4257 1.0416 sec/batch\n",
      "Epoch 6/20  Iteration 1974/7380 Training loss: 5.4253 1.0356 sec/batch\n",
      "Epoch 6/20  Iteration 1975/7380 Training loss: 5.4255 1.0365 sec/batch\n",
      "Epoch 6/20  Iteration 1976/7380 Training loss: 5.4255 1.0349 sec/batch\n",
      "Epoch 6/20  Iteration 1977/7380 Training loss: 5.4257 1.0353 sec/batch\n",
      "Epoch 6/20  Iteration 1978/7380 Training loss: 5.4257 1.0365 sec/batch\n",
      "Epoch 6/20  Iteration 1979/7380 Training loss: 5.4249 1.0524 sec/batch\n",
      "Epoch 6/20  Iteration 1980/7380 Training loss: 5.4246 1.0368 sec/batch\n",
      "Epoch 6/20  Iteration 1981/7380 Training loss: 5.4246 1.0391 sec/batch\n",
      "Epoch 6/20  Iteration 1982/7380 Training loss: 5.4241 1.0380 sec/batch\n",
      "Epoch 6/20  Iteration 1983/7380 Training loss: 5.4240 1.0403 sec/batch\n",
      "Epoch 6/20  Iteration 1984/7380 Training loss: 5.4232 1.0468 sec/batch\n",
      "Epoch 6/20  Iteration 1985/7380 Training loss: 5.4223 1.0405 sec/batch\n",
      "Epoch 6/20  Iteration 1986/7380 Training loss: 5.4238 1.0604 sec/batch\n",
      "Epoch 6/20  Iteration 1987/7380 Training loss: 5.4235 1.0453 sec/batch\n",
      "Epoch 6/20  Iteration 1988/7380 Training loss: 5.4239 1.0338 sec/batch\n",
      "Epoch 6/20  Iteration 1989/7380 Training loss: 5.4237 1.0464 sec/batch\n",
      "Epoch 6/20  Iteration 1990/7380 Training loss: 5.4238 1.0347 sec/batch\n",
      "Epoch 6/20  Iteration 1991/7380 Training loss: 5.4234 1.0345 sec/batch\n",
      "Epoch 6/20  Iteration 1992/7380 Training loss: 5.4233 1.0356 sec/batch\n",
      "Epoch 6/20  Iteration 1993/7380 Training loss: 5.4231 1.0366 sec/batch\n",
      "Epoch 6/20  Iteration 1994/7380 Training loss: 5.4224 1.0612 sec/batch\n",
      "Epoch 6/20  Iteration 1995/7380 Training loss: 5.4227 1.0357 sec/batch\n",
      "Epoch 6/20  Iteration 1996/7380 Training loss: 5.4221 1.0363 sec/batch\n",
      "Epoch 6/20  Iteration 1997/7380 Training loss: 5.4223 1.0332 sec/batch\n",
      "Epoch 6/20  Iteration 1998/7380 Training loss: 5.4222 1.0363 sec/batch\n",
      "Epoch 6/20  Iteration 1999/7380 Training loss: 5.4225 1.0319 sec/batch\n",
      "Epoch 6/20  Iteration 2000/7380 Training loss: 5.4220 1.0346 sec/batch\n",
      "Validation loss: 5.33042 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 2001/7380 Training loss: 5.4218 1.0436 sec/batch\n",
      "Epoch 6/20  Iteration 2002/7380 Training loss: 5.4211 1.0471 sec/batch\n",
      "Epoch 6/20  Iteration 2003/7380 Training loss: 5.4216 1.0509 sec/batch\n",
      "Epoch 6/20  Iteration 2004/7380 Training loss: 5.4211 1.0405 sec/batch\n",
      "Epoch 6/20  Iteration 2005/7380 Training loss: 5.4207 1.0504 sec/batch\n",
      "Epoch 6/20  Iteration 2006/7380 Training loss: 5.4210 1.0416 sec/batch\n",
      "Epoch 6/20  Iteration 2007/7380 Training loss: 5.4214 1.0461 sec/batch\n",
      "Epoch 6/20  Iteration 2008/7380 Training loss: 5.4218 1.0324 sec/batch\n",
      "Epoch 6/20  Iteration 2009/7380 Training loss: 5.4223 1.0331 sec/batch\n",
      "Epoch 6/20  Iteration 2010/7380 Training loss: 5.4225 1.0439 sec/batch\n",
      "Epoch 6/20  Iteration 2011/7380 Training loss: 5.4228 1.0377 sec/batch\n",
      "Epoch 6/20  Iteration 2012/7380 Training loss: 5.4227 1.0342 sec/batch\n",
      "Epoch 6/20  Iteration 2013/7380 Training loss: 5.4230 1.0726 sec/batch\n",
      "Epoch 6/20  Iteration 2014/7380 Training loss: 5.4223 1.0363 sec/batch\n",
      "Epoch 6/20  Iteration 2015/7380 Training loss: 5.4219 1.0380 sec/batch\n",
      "Epoch 6/20  Iteration 2016/7380 Training loss: 5.4219 1.0396 sec/batch\n",
      "Epoch 6/20  Iteration 2017/7380 Training loss: 5.4223 1.0348 sec/batch\n",
      "Epoch 6/20  Iteration 2018/7380 Training loss: 5.4224 1.0406 sec/batch\n",
      "Epoch 6/20  Iteration 2019/7380 Training loss: 5.4221 1.0512 sec/batch\n",
      "Epoch 6/20  Iteration 2020/7380 Training loss: 5.4218 1.0615 sec/batch\n",
      "Epoch 6/20  Iteration 2021/7380 Training loss: 5.4214 1.0542 sec/batch\n",
      "Epoch 6/20  Iteration 2022/7380 Training loss: 5.4213 1.0379 sec/batch\n",
      "Epoch 6/20  Iteration 2023/7380 Training loss: 5.4216 1.0352 sec/batch\n",
      "Epoch 6/20  Iteration 2024/7380 Training loss: 5.4209 1.0361 sec/batch\n",
      "Epoch 6/20  Iteration 2025/7380 Training loss: 5.4201 1.0368 sec/batch\n",
      "Epoch 6/20  Iteration 2026/7380 Training loss: 5.4198 1.0473 sec/batch\n",
      "Epoch 6/20  Iteration 2027/7380 Training loss: 5.4198 1.0373 sec/batch\n",
      "Epoch 6/20  Iteration 2028/7380 Training loss: 5.4204 1.0380 sec/batch\n",
      "Epoch 6/20  Iteration 2029/7380 Training loss: 5.4200 1.0463 sec/batch\n",
      "Epoch 6/20  Iteration 2030/7380 Training loss: 5.4197 1.0369 sec/batch\n",
      "Epoch 6/20  Iteration 2031/7380 Training loss: 5.4198 1.0346 sec/batch\n",
      "Epoch 6/20  Iteration 2032/7380 Training loss: 5.4193 1.0387 sec/batch\n",
      "Epoch 6/20  Iteration 2033/7380 Training loss: 5.4189 1.0403 sec/batch\n",
      "Epoch 6/20  Iteration 2034/7380 Training loss: 5.4185 1.0414 sec/batch\n",
      "Epoch 6/20  Iteration 2035/7380 Training loss: 5.4185 1.0534 sec/batch\n",
      "Epoch 6/20  Iteration 2036/7380 Training loss: 5.4176 1.0472 sec/batch\n",
      "Epoch 6/20  Iteration 2037/7380 Training loss: 5.4180 1.0388 sec/batch\n",
      "Epoch 6/20  Iteration 2038/7380 Training loss: 5.4185 1.0347 sec/batch\n",
      "Epoch 6/20  Iteration 2039/7380 Training loss: 5.4178 1.0365 sec/batch\n",
      "Epoch 6/20  Iteration 2040/7380 Training loss: 5.4180 1.0345 sec/batch\n",
      "Epoch 6/20  Iteration 2041/7380 Training loss: 5.4181 1.0349 sec/batch\n",
      "Epoch 6/20  Iteration 2042/7380 Training loss: 5.4181 1.0420 sec/batch\n",
      "Epoch 6/20  Iteration 2043/7380 Training loss: 5.4185 1.0353 sec/batch\n",
      "Epoch 6/20  Iteration 2044/7380 Training loss: 5.4184 1.0376 sec/batch\n",
      "Epoch 6/20  Iteration 2045/7380 Training loss: 5.4186 1.0340 sec/batch\n",
      "Epoch 6/20  Iteration 2046/7380 Training loss: 5.4188 1.0318 sec/batch\n",
      "Epoch 6/20  Iteration 2047/7380 Training loss: 5.4191 1.0506 sec/batch\n",
      "Epoch 6/20  Iteration 2048/7380 Training loss: 5.4196 1.0321 sec/batch\n",
      "Epoch 6/20  Iteration 2049/7380 Training loss: 5.4195 1.0564 sec/batch\n",
      "Epoch 6/20  Iteration 2050/7380 Training loss: 5.4199 1.0343 sec/batch\n",
      "Validation loss: 5.3189 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 2051/7380 Training loss: 5.4205 1.0436 sec/batch\n",
      "Epoch 6/20  Iteration 2052/7380 Training loss: 5.4196 1.0417 sec/batch\n",
      "Epoch 6/20  Iteration 2053/7380 Training loss: 5.4191 1.0388 sec/batch\n",
      "Epoch 6/20  Iteration 2054/7380 Training loss: 5.4188 1.0445 sec/batch\n",
      "Epoch 6/20  Iteration 2055/7380 Training loss: 5.4182 1.0670 sec/batch\n",
      "Epoch 6/20  Iteration 2056/7380 Training loss: 5.4181 1.0380 sec/batch\n",
      "Epoch 6/20  Iteration 2057/7380 Training loss: 5.4178 1.0440 sec/batch\n",
      "Epoch 6/20  Iteration 2058/7380 Training loss: 5.4179 1.0349 sec/batch\n",
      "Epoch 6/20  Iteration 2059/7380 Training loss: 5.4177 1.0326 sec/batch\n",
      "Epoch 6/20  Iteration 2060/7380 Training loss: 5.4173 1.0340 sec/batch\n",
      "Epoch 6/20  Iteration 2061/7380 Training loss: 5.4168 1.0369 sec/batch\n",
      "Epoch 6/20  Iteration 2062/7380 Training loss: 5.4161 1.0370 sec/batch\n",
      "Epoch 6/20  Iteration 2063/7380 Training loss: 5.4162 1.0452 sec/batch\n",
      "Epoch 6/20  Iteration 2064/7380 Training loss: 5.4167 1.0449 sec/batch\n",
      "Epoch 6/20  Iteration 2065/7380 Training loss: 5.4167 1.0439 sec/batch\n",
      "Epoch 6/20  Iteration 2066/7380 Training loss: 5.4164 1.0338 sec/batch\n",
      "Epoch 6/20  Iteration 2067/7380 Training loss: 5.4169 1.0710 sec/batch\n",
      "Epoch 6/20  Iteration 2068/7380 Training loss: 5.4168 1.0530 sec/batch\n",
      "Epoch 6/20  Iteration 2069/7380 Training loss: 5.4163 1.0376 sec/batch\n",
      "Epoch 6/20  Iteration 2070/7380 Training loss: 5.4163 1.0413 sec/batch\n",
      "Epoch 6/20  Iteration 2071/7380 Training loss: 5.4159 1.0520 sec/batch\n",
      "Epoch 6/20  Iteration 2072/7380 Training loss: 5.4159 1.0484 sec/batch\n",
      "Epoch 6/20  Iteration 2073/7380 Training loss: 5.4154 1.0677 sec/batch\n",
      "Epoch 6/20  Iteration 2074/7380 Training loss: 5.4153 1.0455 sec/batch\n",
      "Epoch 6/20  Iteration 2075/7380 Training loss: 5.4157 1.0675 sec/batch\n",
      "Epoch 6/20  Iteration 2076/7380 Training loss: 5.4151 1.0394 sec/batch\n",
      "Epoch 6/20  Iteration 2077/7380 Training loss: 5.4147 1.0366 sec/batch\n",
      "Epoch 6/20  Iteration 2078/7380 Training loss: 5.4143 1.0352 sec/batch\n",
      "Epoch 6/20  Iteration 2079/7380 Training loss: 5.4138 1.0449 sec/batch\n",
      "Epoch 6/20  Iteration 2080/7380 Training loss: 5.4137 1.0351 sec/batch\n",
      "Epoch 6/20  Iteration 2081/7380 Training loss: 5.4132 1.0374 sec/batch\n",
      "Epoch 6/20  Iteration 2082/7380 Training loss: 5.4127 1.0404 sec/batch\n",
      "Epoch 6/20  Iteration 2083/7380 Training loss: 5.4120 1.0351 sec/batch\n",
      "Epoch 6/20  Iteration 2084/7380 Training loss: 5.4108 1.0449 sec/batch\n",
      "Epoch 6/20  Iteration 2085/7380 Training loss: 5.4101 1.0612 sec/batch\n",
      "Epoch 6/20  Iteration 2086/7380 Training loss: 5.4102 1.0454 sec/batch\n",
      "Epoch 6/20  Iteration 2087/7380 Training loss: 5.4107 1.0377 sec/batch\n",
      "Epoch 6/20  Iteration 2088/7380 Training loss: 5.4102 1.0370 sec/batch\n",
      "Epoch 6/20  Iteration 2089/7380 Training loss: 5.4101 1.0353 sec/batch\n",
      "Epoch 6/20  Iteration 2090/7380 Training loss: 5.4102 1.0454 sec/batch\n",
      "Epoch 6/20  Iteration 2091/7380 Training loss: 5.4102 1.0369 sec/batch\n",
      "Epoch 6/20  Iteration 2092/7380 Training loss: 5.4102 1.0482 sec/batch\n",
      "Epoch 6/20  Iteration 2093/7380 Training loss: 5.4101 1.0387 sec/batch\n",
      "Epoch 6/20  Iteration 2094/7380 Training loss: 5.4098 1.0499 sec/batch\n",
      "Epoch 6/20  Iteration 2095/7380 Training loss: 5.4098 1.0400 sec/batch\n",
      "Epoch 6/20  Iteration 2096/7380 Training loss: 5.4093 1.0475 sec/batch\n",
      "Epoch 6/20  Iteration 2097/7380 Training loss: 5.4091 1.0456 sec/batch\n",
      "Epoch 6/20  Iteration 2098/7380 Training loss: 5.4090 1.0342 sec/batch\n",
      "Epoch 6/20  Iteration 2099/7380 Training loss: 5.4083 1.0393 sec/batch\n",
      "Epoch 6/20  Iteration 2100/7380 Training loss: 5.4087 1.0466 sec/batch\n",
      "Validation loss: 5.31557 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 2101/7380 Training loss: 5.4091 1.0442 sec/batch\n",
      "Epoch 6/20  Iteration 2102/7380 Training loss: 5.4093 1.0474 sec/batch\n",
      "Epoch 6/20  Iteration 2103/7380 Training loss: 5.4091 1.0630 sec/batch\n",
      "Epoch 6/20  Iteration 2104/7380 Training loss: 5.4093 1.0371 sec/batch\n",
      "Epoch 6/20  Iteration 2105/7380 Training loss: 5.4088 1.0447 sec/batch\n",
      "Epoch 6/20  Iteration 2106/7380 Training loss: 5.4085 1.0420 sec/batch\n",
      "Epoch 6/20  Iteration 2107/7380 Training loss: 5.4087 1.0395 sec/batch\n",
      "Epoch 6/20  Iteration 2108/7380 Training loss: 5.4088 1.0465 sec/batch\n",
      "Epoch 6/20  Iteration 2109/7380 Training loss: 5.4086 1.0401 sec/batch\n",
      "Epoch 6/20  Iteration 2110/7380 Training loss: 5.4081 1.0452 sec/batch\n",
      "Epoch 6/20  Iteration 2111/7380 Training loss: 5.4078 1.0354 sec/batch\n",
      "Epoch 6/20  Iteration 2112/7380 Training loss: 5.4079 1.0370 sec/batch\n",
      "Epoch 6/20  Iteration 2113/7380 Training loss: 5.4078 1.0396 sec/batch\n",
      "Epoch 6/20  Iteration 2114/7380 Training loss: 5.4072 1.0453 sec/batch\n",
      "Epoch 6/20  Iteration 2115/7380 Training loss: 5.4073 1.0428 sec/batch\n",
      "Epoch 6/20  Iteration 2116/7380 Training loss: 5.4074 1.0336 sec/batch\n",
      "Epoch 6/20  Iteration 2117/7380 Training loss: 5.4067 1.0363 sec/batch\n",
      "Epoch 6/20  Iteration 2118/7380 Training loss: 5.4069 1.0400 sec/batch\n",
      "Epoch 6/20  Iteration 2119/7380 Training loss: 5.4070 1.0369 sec/batch\n",
      "Epoch 6/20  Iteration 2120/7380 Training loss: 5.4068 1.0427 sec/batch\n",
      "Epoch 6/20  Iteration 2121/7380 Training loss: 5.4063 1.0455 sec/batch\n",
      "Epoch 6/20  Iteration 2122/7380 Training loss: 5.4069 1.0449 sec/batch\n",
      "Epoch 6/20  Iteration 2123/7380 Training loss: 5.4069 1.0501 sec/batch\n",
      "Epoch 6/20  Iteration 2124/7380 Training loss: 5.4070 1.0439 sec/batch\n",
      "Epoch 6/20  Iteration 2125/7380 Training loss: 5.4067 1.0464 sec/batch\n",
      "Epoch 6/20  Iteration 2126/7380 Training loss: 5.4066 1.0364 sec/batch\n",
      "Epoch 6/20  Iteration 2127/7380 Training loss: 5.4066 1.0459 sec/batch\n",
      "Epoch 6/20  Iteration 2128/7380 Training loss: 5.4067 1.0396 sec/batch\n",
      "Epoch 6/20  Iteration 2129/7380 Training loss: 5.4067 1.0650 sec/batch\n",
      "Epoch 6/20  Iteration 2130/7380 Training loss: 5.4069 1.0449 sec/batch\n",
      "Epoch 6/20  Iteration 2131/7380 Training loss: 5.4065 1.0743 sec/batch\n",
      "Epoch 6/20  Iteration 2132/7380 Training loss: 5.4061 1.0512 sec/batch\n",
      "Epoch 6/20  Iteration 2133/7380 Training loss: 5.4060 1.0389 sec/batch\n",
      "Epoch 6/20  Iteration 2134/7380 Training loss: 5.4056 1.0346 sec/batch\n",
      "Epoch 6/20  Iteration 2135/7380 Training loss: 5.4053 1.0360 sec/batch\n",
      "Epoch 6/20  Iteration 2136/7380 Training loss: 5.4048 1.0389 sec/batch\n",
      "Epoch 6/20  Iteration 2137/7380 Training loss: 5.4046 1.0356 sec/batch\n",
      "Epoch 6/20  Iteration 2138/7380 Training loss: 5.4045 1.0353 sec/batch\n",
      "Epoch 6/20  Iteration 2139/7380 Training loss: 5.4043 1.0346 sec/batch\n",
      "Epoch 6/20  Iteration 2140/7380 Training loss: 5.4045 1.0385 sec/batch\n",
      "Epoch 6/20  Iteration 2141/7380 Training loss: 5.4043 1.0470 sec/batch\n",
      "Epoch 6/20  Iteration 2142/7380 Training loss: 5.4040 1.0405 sec/batch\n",
      "Epoch 6/20  Iteration 2143/7380 Training loss: 5.4038 1.0375 sec/batch\n",
      "Epoch 6/20  Iteration 2144/7380 Training loss: 5.4038 1.0383 sec/batch\n",
      "Epoch 6/20  Iteration 2145/7380 Training loss: 5.4043 1.0496 sec/batch\n",
      "Epoch 6/20  Iteration 2146/7380 Training loss: 5.4043 1.0382 sec/batch\n",
      "Epoch 6/20  Iteration 2147/7380 Training loss: 5.4045 1.0334 sec/batch\n",
      "Epoch 6/20  Iteration 2148/7380 Training loss: 5.4045 1.0462 sec/batch\n",
      "Epoch 6/20  Iteration 2149/7380 Training loss: 5.4049 1.0358 sec/batch\n",
      "Epoch 6/20  Iteration 2150/7380 Training loss: 5.4053 1.0363 sec/batch\n",
      "Validation loss: 5.31151 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 2151/7380 Training loss: 5.4054 1.0469 sec/batch\n",
      "Epoch 6/20  Iteration 2152/7380 Training loss: 5.4051 1.0462 sec/batch\n",
      "Epoch 6/20  Iteration 2153/7380 Training loss: 5.4048 1.0491 sec/batch\n",
      "Epoch 6/20  Iteration 2154/7380 Training loss: 5.4044 1.0498 sec/batch\n",
      "Epoch 6/20  Iteration 2155/7380 Training loss: 5.4040 1.0418 sec/batch\n",
      "Epoch 6/20  Iteration 2156/7380 Training loss: 5.4039 1.0459 sec/batch\n",
      "Epoch 6/20  Iteration 2157/7380 Training loss: 5.4035 1.0483 sec/batch\n",
      "Epoch 6/20  Iteration 2158/7380 Training loss: 5.4032 1.0458 sec/batch\n",
      "Epoch 6/20  Iteration 2159/7380 Training loss: 5.4031 1.0454 sec/batch\n",
      "Epoch 6/20  Iteration 2160/7380 Training loss: 5.4031 1.0466 sec/batch\n",
      "Epoch 6/20  Iteration 2161/7380 Training loss: 5.4032 1.0478 sec/batch\n",
      "Epoch 6/20  Iteration 2162/7380 Training loss: 5.4034 1.0403 sec/batch\n",
      "Epoch 6/20  Iteration 2163/7380 Training loss: 5.4035 1.0423 sec/batch\n",
      "Epoch 6/20  Iteration 2164/7380 Training loss: 5.4039 1.0450 sec/batch\n",
      "Epoch 6/20  Iteration 2165/7380 Training loss: 5.4042 1.0388 sec/batch\n",
      "Epoch 6/20  Iteration 2166/7380 Training loss: 5.4040 1.0392 sec/batch\n",
      "Epoch 6/20  Iteration 2167/7380 Training loss: 5.4038 1.1165 sec/batch\n",
      "Epoch 6/20  Iteration 2168/7380 Training loss: 5.4036 1.1049 sec/batch\n",
      "Epoch 6/20  Iteration 2169/7380 Training loss: 5.4034 1.1952 sec/batch\n",
      "Epoch 6/20  Iteration 2170/7380 Training loss: 5.4034 1.3966 sec/batch\n",
      "Epoch 6/20  Iteration 2171/7380 Training loss: 5.4038 1.0696 sec/batch\n",
      "Epoch 6/20  Iteration 2172/7380 Training loss: 5.4035 1.0521 sec/batch\n",
      "Epoch 6/20  Iteration 2173/7380 Training loss: 5.4031 1.0556 sec/batch\n",
      "Epoch 6/20  Iteration 2174/7380 Training loss: 5.4028 1.0642 sec/batch\n",
      "Epoch 6/20  Iteration 2175/7380 Training loss: 5.4024 1.0545 sec/batch\n",
      "Epoch 6/20  Iteration 2176/7380 Training loss: 5.4022 1.0507 sec/batch\n",
      "Epoch 6/20  Iteration 2177/7380 Training loss: 5.4022 1.0389 sec/batch\n",
      "Epoch 6/20  Iteration 2178/7380 Training loss: 5.4018 1.0412 sec/batch\n",
      "Epoch 6/20  Iteration 2179/7380 Training loss: 5.4018 1.0404 sec/batch\n",
      "Epoch 6/20  Iteration 2180/7380 Training loss: 5.4016 1.0400 sec/batch\n",
      "Epoch 6/20  Iteration 2181/7380 Training loss: 5.4017 1.0398 sec/batch\n",
      "Epoch 6/20  Iteration 2182/7380 Training loss: 5.4019 1.0541 sec/batch\n",
      "Epoch 6/20  Iteration 2183/7380 Training loss: 5.4016 1.0756 sec/batch\n",
      "Epoch 6/20  Iteration 2184/7380 Training loss: 5.4016 1.0505 sec/batch\n",
      "Epoch 6/20  Iteration 2185/7380 Training loss: 5.4011 1.0444 sec/batch\n",
      "Epoch 6/20  Iteration 2186/7380 Training loss: 5.4006 1.0470 sec/batch\n",
      "Epoch 6/20  Iteration 2187/7380 Training loss: 5.4003 1.0696 sec/batch\n",
      "Epoch 6/20  Iteration 2188/7380 Training loss: 5.4003 1.0415 sec/batch\n",
      "Epoch 6/20  Iteration 2189/7380 Training loss: 5.4002 1.0544 sec/batch\n",
      "Epoch 6/20  Iteration 2190/7380 Training loss: 5.4000 1.0559 sec/batch\n",
      "Epoch 6/20  Iteration 2191/7380 Training loss: 5.3998 1.0393 sec/batch\n",
      "Epoch 6/20  Iteration 2192/7380 Training loss: 5.4001 1.0407 sec/batch\n",
      "Epoch 6/20  Iteration 2193/7380 Training loss: 5.4000 1.0430 sec/batch\n",
      "Epoch 6/20  Iteration 2194/7380 Training loss: 5.3998 1.0373 sec/batch\n",
      "Epoch 6/20  Iteration 2195/7380 Training loss: 5.3997 1.0379 sec/batch\n",
      "Epoch 6/20  Iteration 2196/7380 Training loss: 5.4000 1.0522 sec/batch\n",
      "Epoch 6/20  Iteration 2197/7380 Training loss: 5.3996 1.0557 sec/batch\n",
      "Epoch 6/20  Iteration 2198/7380 Training loss: 5.3995 1.0387 sec/batch\n",
      "Epoch 6/20  Iteration 2199/7380 Training loss: 5.3992 1.0346 sec/batch\n",
      "Epoch 6/20  Iteration 2200/7380 Training loss: 5.3989 1.0404 sec/batch\n",
      "Validation loss: 5.28525 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 2201/7380 Training loss: 5.3988 1.0459 sec/batch\n",
      "Epoch 6/20  Iteration 2202/7380 Training loss: 5.3989 1.0577 sec/batch\n",
      "Epoch 6/20  Iteration 2203/7380 Training loss: 5.3987 1.0389 sec/batch\n",
      "Epoch 6/20  Iteration 2204/7380 Training loss: 5.3984 1.0364 sec/batch\n",
      "Epoch 6/20  Iteration 2205/7380 Training loss: 5.3981 1.0457 sec/batch\n",
      "Epoch 6/20  Iteration 2206/7380 Training loss: 5.3980 1.0533 sec/batch\n",
      "Epoch 6/20  Iteration 2207/7380 Training loss: 5.3981 1.0772 sec/batch\n",
      "Epoch 6/20  Iteration 2208/7380 Training loss: 5.3978 1.0453 sec/batch\n",
      "Epoch 6/20  Iteration 2209/7380 Training loss: 5.3975 1.0437 sec/batch\n",
      "Epoch 6/20  Iteration 2210/7380 Training loss: 5.3972 1.0578 sec/batch\n",
      "Epoch 6/20  Iteration 2211/7380 Training loss: 5.3969 1.0386 sec/batch\n",
      "Epoch 6/20  Iteration 2212/7380 Training loss: 5.3967 1.0452 sec/batch\n",
      "Epoch 6/20  Iteration 2213/7380 Training loss: 5.3967 1.0590 sec/batch\n",
      "Epoch 6/20  Iteration 2214/7380 Training loss: 5.3964 1.0448 sec/batch\n",
      "Epoch 7/20  Iteration 2215/7380 Training loss: 5.4821 1.0301 sec/batch\n",
      "Epoch 7/20  Iteration 2216/7380 Training loss: 5.4330 1.0407 sec/batch\n",
      "Epoch 7/20  Iteration 2217/7380 Training loss: 5.3944 1.0377 sec/batch\n",
      "Epoch 7/20  Iteration 2218/7380 Training loss: 5.3747 1.0420 sec/batch\n",
      "Epoch 7/20  Iteration 2219/7380 Training loss: 5.3577 1.0409 sec/batch\n",
      "Epoch 7/20  Iteration 2220/7380 Training loss: 5.3633 1.0440 sec/batch\n",
      "Epoch 7/20  Iteration 2221/7380 Training loss: 5.3755 1.0414 sec/batch\n",
      "Epoch 7/20  Iteration 2222/7380 Training loss: 5.3634 1.0439 sec/batch\n",
      "Epoch 7/20  Iteration 2223/7380 Training loss: 5.3591 1.0393 sec/batch\n",
      "Epoch 7/20  Iteration 2224/7380 Training loss: 5.3699 1.0390 sec/batch\n",
      "Epoch 7/20  Iteration 2225/7380 Training loss: 5.3641 1.0426 sec/batch\n",
      "Epoch 7/20  Iteration 2226/7380 Training loss: 5.3541 1.0494 sec/batch\n",
      "Epoch 7/20  Iteration 2227/7380 Training loss: 5.3494 1.0393 sec/batch\n",
      "Epoch 7/20  Iteration 2228/7380 Training loss: 5.3557 1.0460 sec/batch\n",
      "Epoch 7/20  Iteration 2229/7380 Training loss: 5.3598 1.0379 sec/batch\n",
      "Epoch 7/20  Iteration 2230/7380 Training loss: 5.3573 1.0417 sec/batch\n",
      "Epoch 7/20  Iteration 2231/7380 Training loss: 5.3515 1.0402 sec/batch\n",
      "Epoch 7/20  Iteration 2232/7380 Training loss: 5.3391 1.0517 sec/batch\n",
      "Epoch 7/20  Iteration 2233/7380 Training loss: 5.3313 1.0489 sec/batch\n",
      "Epoch 7/20  Iteration 2234/7380 Training loss: 5.3297 1.0402 sec/batch\n",
      "Epoch 7/20  Iteration 2235/7380 Training loss: 5.3255 1.0397 sec/batch\n",
      "Epoch 7/20  Iteration 2236/7380 Training loss: 5.3310 1.0460 sec/batch\n",
      "Epoch 7/20  Iteration 2237/7380 Training loss: 5.3308 1.0529 sec/batch\n",
      "Epoch 7/20  Iteration 2238/7380 Training loss: 5.3378 1.0350 sec/batch\n",
      "Epoch 7/20  Iteration 2239/7380 Training loss: 5.3398 1.0409 sec/batch\n",
      "Epoch 7/20  Iteration 2240/7380 Training loss: 5.3404 1.0390 sec/batch\n",
      "Epoch 7/20  Iteration 2241/7380 Training loss: 5.3354 1.0512 sec/batch\n",
      "Epoch 7/20  Iteration 2242/7380 Training loss: 5.3396 1.0451 sec/batch\n",
      "Epoch 7/20  Iteration 2243/7380 Training loss: 5.3385 1.0539 sec/batch\n",
      "Epoch 7/20  Iteration 2244/7380 Training loss: 5.3391 1.0385 sec/batch\n",
      "Epoch 7/20  Iteration 2245/7380 Training loss: 5.3374 1.0378 sec/batch\n",
      "Epoch 7/20  Iteration 2246/7380 Training loss: 5.3381 1.0489 sec/batch\n",
      "Epoch 7/20  Iteration 2247/7380 Training loss: 5.3386 1.0551 sec/batch\n",
      "Epoch 7/20  Iteration 2248/7380 Training loss: 5.3390 1.0431 sec/batch\n",
      "Epoch 7/20  Iteration 2249/7380 Training loss: 5.3431 1.0372 sec/batch\n",
      "Epoch 7/20  Iteration 2250/7380 Training loss: 5.3416 1.0406 sec/batch\n",
      "Validation loss: 5.27444 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 2251/7380 Training loss: 5.3435 1.0437 sec/batch\n",
      "Epoch 7/20  Iteration 2252/7380 Training loss: 5.3426 1.0559 sec/batch\n",
      "Epoch 7/20  Iteration 2253/7380 Training loss: 5.3447 1.0488 sec/batch\n",
      "Epoch 7/20  Iteration 2254/7380 Training loss: 5.3446 1.0495 sec/batch\n",
      "Epoch 7/20  Iteration 2255/7380 Training loss: 5.3446 1.0505 sec/batch\n",
      "Epoch 7/20  Iteration 2256/7380 Training loss: 5.3433 1.0458 sec/batch\n",
      "Epoch 7/20  Iteration 2257/7380 Training loss: 5.3438 1.0433 sec/batch\n",
      "Epoch 7/20  Iteration 2258/7380 Training loss: 5.3454 1.0421 sec/batch\n",
      "Epoch 7/20  Iteration 2259/7380 Training loss: 5.3457 1.0418 sec/batch\n",
      "Epoch 7/20  Iteration 2260/7380 Training loss: 5.3435 1.0545 sec/batch\n",
      "Epoch 7/20  Iteration 2261/7380 Training loss: 5.3443 1.0409 sec/batch\n",
      "Epoch 7/20  Iteration 2262/7380 Training loss: 5.3411 1.0438 sec/batch\n",
      "Epoch 7/20  Iteration 2263/7380 Training loss: 5.3424 1.0472 sec/batch\n",
      "Epoch 7/20  Iteration 2264/7380 Training loss: 5.3413 1.0796 sec/batch\n",
      "Epoch 7/20  Iteration 2265/7380 Training loss: 5.3400 1.0670 sec/batch\n",
      "Epoch 7/20  Iteration 2266/7380 Training loss: 5.3391 1.0556 sec/batch\n",
      "Epoch 7/20  Iteration 2267/7380 Training loss: 5.3378 1.0411 sec/batch\n",
      "Epoch 7/20  Iteration 2268/7380 Training loss: 5.3369 1.0428 sec/batch\n",
      "Epoch 7/20  Iteration 2269/7380 Training loss: 5.3358 1.0541 sec/batch\n",
      "Epoch 7/20  Iteration 2270/7380 Training loss: 5.3331 1.0518 sec/batch\n",
      "Epoch 7/20  Iteration 2271/7380 Training loss: 5.3324 1.0549 sec/batch\n",
      "Epoch 7/20  Iteration 2272/7380 Training loss: 5.3330 1.0662 sec/batch\n",
      "Epoch 7/20  Iteration 2273/7380 Training loss: 5.3321 1.0391 sec/batch\n",
      "Epoch 7/20  Iteration 2274/7380 Training loss: 5.3328 1.0547 sec/batch\n",
      "Epoch 7/20  Iteration 2275/7380 Training loss: 5.3309 1.0439 sec/batch\n",
      "Epoch 7/20  Iteration 2276/7380 Training loss: 5.3323 1.0754 sec/batch\n",
      "Epoch 7/20  Iteration 2277/7380 Training loss: 5.3306 1.0422 sec/batch\n",
      "Epoch 7/20  Iteration 2278/7380 Training loss: 5.3294 1.0552 sec/batch\n",
      "Epoch 7/20  Iteration 2279/7380 Training loss: 5.3297 1.0662 sec/batch\n",
      "Epoch 7/20  Iteration 2280/7380 Training loss: 5.3291 1.0383 sec/batch\n",
      "Epoch 7/20  Iteration 2281/7380 Training loss: 5.3301 1.0403 sec/batch\n",
      "Epoch 7/20  Iteration 2282/7380 Training loss: 5.3311 1.0605 sec/batch\n",
      "Epoch 7/20  Iteration 2283/7380 Training loss: 5.3325 1.0538 sec/batch\n",
      "Epoch 7/20  Iteration 2284/7380 Training loss: 5.3313 1.0444 sec/batch\n",
      "Epoch 7/20  Iteration 2285/7380 Training loss: 5.3304 1.0503 sec/batch\n",
      "Epoch 7/20  Iteration 2286/7380 Training loss: 5.3316 1.0496 sec/batch\n",
      "Epoch 7/20  Iteration 2287/7380 Training loss: 5.3298 1.0444 sec/batch\n",
      "Epoch 7/20  Iteration 2288/7380 Training loss: 5.3312 1.0641 sec/batch\n",
      "Epoch 7/20  Iteration 2289/7380 Training loss: 5.3324 1.0414 sec/batch\n",
      "Epoch 7/20  Iteration 2290/7380 Training loss: 5.3315 1.0452 sec/batch\n",
      "Epoch 7/20  Iteration 2291/7380 Training loss: 5.3313 1.0432 sec/batch\n",
      "Epoch 7/20  Iteration 2292/7380 Training loss: 5.3313 1.0415 sec/batch\n",
      "Epoch 7/20  Iteration 2293/7380 Training loss: 5.3320 1.0414 sec/batch\n",
      "Epoch 7/20  Iteration 2294/7380 Training loss: 5.3323 1.0400 sec/batch\n",
      "Epoch 7/20  Iteration 2295/7380 Training loss: 5.3317 1.0506 sec/batch\n",
      "Epoch 7/20  Iteration 2296/7380 Training loss: 5.3309 1.0411 sec/batch\n",
      "Epoch 7/20  Iteration 2297/7380 Training loss: 5.3302 1.0569 sec/batch\n",
      "Epoch 7/20  Iteration 2298/7380 Training loss: 5.3296 1.0531 sec/batch\n",
      "Epoch 7/20  Iteration 2299/7380 Training loss: 5.3298 1.0521 sec/batch\n",
      "Epoch 7/20  Iteration 2300/7380 Training loss: 5.3295 1.0504 sec/batch\n",
      "Validation loss: 5.27025 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 2301/7380 Training loss: 5.3300 1.0468 sec/batch\n",
      "Epoch 7/20  Iteration 2302/7380 Training loss: 5.3299 1.0484 sec/batch\n",
      "Epoch 7/20  Iteration 2303/7380 Training loss: 5.3281 1.0503 sec/batch\n",
      "Epoch 7/20  Iteration 2304/7380 Training loss: 5.3278 1.0603 sec/batch\n",
      "Epoch 7/20  Iteration 2305/7380 Training loss: 5.3279 1.0507 sec/batch\n",
      "Epoch 7/20  Iteration 2306/7380 Training loss: 5.3299 1.0461 sec/batch\n",
      "Epoch 7/20  Iteration 2307/7380 Training loss: 5.3297 1.0448 sec/batch\n",
      "Epoch 7/20  Iteration 2308/7380 Training loss: 5.3295 1.0426 sec/batch\n",
      "Epoch 7/20  Iteration 2309/7380 Training loss: 5.3290 1.0407 sec/batch\n",
      "Epoch 7/20  Iteration 2310/7380 Training loss: 5.3291 1.0399 sec/batch\n",
      "Epoch 7/20  Iteration 2311/7380 Training loss: 5.3289 1.0445 sec/batch\n",
      "Epoch 7/20  Iteration 2312/7380 Training loss: 5.3305 1.0496 sec/batch\n",
      "Epoch 7/20  Iteration 2313/7380 Training loss: 5.3301 1.0533 sec/batch\n",
      "Epoch 7/20  Iteration 2314/7380 Training loss: 5.3303 1.0448 sec/batch\n",
      "Epoch 7/20  Iteration 2315/7380 Training loss: 5.3316 1.0404 sec/batch\n",
      "Epoch 7/20  Iteration 2316/7380 Training loss: 5.3319 1.0754 sec/batch\n",
      "Epoch 7/20  Iteration 2317/7380 Training loss: 5.3319 1.0418 sec/batch\n",
      "Epoch 7/20  Iteration 2318/7380 Training loss: 5.3317 1.0422 sec/batch\n",
      "Epoch 7/20  Iteration 2319/7380 Training loss: 5.3322 1.0422 sec/batch\n",
      "Epoch 7/20  Iteration 2320/7380 Training loss: 5.3318 1.0485 sec/batch\n",
      "Epoch 7/20  Iteration 2321/7380 Training loss: 5.3319 1.0432 sec/batch\n",
      "Epoch 7/20  Iteration 2322/7380 Training loss: 5.3315 1.0508 sec/batch\n",
      "Epoch 7/20  Iteration 2323/7380 Training loss: 5.3301 1.0423 sec/batch\n",
      "Epoch 7/20  Iteration 2324/7380 Training loss: 5.3298 1.0460 sec/batch\n",
      "Epoch 7/20  Iteration 2325/7380 Training loss: 5.3293 1.0670 sec/batch\n",
      "Epoch 7/20  Iteration 2326/7380 Training loss: 5.3294 1.0557 sec/batch\n",
      "Epoch 7/20  Iteration 2327/7380 Training loss: 5.3284 1.0413 sec/batch\n",
      "Epoch 7/20  Iteration 2328/7380 Training loss: 5.3289 1.0464 sec/batch\n",
      "Epoch 7/20  Iteration 2329/7380 Training loss: 5.3288 1.0517 sec/batch\n",
      "Epoch 7/20  Iteration 2330/7380 Training loss: 5.3282 1.0434 sec/batch\n",
      "Epoch 7/20  Iteration 2331/7380 Training loss: 5.3273 1.0439 sec/batch\n",
      "Epoch 7/20  Iteration 2332/7380 Training loss: 5.3270 1.0445 sec/batch\n",
      "Epoch 7/20  Iteration 2333/7380 Training loss: 5.3271 1.0557 sec/batch\n",
      "Epoch 7/20  Iteration 2334/7380 Training loss: 5.3268 1.0593 sec/batch\n",
      "Epoch 7/20  Iteration 2335/7380 Training loss: 5.3270 1.0506 sec/batch\n",
      "Epoch 7/20  Iteration 2336/7380 Training loss: 5.3283 1.0650 sec/batch\n",
      "Epoch 7/20  Iteration 2337/7380 Training loss: 5.3281 1.0513 sec/batch\n",
      "Epoch 7/20  Iteration 2338/7380 Training loss: 5.3281 1.0487 sec/batch\n",
      "Epoch 7/20  Iteration 2339/7380 Training loss: 5.3280 1.0473 sec/batch\n",
      "Epoch 7/20  Iteration 2340/7380 Training loss: 5.3272 1.0454 sec/batch\n",
      "Epoch 7/20  Iteration 2341/7380 Training loss: 5.3263 1.0423 sec/batch\n",
      "Epoch 7/20  Iteration 2342/7380 Training loss: 5.3266 1.0404 sec/batch\n",
      "Epoch 7/20  Iteration 2343/7380 Training loss: 5.3262 1.0540 sec/batch\n",
      "Epoch 7/20  Iteration 2344/7380 Training loss: 5.3263 1.0417 sec/batch\n",
      "Epoch 7/20  Iteration 2345/7380 Training loss: 5.3261 1.0518 sec/batch\n",
      "Epoch 7/20  Iteration 2346/7380 Training loss: 5.3263 1.0409 sec/batch\n",
      "Epoch 7/20  Iteration 2347/7380 Training loss: 5.3264 1.0392 sec/batch\n",
      "Epoch 7/20  Iteration 2348/7380 Training loss: 5.3255 1.0430 sec/batch\n",
      "Epoch 7/20  Iteration 2349/7380 Training loss: 5.3252 1.0432 sec/batch\n",
      "Epoch 7/20  Iteration 2350/7380 Training loss: 5.3254 1.0460 sec/batch\n",
      "Validation loss: 5.2692 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 2351/7380 Training loss: 5.3259 1.0564 sec/batch\n",
      "Epoch 7/20  Iteration 2352/7380 Training loss: 5.3259 1.0509 sec/batch\n",
      "Epoch 7/20  Iteration 2353/7380 Training loss: 5.3254 1.0598 sec/batch\n",
      "Epoch 7/20  Iteration 2354/7380 Training loss: 5.3244 1.0445 sec/batch\n",
      "Epoch 7/20  Iteration 2355/7380 Training loss: 5.3256 1.0432 sec/batch\n",
      "Epoch 7/20  Iteration 2356/7380 Training loss: 5.3255 1.0415 sec/batch\n",
      "Epoch 7/20  Iteration 2357/7380 Training loss: 5.3260 1.0431 sec/batch\n",
      "Epoch 7/20  Iteration 2358/7380 Training loss: 5.3260 1.0399 sec/batch\n",
      "Epoch 7/20  Iteration 2359/7380 Training loss: 5.3262 1.0446 sec/batch\n",
      "Epoch 7/20  Iteration 2360/7380 Training loss: 5.3262 1.0758 sec/batch\n",
      "Epoch 7/20  Iteration 2361/7380 Training loss: 5.3264 1.0487 sec/batch\n",
      "Epoch 7/20  Iteration 2362/7380 Training loss: 5.3261 1.0378 sec/batch\n",
      "Epoch 7/20  Iteration 2363/7380 Training loss: 5.3253 1.0450 sec/batch\n",
      "Epoch 7/20  Iteration 2364/7380 Training loss: 5.3255 1.0477 sec/batch\n",
      "Epoch 7/20  Iteration 2365/7380 Training loss: 5.3249 1.0478 sec/batch\n",
      "Epoch 7/20  Iteration 2366/7380 Training loss: 5.3252 1.0402 sec/batch\n",
      "Epoch 7/20  Iteration 2367/7380 Training loss: 5.3255 1.0517 sec/batch\n",
      "Epoch 7/20  Iteration 2368/7380 Training loss: 5.3258 1.0485 sec/batch\n",
      "Epoch 7/20  Iteration 2369/7380 Training loss: 5.3255 1.0419 sec/batch\n",
      "Epoch 7/20  Iteration 2370/7380 Training loss: 5.3249 1.0485 sec/batch\n",
      "Epoch 7/20  Iteration 2371/7380 Training loss: 5.3240 1.0496 sec/batch\n",
      "Epoch 7/20  Iteration 2372/7380 Training loss: 5.3246 1.0425 sec/batch\n",
      "Epoch 7/20  Iteration 2373/7380 Training loss: 5.3243 1.0485 sec/batch\n",
      "Epoch 7/20  Iteration 2374/7380 Training loss: 5.3240 1.0422 sec/batch\n",
      "Epoch 7/20  Iteration 2375/7380 Training loss: 5.3247 1.0468 sec/batch\n",
      "Epoch 7/20  Iteration 2376/7380 Training loss: 5.3252 1.0454 sec/batch\n",
      "Epoch 7/20  Iteration 2377/7380 Training loss: 5.3256 1.0409 sec/batch\n",
      "Epoch 7/20  Iteration 2378/7380 Training loss: 5.3260 1.0854 sec/batch\n",
      "Epoch 7/20  Iteration 2379/7380 Training loss: 5.3261 1.0659 sec/batch\n",
      "Epoch 7/20  Iteration 2380/7380 Training loss: 5.3265 1.0495 sec/batch\n",
      "Epoch 7/20  Iteration 2381/7380 Training loss: 5.3267 1.0521 sec/batch\n",
      "Epoch 7/20  Iteration 2382/7380 Training loss: 5.3270 1.0476 sec/batch\n",
      "Epoch 7/20  Iteration 2383/7380 Training loss: 5.3264 1.0585 sec/batch\n",
      "Epoch 7/20  Iteration 2384/7380 Training loss: 5.3259 1.0409 sec/batch\n",
      "Epoch 7/20  Iteration 2385/7380 Training loss: 5.3260 1.0441 sec/batch\n",
      "Epoch 7/20  Iteration 2386/7380 Training loss: 5.3266 1.0577 sec/batch\n",
      "Epoch 7/20  Iteration 2387/7380 Training loss: 5.3266 1.0440 sec/batch\n",
      "Epoch 7/20  Iteration 2388/7380 Training loss: 5.3263 1.0446 sec/batch\n",
      "Epoch 7/20  Iteration 2389/7380 Training loss: 5.3260 1.0414 sec/batch\n",
      "Epoch 7/20  Iteration 2390/7380 Training loss: 5.3258 1.0436 sec/batch\n",
      "Epoch 7/20  Iteration 2391/7380 Training loss: 5.3259 1.0565 sec/batch\n",
      "Epoch 7/20  Iteration 2392/7380 Training loss: 5.3261 1.0796 sec/batch\n",
      "Epoch 7/20  Iteration 2393/7380 Training loss: 5.3254 1.0476 sec/batch\n",
      "Epoch 7/20  Iteration 2394/7380 Training loss: 5.3248 1.0691 sec/batch\n",
      "Epoch 7/20  Iteration 2395/7380 Training loss: 5.3245 1.0566 sec/batch\n",
      "Epoch 7/20  Iteration 2396/7380 Training loss: 5.3245 1.0444 sec/batch\n",
      "Epoch 7/20  Iteration 2397/7380 Training loss: 5.3251 1.0445 sec/batch\n",
      "Epoch 7/20  Iteration 2398/7380 Training loss: 5.3246 1.0424 sec/batch\n",
      "Epoch 7/20  Iteration 2399/7380 Training loss: 5.3243 1.0508 sec/batch\n",
      "Epoch 7/20  Iteration 2400/7380 Training loss: 5.3242 1.0429 sec/batch\n",
      "Validation loss: 5.25266 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 2401/7380 Training loss: 5.3242 1.0523 sec/batch\n",
      "Epoch 7/20  Iteration 2402/7380 Training loss: 5.3239 1.0588 sec/batch\n",
      "Epoch 7/20  Iteration 2403/7380 Training loss: 5.3234 1.0579 sec/batch\n",
      "Epoch 7/20  Iteration 2404/7380 Training loss: 5.3236 1.0462 sec/batch\n",
      "Epoch 7/20  Iteration 2405/7380 Training loss: 5.3228 1.0486 sec/batch\n",
      "Epoch 7/20  Iteration 2406/7380 Training loss: 5.3232 1.0555 sec/batch\n",
      "Epoch 7/20  Iteration 2407/7380 Training loss: 5.3234 1.0530 sec/batch\n",
      "Epoch 7/20  Iteration 2408/7380 Training loss: 5.3227 1.0471 sec/batch\n",
      "Epoch 7/20  Iteration 2409/7380 Training loss: 5.3229 1.0477 sec/batch\n",
      "Epoch 7/20  Iteration 2410/7380 Training loss: 5.3230 1.0451 sec/batch\n",
      "Epoch 7/20  Iteration 2411/7380 Training loss: 5.3231 1.0538 sec/batch\n",
      "Epoch 7/20  Iteration 2412/7380 Training loss: 5.3235 1.0483 sec/batch\n",
      "Epoch 7/20  Iteration 2413/7380 Training loss: 5.3234 1.0593 sec/batch\n",
      "Epoch 7/20  Iteration 2414/7380 Training loss: 5.3235 1.0509 sec/batch\n",
      "Epoch 7/20  Iteration 2415/7380 Training loss: 5.3239 1.0449 sec/batch\n",
      "Epoch 7/20  Iteration 2416/7380 Training loss: 5.3241 1.0429 sec/batch\n",
      "Epoch 7/20  Iteration 2417/7380 Training loss: 5.3245 1.0396 sec/batch\n",
      "Epoch 7/20  Iteration 2418/7380 Training loss: 5.3245 1.0398 sec/batch\n",
      "Epoch 7/20  Iteration 2419/7380 Training loss: 5.3249 1.0378 sec/batch\n",
      "Epoch 7/20  Iteration 2420/7380 Training loss: 5.3253 1.0401 sec/batch\n",
      "Epoch 7/20  Iteration 2421/7380 Training loss: 5.3244 1.0545 sec/batch\n",
      "Epoch 7/20  Iteration 2422/7380 Training loss: 5.3239 1.0467 sec/batch\n",
      "Epoch 7/20  Iteration 2423/7380 Training loss: 5.3235 1.0563 sec/batch\n",
      "Epoch 7/20  Iteration 2424/7380 Training loss: 5.3229 1.0580 sec/batch\n",
      "Epoch 7/20  Iteration 2425/7380 Training loss: 5.3226 1.0506 sec/batch\n",
      "Epoch 7/20  Iteration 2426/7380 Training loss: 5.3224 1.0466 sec/batch\n",
      "Epoch 7/20  Iteration 2427/7380 Training loss: 5.3223 1.0495 sec/batch\n",
      "Epoch 7/20  Iteration 2428/7380 Training loss: 5.3223 1.0555 sec/batch\n",
      "Epoch 7/20  Iteration 2429/7380 Training loss: 5.3218 1.0492 sec/batch\n",
      "Epoch 7/20  Iteration 2430/7380 Training loss: 5.3212 1.0572 sec/batch\n",
      "Epoch 7/20  Iteration 2431/7380 Training loss: 5.3206 1.0502 sec/batch\n",
      "Epoch 7/20  Iteration 2432/7380 Training loss: 5.3206 1.0439 sec/batch\n",
      "Epoch 7/20  Iteration 2433/7380 Training loss: 5.3210 1.0638 sec/batch\n",
      "Epoch 7/20  Iteration 2434/7380 Training loss: 5.3211 1.0535 sec/batch\n",
      "Epoch 7/20  Iteration 2435/7380 Training loss: 5.3208 1.0464 sec/batch\n",
      "Epoch 7/20  Iteration 2436/7380 Training loss: 5.3214 1.0563 sec/batch\n",
      "Epoch 7/20  Iteration 2437/7380 Training loss: 5.3214 1.0414 sec/batch\n",
      "Epoch 7/20  Iteration 2438/7380 Training loss: 5.3209 1.0421 sec/batch\n",
      "Epoch 7/20  Iteration 2439/7380 Training loss: 5.3210 1.0453 sec/batch\n",
      "Epoch 7/20  Iteration 2440/7380 Training loss: 5.3207 1.0774 sec/batch\n",
      "Epoch 7/20  Iteration 2441/7380 Training loss: 5.3205 1.0425 sec/batch\n",
      "Epoch 7/20  Iteration 2442/7380 Training loss: 5.3200 1.0483 sec/batch\n",
      "Epoch 7/20  Iteration 2443/7380 Training loss: 5.3200 1.0506 sec/batch\n",
      "Epoch 7/20  Iteration 2444/7380 Training loss: 5.3204 1.0441 sec/batch\n",
      "Epoch 7/20  Iteration 2445/7380 Training loss: 5.3199 1.0452 sec/batch\n",
      "Epoch 7/20  Iteration 2446/7380 Training loss: 5.3196 1.0504 sec/batch\n",
      "Epoch 7/20  Iteration 2447/7380 Training loss: 5.3192 1.0455 sec/batch\n",
      "Epoch 7/20  Iteration 2448/7380 Training loss: 5.3189 1.0551 sec/batch\n",
      "Epoch 7/20  Iteration 2449/7380 Training loss: 5.3186 1.0550 sec/batch\n",
      "Epoch 7/20  Iteration 2450/7380 Training loss: 5.3182 1.0501 sec/batch\n",
      "Validation loss: 5.2439 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 2451/7380 Training loss: 5.3181 1.0457 sec/batch\n",
      "Epoch 7/20  Iteration 2452/7380 Training loss: 5.3174 1.0579 sec/batch\n",
      "Epoch 7/20  Iteration 2453/7380 Training loss: 5.3164 1.0467 sec/batch\n",
      "Epoch 7/20  Iteration 2454/7380 Training loss: 5.3157 1.0554 sec/batch\n",
      "Epoch 7/20  Iteration 2455/7380 Training loss: 5.3158 1.0543 sec/batch\n",
      "Epoch 7/20  Iteration 2456/7380 Training loss: 5.3164 1.0740 sec/batch\n",
      "Epoch 7/20  Iteration 2457/7380 Training loss: 5.3160 1.0424 sec/batch\n",
      "Epoch 7/20  Iteration 2458/7380 Training loss: 5.3159 1.0587 sec/batch\n",
      "Epoch 7/20  Iteration 2459/7380 Training loss: 5.3160 1.0446 sec/batch\n",
      "Epoch 7/20  Iteration 2460/7380 Training loss: 5.3161 1.0525 sec/batch\n",
      "Epoch 7/20  Iteration 2461/7380 Training loss: 5.3161 1.0458 sec/batch\n",
      "Epoch 7/20  Iteration 2462/7380 Training loss: 5.3163 1.0508 sec/batch\n",
      "Epoch 7/20  Iteration 2463/7380 Training loss: 5.3161 1.0468 sec/batch\n",
      "Epoch 7/20  Iteration 2464/7380 Training loss: 5.3161 1.0461 sec/batch\n",
      "Epoch 7/20  Iteration 2465/7380 Training loss: 5.3157 1.0595 sec/batch\n",
      "Epoch 7/20  Iteration 2466/7380 Training loss: 5.3156 1.0533 sec/batch\n",
      "Epoch 7/20  Iteration 2467/7380 Training loss: 5.3157 1.0479 sec/batch\n",
      "Epoch 7/20  Iteration 2468/7380 Training loss: 5.3151 1.0936 sec/batch\n",
      "Epoch 7/20  Iteration 2469/7380 Training loss: 5.3153 1.0461 sec/batch\n",
      "Epoch 7/20  Iteration 2470/7380 Training loss: 5.3153 1.0444 sec/batch\n",
      "Epoch 7/20  Iteration 2471/7380 Training loss: 5.3156 1.0558 sec/batch\n",
      "Epoch 7/20  Iteration 2472/7380 Training loss: 5.3153 1.0484 sec/batch\n",
      "Epoch 7/20  Iteration 2473/7380 Training loss: 5.3156 1.0482 sec/batch\n",
      "Epoch 7/20  Iteration 2474/7380 Training loss: 5.3151 1.0489 sec/batch\n",
      "Epoch 7/20  Iteration 2475/7380 Training loss: 5.3149 1.0463 sec/batch\n",
      "Epoch 7/20  Iteration 2476/7380 Training loss: 5.3149 1.0435 sec/batch\n",
      "Epoch 7/20  Iteration 2477/7380 Training loss: 5.3151 1.0584 sec/batch\n",
      "Epoch 7/20  Iteration 2478/7380 Training loss: 5.3148 1.0518 sec/batch\n",
      "Epoch 7/20  Iteration 2479/7380 Training loss: 5.3145 1.0666 sec/batch\n",
      "Epoch 7/20  Iteration 2480/7380 Training loss: 5.3143 1.0486 sec/batch\n",
      "Epoch 7/20  Iteration 2481/7380 Training loss: 5.3144 1.0472 sec/batch\n",
      "Epoch 7/20  Iteration 2482/7380 Training loss: 5.3146 1.0477 sec/batch\n",
      "Epoch 7/20  Iteration 2483/7380 Training loss: 5.3139 1.0688 sec/batch\n",
      "Epoch 7/20  Iteration 2484/7380 Training loss: 5.3140 1.0472 sec/batch\n",
      "Epoch 7/20  Iteration 2485/7380 Training loss: 5.3141 1.0543 sec/batch\n",
      "Epoch 7/20  Iteration 2486/7380 Training loss: 5.3134 1.0547 sec/batch\n",
      "Epoch 7/20  Iteration 2487/7380 Training loss: 5.3135 1.0416 sec/batch\n",
      "Epoch 7/20  Iteration 2488/7380 Training loss: 5.3137 1.0479 sec/batch\n",
      "Epoch 7/20  Iteration 2489/7380 Training loss: 5.3134 1.0458 sec/batch\n",
      "Epoch 7/20  Iteration 2490/7380 Training loss: 5.3132 1.0453 sec/batch\n",
      "Epoch 7/20  Iteration 2491/7380 Training loss: 5.3138 1.0452 sec/batch\n",
      "Epoch 7/20  Iteration 2492/7380 Training loss: 5.3138 1.0457 sec/batch\n",
      "Epoch 7/20  Iteration 2493/7380 Training loss: 5.3139 1.0463 sec/batch\n",
      "Epoch 7/20  Iteration 2494/7380 Training loss: 5.3136 1.0584 sec/batch\n",
      "Epoch 7/20  Iteration 2495/7380 Training loss: 5.3136 1.0461 sec/batch\n",
      "Epoch 7/20  Iteration 2496/7380 Training loss: 5.3135 1.0619 sec/batch\n",
      "Epoch 7/20  Iteration 2497/7380 Training loss: 5.3136 1.0396 sec/batch\n",
      "Epoch 7/20  Iteration 2498/7380 Training loss: 5.3137 1.0433 sec/batch\n",
      "Epoch 7/20  Iteration 2499/7380 Training loss: 5.3139 1.0453 sec/batch\n",
      "Epoch 7/20  Iteration 2500/7380 Training loss: 5.3136 1.0523 sec/batch\n",
      "Validation loss: 5.23372 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 2501/7380 Training loss: 5.3135 1.0550 sec/batch\n",
      "Epoch 7/20  Iteration 2502/7380 Training loss: 5.3134 1.0449 sec/batch\n",
      "Epoch 7/20  Iteration 2503/7380 Training loss: 5.3131 1.0490 sec/batch\n",
      "Epoch 7/20  Iteration 2504/7380 Training loss: 5.3128 1.0497 sec/batch\n",
      "Epoch 7/20  Iteration 2505/7380 Training loss: 5.3124 1.0503 sec/batch\n",
      "Epoch 7/20  Iteration 2506/7380 Training loss: 5.3123 1.0512 sec/batch\n",
      "Epoch 7/20  Iteration 2507/7380 Training loss: 5.3122 1.0889 sec/batch\n",
      "Epoch 7/20  Iteration 2508/7380 Training loss: 5.3121 1.0480 sec/batch\n",
      "Epoch 7/20  Iteration 2509/7380 Training loss: 5.3122 1.0472 sec/batch\n",
      "Epoch 7/20  Iteration 2510/7380 Training loss: 5.3120 1.0465 sec/batch\n",
      "Epoch 7/20  Iteration 2511/7380 Training loss: 5.3118 1.0540 sec/batch\n",
      "Epoch 7/20  Iteration 2512/7380 Training loss: 5.3115 1.0435 sec/batch\n",
      "Epoch 7/20  Iteration 2513/7380 Training loss: 5.3116 1.0573 sec/batch\n",
      "Epoch 7/20  Iteration 2514/7380 Training loss: 5.3121 1.0509 sec/batch\n",
      "Epoch 7/20  Iteration 2515/7380 Training loss: 5.3120 1.0477 sec/batch\n",
      "Epoch 7/20  Iteration 2516/7380 Training loss: 5.3122 1.0593 sec/batch\n",
      "Epoch 7/20  Iteration 2517/7380 Training loss: 5.3122 1.0518 sec/batch\n",
      "Epoch 7/20  Iteration 2518/7380 Training loss: 5.3128 1.0413 sec/batch\n",
      "Epoch 7/20  Iteration 2519/7380 Training loss: 5.3131 1.0479 sec/batch\n",
      "Epoch 7/20  Iteration 2520/7380 Training loss: 5.3130 1.0429 sec/batch\n",
      "Epoch 7/20  Iteration 2521/7380 Training loss: 5.3125 1.0485 sec/batch\n",
      "Epoch 7/20  Iteration 2522/7380 Training loss: 5.3122 1.0504 sec/batch\n",
      "Epoch 7/20  Iteration 2523/7380 Training loss: 5.3119 1.0477 sec/batch\n",
      "Epoch 7/20  Iteration 2524/7380 Training loss: 5.3116 1.0475 sec/batch\n",
      "Epoch 7/20  Iteration 2525/7380 Training loss: 5.3115 1.0606 sec/batch\n",
      "Epoch 7/20  Iteration 2526/7380 Training loss: 5.3112 1.0460 sec/batch\n",
      "Epoch 7/20  Iteration 2527/7380 Training loss: 5.3110 1.0452 sec/batch\n",
      "Epoch 7/20  Iteration 2528/7380 Training loss: 5.3109 1.0435 sec/batch\n",
      "Epoch 7/20  Iteration 2529/7380 Training loss: 5.3109 1.0554 sec/batch\n",
      "Epoch 7/20  Iteration 2530/7380 Training loss: 5.3110 1.0577 sec/batch\n",
      "Epoch 7/20  Iteration 2531/7380 Training loss: 5.3112 1.0557 sec/batch\n",
      "Epoch 7/20  Iteration 2532/7380 Training loss: 5.3112 1.0505 sec/batch\n",
      "Epoch 7/20  Iteration 2533/7380 Training loss: 5.3117 1.0467 sec/batch\n",
      "Epoch 7/20  Iteration 2534/7380 Training loss: 5.3120 1.0432 sec/batch\n",
      "Epoch 7/20  Iteration 2535/7380 Training loss: 5.3117 1.0733 sec/batch\n",
      "Epoch 7/20  Iteration 2536/7380 Training loss: 5.3115 1.0480 sec/batch\n",
      "Epoch 7/20  Iteration 2537/7380 Training loss: 5.3113 1.0523 sec/batch\n",
      "Epoch 7/20  Iteration 2538/7380 Training loss: 5.3111 1.0724 sec/batch\n",
      "Epoch 7/20  Iteration 2539/7380 Training loss: 5.3112 1.0576 sec/batch\n",
      "Epoch 7/20  Iteration 2540/7380 Training loss: 5.3115 1.0481 sec/batch\n",
      "Epoch 7/20  Iteration 2541/7380 Training loss: 5.3112 1.0528 sec/batch\n",
      "Epoch 7/20  Iteration 2542/7380 Training loss: 5.3109 1.0435 sec/batch\n",
      "Epoch 7/20  Iteration 2543/7380 Training loss: 5.3106 1.0565 sec/batch\n",
      "Epoch 7/20  Iteration 2544/7380 Training loss: 5.3103 1.0486 sec/batch\n",
      "Epoch 7/20  Iteration 2545/7380 Training loss: 5.3099 1.0470 sec/batch\n",
      "Epoch 7/20  Iteration 2546/7380 Training loss: 5.3099 1.0519 sec/batch\n",
      "Epoch 7/20  Iteration 2547/7380 Training loss: 5.3095 1.0472 sec/batch\n",
      "Epoch 7/20  Iteration 2548/7380 Training loss: 5.3095 1.0437 sec/batch\n",
      "Epoch 7/20  Iteration 2549/7380 Training loss: 5.3095 1.0466 sec/batch\n",
      "Epoch 7/20  Iteration 2550/7380 Training loss: 5.3096 1.0601 sec/batch\n",
      "Validation loss: 5.23094 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 2551/7380 Training loss: 5.3098 1.0524 sec/batch\n",
      "Epoch 7/20  Iteration 2552/7380 Training loss: 5.3096 1.0512 sec/batch\n",
      "Epoch 7/20  Iteration 2553/7380 Training loss: 5.3095 1.0448 sec/batch\n",
      "Epoch 7/20  Iteration 2554/7380 Training loss: 5.3090 1.0574 sec/batch\n",
      "Epoch 7/20  Iteration 2555/7380 Training loss: 5.3086 1.0480 sec/batch\n",
      "Epoch 7/20  Iteration 2556/7380 Training loss: 5.3083 1.0522 sec/batch\n",
      "Epoch 7/20  Iteration 2557/7380 Training loss: 5.3081 1.0524 sec/batch\n",
      "Epoch 7/20  Iteration 2558/7380 Training loss: 5.3080 1.0466 sec/batch\n",
      "Epoch 7/20  Iteration 2559/7380 Training loss: 5.3079 1.0643 sec/batch\n",
      "Epoch 7/20  Iteration 2560/7380 Training loss: 5.3078 1.0487 sec/batch\n",
      "Epoch 7/20  Iteration 2561/7380 Training loss: 5.3079 1.0486 sec/batch\n",
      "Epoch 7/20  Iteration 2562/7380 Training loss: 5.3078 1.0469 sec/batch\n",
      "Epoch 7/20  Iteration 2563/7380 Training loss: 5.3076 1.0460 sec/batch\n",
      "Epoch 7/20  Iteration 2564/7380 Training loss: 5.3075 1.0452 sec/batch\n",
      "Epoch 7/20  Iteration 2565/7380 Training loss: 5.3078 1.0460 sec/batch\n",
      "Epoch 7/20  Iteration 2566/7380 Training loss: 5.3074 1.0455 sec/batch\n",
      "Epoch 7/20  Iteration 2567/7380 Training loss: 5.3074 1.0820 sec/batch\n",
      "Epoch 7/20  Iteration 2568/7380 Training loss: 5.3070 1.0489 sec/batch\n",
      "Epoch 7/20  Iteration 2569/7380 Training loss: 5.3068 1.0465 sec/batch\n",
      "Epoch 7/20  Iteration 2570/7380 Training loss: 5.3066 1.0515 sec/batch\n",
      "Epoch 7/20  Iteration 2571/7380 Training loss: 5.3067 1.0487 sec/batch\n",
      "Epoch 7/20  Iteration 2572/7380 Training loss: 5.3066 1.0448 sec/batch\n",
      "Epoch 7/20  Iteration 2573/7380 Training loss: 5.3063 1.0464 sec/batch\n",
      "Epoch 7/20  Iteration 2574/7380 Training loss: 5.3061 1.0470 sec/batch\n",
      "Epoch 7/20  Iteration 2575/7380 Training loss: 5.3060 1.0461 sec/batch\n",
      "Epoch 7/20  Iteration 2576/7380 Training loss: 5.3060 1.0592 sec/batch\n",
      "Epoch 7/20  Iteration 2577/7380 Training loss: 5.3058 1.0434 sec/batch\n",
      "Epoch 7/20  Iteration 2578/7380 Training loss: 5.3054 1.0492 sec/batch\n",
      "Epoch 7/20  Iteration 2579/7380 Training loss: 5.3051 1.0482 sec/batch\n",
      "Epoch 7/20  Iteration 2580/7380 Training loss: 5.3048 1.0834 sec/batch\n",
      "Epoch 7/20  Iteration 2581/7380 Training loss: 5.3045 1.0631 sec/batch\n",
      "Epoch 7/20  Iteration 2582/7380 Training loss: 5.3045 1.0632 sec/batch\n",
      "Epoch 7/20  Iteration 2583/7380 Training loss: 5.3042 1.0539 sec/batch\n",
      "Epoch 8/20  Iteration 2584/7380 Training loss: 5.3675 1.0459 sec/batch\n",
      "Epoch 8/20  Iteration 2585/7380 Training loss: 5.3381 1.0484 sec/batch\n",
      "Epoch 8/20  Iteration 2586/7380 Training loss: 5.3097 1.0457 sec/batch\n",
      "Epoch 8/20  Iteration 2587/7380 Training loss: 5.2911 1.0545 sec/batch\n",
      "Epoch 8/20  Iteration 2588/7380 Training loss: 5.2774 1.0526 sec/batch\n",
      "Epoch 8/20  Iteration 2589/7380 Training loss: 5.2799 1.0552 sec/batch\n",
      "Epoch 8/20  Iteration 2590/7380 Training loss: 5.2955 1.0528 sec/batch\n",
      "Epoch 8/20  Iteration 2591/7380 Training loss: 5.2811 1.0558 sec/batch\n",
      "Epoch 8/20  Iteration 2592/7380 Training loss: 5.2753 1.0839 sec/batch\n",
      "Epoch 8/20  Iteration 2593/7380 Training loss: 5.2866 1.0614 sec/batch\n",
      "Epoch 8/20  Iteration 2594/7380 Training loss: 5.2845 1.0588 sec/batch\n",
      "Epoch 8/20  Iteration 2595/7380 Training loss: 5.2744 1.0541 sec/batch\n",
      "Epoch 8/20  Iteration 2596/7380 Training loss: 5.2699 1.0608 sec/batch\n",
      "Epoch 8/20  Iteration 2597/7380 Training loss: 5.2765 1.0462 sec/batch\n",
      "Epoch 8/20  Iteration 2598/7380 Training loss: 5.2813 1.0707 sec/batch\n",
      "Epoch 8/20  Iteration 2599/7380 Training loss: 5.2785 1.0451 sec/batch\n",
      "Epoch 8/20  Iteration 2600/7380 Training loss: 5.2772 1.0451 sec/batch\n",
      "Validation loss: 5.21823 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2601/7380 Training loss: 5.2676 1.0533 sec/batch\n",
      "Epoch 8/20  Iteration 2602/7380 Training loss: 5.2606 1.0508 sec/batch\n",
      "Epoch 8/20  Iteration 2603/7380 Training loss: 5.2592 1.0569 sec/batch\n",
      "Epoch 8/20  Iteration 2604/7380 Training loss: 5.2548 1.0538 sec/batch\n",
      "Epoch 8/20  Iteration 2605/7380 Training loss: 5.2617 1.0633 sec/batch\n",
      "Epoch 8/20  Iteration 2606/7380 Training loss: 5.2620 1.0587 sec/batch\n",
      "Epoch 8/20  Iteration 2607/7380 Training loss: 5.2691 1.0619 sec/batch\n",
      "Epoch 8/20  Iteration 2608/7380 Training loss: 5.2707 1.0563 sec/batch\n",
      "Epoch 8/20  Iteration 2609/7380 Training loss: 5.2703 1.0509 sec/batch\n",
      "Epoch 8/20  Iteration 2610/7380 Training loss: 5.2650 1.0436 sec/batch\n",
      "Epoch 8/20  Iteration 2611/7380 Training loss: 5.2697 1.0581 sec/batch\n",
      "Epoch 8/20  Iteration 2612/7380 Training loss: 5.2680 1.0498 sec/batch\n",
      "Epoch 8/20  Iteration 2613/7380 Training loss: 5.2681 1.0607 sec/batch\n",
      "Epoch 8/20  Iteration 2614/7380 Training loss: 5.2662 1.0585 sec/batch\n",
      "Epoch 8/20  Iteration 2615/7380 Training loss: 5.2666 1.0551 sec/batch\n",
      "Epoch 8/20  Iteration 2616/7380 Training loss: 5.2662 1.0617 sec/batch\n",
      "Epoch 8/20  Iteration 2617/7380 Training loss: 5.2669 1.0474 sec/batch\n",
      "Epoch 8/20  Iteration 2618/7380 Training loss: 5.2710 1.0438 sec/batch\n",
      "Epoch 8/20  Iteration 2619/7380 Training loss: 5.2706 1.0561 sec/batch\n",
      "Epoch 8/20  Iteration 2620/7380 Training loss: 5.2719 1.0496 sec/batch\n",
      "Epoch 8/20  Iteration 2621/7380 Training loss: 5.2709 1.0559 sec/batch\n",
      "Epoch 8/20  Iteration 2622/7380 Training loss: 5.2723 1.0801 sec/batch\n",
      "Epoch 8/20  Iteration 2623/7380 Training loss: 5.2712 1.0492 sec/batch\n",
      "Epoch 8/20  Iteration 2624/7380 Training loss: 5.2710 1.0554 sec/batch\n",
      "Epoch 8/20  Iteration 2625/7380 Training loss: 5.2691 1.0498 sec/batch\n",
      "Epoch 8/20  Iteration 2626/7380 Training loss: 5.2697 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2627/7380 Training loss: 5.2707 1.0658 sec/batch\n",
      "Epoch 8/20  Iteration 2628/7380 Training loss: 5.2716 1.0458 sec/batch\n",
      "Epoch 8/20  Iteration 2629/7380 Training loss: 5.2693 1.0567 sec/batch\n",
      "Epoch 8/20  Iteration 2630/7380 Training loss: 5.2698 1.0503 sec/batch\n",
      "Epoch 8/20  Iteration 2631/7380 Training loss: 5.2672 1.0530 sec/batch\n",
      "Epoch 8/20  Iteration 2632/7380 Training loss: 5.2685 1.0497 sec/batch\n",
      "Epoch 8/20  Iteration 2633/7380 Training loss: 5.2678 1.0489 sec/batch\n",
      "Epoch 8/20  Iteration 2634/7380 Training loss: 5.2668 1.0509 sec/batch\n",
      "Epoch 8/20  Iteration 2635/7380 Training loss: 5.2661 1.0488 sec/batch\n",
      "Epoch 8/20  Iteration 2636/7380 Training loss: 5.2643 1.0519 sec/batch\n",
      "Epoch 8/20  Iteration 2637/7380 Training loss: 5.2637 1.0496 sec/batch\n",
      "Epoch 8/20  Iteration 2638/7380 Training loss: 5.2625 1.0602 sec/batch\n",
      "Epoch 8/20  Iteration 2639/7380 Training loss: 5.2600 1.0538 sec/batch\n",
      "Epoch 8/20  Iteration 2640/7380 Training loss: 5.2592 1.0519 sec/batch\n",
      "Epoch 8/20  Iteration 2641/7380 Training loss: 5.2596 1.0480 sec/batch\n",
      "Epoch 8/20  Iteration 2642/7380 Training loss: 5.2589 1.0655 sec/batch\n",
      "Epoch 8/20  Iteration 2643/7380 Training loss: 5.2594 1.0520 sec/batch\n",
      "Epoch 8/20  Iteration 2644/7380 Training loss: 5.2576 1.0509 sec/batch\n",
      "Epoch 8/20  Iteration 2645/7380 Training loss: 5.2580 1.0508 sec/batch\n",
      "Epoch 8/20  Iteration 2646/7380 Training loss: 5.2562 1.0500 sec/batch\n",
      "Epoch 8/20  Iteration 2647/7380 Training loss: 5.2551 1.0529 sec/batch\n",
      "Epoch 8/20  Iteration 2648/7380 Training loss: 5.2557 1.0563 sec/batch\n",
      "Epoch 8/20  Iteration 2649/7380 Training loss: 5.2557 1.0494 sec/batch\n",
      "Epoch 8/20  Iteration 2650/7380 Training loss: 5.2565 1.0883 sec/batch\n",
      "Validation loss: 5.21197 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2651/7380 Training loss: 5.2584 1.0577 sec/batch\n",
      "Epoch 8/20  Iteration 2652/7380 Training loss: 5.2602 1.0453 sec/batch\n",
      "Epoch 8/20  Iteration 2653/7380 Training loss: 5.2596 1.0576 sec/batch\n",
      "Epoch 8/20  Iteration 2654/7380 Training loss: 5.2583 1.0876 sec/batch\n",
      "Epoch 8/20  Iteration 2655/7380 Training loss: 5.2596 1.0472 sec/batch\n",
      "Epoch 8/20  Iteration 2656/7380 Training loss: 5.2579 1.0601 sec/batch\n",
      "Epoch 8/20  Iteration 2657/7380 Training loss: 5.2600 1.0484 sec/batch\n",
      "Epoch 8/20  Iteration 2658/7380 Training loss: 5.2609 1.0565 sec/batch\n",
      "Epoch 8/20  Iteration 2659/7380 Training loss: 5.2599 1.0611 sec/batch\n",
      "Epoch 8/20  Iteration 2660/7380 Training loss: 5.2595 1.0510 sec/batch\n",
      "Epoch 8/20  Iteration 2661/7380 Training loss: 5.2595 1.0525 sec/batch\n",
      "Epoch 8/20  Iteration 2662/7380 Training loss: 5.2603 1.0563 sec/batch\n",
      "Epoch 8/20  Iteration 2663/7380 Training loss: 5.2609 1.0476 sec/batch\n",
      "Epoch 8/20  Iteration 2664/7380 Training loss: 5.2606 1.0585 sec/batch\n",
      "Epoch 8/20  Iteration 2665/7380 Training loss: 5.2599 1.0841 sec/batch\n",
      "Epoch 8/20  Iteration 2666/7380 Training loss: 5.2588 1.0501 sec/batch\n",
      "Epoch 8/20  Iteration 2667/7380 Training loss: 5.2580 1.0464 sec/batch\n",
      "Epoch 8/20  Iteration 2668/7380 Training loss: 5.2587 1.0485 sec/batch\n",
      "Epoch 8/20  Iteration 2669/7380 Training loss: 5.2584 1.0521 sec/batch\n",
      "Epoch 8/20  Iteration 2670/7380 Training loss: 5.2581 1.0479 sec/batch\n",
      "Epoch 8/20  Iteration 2671/7380 Training loss: 5.2578 1.0568 sec/batch\n",
      "Epoch 8/20  Iteration 2672/7380 Training loss: 5.2561 1.0516 sec/batch\n",
      "Epoch 8/20  Iteration 2673/7380 Training loss: 5.2560 1.0604 sec/batch\n",
      "Epoch 8/20  Iteration 2674/7380 Training loss: 5.2565 1.0503 sec/batch\n",
      "Epoch 8/20  Iteration 2675/7380 Training loss: 5.2583 1.0492 sec/batch\n",
      "Epoch 8/20  Iteration 2676/7380 Training loss: 5.2579 1.0600 sec/batch\n",
      "Epoch 8/20  Iteration 2677/7380 Training loss: 5.2574 1.0501 sec/batch\n",
      "Epoch 8/20  Iteration 2678/7380 Training loss: 5.2572 1.0475 sec/batch\n",
      "Epoch 8/20  Iteration 2679/7380 Training loss: 5.2574 1.0572 sec/batch\n",
      "Epoch 8/20  Iteration 2680/7380 Training loss: 5.2571 1.0569 sec/batch\n",
      "Epoch 8/20  Iteration 2681/7380 Training loss: 5.2586 1.0458 sec/batch\n",
      "Epoch 8/20  Iteration 2682/7380 Training loss: 5.2585 1.0583 sec/batch\n",
      "Epoch 8/20  Iteration 2683/7380 Training loss: 5.2585 1.0495 sec/batch\n",
      "Epoch 8/20  Iteration 2684/7380 Training loss: 5.2597 1.0476 sec/batch\n",
      "Epoch 8/20  Iteration 2685/7380 Training loss: 5.2600 1.0611 sec/batch\n",
      "Epoch 8/20  Iteration 2686/7380 Training loss: 5.2605 1.0460 sec/batch\n",
      "Epoch 8/20  Iteration 2687/7380 Training loss: 5.2600 1.0552 sec/batch\n",
      "Epoch 8/20  Iteration 2688/7380 Training loss: 5.2604 1.0700 sec/batch\n",
      "Epoch 8/20  Iteration 2689/7380 Training loss: 5.2605 1.0608 sec/batch\n",
      "Epoch 8/20  Iteration 2690/7380 Training loss: 5.2605 1.0551 sec/batch\n",
      "Epoch 8/20  Iteration 2691/7380 Training loss: 5.2603 1.0513 sec/batch\n",
      "Epoch 8/20  Iteration 2692/7380 Training loss: 5.2588 1.0503 sec/batch\n",
      "Epoch 8/20  Iteration 2693/7380 Training loss: 5.2584 1.0500 sec/batch\n",
      "Epoch 8/20  Iteration 2694/7380 Training loss: 5.2578 1.0948 sec/batch\n",
      "Epoch 8/20  Iteration 2695/7380 Training loss: 5.2575 1.0506 sec/batch\n",
      "Epoch 8/20  Iteration 2696/7380 Training loss: 5.2567 1.0456 sec/batch\n",
      "Epoch 8/20  Iteration 2697/7380 Training loss: 5.2570 1.0477 sec/batch\n",
      "Epoch 8/20  Iteration 2698/7380 Training loss: 5.2570 1.0499 sec/batch\n",
      "Epoch 8/20  Iteration 2699/7380 Training loss: 5.2563 1.0540 sec/batch\n",
      "Epoch 8/20  Iteration 2700/7380 Training loss: 5.2557 1.0561 sec/batch\n",
      "Validation loss: 5.20577 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2701/7380 Training loss: 5.2558 1.0568 sec/batch\n",
      "Epoch 8/20  Iteration 2702/7380 Training loss: 5.2563 1.0631 sec/batch\n",
      "Epoch 8/20  Iteration 2703/7380 Training loss: 5.2560 1.0587 sec/batch\n",
      "Epoch 8/20  Iteration 2704/7380 Training loss: 5.2565 1.0759 sec/batch\n",
      "Epoch 8/20  Iteration 2705/7380 Training loss: 5.2577 1.0531 sec/batch\n",
      "Epoch 8/20  Iteration 2706/7380 Training loss: 5.2575 1.0492 sec/batch\n",
      "Epoch 8/20  Iteration 2707/7380 Training loss: 5.2578 1.0640 sec/batch\n",
      "Epoch 8/20  Iteration 2708/7380 Training loss: 5.2575 1.0666 sec/batch\n",
      "Epoch 8/20  Iteration 2709/7380 Training loss: 5.2566 1.0607 sec/batch\n",
      "Epoch 8/20  Iteration 2710/7380 Training loss: 5.2555 1.0547 sec/batch\n",
      "Epoch 8/20  Iteration 2711/7380 Training loss: 5.2558 1.0592 sec/batch\n",
      "Epoch 8/20  Iteration 2712/7380 Training loss: 5.2552 1.0629 sec/batch\n",
      "Epoch 8/20  Iteration 2713/7380 Training loss: 5.2551 1.0549 sec/batch\n",
      "Epoch 8/20  Iteration 2714/7380 Training loss: 5.2552 1.0485 sec/batch\n",
      "Epoch 8/20  Iteration 2715/7380 Training loss: 5.2555 1.0540 sec/batch\n",
      "Epoch 8/20  Iteration 2716/7380 Training loss: 5.2558 1.0492 sec/batch\n",
      "Epoch 8/20  Iteration 2717/7380 Training loss: 5.2549 1.0489 sec/batch\n",
      "Epoch 8/20  Iteration 2718/7380 Training loss: 5.2545 1.0585 sec/batch\n",
      "Epoch 8/20  Iteration 2719/7380 Training loss: 5.2548 1.0509 sec/batch\n",
      "Epoch 8/20  Iteration 2720/7380 Training loss: 5.2545 1.0565 sec/batch\n",
      "Epoch 8/20  Iteration 2721/7380 Training loss: 5.2545 1.0710 sec/batch\n",
      "Epoch 8/20  Iteration 2722/7380 Training loss: 5.2536 1.0514 sec/batch\n",
      "Epoch 8/20  Iteration 2723/7380 Training loss: 5.2528 1.0484 sec/batch\n",
      "Epoch 8/20  Iteration 2724/7380 Training loss: 5.2542 1.0603 sec/batch\n",
      "Epoch 8/20  Iteration 2725/7380 Training loss: 5.2543 1.0514 sec/batch\n",
      "Epoch 8/20  Iteration 2726/7380 Training loss: 5.2548 1.1051 sec/batch\n",
      "Epoch 8/20  Iteration 2727/7380 Training loss: 5.2545 1.0651 sec/batch\n",
      "Epoch 8/20  Iteration 2728/7380 Training loss: 5.2547 1.0492 sec/batch\n",
      "Epoch 8/20  Iteration 2729/7380 Training loss: 5.2544 1.0613 sec/batch\n",
      "Epoch 8/20  Iteration 2730/7380 Training loss: 5.2542 1.0495 sec/batch\n",
      "Epoch 8/20  Iteration 2731/7380 Training loss: 5.2542 1.0615 sec/batch\n",
      "Epoch 8/20  Iteration 2732/7380 Training loss: 5.2535 1.0632 sec/batch\n",
      "Epoch 8/20  Iteration 2733/7380 Training loss: 5.2538 1.0629 sec/batch\n",
      "Epoch 8/20  Iteration 2734/7380 Training loss: 5.2531 1.0487 sec/batch\n",
      "Epoch 8/20  Iteration 2735/7380 Training loss: 5.2536 1.0495 sec/batch\n",
      "Epoch 8/20  Iteration 2736/7380 Training loss: 5.2537 1.0506 sec/batch\n",
      "Epoch 8/20  Iteration 2737/7380 Training loss: 5.2542 1.0524 sec/batch\n",
      "Epoch 8/20  Iteration 2738/7380 Training loss: 5.2540 1.0673 sec/batch\n",
      "Epoch 8/20  Iteration 2739/7380 Training loss: 5.2535 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2740/7380 Training loss: 5.2527 1.0579 sec/batch\n",
      "Epoch 8/20  Iteration 2741/7380 Training loss: 5.2534 1.0512 sec/batch\n",
      "Epoch 8/20  Iteration 2742/7380 Training loss: 5.2528 1.0495 sec/batch\n",
      "Epoch 8/20  Iteration 2743/7380 Training loss: 5.2526 1.0535 sec/batch\n",
      "Epoch 8/20  Iteration 2744/7380 Training loss: 5.2531 1.0525 sec/batch\n",
      "Epoch 8/20  Iteration 2745/7380 Training loss: 5.2534 1.0475 sec/batch\n",
      "Epoch 8/20  Iteration 2746/7380 Training loss: 5.2536 1.0627 sec/batch\n",
      "Epoch 8/20  Iteration 2747/7380 Training loss: 5.2541 1.0455 sec/batch\n",
      "Epoch 8/20  Iteration 2748/7380 Training loss: 5.2544 1.0476 sec/batch\n",
      "Epoch 8/20  Iteration 2749/7380 Training loss: 5.2548 1.0662 sec/batch\n",
      "Epoch 8/20  Iteration 2750/7380 Training loss: 5.2548 1.0897 sec/batch\n",
      "Validation loss: 5.20104 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2751/7380 Training loss: 5.2553 1.0645 sec/batch\n",
      "Epoch 8/20  Iteration 2752/7380 Training loss: 5.2546 1.0546 sec/batch\n",
      "Epoch 8/20  Iteration 2753/7380 Training loss: 5.2540 1.0548 sec/batch\n",
      "Epoch 8/20  Iteration 2754/7380 Training loss: 5.2543 1.0506 sec/batch\n",
      "Epoch 8/20  Iteration 2755/7380 Training loss: 5.2549 1.0508 sec/batch\n",
      "Epoch 8/20  Iteration 2756/7380 Training loss: 5.2549 1.0476 sec/batch\n",
      "Epoch 8/20  Iteration 2757/7380 Training loss: 5.2544 1.0509 sec/batch\n",
      "Epoch 8/20  Iteration 2758/7380 Training loss: 5.2541 1.0468 sec/batch\n",
      "Epoch 8/20  Iteration 2759/7380 Training loss: 5.2539 1.0798 sec/batch\n",
      "Epoch 8/20  Iteration 2760/7380 Training loss: 5.2538 1.0511 sec/batch\n",
      "Epoch 8/20  Iteration 2761/7380 Training loss: 5.2540 1.0895 sec/batch\n",
      "Epoch 8/20  Iteration 2762/7380 Training loss: 5.2533 1.0519 sec/batch\n",
      "Epoch 8/20  Iteration 2763/7380 Training loss: 5.2527 1.0578 sec/batch\n",
      "Epoch 8/20  Iteration 2764/7380 Training loss: 5.2524 1.0583 sec/batch\n",
      "Epoch 8/20  Iteration 2765/7380 Training loss: 5.2524 1.0494 sec/batch\n",
      "Epoch 8/20  Iteration 2766/7380 Training loss: 5.2529 1.0601 sec/batch\n",
      "Epoch 8/20  Iteration 2767/7380 Training loss: 5.2525 1.0458 sec/batch\n",
      "Epoch 8/20  Iteration 2768/7380 Training loss: 5.2525 1.0559 sec/batch\n",
      "Epoch 8/20  Iteration 2769/7380 Training loss: 5.2525 1.0595 sec/batch\n",
      "Epoch 8/20  Iteration 2770/7380 Training loss: 5.2520 1.0577 sec/batch\n",
      "Epoch 8/20  Iteration 2771/7380 Training loss: 5.2516 1.0753 sec/batch\n",
      "Epoch 8/20  Iteration 2772/7380 Training loss: 5.2512 1.0633 sec/batch\n",
      "Epoch 8/20  Iteration 2773/7380 Training loss: 5.2512 1.0669 sec/batch\n",
      "Epoch 8/20  Iteration 2774/7380 Training loss: 5.2503 1.0738 sec/batch\n",
      "Epoch 8/20  Iteration 2775/7380 Training loss: 5.2507 1.0657 sec/batch\n",
      "Epoch 8/20  Iteration 2776/7380 Training loss: 5.2512 1.0562 sec/batch\n",
      "Epoch 8/20  Iteration 2777/7380 Training loss: 5.2506 1.0532 sec/batch\n",
      "Epoch 8/20  Iteration 2778/7380 Training loss: 5.2509 1.0517 sec/batch\n",
      "Epoch 8/20  Iteration 2779/7380 Training loss: 5.2509 1.0530 sec/batch\n",
      "Epoch 8/20  Iteration 2780/7380 Training loss: 5.2510 1.0655 sec/batch\n",
      "Epoch 8/20  Iteration 2781/7380 Training loss: 5.2514 1.0910 sec/batch\n",
      "Epoch 8/20  Iteration 2782/7380 Training loss: 5.2512 1.0577 sec/batch\n",
      "Epoch 8/20  Iteration 2783/7380 Training loss: 5.2513 1.0695 sec/batch\n",
      "Epoch 8/20  Iteration 2784/7380 Training loss: 5.2518 1.0542 sec/batch\n",
      "Epoch 8/20  Iteration 2785/7380 Training loss: 5.2521 1.0552 sec/batch\n",
      "Epoch 8/20  Iteration 2786/7380 Training loss: 5.2525 1.0475 sec/batch\n",
      "Epoch 8/20  Iteration 2787/7380 Training loss: 5.2525 1.0463 sec/batch\n",
      "Epoch 8/20  Iteration 2788/7380 Training loss: 5.2528 1.0605 sec/batch\n",
      "Epoch 8/20  Iteration 2789/7380 Training loss: 5.2530 1.0584 sec/batch\n",
      "Epoch 8/20  Iteration 2790/7380 Training loss: 5.2522 1.0546 sec/batch\n",
      "Epoch 8/20  Iteration 2791/7380 Training loss: 5.2519 1.0514 sec/batch\n",
      "Epoch 8/20  Iteration 2792/7380 Training loss: 5.2515 1.0642 sec/batch\n",
      "Epoch 8/20  Iteration 2793/7380 Training loss: 5.2509 1.0548 sec/batch\n",
      "Epoch 8/20  Iteration 2794/7380 Training loss: 5.2508 1.0579 sec/batch\n",
      "Epoch 8/20  Iteration 2795/7380 Training loss: 5.2506 1.0495 sec/batch\n",
      "Epoch 8/20  Iteration 2796/7380 Training loss: 5.2505 1.0582 sec/batch\n",
      "Epoch 8/20  Iteration 2797/7380 Training loss: 5.2505 1.0457 sec/batch\n",
      "Epoch 8/20  Iteration 2798/7380 Training loss: 5.2501 1.0492 sec/batch\n",
      "Epoch 8/20  Iteration 2799/7380 Training loss: 5.2495 1.0930 sec/batch\n",
      "Epoch 8/20  Iteration 2800/7380 Training loss: 5.2489 1.0548 sec/batch\n",
      "Validation loss: 5.20659 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2801/7380 Training loss: 5.2494 1.0623 sec/batch\n",
      "Epoch 8/20  Iteration 2802/7380 Training loss: 5.2498 1.0562 sec/batch\n",
      "Epoch 8/20  Iteration 2803/7380 Training loss: 5.2499 1.0553 sec/batch\n",
      "Epoch 8/20  Iteration 2804/7380 Training loss: 5.2496 1.0619 sec/batch\n",
      "Epoch 8/20  Iteration 2805/7380 Training loss: 5.2500 1.0514 sec/batch\n",
      "Epoch 8/20  Iteration 2806/7380 Training loss: 5.2500 1.0531 sec/batch\n",
      "Epoch 8/20  Iteration 2807/7380 Training loss: 5.2496 1.0562 sec/batch\n",
      "Epoch 8/20  Iteration 2808/7380 Training loss: 5.2497 1.0548 sec/batch\n",
      "Epoch 8/20  Iteration 2809/7380 Training loss: 5.2494 1.0541 sec/batch\n",
      "Epoch 8/20  Iteration 2810/7380 Training loss: 5.2493 1.0527 sec/batch\n",
      "Epoch 8/20  Iteration 2811/7380 Training loss: 5.2487 1.0515 sec/batch\n",
      "Epoch 8/20  Iteration 2812/7380 Training loss: 5.2487 1.0571 sec/batch\n",
      "Epoch 8/20  Iteration 2813/7380 Training loss: 5.2491 1.0618 sec/batch\n",
      "Epoch 8/20  Iteration 2814/7380 Training loss: 5.2488 1.0617 sec/batch\n",
      "Epoch 8/20  Iteration 2815/7380 Training loss: 5.2484 1.0596 sec/batch\n",
      "Epoch 8/20  Iteration 2816/7380 Training loss: 5.2480 1.0579 sec/batch\n",
      "Epoch 8/20  Iteration 2817/7380 Training loss: 5.2476 1.0516 sec/batch\n",
      "Epoch 8/20  Iteration 2818/7380 Training loss: 5.2474 1.0547 sec/batch\n",
      "Epoch 8/20  Iteration 2819/7380 Training loss: 5.2470 1.0922 sec/batch\n",
      "Epoch 8/20  Iteration 2820/7380 Training loss: 5.2467 1.0514 sec/batch\n",
      "Epoch 8/20  Iteration 2821/7380 Training loss: 5.2460 1.0496 sec/batch\n",
      "Epoch 8/20  Iteration 2822/7380 Training loss: 5.2449 1.0512 sec/batch\n",
      "Epoch 8/20  Iteration 2823/7380 Training loss: 5.2443 1.0506 sec/batch\n",
      "Epoch 8/20  Iteration 2824/7380 Training loss: 5.2445 1.0538 sec/batch\n",
      "Epoch 8/20  Iteration 2825/7380 Training loss: 5.2451 1.0508 sec/batch\n",
      "Epoch 8/20  Iteration 2826/7380 Training loss: 5.2447 1.0555 sec/batch\n",
      "Epoch 8/20  Iteration 2827/7380 Training loss: 5.2447 1.0522 sec/batch\n",
      "Epoch 8/20  Iteration 2828/7380 Training loss: 5.2448 1.0528 sec/batch\n",
      "Epoch 8/20  Iteration 2829/7380 Training loss: 5.2449 1.0576 sec/batch\n",
      "Epoch 8/20  Iteration 2830/7380 Training loss: 5.2448 1.0504 sec/batch\n",
      "Epoch 8/20  Iteration 2831/7380 Training loss: 5.2448 1.0513 sec/batch\n",
      "Epoch 8/20  Iteration 2832/7380 Training loss: 5.2447 1.0523 sec/batch\n",
      "Epoch 8/20  Iteration 2833/7380 Training loss: 5.2448 1.0522 sec/batch\n",
      "Epoch 8/20  Iteration 2834/7380 Training loss: 5.2443 1.0514 sec/batch\n",
      "Epoch 8/20  Iteration 2835/7380 Training loss: 5.2442 1.0635 sec/batch\n",
      "Epoch 8/20  Iteration 2836/7380 Training loss: 5.2441 1.0559 sec/batch\n",
      "Epoch 8/20  Iteration 2837/7380 Training loss: 5.2434 1.0600 sec/batch\n",
      "Epoch 8/20  Iteration 2838/7380 Training loss: 5.2437 1.0569 sec/batch\n",
      "Epoch 8/20  Iteration 2839/7380 Training loss: 5.2440 1.0528 sec/batch\n",
      "Epoch 8/20  Iteration 2840/7380 Training loss: 5.2444 1.0502 sec/batch\n",
      "Epoch 8/20  Iteration 2841/7380 Training loss: 5.2440 1.0928 sec/batch\n",
      "Epoch 8/20  Iteration 2842/7380 Training loss: 5.2444 1.0784 sec/batch\n",
      "Epoch 8/20  Iteration 2843/7380 Training loss: 5.2439 1.0504 sec/batch\n",
      "Epoch 8/20  Iteration 2844/7380 Training loss: 5.2437 1.0528 sec/batch\n",
      "Epoch 8/20  Iteration 2845/7380 Training loss: 5.2437 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2846/7380 Training loss: 5.2439 1.0821 sec/batch\n",
      "Epoch 8/20  Iteration 2847/7380 Training loss: 5.2437 1.0703 sec/batch\n",
      "Epoch 8/20  Iteration 2848/7380 Training loss: 5.2434 1.0528 sec/batch\n",
      "Epoch 8/20  Iteration 2849/7380 Training loss: 5.2431 1.0803 sec/batch\n",
      "Epoch 8/20  Iteration 2850/7380 Training loss: 5.2431 1.0527 sec/batch\n",
      "Validation loss: 5.19008 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2851/7380 Training loss: 5.2435 1.0585 sec/batch\n",
      "Epoch 8/20  Iteration 2852/7380 Training loss: 5.2430 1.0614 sec/batch\n",
      "Epoch 8/20  Iteration 2853/7380 Training loss: 5.2433 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2854/7380 Training loss: 5.2433 1.0507 sec/batch\n",
      "Epoch 8/20  Iteration 2855/7380 Training loss: 5.2428 1.0458 sec/batch\n",
      "Epoch 8/20  Iteration 2856/7380 Training loss: 5.2429 1.0665 sec/batch\n",
      "Epoch 8/20  Iteration 2857/7380 Training loss: 5.2430 1.0897 sec/batch\n",
      "Epoch 8/20  Iteration 2858/7380 Training loss: 5.2428 1.0549 sec/batch\n",
      "Epoch 8/20  Iteration 2859/7380 Training loss: 5.2425 1.0612 sec/batch\n",
      "Epoch 8/20  Iteration 2860/7380 Training loss: 5.2431 1.0579 sec/batch\n",
      "Epoch 8/20  Iteration 2861/7380 Training loss: 5.2430 1.0619 sec/batch\n",
      "Epoch 8/20  Iteration 2862/7380 Training loss: 5.2431 1.0588 sec/batch\n",
      "Epoch 8/20  Iteration 2863/7380 Training loss: 5.2428 1.0520 sec/batch\n",
      "Epoch 8/20  Iteration 2864/7380 Training loss: 5.2429 1.0519 sec/batch\n",
      "Epoch 8/20  Iteration 2865/7380 Training loss: 5.2428 1.0529 sec/batch\n",
      "Epoch 8/20  Iteration 2866/7380 Training loss: 5.2430 1.0496 sec/batch\n",
      "Epoch 8/20  Iteration 2867/7380 Training loss: 5.2431 1.0485 sec/batch\n",
      "Epoch 8/20  Iteration 2868/7380 Training loss: 5.2434 1.0500 sec/batch\n",
      "Epoch 8/20  Iteration 2869/7380 Training loss: 5.2432 1.0495 sec/batch\n",
      "Epoch 8/20  Iteration 2870/7380 Training loss: 5.2428 1.0529 sec/batch\n",
      "Epoch 8/20  Iteration 2871/7380 Training loss: 5.2427 1.0630 sec/batch\n",
      "Epoch 8/20  Iteration 2872/7380 Training loss: 5.2424 1.0591 sec/batch\n",
      "Epoch 8/20  Iteration 2873/7380 Training loss: 5.2420 1.0552 sec/batch\n",
      "Epoch 8/20  Iteration 2874/7380 Training loss: 5.2416 1.0538 sec/batch\n",
      "Epoch 8/20  Iteration 2875/7380 Training loss: 5.2414 1.0608 sec/batch\n",
      "Epoch 8/20  Iteration 2876/7380 Training loss: 5.2412 1.0744 sec/batch\n",
      "Epoch 8/20  Iteration 2877/7380 Training loss: 5.2411 1.0559 sec/batch\n",
      "Epoch 8/20  Iteration 2878/7380 Training loss: 5.2413 1.0529 sec/batch\n",
      "Epoch 8/20  Iteration 2879/7380 Training loss: 5.2412 1.0741 sec/batch\n",
      "Epoch 8/20  Iteration 2880/7380 Training loss: 5.2409 1.0582 sec/batch\n",
      "Epoch 8/20  Iteration 2881/7380 Training loss: 5.2405 1.0615 sec/batch\n",
      "Epoch 8/20  Iteration 2882/7380 Training loss: 5.2406 1.0508 sec/batch\n",
      "Epoch 8/20  Iteration 2883/7380 Training loss: 5.2411 1.0564 sec/batch\n",
      "Epoch 8/20  Iteration 2884/7380 Training loss: 5.2408 1.0534 sec/batch\n",
      "Epoch 8/20  Iteration 2885/7380 Training loss: 5.2410 1.0550 sec/batch\n",
      "Epoch 8/20  Iteration 2886/7380 Training loss: 5.2409 1.0750 sec/batch\n",
      "Epoch 8/20  Iteration 2887/7380 Training loss: 5.2415 1.0527 sec/batch\n",
      "Epoch 8/20  Iteration 2888/7380 Training loss: 5.2417 1.0554 sec/batch\n",
      "Epoch 8/20  Iteration 2889/7380 Training loss: 5.2416 1.0549 sec/batch\n",
      "Epoch 8/20  Iteration 2890/7380 Training loss: 5.2412 1.0530 sec/batch\n",
      "Epoch 8/20  Iteration 2891/7380 Training loss: 5.2408 1.0541 sec/batch\n",
      "Epoch 8/20  Iteration 2892/7380 Training loss: 5.2405 1.0538 sec/batch\n",
      "Epoch 8/20  Iteration 2893/7380 Training loss: 5.2401 1.0541 sec/batch\n",
      "Epoch 8/20  Iteration 2894/7380 Training loss: 5.2400 1.0559 sec/batch\n",
      "Epoch 8/20  Iteration 2895/7380 Training loss: 5.2397 1.0528 sec/batch\n",
      "Epoch 8/20  Iteration 2896/7380 Training loss: 5.2395 1.0594 sec/batch\n",
      "Epoch 8/20  Iteration 2897/7380 Training loss: 5.2395 1.0543 sec/batch\n",
      "Epoch 8/20  Iteration 2898/7380 Training loss: 5.2396 1.0537 sec/batch\n",
      "Epoch 8/20  Iteration 2899/7380 Training loss: 5.2397 1.0497 sec/batch\n",
      "Epoch 8/20  Iteration 2900/7380 Training loss: 5.2399 1.0512 sec/batch\n",
      "Validation loss: 5.18789 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2901/7380 Training loss: 5.2403 1.0671 sec/batch\n",
      "Epoch 8/20  Iteration 2902/7380 Training loss: 5.2409 1.0516 sec/batch\n",
      "Epoch 8/20  Iteration 2903/7380 Training loss: 5.2412 1.0581 sec/batch\n",
      "Epoch 8/20  Iteration 2904/7380 Training loss: 5.2409 1.0556 sec/batch\n",
      "Epoch 8/20  Iteration 2905/7380 Training loss: 5.2406 1.0764 sec/batch\n",
      "Epoch 8/20  Iteration 2906/7380 Training loss: 5.2404 1.0597 sec/batch\n",
      "Epoch 8/20  Iteration 2907/7380 Training loss: 5.2402 1.0458 sec/batch\n",
      "Epoch 8/20  Iteration 2908/7380 Training loss: 5.2402 1.0503 sec/batch\n",
      "Epoch 8/20  Iteration 2909/7380 Training loss: 5.2407 1.0567 sec/batch\n",
      "Epoch 8/20  Iteration 2910/7380 Training loss: 5.2403 1.0535 sec/batch\n",
      "Epoch 8/20  Iteration 2911/7380 Training loss: 5.2400 1.0513 sec/batch\n",
      "Epoch 8/20  Iteration 2912/7380 Training loss: 5.2398 1.0611 sec/batch\n",
      "Epoch 8/20  Iteration 2913/7380 Training loss: 5.2395 1.0558 sec/batch\n",
      "Epoch 8/20  Iteration 2914/7380 Training loss: 5.2392 1.0981 sec/batch\n",
      "Epoch 8/20  Iteration 2915/7380 Training loss: 5.2393 1.0539 sec/batch\n",
      "Epoch 8/20  Iteration 2916/7380 Training loss: 5.2389 1.0545 sec/batch\n",
      "Epoch 8/20  Iteration 2917/7380 Training loss: 5.2390 1.0619 sec/batch\n",
      "Epoch 8/20  Iteration 2918/7380 Training loss: 5.2388 1.0630 sec/batch\n",
      "Epoch 8/20  Iteration 2919/7380 Training loss: 5.2389 1.0499 sec/batch\n",
      "Epoch 8/20  Iteration 2920/7380 Training loss: 5.2391 1.0546 sec/batch\n",
      "Epoch 8/20  Iteration 2921/7380 Training loss: 5.2390 1.0535 sec/batch\n",
      "Epoch 8/20  Iteration 2922/7380 Training loss: 5.2389 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2923/7380 Training loss: 5.2386 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2924/7380 Training loss: 5.2381 1.0674 sec/batch\n",
      "Epoch 8/20  Iteration 2925/7380 Training loss: 5.2380 1.0562 sec/batch\n",
      "Epoch 8/20  Iteration 2926/7380 Training loss: 5.2380 1.0526 sec/batch\n",
      "Epoch 8/20  Iteration 2927/7380 Training loss: 5.2379 1.0602 sec/batch\n",
      "Epoch 8/20  Iteration 2928/7380 Training loss: 5.2377 1.0562 sec/batch\n",
      "Epoch 8/20  Iteration 2929/7380 Training loss: 5.2376 1.0533 sec/batch\n",
      "Epoch 8/20  Iteration 2930/7380 Training loss: 5.2377 1.0647 sec/batch\n",
      "Epoch 8/20  Iteration 2931/7380 Training loss: 5.2376 1.0559 sec/batch\n",
      "Epoch 8/20  Iteration 2932/7380 Training loss: 5.2374 1.0959 sec/batch\n",
      "Epoch 8/20  Iteration 2933/7380 Training loss: 5.2374 1.0676 sec/batch\n",
      "Epoch 8/20  Iteration 2934/7380 Training loss: 5.2378 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2935/7380 Training loss: 5.2373 1.0538 sec/batch\n",
      "Epoch 8/20  Iteration 2936/7380 Training loss: 5.2373 1.0551 sec/batch\n",
      "Epoch 8/20  Iteration 2937/7380 Training loss: 5.2370 1.0674 sec/batch\n",
      "Epoch 8/20  Iteration 2938/7380 Training loss: 5.2367 1.0593 sec/batch\n",
      "Epoch 8/20  Iteration 2939/7380 Training loss: 5.2365 1.0628 sec/batch\n",
      "Epoch 8/20  Iteration 2940/7380 Training loss: 5.2366 1.0758 sec/batch\n",
      "Epoch 8/20  Iteration 2941/7380 Training loss: 5.2365 1.0597 sec/batch\n",
      "Epoch 8/20  Iteration 2942/7380 Training loss: 5.2362 1.0536 sec/batch\n",
      "Epoch 8/20  Iteration 2943/7380 Training loss: 5.2359 1.0588 sec/batch\n",
      "Epoch 8/20  Iteration 2944/7380 Training loss: 5.2359 1.0527 sec/batch\n",
      "Epoch 8/20  Iteration 2945/7380 Training loss: 5.2360 1.0542 sec/batch\n",
      "Epoch 8/20  Iteration 2946/7380 Training loss: 5.2358 1.0647 sec/batch\n",
      "Epoch 8/20  Iteration 2947/7380 Training loss: 5.2355 1.0618 sec/batch\n",
      "Epoch 8/20  Iteration 2948/7380 Training loss: 5.2352 1.0560 sec/batch\n",
      "Epoch 8/20  Iteration 2949/7380 Training loss: 5.2349 1.0546 sec/batch\n",
      "Epoch 8/20  Iteration 2950/7380 Training loss: 5.2346 1.0536 sec/batch\n",
      "Validation loss: 5.17965 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2951/7380 Training loss: 5.2348 1.0593 sec/batch\n",
      "Epoch 8/20  Iteration 2952/7380 Training loss: 5.2346 1.0568 sec/batch\n",
      "Epoch 9/20  Iteration 2953/7380 Training loss: 5.2769 1.0582 sec/batch\n",
      "Epoch 9/20  Iteration 2954/7380 Training loss: 5.2495 1.0550 sec/batch\n",
      "Epoch 9/20  Iteration 2955/7380 Training loss: 5.2126 1.0559 sec/batch\n",
      "Epoch 9/20  Iteration 2956/7380 Training loss: 5.1985 1.0639 sec/batch\n",
      "Epoch 9/20  Iteration 2957/7380 Training loss: 5.1927 1.0524 sec/batch\n",
      "Epoch 9/20  Iteration 2958/7380 Training loss: 5.2020 1.0555 sec/batch\n",
      "Epoch 9/20  Iteration 2959/7380 Training loss: 5.2218 1.0557 sec/batch\n",
      "Epoch 9/20  Iteration 2960/7380 Training loss: 5.2092 1.0557 sec/batch\n",
      "Epoch 9/20  Iteration 2961/7380 Training loss: 5.2077 1.0507 sec/batch\n",
      "Epoch 9/20  Iteration 2962/7380 Training loss: 5.2203 1.0486 sec/batch\n",
      "Epoch 9/20  Iteration 2963/7380 Training loss: 5.2187 1.0473 sec/batch\n",
      "Epoch 9/20  Iteration 2964/7380 Training loss: 5.2065 1.0550 sec/batch\n",
      "Epoch 9/20  Iteration 2965/7380 Training loss: 5.2037 1.0554 sec/batch\n",
      "Epoch 9/20  Iteration 2966/7380 Training loss: 5.2129 1.0561 sec/batch\n",
      "Epoch 9/20  Iteration 2967/7380 Training loss: 5.2184 1.0562 sec/batch\n",
      "Epoch 9/20  Iteration 2968/7380 Training loss: 5.2181 1.0558 sec/batch\n",
      "Epoch 9/20  Iteration 2969/7380 Training loss: 5.2144 1.0536 sec/batch\n",
      "Epoch 9/20  Iteration 2970/7380 Training loss: 5.2002 1.0639 sec/batch\n",
      "Epoch 9/20  Iteration 2971/7380 Training loss: 5.1941 1.0543 sec/batch\n",
      "Epoch 9/20  Iteration 2972/7380 Training loss: 5.1922 1.0535 sec/batch\n",
      "Epoch 9/20  Iteration 2973/7380 Training loss: 5.1888 1.0709 sec/batch\n",
      "Epoch 9/20  Iteration 2974/7380 Training loss: 5.1947 1.0909 sec/batch\n",
      "Epoch 9/20  Iteration 2975/7380 Training loss: 5.1946 1.0588 sec/batch\n",
      "Epoch 9/20  Iteration 2976/7380 Training loss: 5.2022 1.0646 sec/batch\n",
      "Epoch 9/20  Iteration 2977/7380 Training loss: 5.2041 1.0606 sec/batch\n",
      "Epoch 9/20  Iteration 2978/7380 Training loss: 5.2045 1.0670 sec/batch\n",
      "Epoch 9/20  Iteration 2979/7380 Training loss: 5.1991 1.0648 sec/batch\n",
      "Epoch 9/20  Iteration 2980/7380 Training loss: 5.2037 1.0507 sec/batch\n",
      "Epoch 9/20  Iteration 2981/7380 Training loss: 5.2027 1.0523 sec/batch\n",
      "Epoch 9/20  Iteration 2982/7380 Training loss: 5.2017 1.0560 sec/batch\n",
      "Epoch 9/20  Iteration 2983/7380 Training loss: 5.2000 1.0649 sec/batch\n",
      "Epoch 9/20  Iteration 2984/7380 Training loss: 5.2011 1.0612 sec/batch\n",
      "Epoch 9/20  Iteration 2985/7380 Training loss: 5.2015 1.0660 sec/batch\n",
      "Epoch 9/20  Iteration 2986/7380 Training loss: 5.2030 1.0611 sec/batch\n",
      "Epoch 9/20  Iteration 2987/7380 Training loss: 5.2063 1.0618 sec/batch\n",
      "Epoch 9/20  Iteration 2988/7380 Training loss: 5.2052 1.0617 sec/batch\n",
      "Epoch 9/20  Iteration 2989/7380 Training loss: 5.2062 1.0768 sec/batch\n",
      "Epoch 9/20  Iteration 2990/7380 Training loss: 5.2048 1.0541 sec/batch\n",
      "Epoch 9/20  Iteration 2991/7380 Training loss: 5.2068 1.0567 sec/batch\n",
      "Epoch 9/20  Iteration 2992/7380 Training loss: 5.2066 1.0939 sec/batch\n",
      "Epoch 9/20  Iteration 2993/7380 Training loss: 5.2067 1.0584 sec/batch\n",
      "Epoch 9/20  Iteration 2994/7380 Training loss: 5.2049 1.0522 sec/batch\n",
      "Epoch 9/20  Iteration 2995/7380 Training loss: 5.2061 1.0669 sec/batch\n",
      "Epoch 9/20  Iteration 2996/7380 Training loss: 5.2079 1.0573 sec/batch\n",
      "Epoch 9/20  Iteration 2997/7380 Training loss: 5.2089 1.0567 sec/batch\n",
      "Epoch 9/20  Iteration 2998/7380 Training loss: 5.2069 1.0565 sec/batch\n",
      "Epoch 9/20  Iteration 2999/7380 Training loss: 5.2072 1.0845 sec/batch\n",
      "Epoch 9/20  Iteration 3000/7380 Training loss: 5.2043 1.0528 sec/batch\n",
      "Validation loss: 5.17198 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 3001/7380 Training loss: 5.2068 1.0689 sec/batch\n",
      "Epoch 9/20  Iteration 3002/7380 Training loss: 5.2055 1.0622 sec/batch\n",
      "Epoch 9/20  Iteration 3003/7380 Training loss: 5.2042 1.0677 sec/batch\n",
      "Epoch 9/20  Iteration 3004/7380 Training loss: 5.2033 1.0564 sec/batch\n",
      "Epoch 9/20  Iteration 3005/7380 Training loss: 5.2020 1.0567 sec/batch\n",
      "Epoch 9/20  Iteration 3006/7380 Training loss: 5.2013 1.0575 sec/batch\n",
      "Epoch 9/20  Iteration 3007/7380 Training loss: 5.1996 1.0566 sec/batch\n",
      "Epoch 9/20  Iteration 3008/7380 Training loss: 5.1971 1.0579 sec/batch\n",
      "Epoch 9/20  Iteration 3009/7380 Training loss: 5.1966 1.0562 sec/batch\n",
      "Epoch 9/20  Iteration 3010/7380 Training loss: 5.1973 1.0537 sec/batch\n",
      "Epoch 9/20  Iteration 3011/7380 Training loss: 5.1969 1.0588 sec/batch\n",
      "Epoch 9/20  Iteration 3012/7380 Training loss: 5.1976 1.0558 sec/batch\n",
      "Epoch 9/20  Iteration 3013/7380 Training loss: 5.1958 1.0580 sec/batch\n",
      "Epoch 9/20  Iteration 3014/7380 Training loss: 5.1965 1.0615 sec/batch\n",
      "Epoch 9/20  Iteration 3015/7380 Training loss: 5.1949 1.0597 sec/batch\n",
      "Epoch 9/20  Iteration 3016/7380 Training loss: 5.1932 1.0603 sec/batch\n",
      "Epoch 9/20  Iteration 3017/7380 Training loss: 5.1940 1.0590 sec/batch\n",
      "Epoch 9/20  Iteration 3018/7380 Training loss: 5.1934 1.0532 sec/batch\n",
      "Epoch 9/20  Iteration 3019/7380 Training loss: 5.1943 1.0600 sec/batch\n",
      "Epoch 9/20  Iteration 3020/7380 Training loss: 5.1949 1.0524 sec/batch\n",
      "Epoch 9/20  Iteration 3021/7380 Training loss: 5.1966 1.0508 sec/batch\n",
      "Epoch 9/20  Iteration 3022/7380 Training loss: 5.1955 1.0733 sec/batch\n",
      "Epoch 9/20  Iteration 3023/7380 Training loss: 5.1942 1.0517 sec/batch\n",
      "Epoch 9/20  Iteration 3024/7380 Training loss: 5.1954 1.0559 sec/batch\n",
      "Epoch 9/20  Iteration 3025/7380 Training loss: 5.1937 1.0534 sec/batch\n",
      "Epoch 9/20  Iteration 3026/7380 Training loss: 5.1955 1.0598 sec/batch\n",
      "Epoch 9/20  Iteration 3027/7380 Training loss: 5.1966 1.0695 sec/batch\n",
      "Epoch 9/20  Iteration 3028/7380 Training loss: 5.1962 1.0826 sec/batch\n",
      "Epoch 9/20  Iteration 3029/7380 Training loss: 5.1961 1.0550 sec/batch\n",
      "Epoch 9/20  Iteration 3030/7380 Training loss: 5.1960 1.0506 sec/batch\n",
      "Epoch 9/20  Iteration 3031/7380 Training loss: 5.1969 1.0498 sec/batch\n",
      "Epoch 9/20  Iteration 3032/7380 Training loss: 5.1975 1.0780 sec/batch\n",
      "Epoch 9/20  Iteration 3033/7380 Training loss: 5.1969 1.0633 sec/batch\n",
      "Epoch 9/20  Iteration 3034/7380 Training loss: 5.1962 1.0612 sec/batch\n",
      "Epoch 9/20  Iteration 3035/7380 Training loss: 5.1949 1.0598 sec/batch\n",
      "Epoch 9/20  Iteration 3036/7380 Training loss: 5.1941 1.0612 sec/batch\n",
      "Epoch 9/20  Iteration 3037/7380 Training loss: 5.1946 1.0651 sec/batch\n",
      "Epoch 9/20  Iteration 3038/7380 Training loss: 5.1939 1.0602 sec/batch\n",
      "Epoch 9/20  Iteration 3039/7380 Training loss: 5.1933 1.0542 sec/batch\n",
      "Epoch 9/20  Iteration 3040/7380 Training loss: 5.1934 1.0565 sec/batch\n",
      "Epoch 9/20  Iteration 3041/7380 Training loss: 5.1916 1.0955 sec/batch\n",
      "Epoch 9/20  Iteration 3042/7380 Training loss: 5.1910 1.0632 sec/batch\n",
      "Epoch 9/20  Iteration 3043/7380 Training loss: 5.1916 1.0538 sec/batch\n",
      "Epoch 9/20  Iteration 3044/7380 Training loss: 5.1935 1.0617 sec/batch\n",
      "Epoch 9/20  Iteration 3045/7380 Training loss: 5.1931 1.0567 sec/batch\n",
      "Epoch 9/20  Iteration 3046/7380 Training loss: 5.1929 1.0529 sec/batch\n",
      "Epoch 9/20  Iteration 3047/7380 Training loss: 5.1923 1.0555 sec/batch\n",
      "Epoch 9/20  Iteration 3048/7380 Training loss: 5.1927 1.0548 sec/batch\n",
      "Epoch 9/20  Iteration 3049/7380 Training loss: 5.1926 1.0670 sec/batch\n",
      "Epoch 9/20  Iteration 3050/7380 Training loss: 5.1938 1.0548 sec/batch\n",
      "Validation loss: 5.16476 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 3051/7380 Training loss: 5.1947 1.0659 sec/batch\n",
      "Epoch 9/20  Iteration 3052/7380 Training loss: 5.1947 1.0559 sec/batch\n",
      "Epoch 9/20  Iteration 3053/7380 Training loss: 5.1961 1.0519 sec/batch\n",
      "Epoch 9/20  Iteration 3054/7380 Training loss: 5.1963 1.0617 sec/batch\n",
      "Epoch 9/20  Iteration 3055/7380 Training loss: 5.1963 1.0545 sec/batch\n",
      "Epoch 9/20  Iteration 3056/7380 Training loss: 5.1956 1.0543 sec/batch\n",
      "Epoch 9/20  Iteration 3057/7380 Training loss: 5.1963 1.0542 sec/batch\n",
      "Epoch 9/20  Iteration 3058/7380 Training loss: 5.1963 1.0526 sec/batch\n",
      "Epoch 9/20  Iteration 3059/7380 Training loss: 5.1965 1.0509 sec/batch\n",
      "Epoch 9/20  Iteration 3060/7380 Training loss: 5.1963 1.0545 sec/batch\n",
      "Epoch 9/20  Iteration 3061/7380 Training loss: 5.1947 1.0511 sec/batch\n",
      "Epoch 9/20  Iteration 3062/7380 Training loss: 5.1944 1.0609 sec/batch\n",
      "Epoch 9/20  Iteration 3063/7380 Training loss: 5.1938 1.0531 sec/batch\n",
      "Epoch 9/20  Iteration 3064/7380 Training loss: 5.1939 1.0672 sec/batch\n",
      "Epoch 9/20  Iteration 3065/7380 Training loss: 5.1929 1.0610 sec/batch\n",
      "Epoch 9/20  Iteration 3066/7380 Training loss: 5.1934 1.0547 sec/batch\n",
      "Epoch 9/20  Iteration 3067/7380 Training loss: 5.1933 1.0600 sec/batch\n",
      "Epoch 9/20  Iteration 3068/7380 Training loss: 5.1927 1.0648 sec/batch\n",
      "Epoch 9/20  Iteration 3069/7380 Training loss: 5.1920 1.0664 sec/batch\n",
      "Epoch 9/20  Iteration 3070/7380 Training loss: 5.1916 1.0632 sec/batch\n",
      "Epoch 9/20  Iteration 3071/7380 Training loss: 5.1920 1.0648 sec/batch\n",
      "Epoch 9/20  Iteration 3072/7380 Training loss: 5.1915 1.0648 sec/batch\n",
      "Epoch 9/20  Iteration 3073/7380 Training loss: 5.1920 1.0626 sec/batch\n",
      "Epoch 9/20  Iteration 3074/7380 Training loss: 5.1930 1.0581 sec/batch\n",
      "Epoch 9/20  Iteration 3075/7380 Training loss: 5.1928 1.0551 sec/batch\n",
      "Epoch 9/20  Iteration 3076/7380 Training loss: 5.1928 1.0737 sec/batch\n",
      "Epoch 9/20  Iteration 3077/7380 Training loss: 5.1926 1.0579 sec/batch\n",
      "Epoch 9/20  Iteration 3078/7380 Training loss: 5.1917 1.0574 sec/batch\n",
      "Epoch 9/20  Iteration 3079/7380 Training loss: 5.1904 1.0562 sec/batch\n",
      "Epoch 9/20  Iteration 3080/7380 Training loss: 5.1905 1.0595 sec/batch\n",
      "Epoch 9/20  Iteration 3081/7380 Training loss: 5.1899 1.0607 sec/batch\n",
      "Epoch 9/20  Iteration 3082/7380 Training loss: 5.1901 1.0959 sec/batch\n",
      "Epoch 9/20  Iteration 3083/7380 Training loss: 5.1902 1.0673 sec/batch\n",
      "Epoch 9/20  Iteration 3084/7380 Training loss: 5.1905 1.0535 sec/batch\n",
      "Epoch 9/20  Iteration 3085/7380 Training loss: 5.1907 1.0512 sec/batch\n",
      "Epoch 9/20  Iteration 3086/7380 Training loss: 5.1898 1.0535 sec/batch\n",
      "Epoch 9/20  Iteration 3087/7380 Training loss: 5.1895 1.0557 sec/batch\n",
      "Epoch 9/20  Iteration 3088/7380 Training loss: 5.1896 1.0654 sec/batch\n",
      "Epoch 9/20  Iteration 3089/7380 Training loss: 5.1890 1.0721 sec/batch\n",
      "Epoch 9/20  Iteration 3090/7380 Training loss: 5.1888 1.0747 sec/batch\n",
      "Epoch 9/20  Iteration 3091/7380 Training loss: 5.1881 1.0700 sec/batch\n",
      "Epoch 9/20  Iteration 3092/7380 Training loss: 5.1873 1.0613 sec/batch\n",
      "Epoch 9/20  Iteration 3093/7380 Training loss: 5.1887 1.0568 sec/batch\n",
      "Epoch 9/20  Iteration 3094/7380 Training loss: 5.1886 1.0560 sec/batch\n",
      "Epoch 9/20  Iteration 3095/7380 Training loss: 5.1891 1.0497 sec/batch\n",
      "Epoch 9/20  Iteration 3096/7380 Training loss: 5.1891 1.0547 sec/batch\n",
      "Epoch 9/20  Iteration 3097/7380 Training loss: 5.1893 1.0551 sec/batch\n",
      "Epoch 9/20  Iteration 3098/7380 Training loss: 5.1891 1.0578 sec/batch\n",
      "Epoch 9/20  Iteration 3099/7380 Training loss: 5.1890 1.0656 sec/batch\n",
      "Epoch 9/20  Iteration 3100/7380 Training loss: 5.1889 1.0674 sec/batch\n",
      "Validation loss: 5.17471 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 3101/7380 Training loss: 5.1890 1.0668 sec/batch\n",
      "Epoch 9/20  Iteration 3102/7380 Training loss: 5.1894 1.0608 sec/batch\n",
      "Epoch 9/20  Iteration 3103/7380 Training loss: 5.1887 1.0626 sec/batch\n",
      "Epoch 9/20  Iteration 3104/7380 Training loss: 5.1892 1.0543 sec/batch\n",
      "Epoch 9/20  Iteration 3105/7380 Training loss: 5.1895 1.0556 sec/batch\n",
      "Epoch 9/20  Iteration 3106/7380 Training loss: 5.1899 1.0533 sec/batch\n",
      "Epoch 9/20  Iteration 3107/7380 Training loss: 5.1898 1.0609 sec/batch\n",
      "Epoch 9/20  Iteration 3108/7380 Training loss: 5.1893 1.0650 sec/batch\n",
      "Epoch 9/20  Iteration 3109/7380 Training loss: 5.1889 1.0632 sec/batch\n",
      "Epoch 9/20  Iteration 3110/7380 Training loss: 5.1897 1.0570 sec/batch\n",
      "Epoch 9/20  Iteration 3111/7380 Training loss: 5.1895 1.0645 sec/batch\n",
      "Epoch 9/20  Iteration 3112/7380 Training loss: 5.1895 1.0777 sec/batch\n",
      "Epoch 9/20  Iteration 3113/7380 Training loss: 5.1900 1.0577 sec/batch\n",
      "Epoch 9/20  Iteration 3114/7380 Training loss: 5.1903 1.0495 sec/batch\n",
      "Epoch 9/20  Iteration 3115/7380 Training loss: 5.1907 1.0513 sec/batch\n",
      "Epoch 9/20  Iteration 3116/7380 Training loss: 5.1911 1.0769 sec/batch\n",
      "Epoch 9/20  Iteration 3117/7380 Training loss: 5.1914 1.0536 sec/batch\n",
      "Epoch 9/20  Iteration 3118/7380 Training loss: 5.1918 1.0567 sec/batch\n",
      "Epoch 9/20  Iteration 3119/7380 Training loss: 5.1918 1.0849 sec/batch\n",
      "Epoch 9/20  Iteration 3120/7380 Training loss: 5.1922 1.0732 sec/batch\n",
      "Epoch 9/20  Iteration 3121/7380 Training loss: 5.1917 1.0669 sec/batch\n",
      "Epoch 9/20  Iteration 3122/7380 Training loss: 5.1912 1.0650 sec/batch\n",
      "Epoch 9/20  Iteration 3123/7380 Training loss: 5.1915 1.0670 sec/batch\n",
      "Epoch 9/20  Iteration 3124/7380 Training loss: 5.1920 1.0816 sec/batch\n",
      "Epoch 9/20  Iteration 3125/7380 Training loss: 5.1921 1.0628 sec/batch\n",
      "Epoch 9/20  Iteration 3126/7380 Training loss: 5.1915 1.0597 sec/batch\n",
      "Epoch 9/20  Iteration 3127/7380 Training loss: 5.1914 1.0622 sec/batch\n",
      "Epoch 9/20  Iteration 3128/7380 Training loss: 5.1910 1.0605 sec/batch\n",
      "Epoch 9/20  Iteration 3129/7380 Training loss: 5.1908 1.0599 sec/batch\n",
      "Epoch 9/20  Iteration 3130/7380 Training loss: 5.1912 1.0553 sec/batch\n",
      "Epoch 9/20  Iteration 3131/7380 Training loss: 5.1905 1.0901 sec/batch\n",
      "Epoch 9/20  Iteration 3132/7380 Training loss: 5.1901 1.0709 sec/batch\n",
      "Epoch 9/20  Iteration 3133/7380 Training loss: 5.1899 1.0770 sec/batch\n",
      "Epoch 9/20  Iteration 3134/7380 Training loss: 5.1900 1.0647 sec/batch\n",
      "Epoch 9/20  Iteration 3135/7380 Training loss: 5.1906 1.0586 sec/batch\n",
      "Epoch 9/20  Iteration 3136/7380 Training loss: 5.1904 1.0633 sec/batch\n",
      "Epoch 9/20  Iteration 3137/7380 Training loss: 5.1903 1.0594 sec/batch\n",
      "Epoch 9/20  Iteration 3138/7380 Training loss: 5.1904 1.0620 sec/batch\n",
      "Epoch 9/20  Iteration 3139/7380 Training loss: 5.1901 1.0560 sec/batch\n",
      "Epoch 9/20  Iteration 3140/7380 Training loss: 5.1899 1.0564 sec/batch\n",
      "Epoch 9/20  Iteration 3141/7380 Training loss: 5.1896 1.0590 sec/batch\n",
      "Epoch 9/20  Iteration 3142/7380 Training loss: 5.1897 1.0681 sec/batch\n",
      "Epoch 9/20  Iteration 3143/7380 Training loss: 5.1889 1.0849 sec/batch\n",
      "Epoch 9/20  Iteration 3144/7380 Training loss: 5.1893 1.0693 sec/batch\n",
      "Epoch 9/20  Iteration 3145/7380 Training loss: 5.1897 1.0673 sec/batch\n",
      "Epoch 9/20  Iteration 3146/7380 Training loss: 5.1891 1.0586 sec/batch\n",
      "Epoch 9/20  Iteration 3147/7380 Training loss: 5.1894 1.0597 sec/batch\n",
      "Epoch 9/20  Iteration 3148/7380 Training loss: 5.1896 1.0574 sec/batch\n",
      "Epoch 9/20  Iteration 3149/7380 Training loss: 5.1897 1.0619 sec/batch\n",
      "Epoch 9/20  Iteration 3150/7380 Training loss: 5.1902 1.0531 sec/batch\n",
      "Validation loss: 5.16088 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 3151/7380 Training loss: 5.1905 1.0639 sec/batch\n",
      "Epoch 9/20  Iteration 3152/7380 Training loss: 5.1905 1.0620 sec/batch\n",
      "Epoch 9/20  Iteration 3153/7380 Training loss: 5.1908 1.0545 sec/batch\n",
      "Epoch 9/20  Iteration 3154/7380 Training loss: 5.1911 1.0612 sec/batch\n",
      "Epoch 9/20  Iteration 3155/7380 Training loss: 5.1917 1.0565 sec/batch\n",
      "Epoch 9/20  Iteration 3156/7380 Training loss: 5.1917 1.0604 sec/batch\n",
      "Epoch 9/20  Iteration 3157/7380 Training loss: 5.1919 1.0605 sec/batch\n",
      "Epoch 9/20  Iteration 3158/7380 Training loss: 5.1923 1.0668 sec/batch\n",
      "Epoch 9/20  Iteration 3159/7380 Training loss: 5.1914 1.0599 sec/batch\n",
      "Epoch 9/20  Iteration 3160/7380 Training loss: 5.1910 1.0677 sec/batch\n",
      "Epoch 9/20  Iteration 3161/7380 Training loss: 5.1905 1.0558 sec/batch\n",
      "Epoch 9/20  Iteration 3162/7380 Training loss: 5.1898 1.0676 sec/batch\n",
      "Epoch 9/20  Iteration 3163/7380 Training loss: 5.1897 1.0631 sec/batch\n",
      "Epoch 9/20  Iteration 3164/7380 Training loss: 5.1894 1.0611 sec/batch\n",
      "Epoch 9/20  Iteration 3165/7380 Training loss: 5.1894 1.0596 sec/batch\n",
      "Epoch 9/20  Iteration 3166/7380 Training loss: 5.1894 1.0601 sec/batch\n",
      "Epoch 9/20  Iteration 3167/7380 Training loss: 5.1892 1.0614 sec/batch\n",
      "Epoch 9/20  Iteration 3168/7380 Training loss: 5.1888 1.0584 sec/batch\n",
      "Epoch 9/20  Iteration 3169/7380 Training loss: 5.1881 1.0648 sec/batch\n",
      "Epoch 9/20  Iteration 3170/7380 Training loss: 5.1882 1.0677 sec/batch\n",
      "Epoch 9/20  Iteration 3171/7380 Training loss: 5.1884 1.0998 sec/batch\n",
      "Epoch 9/20  Iteration 3172/7380 Training loss: 5.1885 1.0736 sec/batch\n",
      "Epoch 9/20  Iteration 3173/7380 Training loss: 5.1883 1.0687 sec/batch\n",
      "Epoch 9/20  Iteration 3174/7380 Training loss: 5.1887 1.0846 sec/batch\n",
      "Epoch 9/20  Iteration 3175/7380 Training loss: 5.1888 1.0589 sec/batch\n",
      "Epoch 9/20  Iteration 3176/7380 Training loss: 5.1883 1.0586 sec/batch\n",
      "Epoch 9/20  Iteration 3177/7380 Training loss: 5.1884 1.0594 sec/batch\n",
      "Epoch 9/20  Iteration 3178/7380 Training loss: 5.1881 1.0652 sec/batch\n",
      "Epoch 9/20  Iteration 3179/7380 Training loss: 5.1880 1.0593 sec/batch\n",
      "Epoch 9/20  Iteration 3180/7380 Training loss: 5.1876 1.0603 sec/batch\n",
      "Epoch 9/20  Iteration 3181/7380 Training loss: 5.1875 1.0604 sec/batch\n",
      "Epoch 9/20  Iteration 3182/7380 Training loss: 5.1880 1.0635 sec/batch\n",
      "Epoch 9/20  Iteration 3183/7380 Training loss: 5.1876 1.0675 sec/batch\n",
      "Epoch 9/20  Iteration 3184/7380 Training loss: 5.1874 1.0627 sec/batch\n",
      "Epoch 9/20  Iteration 3185/7380 Training loss: 5.1871 1.0622 sec/batch\n",
      "Epoch 9/20  Iteration 3186/7380 Training loss: 5.1867 1.0619 sec/batch\n",
      "Epoch 9/20  Iteration 3187/7380 Training loss: 5.1864 1.0707 sec/batch\n",
      "Epoch 9/20  Iteration 3188/7380 Training loss: 5.1861 1.0603 sec/batch\n",
      "Epoch 9/20  Iteration 3189/7380 Training loss: 5.1858 1.0740 sec/batch\n",
      "Epoch 9/20  Iteration 3190/7380 Training loss: 5.1852 1.0559 sec/batch\n",
      "Epoch 9/20  Iteration 3191/7380 Training loss: 5.1841 1.0953 sec/batch\n",
      "Epoch 9/20  Iteration 3192/7380 Training loss: 5.1835 1.0701 sec/batch\n",
      "Epoch 9/20  Iteration 3193/7380 Training loss: 5.1835 1.0733 sec/batch\n",
      "Epoch 9/20  Iteration 3194/7380 Training loss: 5.1841 1.0589 sec/batch\n",
      "Epoch 9/20  Iteration 3195/7380 Training loss: 5.1838 1.0595 sec/batch\n",
      "Epoch 9/20  Iteration 3196/7380 Training loss: 5.1839 1.0728 sec/batch\n",
      "Epoch 9/20  Iteration 3197/7380 Training loss: 5.1839 1.0570 sec/batch\n",
      "Epoch 9/20  Iteration 3198/7380 Training loss: 5.1840 1.0860 sec/batch\n",
      "Epoch 9/20  Iteration 3199/7380 Training loss: 5.1840 1.0767 sec/batch\n",
      "Epoch 9/20  Iteration 3200/7380 Training loss: 5.1841 1.0552 sec/batch\n",
      "Validation loss: 5.16264 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 3201/7380 Training loss: 5.1842 1.0648 sec/batch\n",
      "Epoch 9/20  Iteration 3202/7380 Training loss: 5.1844 1.0576 sec/batch\n",
      "Epoch 9/20  Iteration 3203/7380 Training loss: 5.1839 1.0561 sec/batch\n",
      "Epoch 9/20  Iteration 3204/7380 Training loss: 5.1837 1.0714 sec/batch\n",
      "Epoch 9/20  Iteration 3205/7380 Training loss: 5.1836 1.0624 sec/batch\n",
      "Epoch 9/20  Iteration 3206/7380 Training loss: 5.1830 1.0761 sec/batch\n",
      "Epoch 9/20  Iteration 3207/7380 Training loss: 5.1832 1.0633 sec/batch\n",
      "Epoch 9/20  Iteration 3208/7380 Training loss: 5.1833 1.0632 sec/batch\n",
      "Epoch 9/20  Iteration 3209/7380 Training loss: 5.1836 1.0696 sec/batch\n",
      "Epoch 9/20  Iteration 3210/7380 Training loss: 5.1833 1.1036 sec/batch\n",
      "Epoch 9/20  Iteration 3211/7380 Training loss: 5.1835 1.0740 sec/batch\n",
      "Epoch 9/20  Iteration 3212/7380 Training loss: 5.1830 1.0702 sec/batch\n",
      "Epoch 9/20  Iteration 3213/7380 Training loss: 5.1827 1.0766 sec/batch\n",
      "Epoch 9/20  Iteration 3214/7380 Training loss: 5.1829 1.0555 sec/batch\n",
      "Epoch 9/20  Iteration 3215/7380 Training loss: 5.1832 1.0578 sec/batch\n",
      "Epoch 9/20  Iteration 3216/7380 Training loss: 5.1829 1.0720 sec/batch\n",
      "Epoch 9/20  Iteration 3217/7380 Training loss: 5.1827 1.0603 sec/batch\n",
      "Epoch 9/20  Iteration 3218/7380 Training loss: 5.1823 1.0822 sec/batch\n",
      "Epoch 9/20  Iteration 3219/7380 Training loss: 5.1824 1.0642 sec/batch\n",
      "Epoch 9/20  Iteration 3220/7380 Training loss: 5.1823 1.0608 sec/batch\n",
      "Epoch 9/20  Iteration 3221/7380 Training loss: 5.1817 1.0709 sec/batch\n",
      "Epoch 9/20  Iteration 3222/7380 Training loss: 5.1819 1.0581 sec/batch\n",
      "Epoch 9/20  Iteration 3223/7380 Training loss: 5.1820 1.0616 sec/batch\n",
      "Epoch 9/20  Iteration 3224/7380 Training loss: 5.1814 1.0824 sec/batch\n",
      "Epoch 9/20  Iteration 3225/7380 Training loss: 5.1815 1.0605 sec/batch\n",
      "Epoch 9/20  Iteration 3226/7380 Training loss: 5.1816 1.0573 sec/batch\n",
      "Epoch 9/20  Iteration 3227/7380 Training loss: 5.1814 1.0652 sec/batch\n",
      "Epoch 9/20  Iteration 3228/7380 Training loss: 5.1810 1.0707 sec/batch\n",
      "Epoch 9/20  Iteration 3229/7380 Training loss: 5.1818 1.0596 sec/batch\n",
      "Epoch 9/20  Iteration 3230/7380 Training loss: 5.1816 1.0591 sec/batch\n",
      "Epoch 9/20  Iteration 3231/7380 Training loss: 5.1818 1.0590 sec/batch\n",
      "Epoch 9/20  Iteration 3232/7380 Training loss: 5.1815 1.0575 sec/batch\n",
      "Epoch 9/20  Iteration 3233/7380 Training loss: 5.1814 1.0618 sec/batch\n",
      "Epoch 9/20  Iteration 3234/7380 Training loss: 5.1813 1.0574 sec/batch\n",
      "Epoch 9/20  Iteration 3235/7380 Training loss: 5.1814 1.0586 sec/batch\n",
      "Epoch 9/20  Iteration 3236/7380 Training loss: 5.1815 1.0592 sec/batch\n",
      "Epoch 9/20  Iteration 3237/7380 Training loss: 5.1818 1.0581 sec/batch\n",
      "Epoch 9/20  Iteration 3238/7380 Training loss: 5.1815 1.0636 sec/batch\n",
      "Epoch 9/20  Iteration 3239/7380 Training loss: 5.1811 1.0593 sec/batch\n",
      "Epoch 9/20  Iteration 3240/7380 Training loss: 5.1809 1.0691 sec/batch\n",
      "Epoch 9/20  Iteration 3241/7380 Training loss: 5.1805 1.0701 sec/batch\n",
      "Epoch 9/20  Iteration 3242/7380 Training loss: 5.1801 1.0643 sec/batch\n",
      "Epoch 9/20  Iteration 3243/7380 Training loss: 5.1797 1.0694 sec/batch\n",
      "Epoch 9/20  Iteration 3244/7380 Training loss: 5.1796 1.0635 sec/batch\n",
      "Epoch 9/20  Iteration 3245/7380 Training loss: 5.1794 1.0559 sec/batch\n",
      "Epoch 9/20  Iteration 3246/7380 Training loss: 5.1793 1.0602 sec/batch\n",
      "Epoch 9/20  Iteration 3247/7380 Training loss: 5.1795 1.0622 sec/batch\n",
      "Epoch 9/20  Iteration 3248/7380 Training loss: 5.1794 1.0589 sec/batch\n",
      "Epoch 9/20  Iteration 3249/7380 Training loss: 5.1792 1.0593 sec/batch\n",
      "Epoch 9/20  Iteration 3250/7380 Training loss: 5.1789 1.0571 sec/batch\n",
      "Validation loss: 5.15894 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 3251/7380 Training loss: 5.1795 1.0685 sec/batch\n",
      "Epoch 9/20  Iteration 3252/7380 Training loss: 5.1800 1.0706 sec/batch\n",
      "Epoch 9/20  Iteration 3253/7380 Training loss: 5.1799 1.0598 sec/batch\n",
      "Epoch 9/20  Iteration 3254/7380 Training loss: 5.1801 1.0605 sec/batch\n",
      "Epoch 9/20  Iteration 3255/7380 Training loss: 5.1801 1.0577 sec/batch\n",
      "Epoch 9/20  Iteration 3256/7380 Training loss: 5.1806 1.0540 sec/batch\n",
      "Epoch 9/20  Iteration 3257/7380 Training loss: 5.1809 1.0569 sec/batch\n",
      "Epoch 9/20  Iteration 3258/7380 Training loss: 5.1807 1.0604 sec/batch\n",
      "Epoch 9/20  Iteration 3259/7380 Training loss: 5.1804 1.0579 sec/batch\n",
      "Epoch 9/20  Iteration 3260/7380 Training loss: 5.1800 1.0597 sec/batch\n",
      "Epoch 9/20  Iteration 3261/7380 Training loss: 5.1797 1.0624 sec/batch\n",
      "Epoch 9/20  Iteration 3262/7380 Training loss: 5.1794 1.0574 sec/batch\n",
      "Epoch 9/20  Iteration 3263/7380 Training loss: 5.1794 1.0628 sec/batch\n",
      "Epoch 9/20  Iteration 3264/7380 Training loss: 5.1792 1.0610 sec/batch\n",
      "Epoch 9/20  Iteration 3265/7380 Training loss: 5.1788 1.0801 sec/batch\n",
      "Epoch 9/20  Iteration 3266/7380 Training loss: 5.1788 1.0605 sec/batch\n",
      "Epoch 9/20  Iteration 3267/7380 Training loss: 5.1788 1.0762 sec/batch\n",
      "Epoch 9/20  Iteration 3268/7380 Training loss: 5.1790 1.0603 sec/batch\n",
      "Epoch 9/20  Iteration 3269/7380 Training loss: 5.1792 1.0596 sec/batch\n",
      "Epoch 9/20  Iteration 3270/7380 Training loss: 5.1793 1.0571 sec/batch\n",
      "Epoch 9/20  Iteration 3271/7380 Training loss: 5.1797 1.0709 sec/batch\n",
      "Epoch 9/20  Iteration 3272/7380 Training loss: 5.1801 1.0774 sec/batch\n",
      "Epoch 9/20  Iteration 3273/7380 Training loss: 5.1798 1.0703 sec/batch\n",
      "Epoch 9/20  Iteration 3274/7380 Training loss: 5.1795 1.0611 sec/batch\n",
      "Epoch 9/20  Iteration 3275/7380 Training loss: 5.1795 1.0598 sec/batch\n",
      "Epoch 9/20  Iteration 3276/7380 Training loss: 5.1792 1.0596 sec/batch\n",
      "Epoch 9/20  Iteration 3277/7380 Training loss: 5.1793 1.0581 sec/batch\n",
      "Epoch 9/20  Iteration 3278/7380 Training loss: 5.1796 1.0699 sec/batch\n",
      "Epoch 9/20  Iteration 3279/7380 Training loss: 5.1793 1.0573 sec/batch\n",
      "Epoch 9/20  Iteration 3280/7380 Training loss: 5.1790 1.0585 sec/batch\n",
      "Epoch 9/20  Iteration 3281/7380 Training loss: 5.1787 1.0588 sec/batch\n",
      "Epoch 9/20  Iteration 3282/7380 Training loss: 5.1785 1.1613 sec/batch\n",
      "Epoch 9/20  Iteration 3283/7380 Training loss: 5.1781 1.0696 sec/batch\n",
      "Epoch 9/20  Iteration 3284/7380 Training loss: 5.1782 1.0641 sec/batch\n",
      "Epoch 9/20  Iteration 3285/7380 Training loss: 5.1779 1.0604 sec/batch\n",
      "Epoch 9/20  Iteration 3286/7380 Training loss: 5.1780 1.0626 sec/batch\n",
      "Epoch 9/20  Iteration 3287/7380 Training loss: 5.1779 1.0660 sec/batch\n",
      "Epoch 9/20  Iteration 3288/7380 Training loss: 5.1780 1.0597 sec/batch\n",
      "Epoch 9/20  Iteration 3289/7380 Training loss: 5.1782 1.0720 sec/batch\n",
      "Epoch 9/20  Iteration 3290/7380 Training loss: 5.1781 1.0600 sec/batch\n",
      "Epoch 9/20  Iteration 3291/7380 Training loss: 5.1781 1.0613 sec/batch\n",
      "Epoch 9/20  Iteration 3292/7380 Training loss: 5.1777 1.0659 sec/batch\n",
      "Epoch 9/20  Iteration 3293/7380 Training loss: 5.1774 1.0629 sec/batch\n",
      "Epoch 9/20  Iteration 3294/7380 Training loss: 5.1771 1.0879 sec/batch\n",
      "Epoch 9/20  Iteration 3295/7380 Training loss: 5.1769 1.0636 sec/batch\n",
      "Epoch 9/20  Iteration 3296/7380 Training loss: 5.1769 1.0614 sec/batch\n",
      "Epoch 9/20  Iteration 3297/7380 Training loss: 5.1767 1.0663 sec/batch\n",
      "Epoch 9/20  Iteration 3298/7380 Training loss: 5.1767 1.0729 sec/batch\n",
      "Epoch 9/20  Iteration 3299/7380 Training loss: 5.1769 1.0636 sec/batch\n",
      "Epoch 9/20  Iteration 3300/7380 Training loss: 5.1768 1.0653 sec/batch\n",
      "Validation loss: 5.14586 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 3301/7380 Training loss: 5.1768 1.0638 sec/batch\n",
      "Epoch 9/20  Iteration 3302/7380 Training loss: 5.1767 1.0605 sec/batch\n",
      "Epoch 9/20  Iteration 3303/7380 Training loss: 5.1770 1.0736 sec/batch\n",
      "Epoch 9/20  Iteration 3304/7380 Training loss: 5.1767 1.0594 sec/batch\n",
      "Epoch 9/20  Iteration 3305/7380 Training loss: 5.1767 1.0555 sec/batch\n",
      "Epoch 9/20  Iteration 3306/7380 Training loss: 5.1765 1.0682 sec/batch\n",
      "Epoch 9/20  Iteration 3307/7380 Training loss: 5.1763 1.0615 sec/batch\n",
      "Epoch 9/20  Iteration 3308/7380 Training loss: 5.1761 1.0591 sec/batch\n",
      "Epoch 9/20  Iteration 3309/7380 Training loss: 5.1762 1.0591 sec/batch\n",
      "Epoch 9/20  Iteration 3310/7380 Training loss: 5.1761 1.0670 sec/batch\n",
      "Epoch 9/20  Iteration 3311/7380 Training loss: 5.1758 1.0596 sec/batch\n",
      "Epoch 9/20  Iteration 3312/7380 Training loss: 5.1754 1.0558 sec/batch\n",
      "Epoch 9/20  Iteration 3313/7380 Training loss: 5.1754 1.0560 sec/batch\n",
      "Epoch 9/20  Iteration 3314/7380 Training loss: 5.1754 1.1102 sec/batch\n",
      "Epoch 9/20  Iteration 3315/7380 Training loss: 5.1753 1.0650 sec/batch\n",
      "Epoch 9/20  Iteration 3316/7380 Training loss: 5.1750 1.0598 sec/batch\n",
      "Epoch 9/20  Iteration 3317/7380 Training loss: 5.1748 1.0629 sec/batch\n",
      "Epoch 9/20  Iteration 3318/7380 Training loss: 5.1745 1.0673 sec/batch\n",
      "Epoch 9/20  Iteration 3319/7380 Training loss: 5.1742 1.0654 sec/batch\n",
      "Epoch 9/20  Iteration 3320/7380 Training loss: 5.1743 1.0641 sec/batch\n",
      "Epoch 9/20  Iteration 3321/7380 Training loss: 5.1740 1.0696 sec/batch\n",
      "Epoch 10/20  Iteration 3322/7380 Training loss: 5.1953 1.0635 sec/batch\n",
      "Epoch 10/20  Iteration 3323/7380 Training loss: 5.1834 1.0655 sec/batch\n",
      "Epoch 10/20  Iteration 3324/7380 Training loss: 5.1664 1.0638 sec/batch\n",
      "Epoch 10/20  Iteration 3325/7380 Training loss: 5.1487 1.0596 sec/batch\n",
      "Epoch 10/20  Iteration 3326/7380 Training loss: 5.1346 1.0685 sec/batch\n",
      "Epoch 10/20  Iteration 3327/7380 Training loss: 5.1471 1.0662 sec/batch\n",
      "Epoch 10/20  Iteration 3328/7380 Training loss: 5.1663 1.0634 sec/batch\n",
      "Epoch 10/20  Iteration 3329/7380 Training loss: 5.1511 1.0667 sec/batch\n",
      "Epoch 10/20  Iteration 3330/7380 Training loss: 5.1489 1.0563 sec/batch\n",
      "Epoch 10/20  Iteration 3331/7380 Training loss: 5.1582 1.0669 sec/batch\n",
      "Epoch 10/20  Iteration 3332/7380 Training loss: 5.1556 1.0547 sec/batch\n",
      "Epoch 10/20  Iteration 3333/7380 Training loss: 5.1448 1.0576 sec/batch\n",
      "Epoch 10/20  Iteration 3334/7380 Training loss: 5.1405 1.0628 sec/batch\n",
      "Epoch 10/20  Iteration 3335/7380 Training loss: 5.1491 1.0685 sec/batch\n",
      "Epoch 10/20  Iteration 3336/7380 Training loss: 5.1527 1.0666 sec/batch\n",
      "Epoch 10/20  Iteration 3337/7380 Training loss: 5.1509 1.0599 sec/batch\n",
      "Epoch 10/20  Iteration 3338/7380 Training loss: 5.1494 1.0637 sec/batch\n",
      "Epoch 10/20  Iteration 3339/7380 Training loss: 5.1361 1.0703 sec/batch\n",
      "Epoch 10/20  Iteration 3340/7380 Training loss: 5.1283 1.0703 sec/batch\n",
      "Epoch 10/20  Iteration 3341/7380 Training loss: 5.1293 1.0636 sec/batch\n",
      "Epoch 10/20  Iteration 3342/7380 Training loss: 5.1256 1.0647 sec/batch\n",
      "Epoch 10/20  Iteration 3343/7380 Training loss: 5.1327 1.0606 sec/batch\n",
      "Epoch 10/20  Iteration 3344/7380 Training loss: 5.1336 1.0725 sec/batch\n",
      "Epoch 10/20  Iteration 3345/7380 Training loss: 5.1404 1.0585 sec/batch\n",
      "Epoch 10/20  Iteration 3346/7380 Training loss: 5.1423 1.0584 sec/batch\n",
      "Epoch 10/20  Iteration 3347/7380 Training loss: 5.1444 1.0721 sec/batch\n",
      "Epoch 10/20  Iteration 3348/7380 Training loss: 5.1393 1.0708 sec/batch\n",
      "Epoch 10/20  Iteration 3349/7380 Training loss: 5.1450 1.0629 sec/batch\n",
      "Epoch 10/20  Iteration 3350/7380 Training loss: 5.1448 1.0658 sec/batch\n",
      "Validation loss: 5.14065 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 3351/7380 Training loss: 5.1495 1.0658 sec/batch\n",
      "Epoch 10/20  Iteration 3352/7380 Training loss: 5.1474 1.0759 sec/batch\n",
      "Epoch 10/20  Iteration 3353/7380 Training loss: 5.1476 1.0775 sec/batch\n",
      "Epoch 10/20  Iteration 3354/7380 Training loss: 5.1478 1.0800 sec/batch\n",
      "Epoch 10/20  Iteration 3355/7380 Training loss: 5.1485 1.0633 sec/batch\n",
      "Epoch 10/20  Iteration 3356/7380 Training loss: 5.1514 1.0715 sec/batch\n",
      "Epoch 10/20  Iteration 3357/7380 Training loss: 5.1499 1.0700 sec/batch\n",
      "Epoch 10/20  Iteration 3358/7380 Training loss: 5.1508 1.0690 sec/batch\n",
      "Epoch 10/20  Iteration 3359/7380 Training loss: 5.1488 1.0690 sec/batch\n",
      "Epoch 10/20  Iteration 3360/7380 Training loss: 5.1502 1.0739 sec/batch\n",
      "Epoch 10/20  Iteration 3361/7380 Training loss: 5.1496 1.0681 sec/batch\n",
      "Epoch 10/20  Iteration 3362/7380 Training loss: 5.1493 1.0604 sec/batch\n",
      "Epoch 10/20  Iteration 3363/7380 Training loss: 5.1484 1.0625 sec/batch\n",
      "Epoch 10/20  Iteration 3364/7380 Training loss: 5.1487 1.0931 sec/batch\n",
      "Epoch 10/20  Iteration 3365/7380 Training loss: 5.1492 1.0643 sec/batch\n",
      "Epoch 10/20  Iteration 3366/7380 Training loss: 5.1497 1.1102 sec/batch\n",
      "Epoch 10/20  Iteration 3367/7380 Training loss: 5.1479 1.0608 sec/batch\n",
      "Epoch 10/20  Iteration 3368/7380 Training loss: 5.1485 1.0841 sec/batch\n",
      "Epoch 10/20  Iteration 3369/7380 Training loss: 5.1451 1.0620 sec/batch\n",
      "Epoch 10/20  Iteration 3370/7380 Training loss: 5.1460 1.0798 sec/batch\n",
      "Epoch 10/20  Iteration 3371/7380 Training loss: 5.1448 1.0644 sec/batch\n",
      "Epoch 10/20  Iteration 3372/7380 Training loss: 5.1435 1.0651 sec/batch\n",
      "Epoch 10/20  Iteration 3373/7380 Training loss: 5.1427 1.0666 sec/batch\n",
      "Epoch 10/20  Iteration 3374/7380 Training loss: 5.1420 1.0673 sec/batch\n",
      "Epoch 10/20  Iteration 3375/7380 Training loss: 5.1412 1.0655 sec/batch\n",
      "Epoch 10/20  Iteration 3376/7380 Training loss: 5.1402 1.0646 sec/batch\n",
      "Epoch 10/20  Iteration 3377/7380 Training loss: 5.1380 1.0650 sec/batch\n",
      "Epoch 10/20  Iteration 3378/7380 Training loss: 5.1372 1.0631 sec/batch\n",
      "Epoch 10/20  Iteration 3379/7380 Training loss: 5.1384 1.0614 sec/batch\n",
      "Epoch 10/20  Iteration 3380/7380 Training loss: 5.1375 1.0781 sec/batch\n",
      "Epoch 10/20  Iteration 3381/7380 Training loss: 5.1387 1.0716 sec/batch\n",
      "Epoch 10/20  Iteration 3382/7380 Training loss: 5.1375 1.1297 sec/batch\n",
      "Epoch 10/20  Iteration 3383/7380 Training loss: 5.1384 1.0641 sec/batch\n",
      "Epoch 10/20  Iteration 3384/7380 Training loss: 5.1370 1.0723 sec/batch\n",
      "Epoch 10/20  Iteration 3385/7380 Training loss: 5.1350 1.0635 sec/batch\n",
      "Epoch 10/20  Iteration 3386/7380 Training loss: 5.1358 1.0631 sec/batch\n",
      "Epoch 10/20  Iteration 3387/7380 Training loss: 5.1357 1.0707 sec/batch\n",
      "Epoch 10/20  Iteration 3388/7380 Training loss: 5.1362 1.0731 sec/batch\n",
      "Epoch 10/20  Iteration 3389/7380 Training loss: 5.1365 1.0648 sec/batch\n",
      "Epoch 10/20  Iteration 3390/7380 Training loss: 5.1387 1.0682 sec/batch\n",
      "Epoch 10/20  Iteration 3391/7380 Training loss: 5.1374 1.0712 sec/batch\n",
      "Epoch 10/20  Iteration 3392/7380 Training loss: 5.1363 1.0669 sec/batch\n",
      "Epoch 10/20  Iteration 3393/7380 Training loss: 5.1376 1.0669 sec/batch\n",
      "Epoch 10/20  Iteration 3394/7380 Training loss: 5.1355 1.0680 sec/batch\n",
      "Epoch 10/20  Iteration 3395/7380 Training loss: 5.1372 1.0786 sec/batch\n",
      "Epoch 10/20  Iteration 3396/7380 Training loss: 5.1385 1.0776 sec/batch\n",
      "Epoch 10/20  Iteration 3397/7380 Training loss: 5.1375 1.0707 sec/batch\n",
      "Epoch 10/20  Iteration 3398/7380 Training loss: 5.1379 1.0676 sec/batch\n",
      "Epoch 10/20  Iteration 3399/7380 Training loss: 5.1377 1.0668 sec/batch\n",
      "Epoch 10/20  Iteration 3400/7380 Training loss: 5.1382 1.0649 sec/batch\n",
      "Validation loss: 5.13598 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 3401/7380 Training loss: 5.1400 1.0623 sec/batch\n",
      "Epoch 10/20  Iteration 3402/7380 Training loss: 5.1396 1.0688 sec/batch\n",
      "Epoch 10/20  Iteration 3403/7380 Training loss: 5.1390 1.0635 sec/batch\n",
      "Epoch 10/20  Iteration 3404/7380 Training loss: 5.1381 1.0670 sec/batch\n",
      "Epoch 10/20  Iteration 3405/7380 Training loss: 5.1372 1.0624 sec/batch\n",
      "Epoch 10/20  Iteration 3406/7380 Training loss: 5.1377 1.0907 sec/batch\n",
      "Epoch 10/20  Iteration 3407/7380 Training loss: 5.1371 1.0577 sec/batch\n",
      "Epoch 10/20  Iteration 3408/7380 Training loss: 5.1366 1.0699 sec/batch\n",
      "Epoch 10/20  Iteration 3409/7380 Training loss: 5.1365 1.0577 sec/batch\n",
      "Epoch 10/20  Iteration 3410/7380 Training loss: 5.1347 1.0747 sec/batch\n",
      "Epoch 10/20  Iteration 3411/7380 Training loss: 5.1343 1.0684 sec/batch\n",
      "Epoch 10/20  Iteration 3412/7380 Training loss: 5.1350 1.0656 sec/batch\n",
      "Epoch 10/20  Iteration 3413/7380 Training loss: 5.1368 1.0713 sec/batch\n",
      "Epoch 10/20  Iteration 3414/7380 Training loss: 5.1362 1.0607 sec/batch\n",
      "Epoch 10/20  Iteration 3415/7380 Training loss: 5.1359 1.0974 sec/batch\n",
      "Epoch 10/20  Iteration 3416/7380 Training loss: 5.1356 1.0700 sec/batch\n",
      "Epoch 10/20  Iteration 3417/7380 Training loss: 5.1354 1.0888 sec/batch\n",
      "Epoch 10/20  Iteration 3418/7380 Training loss: 5.1355 1.0623 sec/batch\n",
      "Epoch 10/20  Iteration 3419/7380 Training loss: 5.1366 1.0552 sec/batch\n",
      "Epoch 10/20  Iteration 3420/7380 Training loss: 5.1368 1.0615 sec/batch\n",
      "Epoch 10/20  Iteration 3421/7380 Training loss: 5.1370 1.0623 sec/batch\n",
      "Epoch 10/20  Iteration 3422/7380 Training loss: 5.1384 1.0608 sec/batch\n",
      "Epoch 10/20  Iteration 3423/7380 Training loss: 5.1387 1.0662 sec/batch\n",
      "Epoch 10/20  Iteration 3424/7380 Training loss: 5.1390 1.0684 sec/batch\n",
      "Epoch 10/20  Iteration 3425/7380 Training loss: 5.1388 1.0672 sec/batch\n",
      "Epoch 10/20  Iteration 3426/7380 Training loss: 5.1391 1.0633 sec/batch\n",
      "Epoch 10/20  Iteration 3427/7380 Training loss: 5.1391 1.0724 sec/batch\n",
      "Epoch 10/20  Iteration 3428/7380 Training loss: 5.1388 1.1003 sec/batch\n",
      "Epoch 10/20  Iteration 3429/7380 Training loss: 5.1385 1.0570 sec/batch\n",
      "Epoch 10/20  Iteration 3430/7380 Training loss: 5.1370 1.0617 sec/batch\n",
      "Epoch 10/20  Iteration 3431/7380 Training loss: 5.1367 1.0799 sec/batch\n",
      "Epoch 10/20  Iteration 3432/7380 Training loss: 5.1362 1.0634 sec/batch\n",
      "Epoch 10/20  Iteration 3433/7380 Training loss: 5.1369 1.0633 sec/batch\n",
      "Epoch 10/20  Iteration 3434/7380 Training loss: 5.1359 1.0579 sec/batch\n",
      "Epoch 10/20  Iteration 3435/7380 Training loss: 5.1364 1.0637 sec/batch\n",
      "Epoch 10/20  Iteration 3436/7380 Training loss: 5.1363 1.0662 sec/batch\n",
      "Epoch 10/20  Iteration 3437/7380 Training loss: 5.1361 1.0749 sec/batch\n",
      "Epoch 10/20  Iteration 3438/7380 Training loss: 5.1354 1.0646 sec/batch\n",
      "Epoch 10/20  Iteration 3439/7380 Training loss: 5.1352 1.0578 sec/batch\n",
      "Epoch 10/20  Iteration 3440/7380 Training loss: 5.1358 1.0694 sec/batch\n",
      "Epoch 10/20  Iteration 3441/7380 Training loss: 5.1356 1.0645 sec/batch\n",
      "Epoch 10/20  Iteration 3442/7380 Training loss: 5.1360 1.0615 sec/batch\n",
      "Epoch 10/20  Iteration 3443/7380 Training loss: 5.1371 1.0668 sec/batch\n",
      "Epoch 10/20  Iteration 3444/7380 Training loss: 5.1371 1.0601 sec/batch\n",
      "Epoch 10/20  Iteration 3445/7380 Training loss: 5.1369 1.0624 sec/batch\n",
      "Epoch 10/20  Iteration 3446/7380 Training loss: 5.1371 1.0801 sec/batch\n",
      "Epoch 10/20  Iteration 3447/7380 Training loss: 5.1362 1.0669 sec/batch\n",
      "Epoch 10/20  Iteration 3448/7380 Training loss: 5.1349 1.0737 sec/batch\n",
      "Epoch 10/20  Iteration 3449/7380 Training loss: 5.1352 1.0699 sec/batch\n",
      "Epoch 10/20  Iteration 3450/7380 Training loss: 5.1346 1.0648 sec/batch\n",
      "Validation loss: 5.13325 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 3451/7380 Training loss: 5.1352 1.0668 sec/batch\n",
      "Epoch 10/20  Iteration 3452/7380 Training loss: 5.1352 1.0679 sec/batch\n",
      "Epoch 10/20  Iteration 3453/7380 Training loss: 5.1354 1.0658 sec/batch\n",
      "Epoch 10/20  Iteration 3454/7380 Training loss: 5.1355 1.0629 sec/batch\n",
      "Epoch 10/20  Iteration 3455/7380 Training loss: 5.1346 1.0594 sec/batch\n",
      "Epoch 10/20  Iteration 3456/7380 Training loss: 5.1342 1.0603 sec/batch\n",
      "Epoch 10/20  Iteration 3457/7380 Training loss: 5.1343 1.0796 sec/batch\n",
      "Epoch 10/20  Iteration 3458/7380 Training loss: 5.1340 1.0616 sec/batch\n",
      "Epoch 10/20  Iteration 3459/7380 Training loss: 5.1338 1.0716 sec/batch\n",
      "Epoch 10/20  Iteration 3460/7380 Training loss: 5.1333 1.0744 sec/batch\n",
      "Epoch 10/20  Iteration 3461/7380 Training loss: 5.1325 1.0678 sec/batch\n",
      "Epoch 10/20  Iteration 3462/7380 Training loss: 5.1340 1.0675 sec/batch\n",
      "Epoch 10/20  Iteration 3463/7380 Training loss: 5.1341 1.0660 sec/batch\n",
      "Epoch 10/20  Iteration 3464/7380 Training loss: 5.1345 1.0632 sec/batch\n",
      "Epoch 10/20  Iteration 3465/7380 Training loss: 5.1346 1.0743 sec/batch\n",
      "Epoch 10/20  Iteration 3466/7380 Training loss: 5.1347 1.0630 sec/batch\n",
      "Epoch 10/20  Iteration 3467/7380 Training loss: 5.1345 1.0678 sec/batch\n",
      "Epoch 10/20  Iteration 3468/7380 Training loss: 5.1344 1.0595 sec/batch\n",
      "Epoch 10/20  Iteration 3469/7380 Training loss: 5.1344 1.0605 sec/batch\n",
      "Epoch 10/20  Iteration 3470/7380 Training loss: 5.1338 1.0604 sec/batch\n",
      "Epoch 10/20  Iteration 3471/7380 Training loss: 5.1341 1.0688 sec/batch\n",
      "Epoch 10/20  Iteration 3472/7380 Training loss: 5.1335 1.0669 sec/batch\n",
      "Epoch 10/20  Iteration 3473/7380 Training loss: 5.1338 1.0771 sec/batch\n",
      "Epoch 10/20  Iteration 3474/7380 Training loss: 5.1344 1.0653 sec/batch\n",
      "Epoch 10/20  Iteration 3475/7380 Training loss: 5.1350 1.0695 sec/batch\n",
      "Epoch 10/20  Iteration 3476/7380 Training loss: 5.1347 1.0631 sec/batch\n",
      "Epoch 10/20  Iteration 3477/7380 Training loss: 5.1343 1.0644 sec/batch\n",
      "Epoch 10/20  Iteration 3478/7380 Training loss: 5.1337 1.0613 sec/batch\n",
      "Epoch 10/20  Iteration 3479/7380 Training loss: 5.1346 1.0659 sec/batch\n",
      "Epoch 10/20  Iteration 3480/7380 Training loss: 5.1342 1.0670 sec/batch\n",
      "Epoch 10/20  Iteration 3481/7380 Training loss: 5.1340 1.0693 sec/batch\n",
      "Epoch 10/20  Iteration 3482/7380 Training loss: 5.1346 1.0625 sec/batch\n",
      "Epoch 10/20  Iteration 3483/7380 Training loss: 5.1349 1.0677 sec/batch\n",
      "Epoch 10/20  Iteration 3484/7380 Training loss: 5.1352 1.0637 sec/batch\n",
      "Epoch 10/20  Iteration 3485/7380 Training loss: 5.1355 1.0654 sec/batch\n",
      "Epoch 10/20  Iteration 3486/7380 Training loss: 5.1357 1.0640 sec/batch\n",
      "Epoch 10/20  Iteration 3487/7380 Training loss: 5.1361 1.0554 sec/batch\n",
      "Epoch 10/20  Iteration 3488/7380 Training loss: 5.1361 1.0661 sec/batch\n",
      "Epoch 10/20  Iteration 3489/7380 Training loss: 5.1364 1.0627 sec/batch\n",
      "Epoch 10/20  Iteration 3490/7380 Training loss: 5.1359 1.0841 sec/batch\n",
      "Epoch 10/20  Iteration 3491/7380 Training loss: 5.1355 1.0728 sec/batch\n",
      "Epoch 10/20  Iteration 3492/7380 Training loss: 5.1359 1.0741 sec/batch\n",
      "Epoch 10/20  Iteration 3493/7380 Training loss: 5.1365 1.0739 sec/batch\n",
      "Epoch 10/20  Iteration 3494/7380 Training loss: 5.1365 1.0661 sec/batch\n",
      "Epoch 10/20  Iteration 3495/7380 Training loss: 5.1361 1.0648 sec/batch\n",
      "Epoch 10/20  Iteration 3496/7380 Training loss: 5.1360 1.0626 sec/batch\n",
      "Epoch 10/20  Iteration 3497/7380 Training loss: 5.1359 1.0711 sec/batch\n",
      "Epoch 10/20  Iteration 3498/7380 Training loss: 5.1360 1.0597 sec/batch\n",
      "Epoch 10/20  Iteration 3499/7380 Training loss: 5.1363 1.0649 sec/batch\n",
      "Epoch 10/20  Iteration 3500/7380 Training loss: 5.1356 1.0692 sec/batch\n",
      "Validation loss: 5.14784 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 3501/7380 Training loss: 5.1356 1.0779 sec/batch\n",
      "Epoch 10/20  Iteration 3502/7380 Training loss: 5.1355 1.0674 sec/batch\n",
      "Epoch 10/20  Iteration 3503/7380 Training loss: 5.1355 1.0782 sec/batch\n",
      "Epoch 10/20  Iteration 3504/7380 Training loss: 5.1363 1.0690 sec/batch\n",
      "Epoch 10/20  Iteration 3505/7380 Training loss: 5.1359 1.0682 sec/batch\n",
      "Epoch 10/20  Iteration 3506/7380 Training loss: 5.1358 1.0658 sec/batch\n",
      "Epoch 10/20  Iteration 3507/7380 Training loss: 5.1359 1.0737 sec/batch\n",
      "Epoch 10/20  Iteration 3508/7380 Training loss: 5.1355 1.0642 sec/batch\n",
      "Epoch 10/20  Iteration 3509/7380 Training loss: 5.1352 1.0651 sec/batch\n",
      "Epoch 10/20  Iteration 3510/7380 Training loss: 5.1350 1.0750 sec/batch\n",
      "Epoch 10/20  Iteration 3511/7380 Training loss: 5.1351 1.0784 sec/batch\n",
      "Epoch 10/20  Iteration 3512/7380 Training loss: 5.1344 1.0706 sec/batch\n",
      "Epoch 10/20  Iteration 3513/7380 Training loss: 5.1346 1.0679 sec/batch\n",
      "Epoch 10/20  Iteration 3514/7380 Training loss: 5.1350 1.0697 sec/batch\n",
      "Epoch 10/20  Iteration 3515/7380 Training loss: 5.1345 1.0726 sec/batch\n",
      "Epoch 10/20  Iteration 3516/7380 Training loss: 5.1349 1.0595 sec/batch\n",
      "Epoch 10/20  Iteration 3517/7380 Training loss: 5.1350 1.0692 sec/batch\n",
      "Epoch 10/20  Iteration 3518/7380 Training loss: 5.1349 1.0630 sec/batch\n",
      "Epoch 10/20  Iteration 3519/7380 Training loss: 5.1354 1.0810 sec/batch\n",
      "Epoch 10/20  Iteration 3520/7380 Training loss: 5.1353 1.0650 sec/batch\n",
      "Epoch 10/20  Iteration 3521/7380 Training loss: 5.1353 1.0635 sec/batch\n",
      "Epoch 10/20  Iteration 3522/7380 Training loss: 5.1355 1.0840 sec/batch\n",
      "Epoch 10/20  Iteration 3523/7380 Training loss: 5.1358 1.0678 sec/batch\n",
      "Epoch 10/20  Iteration 3524/7380 Training loss: 5.1363 1.0640 sec/batch\n",
      "Epoch 10/20  Iteration 3525/7380 Training loss: 5.1362 1.0799 sec/batch\n",
      "Epoch 10/20  Iteration 3526/7380 Training loss: 5.1364 1.0627 sec/batch\n",
      "Epoch 10/20  Iteration 3527/7380 Training loss: 5.1369 1.0708 sec/batch\n",
      "Epoch 10/20  Iteration 3528/7380 Training loss: 5.1362 1.0652 sec/batch\n",
      "Epoch 10/20  Iteration 3529/7380 Training loss: 5.1357 1.0687 sec/batch\n",
      "Epoch 10/20  Iteration 3530/7380 Training loss: 5.1354 1.0762 sec/batch\n",
      "Epoch 10/20  Iteration 3531/7380 Training loss: 5.1348 1.0770 sec/batch\n",
      "Epoch 10/20  Iteration 3532/7380 Training loss: 5.1347 1.0738 sec/batch\n",
      "Epoch 10/20  Iteration 3533/7380 Training loss: 5.1343 1.0828 sec/batch\n",
      "Epoch 10/20  Iteration 3534/7380 Training loss: 5.1344 1.0697 sec/batch\n",
      "Epoch 10/20  Iteration 3535/7380 Training loss: 5.1342 1.0620 sec/batch\n",
      "Epoch 10/20  Iteration 3536/7380 Training loss: 5.1338 1.0654 sec/batch\n",
      "Epoch 10/20  Iteration 3537/7380 Training loss: 5.1331 1.0670 sec/batch\n",
      "Epoch 10/20  Iteration 3538/7380 Training loss: 5.1324 1.0701 sec/batch\n",
      "Epoch 10/20  Iteration 3539/7380 Training loss: 5.1327 1.0801 sec/batch\n",
      "Epoch 10/20  Iteration 3540/7380 Training loss: 5.1329 1.0628 sec/batch\n",
      "Epoch 10/20  Iteration 3541/7380 Training loss: 5.1330 1.0647 sec/batch\n",
      "Epoch 10/20  Iteration 3542/7380 Training loss: 5.1327 1.1017 sec/batch\n",
      "Epoch 10/20  Iteration 3543/7380 Training loss: 5.1331 1.0629 sec/batch\n",
      "Epoch 10/20  Iteration 3544/7380 Training loss: 5.1330 1.0666 sec/batch\n",
      "Epoch 10/20  Iteration 3545/7380 Training loss: 5.1325 1.0675 sec/batch\n",
      "Epoch 10/20  Iteration 3546/7380 Training loss: 5.1326 1.0677 sec/batch\n",
      "Epoch 10/20  Iteration 3547/7380 Training loss: 5.1323 1.0662 sec/batch\n",
      "Epoch 10/20  Iteration 3548/7380 Training loss: 5.1322 1.0710 sec/batch\n",
      "Epoch 10/20  Iteration 3549/7380 Training loss: 5.1317 1.0645 sec/batch\n",
      "Epoch 10/20  Iteration 3550/7380 Training loss: 5.1315 1.0693 sec/batch\n",
      "Validation loss: 5.1219 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 3551/7380 Training loss: 5.1325 1.0702 sec/batch\n",
      "Epoch 10/20  Iteration 3552/7380 Training loss: 5.1321 1.0649 sec/batch\n",
      "Epoch 10/20  Iteration 3553/7380 Training loss: 5.1319 1.0725 sec/batch\n",
      "Epoch 10/20  Iteration 3554/7380 Training loss: 5.1315 1.0867 sec/batch\n",
      "Epoch 10/20  Iteration 3555/7380 Training loss: 5.1312 1.0655 sec/batch\n",
      "Epoch 10/20  Iteration 3556/7380 Training loss: 5.1310 1.0712 sec/batch\n",
      "Epoch 10/20  Iteration 3557/7380 Training loss: 5.1305 1.0717 sec/batch\n",
      "Epoch 10/20  Iteration 3558/7380 Training loss: 5.1302 1.0743 sec/batch\n",
      "Epoch 10/20  Iteration 3559/7380 Training loss: 5.1295 1.0690 sec/batch\n",
      "Epoch 10/20  Iteration 3560/7380 Training loss: 5.1285 1.0837 sec/batch\n",
      "Epoch 10/20  Iteration 3561/7380 Training loss: 5.1279 1.0727 sec/batch\n",
      "Epoch 10/20  Iteration 3562/7380 Training loss: 5.1281 1.0669 sec/batch\n",
      "Epoch 10/20  Iteration 3563/7380 Training loss: 5.1287 1.0682 sec/batch\n",
      "Epoch 10/20  Iteration 3564/7380 Training loss: 5.1285 1.0685 sec/batch\n",
      "Epoch 10/20  Iteration 3565/7380 Training loss: 5.1285 1.0716 sec/batch\n",
      "Epoch 10/20  Iteration 3566/7380 Training loss: 5.1287 1.0712 sec/batch\n",
      "Epoch 10/20  Iteration 3567/7380 Training loss: 5.1289 1.0724 sec/batch\n",
      "Epoch 10/20  Iteration 3568/7380 Training loss: 5.1289 1.0701 sec/batch\n",
      "Epoch 10/20  Iteration 3569/7380 Training loss: 5.1288 1.0664 sec/batch\n",
      "Epoch 10/20  Iteration 3570/7380 Training loss: 5.1286 1.0648 sec/batch\n",
      "Epoch 10/20  Iteration 3571/7380 Training loss: 5.1286 1.0630 sec/batch\n",
      "Epoch 10/20  Iteration 3572/7380 Training loss: 5.1282 1.0660 sec/batch\n",
      "Epoch 10/20  Iteration 3573/7380 Training loss: 5.1280 1.0766 sec/batch\n",
      "Epoch 10/20  Iteration 3574/7380 Training loss: 5.1279 1.0985 sec/batch\n",
      "Epoch 10/20  Iteration 3575/7380 Training loss: 5.1274 1.0719 sec/batch\n",
      "Epoch 10/20  Iteration 3576/7380 Training loss: 5.1277 1.0719 sec/batch\n",
      "Epoch 10/20  Iteration 3577/7380 Training loss: 5.1278 1.0739 sec/batch\n",
      "Epoch 10/20  Iteration 3578/7380 Training loss: 5.1281 1.0657 sec/batch\n",
      "Epoch 10/20  Iteration 3579/7380 Training loss: 5.1279 1.0658 sec/batch\n",
      "Epoch 10/20  Iteration 3580/7380 Training loss: 5.1281 1.0691 sec/batch\n",
      "Epoch 10/20  Iteration 3581/7380 Training loss: 5.1277 1.0644 sec/batch\n",
      "Epoch 10/20  Iteration 3582/7380 Training loss: 5.1276 1.1151 sec/batch\n",
      "Epoch 10/20  Iteration 3583/7380 Training loss: 5.1277 1.0642 sec/batch\n",
      "Epoch 10/20  Iteration 3584/7380 Training loss: 5.1279 1.0614 sec/batch\n",
      "Epoch 10/20  Iteration 3585/7380 Training loss: 5.1276 1.0626 sec/batch\n",
      "Epoch 10/20  Iteration 3586/7380 Training loss: 5.1274 1.0701 sec/batch\n",
      "Epoch 10/20  Iteration 3587/7380 Training loss: 5.1271 1.0809 sec/batch\n",
      "Epoch 10/20  Iteration 3588/7380 Training loss: 5.1271 1.1161 sec/batch\n",
      "Epoch 10/20  Iteration 3589/7380 Training loss: 5.1273 1.0671 sec/batch\n",
      "Epoch 10/20  Iteration 3590/7380 Training loss: 5.1267 1.0669 sec/batch\n",
      "Epoch 10/20  Iteration 3591/7380 Training loss: 5.1270 1.0686 sec/batch\n",
      "Epoch 10/20  Iteration 3592/7380 Training loss: 5.1270 1.0621 sec/batch\n",
      "Epoch 10/20  Iteration 3593/7380 Training loss: 5.1264 1.0658 sec/batch\n",
      "Epoch 10/20  Iteration 3594/7380 Training loss: 5.1267 1.0596 sec/batch\n",
      "Epoch 10/20  Iteration 3595/7380 Training loss: 5.1269 1.0663 sec/batch\n",
      "Epoch 10/20  Iteration 3596/7380 Training loss: 5.1268 1.0737 sec/batch\n",
      "Epoch 10/20  Iteration 3597/7380 Training loss: 5.1264 1.0717 sec/batch\n",
      "Epoch 10/20  Iteration 3598/7380 Training loss: 5.1269 1.0634 sec/batch\n",
      "Epoch 10/20  Iteration 3599/7380 Training loss: 5.1268 1.0693 sec/batch\n",
      "Epoch 10/20  Iteration 3600/7380 Training loss: 5.1270 1.0685 sec/batch\n",
      "Validation loss: 5.12418 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 3601/7380 Training loss: 5.1270 1.0798 sec/batch\n",
      "Epoch 10/20  Iteration 3602/7380 Training loss: 5.1271 1.0710 sec/batch\n",
      "Epoch 10/20  Iteration 3603/7380 Training loss: 5.1271 1.0686 sec/batch\n",
      "Epoch 10/20  Iteration 3604/7380 Training loss: 5.1273 1.0653 sec/batch\n",
      "Epoch 10/20  Iteration 3605/7380 Training loss: 5.1274 1.0600 sec/batch\n",
      "Epoch 10/20  Iteration 3606/7380 Training loss: 5.1277 1.0694 sec/batch\n",
      "Epoch 10/20  Iteration 3607/7380 Training loss: 5.1274 1.0717 sec/batch\n",
      "Epoch 10/20  Iteration 3608/7380 Training loss: 5.1270 1.0622 sec/batch\n",
      "Epoch 10/20  Iteration 3609/7380 Training loss: 5.1268 1.0708 sec/batch\n",
      "Epoch 10/20  Iteration 3610/7380 Training loss: 5.1265 1.0644 sec/batch\n",
      "Epoch 10/20  Iteration 3611/7380 Training loss: 5.1261 1.0705 sec/batch\n",
      "Epoch 10/20  Iteration 3612/7380 Training loss: 5.1258 1.0760 sec/batch\n",
      "Epoch 10/20  Iteration 3613/7380 Training loss: 5.1255 1.0857 sec/batch\n",
      "Epoch 10/20  Iteration 3614/7380 Training loss: 5.1254 1.0652 sec/batch\n",
      "Epoch 10/20  Iteration 3615/7380 Training loss: 5.1253 1.0657 sec/batch\n",
      "Epoch 10/20  Iteration 3616/7380 Training loss: 5.1255 1.0701 sec/batch\n",
      "Epoch 10/20  Iteration 3617/7380 Training loss: 5.1253 1.0910 sec/batch\n",
      "Epoch 10/20  Iteration 3618/7380 Training loss: 5.1251 1.0656 sec/batch\n",
      "Epoch 10/20  Iteration 3619/7380 Training loss: 5.1249 1.0710 sec/batch\n",
      "Epoch 10/20  Iteration 3620/7380 Training loss: 5.1250 1.0657 sec/batch\n",
      "Epoch 10/20  Iteration 3621/7380 Training loss: 5.1255 1.0947 sec/batch\n",
      "Epoch 10/20  Iteration 3622/7380 Training loss: 5.1252 1.0666 sec/batch\n",
      "Epoch 10/20  Iteration 3623/7380 Training loss: 5.1254 1.0651 sec/batch\n",
      "Epoch 10/20  Iteration 3624/7380 Training loss: 5.1255 1.0699 sec/batch\n",
      "Epoch 10/20  Iteration 3625/7380 Training loss: 5.1260 1.1159 sec/batch\n",
      "Epoch 10/20  Iteration 3626/7380 Training loss: 5.1261 1.0667 sec/batch\n",
      "Epoch 10/20  Iteration 3627/7380 Training loss: 5.1262 1.0757 sec/batch\n",
      "Epoch 10/20  Iteration 3628/7380 Training loss: 5.1259 1.0692 sec/batch\n",
      "Epoch 10/20  Iteration 3629/7380 Training loss: 5.1255 1.0665 sec/batch\n",
      "Epoch 10/20  Iteration 3630/7380 Training loss: 5.1252 1.0706 sec/batch\n",
      "Epoch 10/20  Iteration 3631/7380 Training loss: 5.1248 1.0580 sec/batch\n",
      "Epoch 10/20  Iteration 3632/7380 Training loss: 5.1247 1.0724 sec/batch\n",
      "Epoch 10/20  Iteration 3633/7380 Training loss: 5.1244 1.0638 sec/batch\n",
      "Epoch 10/20  Iteration 3634/7380 Training loss: 5.1241 1.0710 sec/batch\n",
      "Epoch 10/20  Iteration 3635/7380 Training loss: 5.1241 1.0718 sec/batch\n",
      "Epoch 10/20  Iteration 3636/7380 Training loss: 5.1242 1.0766 sec/batch\n",
      "Epoch 10/20  Iteration 3637/7380 Training loss: 5.1244 1.0730 sec/batch\n",
      "Epoch 10/20  Iteration 3638/7380 Training loss: 5.1244 1.0672 sec/batch\n",
      "Epoch 10/20  Iteration 3639/7380 Training loss: 5.1245 1.0704 sec/batch\n",
      "Epoch 10/20  Iteration 3640/7380 Training loss: 5.1251 1.0615 sec/batch\n",
      "Epoch 10/20  Iteration 3641/7380 Training loss: 5.1254 1.0713 sec/batch\n",
      "Epoch 10/20  Iteration 3642/7380 Training loss: 5.1250 1.0670 sec/batch\n",
      "Epoch 10/20  Iteration 3643/7380 Training loss: 5.1246 1.0814 sec/batch\n",
      "Epoch 10/20  Iteration 3644/7380 Training loss: 5.1245 1.0632 sec/batch\n",
      "Epoch 10/20  Iteration 3645/7380 Training loss: 5.1244 1.0628 sec/batch\n",
      "Epoch 10/20  Iteration 3646/7380 Training loss: 5.1245 1.0617 sec/batch\n",
      "Epoch 10/20  Iteration 3647/7380 Training loss: 5.1249 1.0711 sec/batch\n",
      "Epoch 10/20  Iteration 3648/7380 Training loss: 5.1247 1.0698 sec/batch\n",
      "Epoch 10/20  Iteration 3649/7380 Training loss: 5.1244 1.0754 sec/batch\n",
      "Epoch 10/20  Iteration 3650/7380 Training loss: 5.1241 1.0662 sec/batch\n",
      "Validation loss: 5.12045 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 3651/7380 Training loss: 5.1241 1.0797 sec/batch\n",
      "Epoch 10/20  Iteration 3652/7380 Training loss: 5.1237 1.0734 sec/batch\n",
      "Epoch 10/20  Iteration 3653/7380 Training loss: 5.1237 1.0680 sec/batch\n",
      "Epoch 10/20  Iteration 3654/7380 Training loss: 5.1233 1.0827 sec/batch\n",
      "Epoch 10/20  Iteration 3655/7380 Training loss: 5.1233 1.0698 sec/batch\n",
      "Epoch 10/20  Iteration 3656/7380 Training loss: 5.1233 1.0783 sec/batch\n",
      "Epoch 10/20  Iteration 3657/7380 Training loss: 5.1235 1.0657 sec/batch\n",
      "Epoch 10/20  Iteration 3658/7380 Training loss: 5.1236 1.0759 sec/batch\n",
      "Epoch 10/20  Iteration 3659/7380 Training loss: 5.1236 1.0810 sec/batch\n",
      "Epoch 10/20  Iteration 3660/7380 Training loss: 5.1236 1.1134 sec/batch\n",
      "Epoch 10/20  Iteration 3661/7380 Training loss: 5.1233 1.0703 sec/batch\n",
      "Epoch 10/20  Iteration 3662/7380 Training loss: 5.1230 1.0709 sec/batch\n",
      "Epoch 10/20  Iteration 3663/7380 Training loss: 5.1227 1.0743 sec/batch\n",
      "Epoch 10/20  Iteration 3664/7380 Training loss: 5.1227 1.0785 sec/batch\n",
      "Epoch 10/20  Iteration 3665/7380 Training loss: 5.1227 1.0720 sec/batch\n",
      "Epoch 10/20  Iteration 3666/7380 Training loss: 5.1226 1.0884 sec/batch\n",
      "Epoch 10/20  Iteration 3667/7380 Training loss: 5.1225 1.0695 sec/batch\n",
      "Epoch 10/20  Iteration 3668/7380 Training loss: 5.1228 1.0612 sec/batch\n",
      "Epoch 10/20  Iteration 3669/7380 Training loss: 5.1229 1.0680 sec/batch\n",
      "Epoch 10/20  Iteration 3670/7380 Training loss: 5.1227 1.0668 sec/batch\n",
      "Epoch 10/20  Iteration 3671/7380 Training loss: 5.1226 1.0729 sec/batch\n",
      "Epoch 10/20  Iteration 3672/7380 Training loss: 5.1230 1.0663 sec/batch\n",
      "Epoch 10/20  Iteration 3673/7380 Training loss: 5.1227 1.0690 sec/batch\n",
      "Epoch 10/20  Iteration 3674/7380 Training loss: 5.1228 1.0695 sec/batch\n",
      "Epoch 10/20  Iteration 3675/7380 Training loss: 5.1225 1.0697 sec/batch\n",
      "Epoch 10/20  Iteration 3676/7380 Training loss: 5.1223 1.0686 sec/batch\n",
      "Epoch 10/20  Iteration 3677/7380 Training loss: 5.1221 1.0647 sec/batch\n",
      "Epoch 10/20  Iteration 3678/7380 Training loss: 5.1224 1.0681 sec/batch\n",
      "Epoch 10/20  Iteration 3679/7380 Training loss: 5.1224 1.0658 sec/batch\n",
      "Epoch 10/20  Iteration 3680/7380 Training loss: 5.1221 1.0613 sec/batch\n",
      "Epoch 10/20  Iteration 3681/7380 Training loss: 5.1219 1.0707 sec/batch\n",
      "Epoch 10/20  Iteration 3682/7380 Training loss: 5.1219 1.0681 sec/batch\n",
      "Epoch 10/20  Iteration 3683/7380 Training loss: 5.1220 1.0640 sec/batch\n",
      "Epoch 10/20  Iteration 3684/7380 Training loss: 5.1220 1.1111 sec/batch\n",
      "Epoch 10/20  Iteration 3685/7380 Training loss: 5.1217 1.0742 sec/batch\n",
      "Epoch 10/20  Iteration 3686/7380 Training loss: 5.1215 1.0700 sec/batch\n",
      "Epoch 10/20  Iteration 3687/7380 Training loss: 5.1212 1.0724 sec/batch\n",
      "Epoch 10/20  Iteration 3688/7380 Training loss: 5.1210 1.0751 sec/batch\n",
      "Epoch 10/20  Iteration 3689/7380 Training loss: 5.1212 1.0689 sec/batch\n",
      "Epoch 10/20  Iteration 3690/7380 Training loss: 5.1208 1.0761 sec/batch\n",
      "Epoch 11/20  Iteration 3691/7380 Training loss: 5.1572 1.0707 sec/batch\n",
      "Epoch 11/20  Iteration 3692/7380 Training loss: 5.1427 1.0699 sec/batch\n",
      "Epoch 11/20  Iteration 3693/7380 Training loss: 5.1139 1.1160 sec/batch\n",
      "Epoch 11/20  Iteration 3694/7380 Training loss: 5.1118 1.0668 sec/batch\n",
      "Epoch 11/20  Iteration 3695/7380 Training loss: 5.1007 1.0777 sec/batch\n",
      "Epoch 11/20  Iteration 3696/7380 Training loss: 5.1078 1.0637 sec/batch\n",
      "Epoch 11/20  Iteration 3697/7380 Training loss: 5.1231 1.0637 sec/batch\n",
      "Epoch 11/20  Iteration 3698/7380 Training loss: 5.1084 1.0678 sec/batch\n",
      "Epoch 11/20  Iteration 3699/7380 Training loss: 5.1055 1.0660 sec/batch\n",
      "Epoch 11/20  Iteration 3700/7380 Training loss: 5.1146 1.0662 sec/batch\n",
      "Validation loss: 5.11469 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3701/7380 Training loss: 5.1237 1.0919 sec/batch\n",
      "Epoch 11/20  Iteration 3702/7380 Training loss: 5.1155 1.0792 sec/batch\n",
      "Epoch 11/20  Iteration 3703/7380 Training loss: 5.1122 1.0867 sec/batch\n",
      "Epoch 11/20  Iteration 3704/7380 Training loss: 5.1221 1.0955 sec/batch\n",
      "Epoch 11/20  Iteration 3705/7380 Training loss: 5.1245 1.0635 sec/batch\n",
      "Epoch 11/20  Iteration 3706/7380 Training loss: 5.1223 1.0823 sec/batch\n",
      "Epoch 11/20  Iteration 3707/7380 Training loss: 5.1198 1.0642 sec/batch\n",
      "Epoch 11/20  Iteration 3708/7380 Training loss: 5.1056 1.0770 sec/batch\n",
      "Epoch 11/20  Iteration 3709/7380 Training loss: 5.0993 1.0691 sec/batch\n",
      "Epoch 11/20  Iteration 3710/7380 Training loss: 5.0976 1.0675 sec/batch\n",
      "Epoch 11/20  Iteration 3711/7380 Training loss: 5.0932 1.0708 sec/batch\n",
      "Epoch 11/20  Iteration 3712/7380 Training loss: 5.0994 1.0714 sec/batch\n",
      "Epoch 11/20  Iteration 3713/7380 Training loss: 5.0994 1.0732 sec/batch\n",
      "Epoch 11/20  Iteration 3714/7380 Training loss: 5.1051 1.0678 sec/batch\n",
      "Epoch 11/20  Iteration 3715/7380 Training loss: 5.1068 1.0672 sec/batch\n",
      "Epoch 11/20  Iteration 3716/7380 Training loss: 5.1076 1.0672 sec/batch\n",
      "Epoch 11/20  Iteration 3717/7380 Training loss: 5.1018 1.0727 sec/batch\n",
      "Epoch 11/20  Iteration 3718/7380 Training loss: 5.1077 1.0654 sec/batch\n",
      "Epoch 11/20  Iteration 3719/7380 Training loss: 5.1073 1.0645 sec/batch\n",
      "Epoch 11/20  Iteration 3720/7380 Training loss: 5.1072 1.0742 sec/batch\n",
      "Epoch 11/20  Iteration 3721/7380 Training loss: 5.1054 1.0622 sec/batch\n",
      "Epoch 11/20  Iteration 3722/7380 Training loss: 5.1068 1.0707 sec/batch\n",
      "Epoch 11/20  Iteration 3723/7380 Training loss: 5.1065 1.0664 sec/batch\n",
      "Epoch 11/20  Iteration 3724/7380 Training loss: 5.1073 1.0833 sec/batch\n",
      "Epoch 11/20  Iteration 3725/7380 Training loss: 5.1104 1.0737 sec/batch\n",
      "Epoch 11/20  Iteration 3726/7380 Training loss: 5.1088 1.0697 sec/batch\n",
      "Epoch 11/20  Iteration 3727/7380 Training loss: 5.1093 1.0814 sec/batch\n",
      "Epoch 11/20  Iteration 3728/7380 Training loss: 5.1075 1.1210 sec/batch\n",
      "Epoch 11/20  Iteration 3729/7380 Training loss: 5.1095 1.0727 sec/batch\n",
      "Epoch 11/20  Iteration 3730/7380 Training loss: 5.1087 1.0680 sec/batch\n",
      "Epoch 11/20  Iteration 3731/7380 Training loss: 5.1088 1.0708 sec/batch\n",
      "Epoch 11/20  Iteration 3732/7380 Training loss: 5.1079 1.0671 sec/batch\n",
      "Epoch 11/20  Iteration 3733/7380 Training loss: 5.1083 1.0709 sec/batch\n",
      "Epoch 11/20  Iteration 3734/7380 Training loss: 5.1091 1.0657 sec/batch\n",
      "Epoch 11/20  Iteration 3735/7380 Training loss: 5.1102 1.0712 sec/batch\n",
      "Epoch 11/20  Iteration 3736/7380 Training loss: 5.1079 1.0801 sec/batch\n",
      "Epoch 11/20  Iteration 3737/7380 Training loss: 5.1084 1.0676 sec/batch\n",
      "Epoch 11/20  Iteration 3738/7380 Training loss: 5.1059 1.0733 sec/batch\n",
      "Epoch 11/20  Iteration 3739/7380 Training loss: 5.1064 1.1164 sec/batch\n",
      "Epoch 11/20  Iteration 3740/7380 Training loss: 5.1049 1.0800 sec/batch\n",
      "Epoch 11/20  Iteration 3741/7380 Training loss: 5.1032 1.0790 sec/batch\n",
      "Epoch 11/20  Iteration 3742/7380 Training loss: 5.1024 1.0783 sec/batch\n",
      "Epoch 11/20  Iteration 3743/7380 Training loss: 5.1009 1.0676 sec/batch\n",
      "Epoch 11/20  Iteration 3744/7380 Training loss: 5.1000 1.0695 sec/batch\n",
      "Epoch 11/20  Iteration 3745/7380 Training loss: 5.0984 1.0659 sec/batch\n",
      "Epoch 11/20  Iteration 3746/7380 Training loss: 5.0958 1.0674 sec/batch\n",
      "Epoch 11/20  Iteration 3747/7380 Training loss: 5.0949 1.0719 sec/batch\n",
      "Epoch 11/20  Iteration 3748/7380 Training loss: 5.0955 1.0644 sec/batch\n",
      "Epoch 11/20  Iteration 3749/7380 Training loss: 5.0953 1.0658 sec/batch\n",
      "Epoch 11/20  Iteration 3750/7380 Training loss: 5.0962 1.0663 sec/batch\n",
      "Validation loss: 5.11053 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3751/7380 Training loss: 5.0966 1.1017 sec/batch\n",
      "Epoch 11/20  Iteration 3752/7380 Training loss: 5.0982 1.0785 sec/batch\n",
      "Epoch 11/20  Iteration 3753/7380 Training loss: 5.0966 1.0713 sec/batch\n",
      "Epoch 11/20  Iteration 3754/7380 Training loss: 5.0947 1.0773 sec/batch\n",
      "Epoch 11/20  Iteration 3755/7380 Training loss: 5.0946 1.0728 sec/batch\n",
      "Epoch 11/20  Iteration 3756/7380 Training loss: 5.0947 1.0686 sec/batch\n",
      "Epoch 11/20  Iteration 3757/7380 Training loss: 5.0955 1.0695 sec/batch\n",
      "Epoch 11/20  Iteration 3758/7380 Training loss: 5.0959 1.0665 sec/batch\n",
      "Epoch 11/20  Iteration 3759/7380 Training loss: 5.0976 1.0678 sec/batch\n",
      "Epoch 11/20  Iteration 3760/7380 Training loss: 5.0967 1.0605 sec/batch\n",
      "Epoch 11/20  Iteration 3761/7380 Training loss: 5.0952 1.0661 sec/batch\n",
      "Epoch 11/20  Iteration 3762/7380 Training loss: 5.0961 1.0655 sec/batch\n",
      "Epoch 11/20  Iteration 3763/7380 Training loss: 5.0945 1.0767 sec/batch\n",
      "Epoch 11/20  Iteration 3764/7380 Training loss: 5.0958 1.0687 sec/batch\n",
      "Epoch 11/20  Iteration 3765/7380 Training loss: 5.0968 1.0749 sec/batch\n",
      "Epoch 11/20  Iteration 3766/7380 Training loss: 5.0960 1.1056 sec/batch\n",
      "Epoch 11/20  Iteration 3767/7380 Training loss: 5.0962 1.0704 sec/batch\n",
      "Epoch 11/20  Iteration 3768/7380 Training loss: 5.0959 1.0814 sec/batch\n",
      "Epoch 11/20  Iteration 3769/7380 Training loss: 5.0967 1.0715 sec/batch\n",
      "Epoch 11/20  Iteration 3770/7380 Training loss: 5.0970 1.0628 sec/batch\n",
      "Epoch 11/20  Iteration 3771/7380 Training loss: 5.0964 1.0612 sec/batch\n",
      "Epoch 11/20  Iteration 3772/7380 Training loss: 5.0961 1.0655 sec/batch\n",
      "Epoch 11/20  Iteration 3773/7380 Training loss: 5.0949 1.0674 sec/batch\n",
      "Epoch 11/20  Iteration 3774/7380 Training loss: 5.0943 1.0650 sec/batch\n",
      "Epoch 11/20  Iteration 3775/7380 Training loss: 5.0945 1.0675 sec/batch\n",
      "Epoch 11/20  Iteration 3776/7380 Training loss: 5.0937 1.0859 sec/batch\n",
      "Epoch 11/20  Iteration 3777/7380 Training loss: 5.0933 1.0699 sec/batch\n",
      "Epoch 11/20  Iteration 3778/7380 Training loss: 5.0930 1.0713 sec/batch\n",
      "Epoch 11/20  Iteration 3779/7380 Training loss: 5.0908 1.0790 sec/batch\n",
      "Epoch 11/20  Iteration 3780/7380 Training loss: 5.0905 1.0794 sec/batch\n",
      "Epoch 11/20  Iteration 3781/7380 Training loss: 5.0914 1.0719 sec/batch\n",
      "Epoch 11/20  Iteration 3782/7380 Training loss: 5.0933 1.0655 sec/batch\n",
      "Epoch 11/20  Iteration 3783/7380 Training loss: 5.0927 1.0776 sec/batch\n",
      "Epoch 11/20  Iteration 3784/7380 Training loss: 5.0924 1.0681 sec/batch\n",
      "Epoch 11/20  Iteration 3785/7380 Training loss: 5.0920 1.0671 sec/batch\n",
      "Epoch 11/20  Iteration 3786/7380 Training loss: 5.0922 1.0740 sec/batch\n",
      "Epoch 11/20  Iteration 3787/7380 Training loss: 5.0920 1.0717 sec/batch\n",
      "Epoch 11/20  Iteration 3788/7380 Training loss: 5.0933 1.0781 sec/batch\n",
      "Epoch 11/20  Iteration 3789/7380 Training loss: 5.0933 1.0677 sec/batch\n",
      "Epoch 11/20  Iteration 3790/7380 Training loss: 5.0932 1.0662 sec/batch\n",
      "Epoch 11/20  Iteration 3791/7380 Training loss: 5.0944 1.0769 sec/batch\n",
      "Epoch 11/20  Iteration 3792/7380 Training loss: 5.0946 1.0689 sec/batch\n",
      "Epoch 11/20  Iteration 3793/7380 Training loss: 5.0949 1.0678 sec/batch\n",
      "Epoch 11/20  Iteration 3794/7380 Training loss: 5.0950 1.0682 sec/batch\n",
      "Epoch 11/20  Iteration 3795/7380 Training loss: 5.0956 1.0761 sec/batch\n",
      "Epoch 11/20  Iteration 3796/7380 Training loss: 5.0956 1.0788 sec/batch\n",
      "Epoch 11/20  Iteration 3797/7380 Training loss: 5.0953 1.0685 sec/batch\n",
      "Epoch 11/20  Iteration 3798/7380 Training loss: 5.0951 1.0658 sec/batch\n",
      "Epoch 11/20  Iteration 3799/7380 Training loss: 5.0936 1.0716 sec/batch\n",
      "Epoch 11/20  Iteration 3800/7380 Training loss: 5.0935 1.0657 sec/batch\n",
      "Validation loss: 5.10179 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3801/7380 Training loss: 5.0941 1.0674 sec/batch\n",
      "Epoch 11/20  Iteration 3802/7380 Training loss: 5.0942 1.0691 sec/batch\n",
      "Epoch 11/20  Iteration 3803/7380 Training loss: 5.0935 1.0669 sec/batch\n",
      "Epoch 11/20  Iteration 3804/7380 Training loss: 5.0942 1.0744 sec/batch\n",
      "Epoch 11/20  Iteration 3805/7380 Training loss: 5.0940 1.0778 sec/batch\n",
      "Epoch 11/20  Iteration 3806/7380 Training loss: 5.0935 1.0684 sec/batch\n",
      "Epoch 11/20  Iteration 3807/7380 Training loss: 5.0928 1.0732 sec/batch\n",
      "Epoch 11/20  Iteration 3808/7380 Training loss: 5.0926 1.1042 sec/batch\n",
      "Epoch 11/20  Iteration 3809/7380 Training loss: 5.0929 1.0821 sec/batch\n",
      "Epoch 11/20  Iteration 3810/7380 Training loss: 5.0925 1.0738 sec/batch\n",
      "Epoch 11/20  Iteration 3811/7380 Training loss: 5.0927 1.0669 sec/batch\n",
      "Epoch 11/20  Iteration 3812/7380 Training loss: 5.0937 1.0663 sec/batch\n",
      "Epoch 11/20  Iteration 3813/7380 Training loss: 5.0934 1.0700 sec/batch\n",
      "Epoch 11/20  Iteration 3814/7380 Training loss: 5.0933 1.0770 sec/batch\n",
      "Epoch 11/20  Iteration 3815/7380 Training loss: 5.0932 1.0740 sec/batch\n",
      "Epoch 11/20  Iteration 3816/7380 Training loss: 5.0922 1.0806 sec/batch\n",
      "Epoch 11/20  Iteration 3817/7380 Training loss: 5.0907 1.0820 sec/batch\n",
      "Epoch 11/20  Iteration 3818/7380 Training loss: 5.0909 1.0682 sec/batch\n",
      "Epoch 11/20  Iteration 3819/7380 Training loss: 5.0904 1.0830 sec/batch\n",
      "Epoch 11/20  Iteration 3820/7380 Training loss: 5.0906 1.0788 sec/batch\n",
      "Epoch 11/20  Iteration 3821/7380 Training loss: 5.0906 1.0661 sec/batch\n",
      "Epoch 11/20  Iteration 3822/7380 Training loss: 5.0907 1.1016 sec/batch\n",
      "Epoch 11/20  Iteration 3823/7380 Training loss: 5.0908 1.0692 sec/batch\n",
      "Epoch 11/20  Iteration 3824/7380 Training loss: 5.0897 1.0791 sec/batch\n",
      "Epoch 11/20  Iteration 3825/7380 Training loss: 5.0895 1.0668 sec/batch\n",
      "Epoch 11/20  Iteration 3826/7380 Training loss: 5.0896 1.0803 sec/batch\n",
      "Epoch 11/20  Iteration 3827/7380 Training loss: 5.0893 1.0747 sec/batch\n",
      "Epoch 11/20  Iteration 3828/7380 Training loss: 5.0890 1.0867 sec/batch\n",
      "Epoch 11/20  Iteration 3829/7380 Training loss: 5.0882 1.0994 sec/batch\n",
      "Epoch 11/20  Iteration 3830/7380 Training loss: 5.0875 1.0838 sec/batch\n",
      "Epoch 11/20  Iteration 3831/7380 Training loss: 5.0891 1.0733 sec/batch\n",
      "Epoch 11/20  Iteration 3832/7380 Training loss: 5.0891 1.0767 sec/batch\n",
      "Epoch 11/20  Iteration 3833/7380 Training loss: 5.0896 1.0710 sec/batch\n",
      "Epoch 11/20  Iteration 3834/7380 Training loss: 5.0896 1.0715 sec/batch\n",
      "Epoch 11/20  Iteration 3835/7380 Training loss: 5.0899 1.0673 sec/batch\n",
      "Epoch 11/20  Iteration 3836/7380 Training loss: 5.0898 1.0695 sec/batch\n",
      "Epoch 11/20  Iteration 3837/7380 Training loss: 5.0896 1.0792 sec/batch\n",
      "Epoch 11/20  Iteration 3838/7380 Training loss: 5.0895 1.0662 sec/batch\n",
      "Epoch 11/20  Iteration 3839/7380 Training loss: 5.0889 1.0769 sec/batch\n",
      "Epoch 11/20  Iteration 3840/7380 Training loss: 5.0893 1.0817 sec/batch\n",
      "Epoch 11/20  Iteration 3841/7380 Training loss: 5.0886 1.0680 sec/batch\n",
      "Epoch 11/20  Iteration 3842/7380 Training loss: 5.0890 1.0667 sec/batch\n",
      "Epoch 11/20  Iteration 3843/7380 Training loss: 5.0894 1.1111 sec/batch\n",
      "Epoch 11/20  Iteration 3844/7380 Training loss: 5.0899 1.0732 sec/batch\n",
      "Epoch 11/20  Iteration 3845/7380 Training loss: 5.0896 1.0769 sec/batch\n",
      "Epoch 11/20  Iteration 3846/7380 Training loss: 5.0892 1.0735 sec/batch\n",
      "Epoch 11/20  Iteration 3847/7380 Training loss: 5.0886 1.0756 sec/batch\n",
      "Epoch 11/20  Iteration 3848/7380 Training loss: 5.0893 1.0709 sec/batch\n",
      "Epoch 11/20  Iteration 3849/7380 Training loss: 5.0890 1.0687 sec/batch\n",
      "Epoch 11/20  Iteration 3850/7380 Training loss: 5.0890 1.0673 sec/batch\n",
      "Validation loss: 5.10998 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3851/7380 Training loss: 5.0903 1.0801 sec/batch\n",
      "Epoch 11/20  Iteration 3852/7380 Training loss: 5.0907 1.0747 sec/batch\n",
      "Epoch 11/20  Iteration 3853/7380 Training loss: 5.0911 1.0678 sec/batch\n",
      "Epoch 11/20  Iteration 3854/7380 Training loss: 5.0913 1.0709 sec/batch\n",
      "Epoch 11/20  Iteration 3855/7380 Training loss: 5.0916 1.0688 sec/batch\n",
      "Epoch 11/20  Iteration 3856/7380 Training loss: 5.0921 1.0686 sec/batch\n",
      "Epoch 11/20  Iteration 3857/7380 Training loss: 5.0922 1.0701 sec/batch\n",
      "Epoch 11/20  Iteration 3858/7380 Training loss: 5.0926 1.0713 sec/batch\n",
      "Epoch 11/20  Iteration 3859/7380 Training loss: 5.0920 1.0779 sec/batch\n",
      "Epoch 11/20  Iteration 3860/7380 Training loss: 5.0915 1.0785 sec/batch\n",
      "Epoch 11/20  Iteration 3861/7380 Training loss: 5.0918 1.0707 sec/batch\n",
      "Epoch 11/20  Iteration 3862/7380 Training loss: 5.0924 1.0833 sec/batch\n",
      "Epoch 11/20  Iteration 3863/7380 Training loss: 5.0925 1.0727 sec/batch\n",
      "Epoch 11/20  Iteration 3864/7380 Training loss: 5.0921 1.0793 sec/batch\n",
      "Epoch 11/20  Iteration 3865/7380 Training loss: 5.0917 1.0675 sec/batch\n",
      "Epoch 11/20  Iteration 3866/7380 Training loss: 5.0913 1.0649 sec/batch\n",
      "Epoch 11/20  Iteration 3867/7380 Training loss: 5.0913 1.0662 sec/batch\n",
      "Epoch 11/20  Iteration 3868/7380 Training loss: 5.0916 1.0705 sec/batch\n",
      "Epoch 11/20  Iteration 3869/7380 Training loss: 5.0907 1.0776 sec/batch\n",
      "Epoch 11/20  Iteration 3870/7380 Training loss: 5.0900 1.0683 sec/batch\n",
      "Epoch 11/20  Iteration 3871/7380 Training loss: 5.0897 1.0664 sec/batch\n",
      "Epoch 11/20  Iteration 3872/7380 Training loss: 5.0898 1.0788 sec/batch\n",
      "Epoch 11/20  Iteration 3873/7380 Training loss: 5.0905 1.1276 sec/batch\n",
      "Epoch 11/20  Iteration 3874/7380 Training loss: 5.0902 1.0722 sec/batch\n",
      "Epoch 11/20  Iteration 3875/7380 Training loss: 5.0900 1.1017 sec/batch\n",
      "Epoch 11/20  Iteration 3876/7380 Training loss: 5.0899 1.0709 sec/batch\n",
      "Epoch 11/20  Iteration 3877/7380 Training loss: 5.0896 1.0770 sec/batch\n",
      "Epoch 11/20  Iteration 3878/7380 Training loss: 5.0892 1.0711 sec/batch\n",
      "Epoch 11/20  Iteration 3879/7380 Training loss: 5.0887 1.0862 sec/batch\n",
      "Epoch 11/20  Iteration 3880/7380 Training loss: 5.0887 1.0764 sec/batch\n",
      "Epoch 11/20  Iteration 3881/7380 Training loss: 5.0879 1.0814 sec/batch\n",
      "Epoch 11/20  Iteration 3882/7380 Training loss: 5.0881 1.0735 sec/batch\n",
      "Epoch 11/20  Iteration 3883/7380 Training loss: 5.0884 1.0827 sec/batch\n",
      "Epoch 11/20  Iteration 3884/7380 Training loss: 5.0877 1.0864 sec/batch\n",
      "Epoch 11/20  Iteration 3885/7380 Training loss: 5.0880 1.0698 sec/batch\n",
      "Epoch 11/20  Iteration 3886/7380 Training loss: 5.0880 1.1124 sec/batch\n",
      "Epoch 11/20  Iteration 3887/7380 Training loss: 5.0881 1.0963 sec/batch\n",
      "Epoch 11/20  Iteration 3888/7380 Training loss: 5.0885 1.0772 sec/batch\n",
      "Epoch 11/20  Iteration 3889/7380 Training loss: 5.0884 1.0799 sec/batch\n",
      "Epoch 11/20  Iteration 3890/7380 Training loss: 5.0883 1.0695 sec/batch\n",
      "Epoch 11/20  Iteration 3891/7380 Training loss: 5.0886 1.0701 sec/batch\n",
      "Epoch 11/20  Iteration 3892/7380 Training loss: 5.0891 1.0762 sec/batch\n",
      "Epoch 11/20  Iteration 3893/7380 Training loss: 5.0896 1.0672 sec/batch\n",
      "Epoch 11/20  Iteration 3894/7380 Training loss: 5.0895 1.0862 sec/batch\n",
      "Epoch 11/20  Iteration 3895/7380 Training loss: 5.0897 1.0672 sec/batch\n",
      "Epoch 11/20  Iteration 3896/7380 Training loss: 5.0900 1.0725 sec/batch\n",
      "Epoch 11/20  Iteration 3897/7380 Training loss: 5.0892 1.0739 sec/batch\n",
      "Epoch 11/20  Iteration 3898/7380 Training loss: 5.0887 1.0720 sec/batch\n",
      "Epoch 11/20  Iteration 3899/7380 Training loss: 5.0883 1.0703 sec/batch\n",
      "Epoch 11/20  Iteration 3900/7380 Training loss: 5.0877 1.0723 sec/batch\n",
      "Validation loss: 5.10897 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3901/7380 Training loss: 5.0880 1.0777 sec/batch\n",
      "Epoch 11/20  Iteration 3902/7380 Training loss: 5.0876 1.0749 sec/batch\n",
      "Epoch 11/20  Iteration 3903/7380 Training loss: 5.0876 1.0784 sec/batch\n",
      "Epoch 11/20  Iteration 3904/7380 Training loss: 5.0875 1.0732 sec/batch\n",
      "Epoch 11/20  Iteration 3905/7380 Training loss: 5.0871 1.0768 sec/batch\n",
      "Epoch 11/20  Iteration 3906/7380 Training loss: 5.0867 1.0803 sec/batch\n",
      "Epoch 11/20  Iteration 3907/7380 Training loss: 5.0860 1.0723 sec/batch\n",
      "Epoch 11/20  Iteration 3908/7380 Training loss: 5.0863 1.0689 sec/batch\n",
      "Epoch 11/20  Iteration 3909/7380 Training loss: 5.0866 1.0734 sec/batch\n",
      "Epoch 11/20  Iteration 3910/7380 Training loss: 5.0866 1.0769 sec/batch\n",
      "Epoch 11/20  Iteration 3911/7380 Training loss: 5.0863 1.0697 sec/batch\n",
      "Epoch 11/20  Iteration 3912/7380 Training loss: 5.0867 1.0798 sec/batch\n",
      "Epoch 11/20  Iteration 3913/7380 Training loss: 5.0867 1.0760 sec/batch\n",
      "Epoch 11/20  Iteration 3914/7380 Training loss: 5.0862 1.0803 sec/batch\n",
      "Epoch 11/20  Iteration 3915/7380 Training loss: 5.0862 1.0820 sec/batch\n",
      "Epoch 11/20  Iteration 3916/7380 Training loss: 5.0860 1.0993 sec/batch\n",
      "Epoch 11/20  Iteration 3917/7380 Training loss: 5.0858 1.0701 sec/batch\n",
      "Epoch 11/20  Iteration 3918/7380 Training loss: 5.0853 1.0713 sec/batch\n",
      "Epoch 11/20  Iteration 3919/7380 Training loss: 5.0852 1.0757 sec/batch\n",
      "Epoch 11/20  Iteration 3920/7380 Training loss: 5.0857 1.0731 sec/batch\n",
      "Epoch 11/20  Iteration 3921/7380 Training loss: 5.0853 1.0783 sec/batch\n",
      "Epoch 11/20  Iteration 3922/7380 Training loss: 5.0851 1.0867 sec/batch\n",
      "Epoch 11/20  Iteration 3923/7380 Training loss: 5.0846 1.0847 sec/batch\n",
      "Epoch 11/20  Iteration 3924/7380 Training loss: 5.0843 1.0758 sec/batch\n",
      "Epoch 11/20  Iteration 3925/7380 Training loss: 5.0840 1.0713 sec/batch\n",
      "Epoch 11/20  Iteration 3926/7380 Training loss: 5.0836 1.0738 sec/batch\n",
      "Epoch 11/20  Iteration 3927/7380 Training loss: 5.0834 1.0748 sec/batch\n",
      "Epoch 11/20  Iteration 3928/7380 Training loss: 5.0828 1.1052 sec/batch\n",
      "Epoch 11/20  Iteration 3929/7380 Training loss: 5.0817 1.0760 sec/batch\n",
      "Epoch 11/20  Iteration 3930/7380 Training loss: 5.0810 1.0701 sec/batch\n",
      "Epoch 11/20  Iteration 3931/7380 Training loss: 5.0810 1.0700 sec/batch\n",
      "Epoch 11/20  Iteration 3932/7380 Training loss: 5.0817 1.0690 sec/batch\n",
      "Epoch 11/20  Iteration 3933/7380 Training loss: 5.0813 1.0817 sec/batch\n",
      "Epoch 11/20  Iteration 3934/7380 Training loss: 5.0814 1.0795 sec/batch\n",
      "Epoch 11/20  Iteration 3935/7380 Training loss: 5.0815 1.0753 sec/batch\n",
      "Epoch 11/20  Iteration 3936/7380 Training loss: 5.0817 1.0793 sec/batch\n",
      "Epoch 11/20  Iteration 3937/7380 Training loss: 5.0816 1.0718 sec/batch\n",
      "Epoch 11/20  Iteration 3938/7380 Training loss: 5.0816 1.0722 sec/batch\n",
      "Epoch 11/20  Iteration 3939/7380 Training loss: 5.0815 1.0707 sec/batch\n",
      "Epoch 11/20  Iteration 3940/7380 Training loss: 5.0816 1.0731 sec/batch\n",
      "Epoch 11/20  Iteration 3941/7380 Training loss: 5.0812 1.0726 sec/batch\n",
      "Epoch 11/20  Iteration 3942/7380 Training loss: 5.0811 1.0776 sec/batch\n",
      "Epoch 11/20  Iteration 3943/7380 Training loss: 5.0809 1.0730 sec/batch\n",
      "Epoch 11/20  Iteration 3944/7380 Training loss: 5.0803 1.0740 sec/batch\n",
      "Epoch 11/20  Iteration 3945/7380 Training loss: 5.0803 1.0737 sec/batch\n",
      "Epoch 11/20  Iteration 3946/7380 Training loss: 5.0805 1.0757 sec/batch\n",
      "Epoch 11/20  Iteration 3947/7380 Training loss: 5.0808 1.0721 sec/batch\n",
      "Epoch 11/20  Iteration 3948/7380 Training loss: 5.0805 1.0670 sec/batch\n",
      "Epoch 11/20  Iteration 3949/7380 Training loss: 5.0807 1.0720 sec/batch\n",
      "Epoch 11/20  Iteration 3950/7380 Training loss: 5.0803 1.0758 sec/batch\n",
      "Validation loss: 5.09721 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3951/7380 Training loss: 5.0803 1.0731 sec/batch\n",
      "Epoch 11/20  Iteration 3952/7380 Training loss: 5.0804 1.0716 sec/batch\n",
      "Epoch 11/20  Iteration 3953/7380 Training loss: 5.0806 1.0810 sec/batch\n",
      "Epoch 11/20  Iteration 3954/7380 Training loss: 5.0804 1.0708 sec/batch\n",
      "Epoch 11/20  Iteration 3955/7380 Training loss: 5.0801 1.0794 sec/batch\n",
      "Epoch 11/20  Iteration 3956/7380 Training loss: 5.0798 1.0925 sec/batch\n",
      "Epoch 11/20  Iteration 3957/7380 Training loss: 5.0799 1.0731 sec/batch\n",
      "Epoch 11/20  Iteration 3958/7380 Training loss: 5.0800 1.0996 sec/batch\n",
      "Epoch 11/20  Iteration 3959/7380 Training loss: 5.0794 1.1029 sec/batch\n",
      "Epoch 11/20  Iteration 3960/7380 Training loss: 5.0796 1.0761 sec/batch\n",
      "Epoch 11/20  Iteration 3961/7380 Training loss: 5.0798 1.0732 sec/batch\n",
      "Epoch 11/20  Iteration 3962/7380 Training loss: 5.0792 1.0749 sec/batch\n",
      "Epoch 11/20  Iteration 3963/7380 Training loss: 5.0794 1.0722 sec/batch\n",
      "Epoch 11/20  Iteration 3964/7380 Training loss: 5.0796 1.0695 sec/batch\n",
      "Epoch 11/20  Iteration 3965/7380 Training loss: 5.0795 1.0690 sec/batch\n",
      "Epoch 11/20  Iteration 3966/7380 Training loss: 5.0792 1.0702 sec/batch\n",
      "Epoch 11/20  Iteration 3967/7380 Training loss: 5.0798 1.0699 sec/batch\n",
      "Epoch 11/20  Iteration 3968/7380 Training loss: 5.0798 1.0977 sec/batch\n",
      "Epoch 11/20  Iteration 3969/7380 Training loss: 5.0800 1.0720 sec/batch\n",
      "Epoch 11/20  Iteration 3970/7380 Training loss: 5.0798 1.0760 sec/batch\n",
      "Epoch 11/20  Iteration 3971/7380 Training loss: 5.0799 1.0691 sec/batch\n",
      "Epoch 11/20  Iteration 3972/7380 Training loss: 5.0799 1.0768 sec/batch\n",
      "Epoch 11/20  Iteration 3973/7380 Training loss: 5.0801 1.0699 sec/batch\n",
      "Epoch 11/20  Iteration 3974/7380 Training loss: 5.0802 1.0793 sec/batch\n",
      "Epoch 11/20  Iteration 3975/7380 Training loss: 5.0805 1.0725 sec/batch\n",
      "Epoch 11/20  Iteration 3976/7380 Training loss: 5.0803 1.0766 sec/batch\n",
      "Epoch 11/20  Iteration 3977/7380 Training loss: 5.0800 1.0757 sec/batch\n",
      "Epoch 11/20  Iteration 3978/7380 Training loss: 5.0798 1.0754 sec/batch\n",
      "Epoch 11/20  Iteration 3979/7380 Training loss: 5.0796 1.0794 sec/batch\n",
      "Epoch 11/20  Iteration 3980/7380 Training loss: 5.0791 1.1086 sec/batch\n",
      "Epoch 11/20  Iteration 3981/7380 Training loss: 5.0786 1.0900 sec/batch\n",
      "Epoch 11/20  Iteration 3982/7380 Training loss: 5.0783 1.0754 sec/batch\n",
      "Epoch 11/20  Iteration 3983/7380 Training loss: 5.0781 1.0759 sec/batch\n",
      "Epoch 11/20  Iteration 3984/7380 Training loss: 5.0780 1.0694 sec/batch\n",
      "Epoch 11/20  Iteration 3985/7380 Training loss: 5.0780 1.0807 sec/batch\n",
      "Epoch 11/20  Iteration 3986/7380 Training loss: 5.0779 1.0714 sec/batch\n",
      "Epoch 11/20  Iteration 3987/7380 Training loss: 5.0776 1.0804 sec/batch\n",
      "Epoch 11/20  Iteration 3988/7380 Training loss: 5.0773 1.0803 sec/batch\n",
      "Epoch 11/20  Iteration 3989/7380 Training loss: 5.0775 1.0757 sec/batch\n",
      "Epoch 11/20  Iteration 3990/7380 Training loss: 5.0780 1.0755 sec/batch\n",
      "Epoch 11/20  Iteration 3991/7380 Training loss: 5.0780 1.0684 sec/batch\n",
      "Epoch 11/20  Iteration 3992/7380 Training loss: 5.0782 1.0781 sec/batch\n",
      "Epoch 11/20  Iteration 3993/7380 Training loss: 5.0783 1.0700 sec/batch\n",
      "Epoch 11/20  Iteration 3994/7380 Training loss: 5.0788 1.0695 sec/batch\n",
      "Epoch 11/20  Iteration 3995/7380 Training loss: 5.0791 1.0726 sec/batch\n",
      "Epoch 11/20  Iteration 3996/7380 Training loss: 5.0790 1.0851 sec/batch\n",
      "Epoch 11/20  Iteration 3997/7380 Training loss: 5.0786 1.0732 sec/batch\n",
      "Epoch 11/20  Iteration 3998/7380 Training loss: 5.0782 1.0711 sec/batch\n",
      "Epoch 11/20  Iteration 3999/7380 Training loss: 5.0780 1.0822 sec/batch\n",
      "Epoch 11/20  Iteration 4000/7380 Training loss: 5.0776 1.0741 sec/batch\n",
      "Validation loss: 5.0956 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 4001/7380 Training loss: 5.0779 1.0825 sec/batch\n",
      "Epoch 11/20  Iteration 4002/7380 Training loss: 5.0777 1.1142 sec/batch\n",
      "Epoch 11/20  Iteration 4003/7380 Training loss: 5.0773 1.0767 sec/batch\n",
      "Epoch 11/20  Iteration 4004/7380 Training loss: 5.0772 1.0818 sec/batch\n",
      "Epoch 11/20  Iteration 4005/7380 Training loss: 5.0774 1.0699 sec/batch\n",
      "Epoch 11/20  Iteration 4006/7380 Training loss: 5.0776 1.0738 sec/batch\n",
      "Epoch 11/20  Iteration 4007/7380 Training loss: 5.0778 1.0733 sec/batch\n",
      "Epoch 11/20  Iteration 4008/7380 Training loss: 5.0779 1.0824 sec/batch\n",
      "Epoch 11/20  Iteration 4009/7380 Training loss: 5.0784 1.0716 sec/batch\n",
      "Epoch 11/20  Iteration 4010/7380 Training loss: 5.0787 1.0682 sec/batch\n",
      "Epoch 11/20  Iteration 4011/7380 Training loss: 5.0785 1.0767 sec/batch\n",
      "Epoch 11/20  Iteration 4012/7380 Training loss: 5.0781 1.0803 sec/batch\n",
      "Epoch 11/20  Iteration 4013/7380 Training loss: 5.0780 1.0748 sec/batch\n",
      "Epoch 11/20  Iteration 4014/7380 Training loss: 5.0778 1.0822 sec/batch\n",
      "Epoch 11/20  Iteration 4015/7380 Training loss: 5.0778 1.0891 sec/batch\n",
      "Epoch 11/20  Iteration 4016/7380 Training loss: 5.0782 1.0788 sec/batch\n",
      "Epoch 11/20  Iteration 4017/7380 Training loss: 5.0778 1.0784 sec/batch\n",
      "Epoch 11/20  Iteration 4018/7380 Training loss: 5.0775 1.0728 sec/batch\n",
      "Epoch 11/20  Iteration 4019/7380 Training loss: 5.0773 1.0718 sec/batch\n",
      "Epoch 11/20  Iteration 4020/7380 Training loss: 5.0770 1.0794 sec/batch\n",
      "Epoch 11/20  Iteration 4021/7380 Training loss: 5.0766 1.0763 sec/batch\n",
      "Epoch 11/20  Iteration 4022/7380 Training loss: 5.0769 1.0769 sec/batch\n",
      "Epoch 11/20  Iteration 4023/7380 Training loss: 5.0766 1.0839 sec/batch\n",
      "Epoch 11/20  Iteration 4024/7380 Training loss: 5.0768 1.0706 sec/batch\n",
      "Epoch 11/20  Iteration 4025/7380 Training loss: 5.0769 1.0885 sec/batch\n",
      "Epoch 11/20  Iteration 4026/7380 Training loss: 5.0770 1.0861 sec/batch\n",
      "Epoch 11/20  Iteration 4027/7380 Training loss: 5.0772 1.0869 sec/batch\n",
      "Epoch 11/20  Iteration 4028/7380 Training loss: 5.0771 1.0683 sec/batch\n",
      "Epoch 11/20  Iteration 4029/7380 Training loss: 5.0772 1.0840 sec/batch\n",
      "Epoch 11/20  Iteration 4030/7380 Training loss: 5.0768 1.0842 sec/batch\n",
      "Epoch 11/20  Iteration 4031/7380 Training loss: 5.0765 1.0997 sec/batch\n",
      "Epoch 11/20  Iteration 4032/7380 Training loss: 5.0762 1.1021 sec/batch\n",
      "Epoch 11/20  Iteration 4033/7380 Training loss: 5.0761 1.0803 sec/batch\n",
      "Epoch 11/20  Iteration 4034/7380 Training loss: 5.0760 1.0788 sec/batch\n",
      "Epoch 11/20  Iteration 4035/7380 Training loss: 5.0759 1.0773 sec/batch\n",
      "Epoch 11/20  Iteration 4036/7380 Training loss: 5.0758 1.0757 sec/batch\n",
      "Epoch 11/20  Iteration 4037/7380 Training loss: 5.0761 1.0739 sec/batch\n",
      "Epoch 11/20  Iteration 4038/7380 Training loss: 5.0762 1.0823 sec/batch\n",
      "Epoch 11/20  Iteration 4039/7380 Training loss: 5.0760 1.0721 sec/batch\n",
      "Epoch 11/20  Iteration 4040/7380 Training loss: 5.0759 1.0667 sec/batch\n",
      "Epoch 11/20  Iteration 4041/7380 Training loss: 5.0764 1.0682 sec/batch\n",
      "Epoch 11/20  Iteration 4042/7380 Training loss: 5.0762 1.0820 sec/batch\n",
      "Epoch 11/20  Iteration 4043/7380 Training loss: 5.0762 1.0718 sec/batch\n",
      "Epoch 11/20  Iteration 4044/7380 Training loss: 5.0759 1.0847 sec/batch\n",
      "Epoch 11/20  Iteration 4045/7380 Training loss: 5.0757 1.0730 sec/batch\n",
      "Epoch 11/20  Iteration 4046/7380 Training loss: 5.0755 1.0885 sec/batch\n",
      "Epoch 11/20  Iteration 4047/7380 Training loss: 5.0757 1.0905 sec/batch\n",
      "Epoch 11/20  Iteration 4048/7380 Training loss: 5.0757 1.0681 sec/batch\n",
      "Epoch 11/20  Iteration 4049/7380 Training loss: 5.0754 1.0716 sec/batch\n",
      "Epoch 11/20  Iteration 4050/7380 Training loss: 5.0751 1.0727 sec/batch\n",
      "Validation loss: 5.08929 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 4051/7380 Training loss: 5.0754 1.0747 sec/batch\n",
      "Epoch 11/20  Iteration 4052/7380 Training loss: 5.0754 1.0812 sec/batch\n",
      "Epoch 11/20  Iteration 4053/7380 Training loss: 5.0753 1.0756 sec/batch\n",
      "Epoch 11/20  Iteration 4054/7380 Training loss: 5.0749 1.0720 sec/batch\n",
      "Epoch 11/20  Iteration 4055/7380 Training loss: 5.0747 1.0779 sec/batch\n",
      "Epoch 11/20  Iteration 4056/7380 Training loss: 5.0744 1.0756 sec/batch\n",
      "Epoch 11/20  Iteration 4057/7380 Training loss: 5.0742 1.0729 sec/batch\n",
      "Epoch 11/20  Iteration 4058/7380 Training loss: 5.0743 1.0849 sec/batch\n",
      "Epoch 11/20  Iteration 4059/7380 Training loss: 5.0740 1.0759 sec/batch\n",
      "Epoch 12/20  Iteration 4060/7380 Training loss: 5.1100 1.0727 sec/batch\n",
      "Epoch 12/20  Iteration 4061/7380 Training loss: 5.0812 1.0764 sec/batch\n",
      "Epoch 12/20  Iteration 4062/7380 Training loss: 5.0564 1.0926 sec/batch\n",
      "Epoch 12/20  Iteration 4063/7380 Training loss: 5.0545 1.0743 sec/batch\n",
      "Epoch 12/20  Iteration 4064/7380 Training loss: 5.0448 1.0774 sec/batch\n",
      "Epoch 12/20  Iteration 4065/7380 Training loss: 5.0546 1.0830 sec/batch\n",
      "Epoch 12/20  Iteration 4066/7380 Training loss: 5.0761 1.0983 sec/batch\n",
      "Epoch 12/20  Iteration 4067/7380 Training loss: 5.0613 1.0913 sec/batch\n",
      "Epoch 12/20  Iteration 4068/7380 Training loss: 5.0568 1.0751 sec/batch\n",
      "Epoch 12/20  Iteration 4069/7380 Training loss: 5.0670 1.0723 sec/batch\n",
      "Epoch 12/20  Iteration 4070/7380 Training loss: 5.0666 1.0826 sec/batch\n",
      "Epoch 12/20  Iteration 4071/7380 Training loss: 5.0574 1.0758 sec/batch\n",
      "Epoch 12/20  Iteration 4072/7380 Training loss: 5.0545 1.0747 sec/batch\n",
      "Epoch 12/20  Iteration 4073/7380 Training loss: 5.0642 1.0734 sec/batch\n",
      "Epoch 12/20  Iteration 4074/7380 Training loss: 5.0684 1.0843 sec/batch\n",
      "Epoch 12/20  Iteration 4075/7380 Training loss: 5.0665 1.0836 sec/batch\n",
      "Epoch 12/20  Iteration 4076/7380 Training loss: 5.0633 1.0861 sec/batch\n",
      "Epoch 12/20  Iteration 4077/7380 Training loss: 5.0492 1.0898 sec/batch\n",
      "Epoch 12/20  Iteration 4078/7380 Training loss: 5.0438 1.0733 sec/batch\n",
      "Epoch 12/20  Iteration 4079/7380 Training loss: 5.0419 1.0870 sec/batch\n",
      "Epoch 12/20  Iteration 4080/7380 Training loss: 5.0374 1.0761 sec/batch\n",
      "Epoch 12/20  Iteration 4081/7380 Training loss: 5.0449 1.0770 sec/batch\n",
      "Epoch 12/20  Iteration 4082/7380 Training loss: 5.0447 1.0786 sec/batch\n",
      "Epoch 12/20  Iteration 4083/7380 Training loss: 5.0519 1.0767 sec/batch\n",
      "Epoch 12/20  Iteration 4084/7380 Training loss: 5.0547 1.0727 sec/batch\n",
      "Epoch 12/20  Iteration 4085/7380 Training loss: 5.0561 1.0796 sec/batch\n",
      "Epoch 12/20  Iteration 4086/7380 Training loss: 5.0500 1.0860 sec/batch\n",
      "Epoch 12/20  Iteration 4087/7380 Training loss: 5.0563 1.0781 sec/batch\n",
      "Epoch 12/20  Iteration 4088/7380 Training loss: 5.0562 1.0823 sec/batch\n",
      "Epoch 12/20  Iteration 4089/7380 Training loss: 5.0563 1.0749 sec/batch\n",
      "Epoch 12/20  Iteration 4090/7380 Training loss: 5.0554 1.0782 sec/batch\n",
      "Epoch 12/20  Iteration 4091/7380 Training loss: 5.0564 1.0759 sec/batch\n",
      "Epoch 12/20  Iteration 4092/7380 Training loss: 5.0554 1.0786 sec/batch\n",
      "Epoch 12/20  Iteration 4093/7380 Training loss: 5.0562 1.0863 sec/batch\n",
      "Epoch 12/20  Iteration 4094/7380 Training loss: 5.0602 1.0834 sec/batch\n",
      "Epoch 12/20  Iteration 4095/7380 Training loss: 5.0594 1.0744 sec/batch\n",
      "Epoch 12/20  Iteration 4096/7380 Training loss: 5.0601 1.0840 sec/batch\n",
      "Epoch 12/20  Iteration 4097/7380 Training loss: 5.0577 1.0809 sec/batch\n",
      "Epoch 12/20  Iteration 4098/7380 Training loss: 5.0591 1.0785 sec/batch\n",
      "Epoch 12/20  Iteration 4099/7380 Training loss: 5.0594 1.0775 sec/batch\n",
      "Epoch 12/20  Iteration 4100/7380 Training loss: 5.0596 1.0765 sec/batch\n",
      "Validation loss: 5.09067 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 4101/7380 Training loss: 5.0605 1.0784 sec/batch\n",
      "Epoch 12/20  Iteration 4102/7380 Training loss: 5.0615 1.1037 sec/batch\n",
      "Epoch 12/20  Iteration 4103/7380 Training loss: 5.0631 1.0744 sec/batch\n",
      "Epoch 12/20  Iteration 4104/7380 Training loss: 5.0643 1.0730 sec/batch\n",
      "Epoch 12/20  Iteration 4105/7380 Training loss: 5.0626 1.0732 sec/batch\n",
      "Epoch 12/20  Iteration 4106/7380 Training loss: 5.0632 1.0868 sec/batch\n",
      "Epoch 12/20  Iteration 4107/7380 Training loss: 5.0608 1.0705 sec/batch\n",
      "Epoch 12/20  Iteration 4108/7380 Training loss: 5.0623 1.0838 sec/batch\n",
      "Epoch 12/20  Iteration 4109/7380 Training loss: 5.0618 1.0865 sec/batch\n",
      "Epoch 12/20  Iteration 4110/7380 Training loss: 5.0599 1.0834 sec/batch\n",
      "Epoch 12/20  Iteration 4111/7380 Training loss: 5.0587 1.0768 sec/batch\n",
      "Epoch 12/20  Iteration 4112/7380 Training loss: 5.0569 1.0773 sec/batch\n",
      "Epoch 12/20  Iteration 4113/7380 Training loss: 5.0554 1.0798 sec/batch\n",
      "Epoch 12/20  Iteration 4114/7380 Training loss: 5.0538 1.0767 sec/batch\n",
      "Epoch 12/20  Iteration 4115/7380 Training loss: 5.0513 1.0802 sec/batch\n",
      "Epoch 12/20  Iteration 4116/7380 Training loss: 5.0512 1.0783 sec/batch\n",
      "Epoch 12/20  Iteration 4117/7380 Training loss: 5.0517 1.0755 sec/batch\n",
      "Epoch 12/20  Iteration 4118/7380 Training loss: 5.0514 1.0757 sec/batch\n",
      "Epoch 12/20  Iteration 4119/7380 Training loss: 5.0518 1.0785 sec/batch\n",
      "Epoch 12/20  Iteration 4120/7380 Training loss: 5.0500 1.0747 sec/batch\n",
      "Epoch 12/20  Iteration 4121/7380 Training loss: 5.0507 1.0802 sec/batch\n",
      "Epoch 12/20  Iteration 4122/7380 Training loss: 5.0491 1.0802 sec/batch\n",
      "Epoch 12/20  Iteration 4123/7380 Training loss: 5.0468 1.0744 sec/batch\n",
      "Epoch 12/20  Iteration 4124/7380 Training loss: 5.0466 1.0720 sec/batch\n",
      "Epoch 12/20  Iteration 4125/7380 Training loss: 5.0466 1.0801 sec/batch\n",
      "Epoch 12/20  Iteration 4126/7380 Training loss: 5.0473 1.0706 sec/batch\n",
      "Epoch 12/20  Iteration 4127/7380 Training loss: 5.0477 1.0782 sec/batch\n",
      "Epoch 12/20  Iteration 4128/7380 Training loss: 5.0492 1.0798 sec/batch\n",
      "Epoch 12/20  Iteration 4129/7380 Training loss: 5.0479 1.0738 sec/batch\n",
      "Epoch 12/20  Iteration 4130/7380 Training loss: 5.0465 1.0779 sec/batch\n",
      "Epoch 12/20  Iteration 4131/7380 Training loss: 5.0471 1.0777 sec/batch\n",
      "Epoch 12/20  Iteration 4132/7380 Training loss: 5.0455 1.0808 sec/batch\n",
      "Epoch 12/20  Iteration 4133/7380 Training loss: 5.0470 1.1050 sec/batch\n",
      "Epoch 12/20  Iteration 4134/7380 Training loss: 5.0477 1.0735 sec/batch\n",
      "Epoch 12/20  Iteration 4135/7380 Training loss: 5.0473 1.0989 sec/batch\n",
      "Epoch 12/20  Iteration 4136/7380 Training loss: 5.0472 1.0733 sec/batch\n",
      "Epoch 12/20  Iteration 4137/7380 Training loss: 5.0470 1.0815 sec/batch\n",
      "Epoch 12/20  Iteration 4138/7380 Training loss: 5.0475 1.0852 sec/batch\n",
      "Epoch 12/20  Iteration 4139/7380 Training loss: 5.0481 1.0740 sec/batch\n",
      "Epoch 12/20  Iteration 4140/7380 Training loss: 5.0475 1.0822 sec/batch\n",
      "Epoch 12/20  Iteration 4141/7380 Training loss: 5.0473 1.0722 sec/batch\n",
      "Epoch 12/20  Iteration 4142/7380 Training loss: 5.0461 1.0749 sec/batch\n",
      "Epoch 12/20  Iteration 4143/7380 Training loss: 5.0456 1.0669 sec/batch\n",
      "Epoch 12/20  Iteration 4144/7380 Training loss: 5.0459 1.0795 sec/batch\n",
      "Epoch 12/20  Iteration 4145/7380 Training loss: 5.0454 1.0710 sec/batch\n",
      "Epoch 12/20  Iteration 4146/7380 Training loss: 5.0453 1.0733 sec/batch\n",
      "Epoch 12/20  Iteration 4147/7380 Training loss: 5.0451 1.0745 sec/batch\n",
      "Epoch 12/20  Iteration 4148/7380 Training loss: 5.0433 1.0765 sec/batch\n",
      "Epoch 12/20  Iteration 4149/7380 Training loss: 5.0432 1.0765 sec/batch\n",
      "Epoch 12/20  Iteration 4150/7380 Training loss: 5.0439 1.0827 sec/batch\n",
      "Validation loss: 5.08906 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 4151/7380 Training loss: 5.0470 1.0783 sec/batch\n",
      "Epoch 12/20  Iteration 4152/7380 Training loss: 5.0465 1.0766 sec/batch\n",
      "Epoch 12/20  Iteration 4153/7380 Training loss: 5.0461 1.0752 sec/batch\n",
      "Epoch 12/20  Iteration 4154/7380 Training loss: 5.0456 1.0746 sec/batch\n",
      "Epoch 12/20  Iteration 4155/7380 Training loss: 5.0456 1.1046 sec/batch\n",
      "Epoch 12/20  Iteration 4156/7380 Training loss: 5.0455 1.0836 sec/batch\n",
      "Epoch 12/20  Iteration 4157/7380 Training loss: 5.0468 1.0735 sec/batch\n",
      "Epoch 12/20  Iteration 4158/7380 Training loss: 5.0469 1.0823 sec/batch\n",
      "Epoch 12/20  Iteration 4159/7380 Training loss: 5.0466 1.0739 sec/batch\n",
      "Epoch 12/20  Iteration 4160/7380 Training loss: 5.0474 1.0725 sec/batch\n",
      "Epoch 12/20  Iteration 4161/7380 Training loss: 5.0477 1.0693 sec/batch\n",
      "Epoch 12/20  Iteration 4162/7380 Training loss: 5.0477 1.0795 sec/batch\n",
      "Epoch 12/20  Iteration 4163/7380 Training loss: 5.0472 1.0764 sec/batch\n",
      "Epoch 12/20  Iteration 4164/7380 Training loss: 5.0477 1.0738 sec/batch\n",
      "Epoch 12/20  Iteration 4165/7380 Training loss: 5.0477 1.0797 sec/batch\n",
      "Epoch 12/20  Iteration 4166/7380 Training loss: 5.0475 1.1137 sec/batch\n",
      "Epoch 12/20  Iteration 4167/7380 Training loss: 5.0473 1.0694 sec/batch\n",
      "Epoch 12/20  Iteration 4168/7380 Training loss: 5.0455 1.0830 sec/batch\n",
      "Epoch 12/20  Iteration 4169/7380 Training loss: 5.0453 1.0828 sec/batch\n",
      "Epoch 12/20  Iteration 4170/7380 Training loss: 5.0448 1.0750 sec/batch\n",
      "Epoch 12/20  Iteration 4171/7380 Training loss: 5.0450 1.0824 sec/batch\n",
      "Epoch 12/20  Iteration 4172/7380 Training loss: 5.0441 1.0818 sec/batch\n",
      "Epoch 12/20  Iteration 4173/7380 Training loss: 5.0442 1.0820 sec/batch\n",
      "Epoch 12/20  Iteration 4174/7380 Training loss: 5.0444 1.0770 sec/batch\n",
      "Epoch 12/20  Iteration 4175/7380 Training loss: 5.0441 1.0994 sec/batch\n",
      "Epoch 12/20  Iteration 4176/7380 Training loss: 5.0432 1.0804 sec/batch\n",
      "Epoch 12/20  Iteration 4177/7380 Training loss: 5.0429 1.0730 sec/batch\n",
      "Epoch 12/20  Iteration 4178/7380 Training loss: 5.0437 1.0743 sec/batch\n",
      "Epoch 12/20  Iteration 4179/7380 Training loss: 5.0432 1.0736 sec/batch\n",
      "Epoch 12/20  Iteration 4180/7380 Training loss: 5.0438 1.0707 sec/batch\n",
      "Epoch 12/20  Iteration 4181/7380 Training loss: 5.0448 1.0804 sec/batch\n",
      "Epoch 12/20  Iteration 4182/7380 Training loss: 5.0447 1.0776 sec/batch\n",
      "Epoch 12/20  Iteration 4183/7380 Training loss: 5.0448 1.0849 sec/batch\n",
      "Epoch 12/20  Iteration 4184/7380 Training loss: 5.0448 1.0799 sec/batch\n",
      "Epoch 12/20  Iteration 4185/7380 Training loss: 5.0439 1.0913 sec/batch\n",
      "Epoch 12/20  Iteration 4186/7380 Training loss: 5.0425 1.1530 sec/batch\n",
      "Epoch 12/20  Iteration 4187/7380 Training loss: 5.0425 1.0728 sec/batch\n",
      "Epoch 12/20  Iteration 4188/7380 Training loss: 5.0420 1.0766 sec/batch\n",
      "Epoch 12/20  Iteration 4189/7380 Training loss: 5.0420 1.0878 sec/batch\n",
      "Epoch 12/20  Iteration 4190/7380 Training loss: 5.0421 1.0728 sec/batch\n",
      "Epoch 12/20  Iteration 4191/7380 Training loss: 5.0423 1.0759 sec/batch\n",
      "Epoch 12/20  Iteration 4192/7380 Training loss: 5.0423 1.0767 sec/batch\n",
      "Epoch 12/20  Iteration 4193/7380 Training loss: 5.0411 1.0790 sec/batch\n",
      "Epoch 12/20  Iteration 4194/7380 Training loss: 5.0408 1.0783 sec/batch\n",
      "Epoch 12/20  Iteration 4195/7380 Training loss: 5.0409 1.0817 sec/batch\n",
      "Epoch 12/20  Iteration 4196/7380 Training loss: 5.0406 1.0805 sec/batch\n",
      "Epoch 12/20  Iteration 4197/7380 Training loss: 5.0406 1.0921 sec/batch\n",
      "Epoch 12/20  Iteration 4198/7380 Training loss: 5.0398 1.0741 sec/batch\n",
      "Epoch 12/20  Iteration 4199/7380 Training loss: 5.0391 1.0890 sec/batch\n",
      "Epoch 12/20  Iteration 4200/7380 Training loss: 5.0405 1.0817 sec/batch\n",
      "Validation loss: 5.08422 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 4201/7380 Training loss: 5.0412 1.0801 sec/batch\n",
      "Epoch 12/20  Iteration 4202/7380 Training loss: 5.0417 1.0798 sec/batch\n",
      "Epoch 12/20  Iteration 4203/7380 Training loss: 5.0415 1.0749 sec/batch\n",
      "Epoch 12/20  Iteration 4204/7380 Training loss: 5.0416 1.0781 sec/batch\n",
      "Epoch 12/20  Iteration 4205/7380 Training loss: 5.0412 1.0758 sec/batch\n",
      "Epoch 12/20  Iteration 4206/7380 Training loss: 5.0411 1.0809 sec/batch\n",
      "Epoch 12/20  Iteration 4207/7380 Training loss: 5.0410 1.0847 sec/batch\n",
      "Epoch 12/20  Iteration 4208/7380 Training loss: 5.0404 1.0756 sec/batch\n",
      "Epoch 12/20  Iteration 4209/7380 Training loss: 5.0408 1.0805 sec/batch\n",
      "Epoch 12/20  Iteration 4210/7380 Training loss: 5.0401 1.0799 sec/batch\n",
      "Epoch 12/20  Iteration 4211/7380 Training loss: 5.0405 1.0820 sec/batch\n",
      "Epoch 12/20  Iteration 4212/7380 Training loss: 5.0407 1.0949 sec/batch\n",
      "Epoch 12/20  Iteration 4213/7380 Training loss: 5.0413 1.0750 sec/batch\n",
      "Epoch 12/20  Iteration 4214/7380 Training loss: 5.0410 1.0742 sec/batch\n",
      "Epoch 12/20  Iteration 4215/7380 Training loss: 5.0405 1.0801 sec/batch\n",
      "Epoch 12/20  Iteration 4216/7380 Training loss: 5.0400 1.0775 sec/batch\n",
      "Epoch 12/20  Iteration 4217/7380 Training loss: 5.0407 1.0761 sec/batch\n",
      "Epoch 12/20  Iteration 4218/7380 Training loss: 5.0403 1.0807 sec/batch\n",
      "Epoch 12/20  Iteration 4219/7380 Training loss: 5.0403 1.0833 sec/batch\n",
      "Epoch 12/20  Iteration 4220/7380 Training loss: 5.0410 1.0833 sec/batch\n",
      "Epoch 12/20  Iteration 4221/7380 Training loss: 5.0411 1.0786 sec/batch\n",
      "Epoch 12/20  Iteration 4222/7380 Training loss: 5.0415 1.0738 sec/batch\n",
      "Epoch 12/20  Iteration 4223/7380 Training loss: 5.0419 1.0734 sec/batch\n",
      "Epoch 12/20  Iteration 4224/7380 Training loss: 5.0423 1.0759 sec/batch\n",
      "Epoch 12/20  Iteration 4225/7380 Training loss: 5.0426 1.0757 sec/batch\n",
      "Epoch 12/20  Iteration 4226/7380 Training loss: 5.0427 1.0744 sec/batch\n",
      "Epoch 12/20  Iteration 4227/7380 Training loss: 5.0431 1.0786 sec/batch\n",
      "Epoch 12/20  Iteration 4228/7380 Training loss: 5.0426 1.0763 sec/batch\n",
      "Epoch 12/20  Iteration 4229/7380 Training loss: 5.0422 1.0831 sec/batch\n",
      "Epoch 12/20  Iteration 4230/7380 Training loss: 5.0425 1.0854 sec/batch\n",
      "Epoch 12/20  Iteration 4231/7380 Training loss: 5.0431 1.0760 sec/batch\n",
      "Epoch 12/20  Iteration 4232/7380 Training loss: 5.0433 1.0848 sec/batch\n",
      "Epoch 12/20  Iteration 4233/7380 Training loss: 5.0430 1.0794 sec/batch\n",
      "Epoch 12/20  Iteration 4234/7380 Training loss: 5.0427 1.0792 sec/batch\n",
      "Epoch 12/20  Iteration 4235/7380 Training loss: 5.0423 1.0748 sec/batch\n",
      "Epoch 12/20  Iteration 4236/7380 Training loss: 5.0423 1.0742 sec/batch\n",
      "Epoch 12/20  Iteration 4237/7380 Training loss: 5.0425 1.0861 sec/batch\n",
      "Epoch 12/20  Iteration 4238/7380 Training loss: 5.0416 1.0870 sec/batch\n",
      "Epoch 12/20  Iteration 4239/7380 Training loss: 5.0411 1.1124 sec/batch\n",
      "Epoch 12/20  Iteration 4240/7380 Training loss: 5.0408 1.0892 sec/batch\n",
      "Epoch 12/20  Iteration 4241/7380 Training loss: 5.0410 1.0767 sec/batch\n",
      "Epoch 12/20  Iteration 4242/7380 Training loss: 5.0417 1.0757 sec/batch\n",
      "Epoch 12/20  Iteration 4243/7380 Training loss: 5.0416 1.0946 sec/batch\n",
      "Epoch 12/20  Iteration 4244/7380 Training loss: 5.0415 1.0794 sec/batch\n",
      "Epoch 12/20  Iteration 4245/7380 Training loss: 5.0415 1.0857 sec/batch\n",
      "Epoch 12/20  Iteration 4246/7380 Training loss: 5.0409 1.0747 sec/batch\n",
      "Epoch 12/20  Iteration 4247/7380 Training loss: 5.0407 1.0874 sec/batch\n",
      "Epoch 12/20  Iteration 4248/7380 Training loss: 5.0404 1.0785 sec/batch\n",
      "Epoch 12/20  Iteration 4249/7380 Training loss: 5.0405 1.0773 sec/batch\n",
      "Epoch 12/20  Iteration 4250/7380 Training loss: 5.0397 1.0843 sec/batch\n",
      "Validation loss: 5.08675 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 4251/7380 Training loss: 5.0406 1.1346 sec/batch\n",
      "Epoch 12/20  Iteration 4252/7380 Training loss: 5.0412 1.0923 sec/batch\n",
      "Epoch 12/20  Iteration 4253/7380 Training loss: 5.0406 1.0795 sec/batch\n",
      "Epoch 12/20  Iteration 4254/7380 Training loss: 5.0410 1.0783 sec/batch\n",
      "Epoch 12/20  Iteration 4255/7380 Training loss: 5.0412 1.0759 sec/batch\n",
      "Epoch 12/20  Iteration 4256/7380 Training loss: 5.0412 1.0941 sec/batch\n",
      "Epoch 12/20  Iteration 4257/7380 Training loss: 5.0416 1.0996 sec/batch\n",
      "Epoch 12/20  Iteration 4258/7380 Training loss: 5.0414 1.0903 sec/batch\n",
      "Epoch 12/20  Iteration 4259/7380 Training loss: 5.0416 1.0868 sec/batch\n",
      "Epoch 12/20  Iteration 4260/7380 Training loss: 5.0420 1.0752 sec/batch\n",
      "Epoch 12/20  Iteration 4261/7380 Training loss: 5.0424 1.0744 sec/batch\n",
      "Epoch 12/20  Iteration 4262/7380 Training loss: 5.0430 1.0858 sec/batch\n",
      "Epoch 12/20  Iteration 4263/7380 Training loss: 5.0430 1.1213 sec/batch\n",
      "Epoch 12/20  Iteration 4264/7380 Training loss: 5.0433 1.0759 sec/batch\n",
      "Epoch 12/20  Iteration 4265/7380 Training loss: 5.0436 1.0858 sec/batch\n",
      "Epoch 12/20  Iteration 4266/7380 Training loss: 5.0425 1.1084 sec/batch\n",
      "Epoch 12/20  Iteration 4267/7380 Training loss: 5.0421 1.0800 sec/batch\n",
      "Epoch 12/20  Iteration 4268/7380 Training loss: 5.0418 1.0726 sec/batch\n",
      "Epoch 12/20  Iteration 4269/7380 Training loss: 5.0412 1.0844 sec/batch\n",
      "Epoch 12/20  Iteration 4270/7380 Training loss: 5.0412 1.0768 sec/batch\n",
      "Epoch 12/20  Iteration 4271/7380 Training loss: 5.0407 1.0793 sec/batch\n",
      "Epoch 12/20  Iteration 4272/7380 Training loss: 5.0408 1.0745 sec/batch\n",
      "Epoch 12/20  Iteration 4273/7380 Training loss: 5.0407 1.0767 sec/batch\n",
      "Epoch 12/20  Iteration 4274/7380 Training loss: 5.0403 1.0777 sec/batch\n",
      "Epoch 12/20  Iteration 4275/7380 Training loss: 5.0397 1.0835 sec/batch\n",
      "Epoch 12/20  Iteration 4276/7380 Training loss: 5.0389 1.0805 sec/batch\n",
      "Epoch 12/20  Iteration 4277/7380 Training loss: 5.0391 1.0846 sec/batch\n",
      "Epoch 12/20  Iteration 4278/7380 Training loss: 5.0395 1.0826 sec/batch\n",
      "Epoch 12/20  Iteration 4279/7380 Training loss: 5.0396 1.0746 sec/batch\n",
      "Epoch 12/20  Iteration 4280/7380 Training loss: 5.0394 1.0886 sec/batch\n",
      "Epoch 12/20  Iteration 4281/7380 Training loss: 5.0399 1.0779 sec/batch\n",
      "Epoch 12/20  Iteration 4282/7380 Training loss: 5.0399 1.0904 sec/batch\n",
      "Epoch 12/20  Iteration 4283/7380 Training loss: 5.0394 1.0845 sec/batch\n",
      "Epoch 12/20  Iteration 4284/7380 Training loss: 5.0394 1.0846 sec/batch\n",
      "Epoch 12/20  Iteration 4285/7380 Training loss: 5.0393 1.0835 sec/batch\n",
      "Epoch 12/20  Iteration 4286/7380 Training loss: 5.0393 1.0811 sec/batch\n",
      "Epoch 12/20  Iteration 4287/7380 Training loss: 5.0388 1.0737 sec/batch\n",
      "Epoch 12/20  Iteration 4288/7380 Training loss: 5.0388 1.0737 sec/batch\n",
      "Epoch 12/20  Iteration 4289/7380 Training loss: 5.0393 1.0745 sec/batch\n",
      "Epoch 12/20  Iteration 4290/7380 Training loss: 5.0390 1.0766 sec/batch\n",
      "Epoch 12/20  Iteration 4291/7380 Training loss: 5.0386 1.0840 sec/batch\n",
      "Epoch 12/20  Iteration 4292/7380 Training loss: 5.0382 1.0740 sec/batch\n",
      "Epoch 12/20  Iteration 4293/7380 Training loss: 5.0379 1.0815 sec/batch\n",
      "Epoch 12/20  Iteration 4294/7380 Training loss: 5.0376 1.0814 sec/batch\n",
      "Epoch 12/20  Iteration 4295/7380 Training loss: 5.0372 1.0838 sec/batch\n",
      "Epoch 12/20  Iteration 4296/7380 Training loss: 5.0370 1.0796 sec/batch\n",
      "Epoch 12/20  Iteration 4297/7380 Training loss: 5.0364 1.0821 sec/batch\n",
      "Epoch 12/20  Iteration 4298/7380 Training loss: 5.0352 1.0786 sec/batch\n",
      "Epoch 12/20  Iteration 4299/7380 Training loss: 5.0346 1.0805 sec/batch\n",
      "Epoch 12/20  Iteration 4300/7380 Training loss: 5.0348 1.0828 sec/batch\n",
      "Validation loss: 5.07577 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 4301/7380 Training loss: 5.0358 1.0826 sec/batch\n",
      "Epoch 12/20  Iteration 4302/7380 Training loss: 5.0354 1.1004 sec/batch\n",
      "Epoch 12/20  Iteration 4303/7380 Training loss: 5.0354 1.0779 sec/batch\n",
      "Epoch 12/20  Iteration 4304/7380 Training loss: 5.0356 1.0801 sec/batch\n",
      "Epoch 12/20  Iteration 4305/7380 Training loss: 5.0357 1.0777 sec/batch\n",
      "Epoch 12/20  Iteration 4306/7380 Training loss: 5.0358 1.0797 sec/batch\n",
      "Epoch 12/20  Iteration 4307/7380 Training loss: 5.0357 1.0796 sec/batch\n",
      "Epoch 12/20  Iteration 4308/7380 Training loss: 5.0355 1.0793 sec/batch\n",
      "Epoch 12/20  Iteration 4309/7380 Training loss: 5.0357 1.0782 sec/batch\n",
      "Epoch 12/20  Iteration 4310/7380 Training loss: 5.0354 1.0780 sec/batch\n",
      "Epoch 12/20  Iteration 4311/7380 Training loss: 5.0353 1.0854 sec/batch\n",
      "Epoch 12/20  Iteration 4312/7380 Training loss: 5.0352 1.0877 sec/batch\n",
      "Epoch 12/20  Iteration 4313/7380 Training loss: 5.0347 1.0857 sec/batch\n",
      "Epoch 12/20  Iteration 4314/7380 Training loss: 5.0350 1.0866 sec/batch\n",
      "Epoch 12/20  Iteration 4315/7380 Training loss: 5.0351 1.1202 sec/batch\n",
      "Epoch 12/20  Iteration 4316/7380 Training loss: 5.0354 1.0763 sec/batch\n",
      "Epoch 12/20  Iteration 4317/7380 Training loss: 5.0353 1.0939 sec/batch\n",
      "Epoch 12/20  Iteration 4318/7380 Training loss: 5.0356 1.0838 sec/batch\n",
      "Epoch 12/20  Iteration 4319/7380 Training loss: 5.0350 1.0787 sec/batch\n",
      "Epoch 12/20  Iteration 4320/7380 Training loss: 5.0347 1.0750 sec/batch\n",
      "Epoch 12/20  Iteration 4321/7380 Training loss: 5.0349 1.0785 sec/batch\n",
      "Epoch 12/20  Iteration 4322/7380 Training loss: 5.0351 1.0861 sec/batch\n",
      "Epoch 12/20  Iteration 4323/7380 Training loss: 5.0348 1.0757 sec/batch\n",
      "Epoch 12/20  Iteration 4324/7380 Training loss: 5.0346 1.0942 sec/batch\n",
      "Epoch 12/20  Iteration 4325/7380 Training loss: 5.0344 1.1061 sec/batch\n",
      "Epoch 12/20  Iteration 4326/7380 Training loss: 5.0345 1.0935 sec/batch\n",
      "Epoch 12/20  Iteration 4327/7380 Training loss: 5.0345 1.0795 sec/batch\n",
      "Epoch 12/20  Iteration 4328/7380 Training loss: 5.0338 1.0745 sec/batch\n",
      "Epoch 12/20  Iteration 4329/7380 Training loss: 5.0341 1.0795 sec/batch\n",
      "Epoch 12/20  Iteration 4330/7380 Training loss: 5.0343 1.0790 sec/batch\n",
      "Epoch 12/20  Iteration 4331/7380 Training loss: 5.0338 1.0799 sec/batch\n",
      "Epoch 12/20  Iteration 4332/7380 Training loss: 5.0340 1.0827 sec/batch\n",
      "Epoch 12/20  Iteration 4333/7380 Training loss: 5.0342 1.0890 sec/batch\n",
      "Epoch 12/20  Iteration 4334/7380 Training loss: 5.0342 1.0907 sec/batch\n",
      "Epoch 12/20  Iteration 4335/7380 Training loss: 5.0339 1.0839 sec/batch\n",
      "Epoch 12/20  Iteration 4336/7380 Training loss: 5.0346 1.0704 sec/batch\n",
      "Epoch 12/20  Iteration 4337/7380 Training loss: 5.0345 1.0839 sec/batch\n",
      "Epoch 12/20  Iteration 4338/7380 Training loss: 5.0347 1.0769 sec/batch\n",
      "Epoch 12/20  Iteration 4339/7380 Training loss: 5.0346 1.0800 sec/batch\n",
      "Epoch 12/20  Iteration 4340/7380 Training loss: 5.0347 1.0782 sec/batch\n",
      "Epoch 12/20  Iteration 4341/7380 Training loss: 5.0348 1.0831 sec/batch\n",
      "Epoch 12/20  Iteration 4342/7380 Training loss: 5.0350 1.0792 sec/batch\n",
      "Epoch 12/20  Iteration 4343/7380 Training loss: 5.0349 1.0799 sec/batch\n",
      "Epoch 12/20  Iteration 4344/7380 Training loss: 5.0354 1.0811 sec/batch\n",
      "Epoch 12/20  Iteration 4345/7380 Training loss: 5.0352 1.0783 sec/batch\n",
      "Epoch 12/20  Iteration 4346/7380 Training loss: 5.0348 1.1047 sec/batch\n",
      "Epoch 12/20  Iteration 4347/7380 Training loss: 5.0347 1.0729 sec/batch\n",
      "Epoch 12/20  Iteration 4348/7380 Training loss: 5.0343 1.0827 sec/batch\n",
      "Epoch 12/20  Iteration 4349/7380 Training loss: 5.0340 1.0837 sec/batch\n",
      "Epoch 12/20  Iteration 4350/7380 Training loss: 5.0336 1.0815 sec/batch\n",
      "Validation loss: 5.08237 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 4351/7380 Training loss: 5.0339 1.0885 sec/batch\n",
      "Epoch 12/20  Iteration 4352/7380 Training loss: 5.0338 1.0892 sec/batch\n",
      "Epoch 12/20  Iteration 4353/7380 Training loss: 5.0338 1.0853 sec/batch\n",
      "Epoch 12/20  Iteration 4354/7380 Training loss: 5.0341 1.0797 sec/batch\n",
      "Epoch 12/20  Iteration 4355/7380 Training loss: 5.0339 1.0866 sec/batch\n",
      "Epoch 12/20  Iteration 4356/7380 Training loss: 5.0338 1.0840 sec/batch\n",
      "Epoch 12/20  Iteration 4357/7380 Training loss: 5.0335 1.0890 sec/batch\n",
      "Epoch 12/20  Iteration 4358/7380 Training loss: 5.0336 1.0851 sec/batch\n",
      "Epoch 12/20  Iteration 4359/7380 Training loss: 5.0342 1.0841 sec/batch\n",
      "Epoch 12/20  Iteration 4360/7380 Training loss: 5.0342 1.0792 sec/batch\n",
      "Epoch 12/20  Iteration 4361/7380 Training loss: 5.0343 1.0797 sec/batch\n",
      "Epoch 12/20  Iteration 4362/7380 Training loss: 5.0343 1.0768 sec/batch\n",
      "Epoch 12/20  Iteration 4363/7380 Training loss: 5.0348 1.0789 sec/batch\n",
      "Epoch 12/20  Iteration 4364/7380 Training loss: 5.0350 1.0807 sec/batch\n",
      "Epoch 12/20  Iteration 4365/7380 Training loss: 5.0350 1.0789 sec/batch\n",
      "Epoch 12/20  Iteration 4366/7380 Training loss: 5.0346 1.0872 sec/batch\n",
      "Epoch 12/20  Iteration 4367/7380 Training loss: 5.0344 1.0793 sec/batch\n",
      "Epoch 12/20  Iteration 4368/7380 Training loss: 5.0342 1.0911 sec/batch\n",
      "Epoch 12/20  Iteration 4369/7380 Training loss: 5.0338 1.0804 sec/batch\n",
      "Epoch 12/20  Iteration 4370/7380 Training loss: 5.0338 1.0826 sec/batch\n",
      "Epoch 12/20  Iteration 4371/7380 Training loss: 5.0336 1.0868 sec/batch\n",
      "Epoch 12/20  Iteration 4372/7380 Training loss: 5.0333 1.0918 sec/batch\n",
      "Epoch 12/20  Iteration 4373/7380 Training loss: 5.0332 1.0838 sec/batch\n",
      "Epoch 12/20  Iteration 4374/7380 Training loss: 5.0334 1.0837 sec/batch\n",
      "Epoch 12/20  Iteration 4375/7380 Training loss: 5.0336 1.0925 sec/batch\n",
      "Epoch 12/20  Iteration 4376/7380 Training loss: 5.0337 1.0793 sec/batch\n",
      "Epoch 12/20  Iteration 4377/7380 Training loss: 5.0339 1.0957 sec/batch\n",
      "Epoch 12/20  Iteration 4378/7380 Training loss: 5.0343 1.0801 sec/batch\n",
      "Epoch 12/20  Iteration 4379/7380 Training loss: 5.0346 1.0858 sec/batch\n",
      "Epoch 12/20  Iteration 4380/7380 Training loss: 5.0343 1.1527 sec/batch\n",
      "Epoch 12/20  Iteration 4381/7380 Training loss: 5.0340 1.0831 sec/batch\n",
      "Epoch 12/20  Iteration 4382/7380 Training loss: 5.0339 1.0786 sec/batch\n",
      "Epoch 12/20  Iteration 4383/7380 Training loss: 5.0338 1.0819 sec/batch\n",
      "Epoch 12/20  Iteration 4384/7380 Training loss: 5.0339 1.0893 sec/batch\n",
      "Epoch 12/20  Iteration 4385/7380 Training loss: 5.0345 1.0783 sec/batch\n",
      "Epoch 12/20  Iteration 4386/7380 Training loss: 5.0342 1.0738 sec/batch\n",
      "Epoch 12/20  Iteration 4387/7380 Training loss: 5.0339 1.0778 sec/batch\n",
      "Epoch 12/20  Iteration 4388/7380 Training loss: 5.0336 1.0888 sec/batch\n",
      "Epoch 12/20  Iteration 4389/7380 Training loss: 5.0335 1.0824 sec/batch\n",
      "Epoch 12/20  Iteration 4390/7380 Training loss: 5.0331 1.0809 sec/batch\n",
      "Epoch 12/20  Iteration 4391/7380 Training loss: 5.0332 1.0844 sec/batch\n",
      "Epoch 12/20  Iteration 4392/7380 Training loss: 5.0329 1.0844 sec/batch\n",
      "Epoch 12/20  Iteration 4393/7380 Training loss: 5.0330 1.0787 sec/batch\n",
      "Epoch 12/20  Iteration 4394/7380 Training loss: 5.0330 1.0771 sec/batch\n",
      "Epoch 12/20  Iteration 4395/7380 Training loss: 5.0331 1.1261 sec/batch\n",
      "Epoch 12/20  Iteration 4396/7380 Training loss: 5.0333 1.0760 sec/batch\n",
      "Epoch 12/20  Iteration 4397/7380 Training loss: 5.0332 1.0794 sec/batch\n",
      "Epoch 12/20  Iteration 4398/7380 Training loss: 5.0333 1.0794 sec/batch\n",
      "Epoch 12/20  Iteration 4399/7380 Training loss: 5.0329 1.0869 sec/batch\n",
      "Epoch 12/20  Iteration 4400/7380 Training loss: 5.0326 1.0973 sec/batch\n",
      "Validation loss: 5.08991 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 4401/7380 Training loss: 5.0327 1.0882 sec/batch\n",
      "Epoch 12/20  Iteration 4402/7380 Training loss: 5.0327 1.0840 sec/batch\n",
      "Epoch 12/20  Iteration 4403/7380 Training loss: 5.0327 1.1082 sec/batch\n",
      "Epoch 12/20  Iteration 4404/7380 Training loss: 5.0325 1.0936 sec/batch\n",
      "Epoch 12/20  Iteration 4405/7380 Training loss: 5.0324 1.0828 sec/batch\n",
      "Epoch 12/20  Iteration 4406/7380 Training loss: 5.0327 1.0789 sec/batch\n",
      "Epoch 12/20  Iteration 4407/7380 Training loss: 5.0327 1.1201 sec/batch\n",
      "Epoch 12/20  Iteration 4408/7380 Training loss: 5.0325 1.0808 sec/batch\n",
      "Epoch 12/20  Iteration 4409/7380 Training loss: 5.0325 1.0828 sec/batch\n",
      "Epoch 12/20  Iteration 4410/7380 Training loss: 5.0328 1.0787 sec/batch\n",
      "Epoch 12/20  Iteration 4411/7380 Training loss: 5.0325 1.1071 sec/batch\n",
      "Epoch 12/20  Iteration 4412/7380 Training loss: 5.0327 1.0774 sec/batch\n",
      "Epoch 12/20  Iteration 4413/7380 Training loss: 5.0324 1.0795 sec/batch\n",
      "Epoch 12/20  Iteration 4414/7380 Training loss: 5.0322 1.0791 sec/batch\n",
      "Epoch 12/20  Iteration 4415/7380 Training loss: 5.0320 1.0789 sec/batch\n",
      "Epoch 12/20  Iteration 4416/7380 Training loss: 5.0321 1.0907 sec/batch\n",
      "Epoch 12/20  Iteration 4417/7380 Training loss: 5.0321 1.0853 sec/batch\n",
      "Epoch 12/20  Iteration 4418/7380 Training loss: 5.0319 1.0810 sec/batch\n",
      "Epoch 12/20  Iteration 4419/7380 Training loss: 5.0317 1.0781 sec/batch\n",
      "Epoch 12/20  Iteration 4420/7380 Training loss: 5.0316 1.0840 sec/batch\n",
      "Epoch 12/20  Iteration 4421/7380 Training loss: 5.0317 1.0875 sec/batch\n",
      "Epoch 12/20  Iteration 4422/7380 Training loss: 5.0315 1.0820 sec/batch\n",
      "Epoch 12/20  Iteration 4423/7380 Training loss: 5.0312 1.0791 sec/batch\n",
      "Epoch 12/20  Iteration 4424/7380 Training loss: 5.0310 1.0906 sec/batch\n",
      "Epoch 12/20  Iteration 4425/7380 Training loss: 5.0308 1.1035 sec/batch\n",
      "Epoch 12/20  Iteration 4426/7380 Training loss: 5.0305 1.0782 sec/batch\n",
      "Epoch 12/20  Iteration 4427/7380 Training loss: 5.0306 1.0821 sec/batch\n",
      "Epoch 12/20  Iteration 4428/7380 Training loss: 5.0304 1.0893 sec/batch\n",
      "Epoch 13/20  Iteration 4429/7380 Training loss: 5.0745 1.0744 sec/batch\n",
      "Epoch 13/20  Iteration 4430/7380 Training loss: 5.0530 1.0883 sec/batch\n",
      "Epoch 13/20  Iteration 4431/7380 Training loss: 5.0223 1.0868 sec/batch\n",
      "Epoch 13/20  Iteration 4432/7380 Training loss: 5.0090 1.0819 sec/batch\n",
      "Epoch 13/20  Iteration 4433/7380 Training loss: 5.0002 1.0775 sec/batch\n",
      "Epoch 13/20  Iteration 4434/7380 Training loss: 5.0116 1.0874 sec/batch\n",
      "Epoch 13/20  Iteration 4435/7380 Training loss: 5.0337 1.0766 sec/batch\n",
      "Epoch 13/20  Iteration 4436/7380 Training loss: 5.0192 1.0804 sec/batch\n",
      "Epoch 13/20  Iteration 4437/7380 Training loss: 5.0184 1.0946 sec/batch\n",
      "Epoch 13/20  Iteration 4438/7380 Training loss: 5.0272 1.0816 sec/batch\n",
      "Epoch 13/20  Iteration 4439/7380 Training loss: 5.0245 1.0796 sec/batch\n",
      "Epoch 13/20  Iteration 4440/7380 Training loss: 5.0149 1.0852 sec/batch\n",
      "Epoch 13/20  Iteration 4441/7380 Training loss: 5.0112 1.0912 sec/batch\n",
      "Epoch 13/20  Iteration 4442/7380 Training loss: 5.0219 1.0885 sec/batch\n",
      "Epoch 13/20  Iteration 4443/7380 Training loss: 5.0246 1.0806 sec/batch\n",
      "Epoch 13/20  Iteration 4444/7380 Training loss: 5.0231 1.0834 sec/batch\n",
      "Epoch 13/20  Iteration 4445/7380 Training loss: 5.0186 1.0860 sec/batch\n",
      "Epoch 13/20  Iteration 4446/7380 Training loss: 5.0059 1.0869 sec/batch\n",
      "Epoch 13/20  Iteration 4447/7380 Training loss: 4.9998 1.0787 sec/batch\n",
      "Epoch 13/20  Iteration 4448/7380 Training loss: 4.9989 1.0789 sec/batch\n",
      "Epoch 13/20  Iteration 4449/7380 Training loss: 4.9942 1.1453 sec/batch\n",
      "Epoch 13/20  Iteration 4450/7380 Training loss: 5.0015 1.0838 sec/batch\n",
      "Validation loss: 5.06771 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 4451/7380 Training loss: 5.0052 1.1095 sec/batch\n",
      "Epoch 13/20  Iteration 4452/7380 Training loss: 5.0126 1.0789 sec/batch\n",
      "Epoch 13/20  Iteration 4453/7380 Training loss: 5.0159 1.0799 sec/batch\n",
      "Epoch 13/20  Iteration 4454/7380 Training loss: 5.0181 1.0793 sec/batch\n",
      "Epoch 13/20  Iteration 4455/7380 Training loss: 5.0119 1.0815 sec/batch\n",
      "Epoch 13/20  Iteration 4456/7380 Training loss: 5.0165 1.0805 sec/batch\n",
      "Epoch 13/20  Iteration 4457/7380 Training loss: 5.0149 1.0812 sec/batch\n",
      "Epoch 13/20  Iteration 4458/7380 Training loss: 5.0148 1.0769 sec/batch\n",
      "Epoch 13/20  Iteration 4459/7380 Training loss: 5.0135 1.0863 sec/batch\n",
      "Epoch 13/20  Iteration 4460/7380 Training loss: 5.0152 1.0802 sec/batch\n",
      "Epoch 13/20  Iteration 4461/7380 Training loss: 5.0152 1.0783 sec/batch\n",
      "Epoch 13/20  Iteration 4462/7380 Training loss: 5.0175 1.0862 sec/batch\n",
      "Epoch 13/20  Iteration 4463/7380 Training loss: 5.0214 1.0863 sec/batch\n",
      "Epoch 13/20  Iteration 4464/7380 Training loss: 5.0198 1.0969 sec/batch\n",
      "Epoch 13/20  Iteration 4465/7380 Training loss: 5.0208 1.0876 sec/batch\n",
      "Epoch 13/20  Iteration 4466/7380 Training loss: 5.0193 1.0927 sec/batch\n",
      "Epoch 13/20  Iteration 4467/7380 Training loss: 5.0207 1.0821 sec/batch\n",
      "Epoch 13/20  Iteration 4468/7380 Training loss: 5.0208 1.0821 sec/batch\n",
      "Epoch 13/20  Iteration 4469/7380 Training loss: 5.0204 1.0841 sec/batch\n",
      "Epoch 13/20  Iteration 4470/7380 Training loss: 5.0193 1.0820 sec/batch\n",
      "Epoch 13/20  Iteration 4471/7380 Training loss: 5.0192 1.0798 sec/batch\n",
      "Epoch 13/20  Iteration 4472/7380 Training loss: 5.0204 1.0803 sec/batch\n",
      "Epoch 13/20  Iteration 4473/7380 Training loss: 5.0214 1.0774 sec/batch\n",
      "Epoch 13/20  Iteration 4474/7380 Training loss: 5.0189 1.0865 sec/batch\n",
      "Epoch 13/20  Iteration 4475/7380 Training loss: 5.0194 1.0787 sec/batch\n",
      "Epoch 13/20  Iteration 4476/7380 Training loss: 5.0168 1.0857 sec/batch\n",
      "Epoch 13/20  Iteration 4477/7380 Training loss: 5.0179 1.0735 sec/batch\n",
      "Epoch 13/20  Iteration 4478/7380 Training loss: 5.0168 1.1037 sec/batch\n",
      "Epoch 13/20  Iteration 4479/7380 Training loss: 5.0158 1.0788 sec/batch\n",
      "Epoch 13/20  Iteration 4480/7380 Training loss: 5.0142 1.0930 sec/batch\n",
      "Epoch 13/20  Iteration 4481/7380 Training loss: 5.0128 1.1112 sec/batch\n",
      "Epoch 13/20  Iteration 4482/7380 Training loss: 5.0117 1.0843 sec/batch\n",
      "Epoch 13/20  Iteration 4483/7380 Training loss: 5.0109 1.0775 sec/batch\n",
      "Epoch 13/20  Iteration 4484/7380 Training loss: 5.0088 1.0845 sec/batch\n",
      "Epoch 13/20  Iteration 4485/7380 Training loss: 5.0079 1.0762 sec/batch\n",
      "Epoch 13/20  Iteration 4486/7380 Training loss: 5.0089 1.0774 sec/batch\n",
      "Epoch 13/20  Iteration 4487/7380 Training loss: 5.0086 1.0807 sec/batch\n",
      "Epoch 13/20  Iteration 4488/7380 Training loss: 5.0096 1.0785 sec/batch\n",
      "Epoch 13/20  Iteration 4489/7380 Training loss: 5.0080 1.0798 sec/batch\n",
      "Epoch 13/20  Iteration 4490/7380 Training loss: 5.0087 1.0826 sec/batch\n",
      "Epoch 13/20  Iteration 4491/7380 Training loss: 5.0074 1.0766 sec/batch\n",
      "Epoch 13/20  Iteration 4492/7380 Training loss: 5.0054 1.0861 sec/batch\n",
      "Epoch 13/20  Iteration 4493/7380 Training loss: 5.0058 1.0878 sec/batch\n",
      "Epoch 13/20  Iteration 4494/7380 Training loss: 5.0057 1.0824 sec/batch\n",
      "Epoch 13/20  Iteration 4495/7380 Training loss: 5.0061 1.0927 sec/batch\n",
      "Epoch 13/20  Iteration 4496/7380 Training loss: 5.0066 1.0815 sec/batch\n",
      "Epoch 13/20  Iteration 4497/7380 Training loss: 5.0081 1.1351 sec/batch\n",
      "Epoch 13/20  Iteration 4498/7380 Training loss: 5.0070 1.0860 sec/batch\n",
      "Epoch 13/20  Iteration 4499/7380 Training loss: 5.0059 1.1355 sec/batch\n",
      "Epoch 13/20  Iteration 4500/7380 Training loss: 5.0073 1.0784 sec/batch\n",
      "Validation loss: 5.07082 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 4501/7380 Training loss: 5.0073 1.0842 sec/batch\n",
      "Epoch 13/20  Iteration 4502/7380 Training loss: 5.0089 1.0937 sec/batch\n",
      "Epoch 13/20  Iteration 4503/7380 Training loss: 5.0099 1.0868 sec/batch\n",
      "Epoch 13/20  Iteration 4504/7380 Training loss: 5.0091 1.0811 sec/batch\n",
      "Epoch 13/20  Iteration 4505/7380 Training loss: 5.0090 1.0787 sec/batch\n",
      "Epoch 13/20  Iteration 4506/7380 Training loss: 5.0085 1.0761 sec/batch\n",
      "Epoch 13/20  Iteration 4507/7380 Training loss: 5.0095 1.0779 sec/batch\n",
      "Epoch 13/20  Iteration 4508/7380 Training loss: 5.0103 1.1037 sec/batch\n",
      "Epoch 13/20  Iteration 4509/7380 Training loss: 5.0095 1.0830 sec/batch\n",
      "Epoch 13/20  Iteration 4510/7380 Training loss: 5.0091 1.0809 sec/batch\n",
      "Epoch 13/20  Iteration 4511/7380 Training loss: 5.0078 1.0823 sec/batch\n",
      "Epoch 13/20  Iteration 4512/7380 Training loss: 5.0075 1.0751 sec/batch\n",
      "Epoch 13/20  Iteration 4513/7380 Training loss: 5.0079 1.0901 sec/batch\n",
      "Epoch 13/20  Iteration 4514/7380 Training loss: 5.0072 1.1186 sec/batch\n",
      "Epoch 13/20  Iteration 4515/7380 Training loss: 5.0071 1.1137 sec/batch\n",
      "Epoch 13/20  Iteration 4516/7380 Training loss: 5.0069 1.0865 sec/batch\n",
      "Epoch 13/20  Iteration 4517/7380 Training loss: 5.0051 1.0823 sec/batch\n",
      "Epoch 13/20  Iteration 4518/7380 Training loss: 5.0042 1.0817 sec/batch\n",
      "Epoch 13/20  Iteration 4519/7380 Training loss: 5.0051 1.0835 sec/batch\n",
      "Epoch 13/20  Iteration 4520/7380 Training loss: 5.0072 1.0794 sec/batch\n",
      "Epoch 13/20  Iteration 4521/7380 Training loss: 5.0069 1.0781 sec/batch\n",
      "Epoch 13/20  Iteration 4522/7380 Training loss: 5.0066 1.0823 sec/batch\n",
      "Epoch 13/20  Iteration 4523/7380 Training loss: 5.0057 1.0911 sec/batch\n",
      "Epoch 13/20  Iteration 4524/7380 Training loss: 5.0057 1.0824 sec/batch\n",
      "Epoch 13/20  Iteration 4525/7380 Training loss: 5.0055 1.0803 sec/batch\n",
      "Epoch 13/20  Iteration 4526/7380 Training loss: 5.0065 1.0799 sec/batch\n",
      "Epoch 13/20  Iteration 4527/7380 Training loss: 5.0065 1.0794 sec/batch\n",
      "Epoch 13/20  Iteration 4528/7380 Training loss: 5.0062 1.0768 sec/batch\n",
      "Epoch 13/20  Iteration 4529/7380 Training loss: 5.0075 1.0731 sec/batch\n",
      "Epoch 13/20  Iteration 4530/7380 Training loss: 5.0080 1.0769 sec/batch\n",
      "Epoch 13/20  Iteration 4531/7380 Training loss: 5.0082 1.0857 sec/batch\n",
      "Epoch 13/20  Iteration 4532/7380 Training loss: 5.0076 1.0988 sec/batch\n",
      "Epoch 13/20  Iteration 4533/7380 Training loss: 5.0076 1.0818 sec/batch\n",
      "Epoch 13/20  Iteration 4534/7380 Training loss: 5.0075 1.0960 sec/batch\n",
      "Epoch 13/20  Iteration 4535/7380 Training loss: 5.0075 1.0833 sec/batch\n",
      "Epoch 13/20  Iteration 4536/7380 Training loss: 5.0075 1.0841 sec/batch\n",
      "Epoch 13/20  Iteration 4537/7380 Training loss: 5.0058 1.1026 sec/batch\n",
      "Epoch 13/20  Iteration 4538/7380 Training loss: 5.0055 1.0918 sec/batch\n",
      "Epoch 13/20  Iteration 4539/7380 Training loss: 5.0052 1.0875 sec/batch\n",
      "Epoch 13/20  Iteration 4540/7380 Training loss: 5.0055 1.0930 sec/batch\n",
      "Epoch 13/20  Iteration 4541/7380 Training loss: 5.0047 1.0891 sec/batch\n",
      "Epoch 13/20  Iteration 4542/7380 Training loss: 5.0052 1.0795 sec/batch\n",
      "Epoch 13/20  Iteration 4543/7380 Training loss: 5.0052 1.0861 sec/batch\n",
      "Epoch 13/20  Iteration 4544/7380 Training loss: 5.0050 1.0943 sec/batch\n",
      "Epoch 13/20  Iteration 4545/7380 Training loss: 5.0042 1.0813 sec/batch\n",
      "Epoch 13/20  Iteration 4546/7380 Training loss: 5.0038 1.0774 sec/batch\n",
      "Epoch 13/20  Iteration 4547/7380 Training loss: 5.0041 1.0820 sec/batch\n",
      "Epoch 13/20  Iteration 4548/7380 Training loss: 5.0035 1.0842 sec/batch\n",
      "Epoch 13/20  Iteration 4549/7380 Training loss: 5.0037 1.0852 sec/batch\n",
      "Epoch 13/20  Iteration 4550/7380 Training loss: 5.0050 1.0793 sec/batch\n",
      "Validation loss: 5.06157 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 4551/7380 Training loss: 5.0058 1.0870 sec/batch\n",
      "Epoch 13/20  Iteration 4552/7380 Training loss: 5.0058 1.0852 sec/batch\n",
      "Epoch 13/20  Iteration 4553/7380 Training loss: 5.0057 1.0808 sec/batch\n",
      "Epoch 13/20  Iteration 4554/7380 Training loss: 5.0050 1.0894 sec/batch\n",
      "Epoch 13/20  Iteration 4555/7380 Training loss: 5.0035 1.0843 sec/batch\n",
      "Epoch 13/20  Iteration 4556/7380 Training loss: 5.0036 1.0912 sec/batch\n",
      "Epoch 13/20  Iteration 4557/7380 Training loss: 5.0031 1.0919 sec/batch\n",
      "Epoch 13/20  Iteration 4558/7380 Training loss: 5.0031 1.0862 sec/batch\n",
      "Epoch 13/20  Iteration 4559/7380 Training loss: 5.0033 1.0798 sec/batch\n",
      "Epoch 13/20  Iteration 4560/7380 Training loss: 5.0033 1.0824 sec/batch\n",
      "Epoch 13/20  Iteration 4561/7380 Training loss: 5.0034 1.1028 sec/batch\n",
      "Epoch 13/20  Iteration 4562/7380 Training loss: 5.0027 1.0969 sec/batch\n",
      "Epoch 13/20  Iteration 4563/7380 Training loss: 5.0024 1.0923 sec/batch\n",
      "Epoch 13/20  Iteration 4564/7380 Training loss: 5.0025 1.0958 sec/batch\n",
      "Epoch 13/20  Iteration 4565/7380 Training loss: 5.0021 1.0970 sec/batch\n",
      "Epoch 13/20  Iteration 4566/7380 Training loss: 5.0021 1.0848 sec/batch\n",
      "Epoch 13/20  Iteration 4567/7380 Training loss: 5.0015 1.0828 sec/batch\n",
      "Epoch 13/20  Iteration 4568/7380 Training loss: 5.0007 1.0837 sec/batch\n",
      "Epoch 13/20  Iteration 4569/7380 Training loss: 5.0019 1.0870 sec/batch\n",
      "Epoch 13/20  Iteration 4570/7380 Training loss: 5.0020 1.0959 sec/batch\n",
      "Epoch 13/20  Iteration 4571/7380 Training loss: 5.0024 1.0891 sec/batch\n",
      "Epoch 13/20  Iteration 4572/7380 Training loss: 5.0022 1.0822 sec/batch\n",
      "Epoch 13/20  Iteration 4573/7380 Training loss: 5.0024 1.0856 sec/batch\n",
      "Epoch 13/20  Iteration 4574/7380 Training loss: 5.0024 1.0819 sec/batch\n",
      "Epoch 13/20  Iteration 4575/7380 Training loss: 5.0020 1.0856 sec/batch\n",
      "Epoch 13/20  Iteration 4576/7380 Training loss: 5.0020 1.0802 sec/batch\n",
      "Epoch 13/20  Iteration 4577/7380 Training loss: 5.0014 1.0969 sec/batch\n",
      "Epoch 13/20  Iteration 4578/7380 Training loss: 5.0017 1.0907 sec/batch\n",
      "Epoch 13/20  Iteration 4579/7380 Training loss: 5.0011 1.0876 sec/batch\n",
      "Epoch 13/20  Iteration 4580/7380 Training loss: 5.0015 1.1029 sec/batch\n",
      "Epoch 13/20  Iteration 4581/7380 Training loss: 5.0017 1.1118 sec/batch\n",
      "Epoch 13/20  Iteration 4582/7380 Training loss: 5.0021 1.0831 sec/batch\n",
      "Epoch 13/20  Iteration 4583/7380 Training loss: 5.0017 1.0852 sec/batch\n",
      "Epoch 13/20  Iteration 4584/7380 Training loss: 5.0012 1.0836 sec/batch\n",
      "Epoch 13/20  Iteration 4585/7380 Training loss: 5.0006 1.0904 sec/batch\n",
      "Epoch 13/20  Iteration 4586/7380 Training loss: 5.0015 1.0811 sec/batch\n",
      "Epoch 13/20  Iteration 4587/7380 Training loss: 5.0012 1.0825 sec/batch\n",
      "Epoch 13/20  Iteration 4588/7380 Training loss: 5.0010 1.1104 sec/batch\n",
      "Epoch 13/20  Iteration 4589/7380 Training loss: 5.0016 1.0845 sec/batch\n",
      "Epoch 13/20  Iteration 4590/7380 Training loss: 5.0019 1.0867 sec/batch\n",
      "Epoch 13/20  Iteration 4591/7380 Training loss: 5.0023 1.0806 sec/batch\n",
      "Epoch 13/20  Iteration 4592/7380 Training loss: 5.0025 1.0779 sec/batch\n",
      "Epoch 13/20  Iteration 4593/7380 Training loss: 5.0028 1.0796 sec/batch\n",
      "Epoch 13/20  Iteration 4594/7380 Training loss: 5.0034 1.0777 sec/batch\n",
      "Epoch 13/20  Iteration 4595/7380 Training loss: 5.0034 1.0781 sec/batch\n",
      "Epoch 13/20  Iteration 4596/7380 Training loss: 5.0037 1.0796 sec/batch\n",
      "Epoch 13/20  Iteration 4597/7380 Training loss: 5.0030 1.0898 sec/batch\n",
      "Epoch 13/20  Iteration 4598/7380 Training loss: 5.0025 1.0951 sec/batch\n",
      "Epoch 13/20  Iteration 4599/7380 Training loss: 5.0029 1.1015 sec/batch\n",
      "Epoch 13/20  Iteration 4600/7380 Training loss: 5.0033 1.0795 sec/batch\n",
      "Validation loss: 5.06475 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 4601/7380 Training loss: 5.0040 1.0951 sec/batch\n",
      "Epoch 13/20  Iteration 4602/7380 Training loss: 5.0035 1.0854 sec/batch\n",
      "Epoch 13/20  Iteration 4603/7380 Training loss: 5.0033 1.0831 sec/batch\n",
      "Epoch 13/20  Iteration 4604/7380 Training loss: 5.0029 1.0814 sec/batch\n",
      "Epoch 13/20  Iteration 4605/7380 Training loss: 5.0029 1.0763 sec/batch\n",
      "Epoch 13/20  Iteration 4606/7380 Training loss: 5.0033 1.0756 sec/batch\n",
      "Epoch 13/20  Iteration 4607/7380 Training loss: 5.0027 1.0817 sec/batch\n",
      "Epoch 13/20  Iteration 4608/7380 Training loss: 5.0022 1.0939 sec/batch\n",
      "Epoch 13/20  Iteration 4609/7380 Training loss: 5.0020 1.0912 sec/batch\n",
      "Epoch 13/20  Iteration 4610/7380 Training loss: 5.0019 1.0829 sec/batch\n",
      "Epoch 13/20  Iteration 4611/7380 Training loss: 5.0026 1.0855 sec/batch\n",
      "Epoch 13/20  Iteration 4612/7380 Training loss: 5.0024 1.0985 sec/batch\n",
      "Epoch 13/20  Iteration 4613/7380 Training loss: 5.0023 1.0873 sec/batch\n",
      "Epoch 13/20  Iteration 4614/7380 Training loss: 5.0025 1.0923 sec/batch\n",
      "Epoch 13/20  Iteration 4615/7380 Training loss: 5.0021 1.0845 sec/batch\n",
      "Epoch 13/20  Iteration 4616/7380 Training loss: 5.0018 1.0769 sec/batch\n",
      "Epoch 13/20  Iteration 4617/7380 Training loss: 5.0016 1.0839 sec/batch\n",
      "Epoch 13/20  Iteration 4618/7380 Training loss: 5.0016 1.0903 sec/batch\n",
      "Epoch 13/20  Iteration 4619/7380 Training loss: 5.0007 1.0968 sec/batch\n",
      "Epoch 13/20  Iteration 4620/7380 Training loss: 5.0011 1.0833 sec/batch\n",
      "Epoch 13/20  Iteration 4621/7380 Training loss: 5.0016 1.0854 sec/batch\n",
      "Epoch 13/20  Iteration 4622/7380 Training loss: 5.0009 1.0877 sec/batch\n",
      "Epoch 13/20  Iteration 4623/7380 Training loss: 5.0013 1.0809 sec/batch\n",
      "Epoch 13/20  Iteration 4624/7380 Training loss: 5.0015 1.0856 sec/batch\n",
      "Epoch 13/20  Iteration 4625/7380 Training loss: 5.0016 1.0926 sec/batch\n",
      "Epoch 13/20  Iteration 4626/7380 Training loss: 5.0018 1.0941 sec/batch\n",
      "Epoch 13/20  Iteration 4627/7380 Training loss: 5.0015 1.0858 sec/batch\n",
      "Epoch 13/20  Iteration 4628/7380 Training loss: 5.0017 1.0823 sec/batch\n",
      "Epoch 13/20  Iteration 4629/7380 Training loss: 5.0020 1.0865 sec/batch\n",
      "Epoch 13/20  Iteration 4630/7380 Training loss: 5.0022 1.0869 sec/batch\n",
      "Epoch 13/20  Iteration 4631/7380 Training loss: 5.0027 1.1116 sec/batch\n",
      "Epoch 13/20  Iteration 4632/7380 Training loss: 5.0028 1.0834 sec/batch\n",
      "Epoch 13/20  Iteration 4633/7380 Training loss: 5.0030 1.0873 sec/batch\n",
      "Epoch 13/20  Iteration 4634/7380 Training loss: 5.0033 1.0914 sec/batch\n",
      "Epoch 13/20  Iteration 4635/7380 Training loss: 5.0024 1.0962 sec/batch\n",
      "Epoch 13/20  Iteration 4636/7380 Training loss: 5.0022 1.0813 sec/batch\n",
      "Epoch 13/20  Iteration 4637/7380 Training loss: 5.0018 1.0949 sec/batch\n",
      "Epoch 13/20  Iteration 4638/7380 Training loss: 5.0012 1.0817 sec/batch\n",
      "Epoch 13/20  Iteration 4639/7380 Training loss: 5.0011 1.0861 sec/batch\n",
      "Epoch 13/20  Iteration 4640/7380 Training loss: 5.0008 1.0947 sec/batch\n",
      "Epoch 13/20  Iteration 4641/7380 Training loss: 5.0009 1.0920 sec/batch\n",
      "Epoch 13/20  Iteration 4642/7380 Training loss: 5.0008 1.0874 sec/batch\n",
      "Epoch 13/20  Iteration 4643/7380 Training loss: 5.0004 1.0987 sec/batch\n",
      "Epoch 13/20  Iteration 4644/7380 Training loss: 4.9998 1.0904 sec/batch\n",
      "Epoch 13/20  Iteration 4645/7380 Training loss: 4.9994 1.0907 sec/batch\n",
      "Epoch 13/20  Iteration 4646/7380 Training loss: 4.9995 1.0877 sec/batch\n",
      "Epoch 13/20  Iteration 4647/7380 Training loss: 4.9998 1.0768 sec/batch\n",
      "Epoch 13/20  Iteration 4648/7380 Training loss: 5.0000 1.1078 sec/batch\n",
      "Epoch 13/20  Iteration 4649/7380 Training loss: 4.9998 1.0919 sec/batch\n",
      "Epoch 13/20  Iteration 4650/7380 Training loss: 5.0003 1.0864 sec/batch\n",
      "Validation loss: 5.07786 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 4651/7380 Training loss: 5.0006 1.0869 sec/batch\n",
      "Epoch 13/20  Iteration 4652/7380 Training loss: 5.0001 1.0906 sec/batch\n",
      "Epoch 13/20  Iteration 4653/7380 Training loss: 5.0002 1.0885 sec/batch\n",
      "Epoch 13/20  Iteration 4654/7380 Training loss: 5.0000 1.0870 sec/batch\n",
      "Epoch 13/20  Iteration 4655/7380 Training loss: 4.9999 1.0827 sec/batch\n",
      "Epoch 13/20  Iteration 4656/7380 Training loss: 4.9995 1.0848 sec/batch\n",
      "Epoch 13/20  Iteration 4657/7380 Training loss: 4.9994 1.0834 sec/batch\n",
      "Epoch 13/20  Iteration 4658/7380 Training loss: 4.9999 1.0923 sec/batch\n",
      "Epoch 13/20  Iteration 4659/7380 Training loss: 4.9994 1.0885 sec/batch\n",
      "Epoch 13/20  Iteration 4660/7380 Training loss: 4.9990 1.0849 sec/batch\n",
      "Epoch 13/20  Iteration 4661/7380 Training loss: 4.9986 1.1320 sec/batch\n",
      "Epoch 13/20  Iteration 4662/7380 Training loss: 4.9983 1.0995 sec/batch\n",
      "Epoch 13/20  Iteration 4663/7380 Training loss: 4.9980 1.1097 sec/batch\n",
      "Epoch 13/20  Iteration 4664/7380 Training loss: 4.9977 1.0877 sec/batch\n",
      "Epoch 13/20  Iteration 4665/7380 Training loss: 4.9974 1.0876 sec/batch\n",
      "Epoch 13/20  Iteration 4666/7380 Training loss: 4.9969 1.0863 sec/batch\n",
      "Epoch 13/20  Iteration 4667/7380 Training loss: 4.9958 1.0931 sec/batch\n",
      "Epoch 13/20  Iteration 4668/7380 Training loss: 4.9951 1.0851 sec/batch\n",
      "Epoch 13/20  Iteration 4669/7380 Training loss: 4.9953 1.0876 sec/batch\n",
      "Epoch 13/20  Iteration 4670/7380 Training loss: 4.9960 1.0816 sec/batch\n",
      "Epoch 13/20  Iteration 4671/7380 Training loss: 4.9955 1.0930 sec/batch\n",
      "Epoch 13/20  Iteration 4672/7380 Training loss: 4.9955 1.1099 sec/batch\n",
      "Epoch 13/20  Iteration 4673/7380 Training loss: 4.9955 1.1059 sec/batch\n",
      "Epoch 13/20  Iteration 4674/7380 Training loss: 4.9958 1.0857 sec/batch\n",
      "Epoch 13/20  Iteration 4675/7380 Training loss: 4.9958 1.0817 sec/batch\n",
      "Epoch 13/20  Iteration 4676/7380 Training loss: 4.9958 1.0846 sec/batch\n",
      "Epoch 13/20  Iteration 4677/7380 Training loss: 4.9956 1.0836 sec/batch\n",
      "Epoch 13/20  Iteration 4678/7380 Training loss: 4.9958 1.0842 sec/batch\n",
      "Epoch 13/20  Iteration 4679/7380 Training loss: 4.9954 1.0915 sec/batch\n",
      "Epoch 13/20  Iteration 4680/7380 Training loss: 4.9953 1.1053 sec/batch\n",
      "Epoch 13/20  Iteration 4681/7380 Training loss: 4.9952 1.0841 sec/batch\n",
      "Epoch 13/20  Iteration 4682/7380 Training loss: 4.9947 1.0801 sec/batch\n",
      "Epoch 13/20  Iteration 4683/7380 Training loss: 4.9949 1.0840 sec/batch\n",
      "Epoch 13/20  Iteration 4684/7380 Training loss: 4.9950 1.0897 sec/batch\n",
      "Epoch 13/20  Iteration 4685/7380 Training loss: 4.9955 1.0906 sec/batch\n",
      "Epoch 13/20  Iteration 4686/7380 Training loss: 4.9953 1.0815 sec/batch\n",
      "Epoch 13/20  Iteration 4687/7380 Training loss: 4.9956 1.0858 sec/batch\n",
      "Epoch 13/20  Iteration 4688/7380 Training loss: 4.9951 1.0837 sec/batch\n",
      "Epoch 13/20  Iteration 4689/7380 Training loss: 4.9948 1.0863 sec/batch\n",
      "Epoch 13/20  Iteration 4690/7380 Training loss: 4.9948 1.0788 sec/batch\n",
      "Epoch 13/20  Iteration 4691/7380 Training loss: 4.9951 1.0900 sec/batch\n",
      "Epoch 13/20  Iteration 4692/7380 Training loss: 4.9947 1.0844 sec/batch\n",
      "Epoch 13/20  Iteration 4693/7380 Training loss: 4.9944 1.0836 sec/batch\n",
      "Epoch 13/20  Iteration 4694/7380 Training loss: 4.9942 1.0885 sec/batch\n",
      "Epoch 13/20  Iteration 4695/7380 Training loss: 4.9943 1.0888 sec/batch\n",
      "Epoch 13/20  Iteration 4696/7380 Training loss: 4.9943 1.0832 sec/batch\n",
      "Epoch 13/20  Iteration 4697/7380 Training loss: 4.9937 1.0869 sec/batch\n",
      "Epoch 13/20  Iteration 4698/7380 Training loss: 4.9940 1.0942 sec/batch\n",
      "Epoch 13/20  Iteration 4699/7380 Training loss: 4.9942 1.0840 sec/batch\n",
      "Epoch 13/20  Iteration 4700/7380 Training loss: 4.9937 1.1119 sec/batch\n",
      "Validation loss: 5.0577 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 4701/7380 Training loss: 4.9943 1.0915 sec/batch\n",
      "Epoch 13/20  Iteration 4702/7380 Training loss: 4.9946 1.0876 sec/batch\n",
      "Epoch 13/20  Iteration 4703/7380 Training loss: 4.9947 1.0876 sec/batch\n",
      "Epoch 13/20  Iteration 4704/7380 Training loss: 4.9943 1.0908 sec/batch\n",
      "Epoch 13/20  Iteration 4705/7380 Training loss: 4.9950 1.0853 sec/batch\n",
      "Epoch 13/20  Iteration 4706/7380 Training loss: 4.9949 1.0837 sec/batch\n",
      "Epoch 13/20  Iteration 4707/7380 Training loss: 4.9951 1.0865 sec/batch\n",
      "Epoch 13/20  Iteration 4708/7380 Training loss: 4.9949 1.0940 sec/batch\n",
      "Epoch 13/20  Iteration 4709/7380 Training loss: 4.9948 1.0918 sec/batch\n",
      "Epoch 13/20  Iteration 4710/7380 Training loss: 4.9948 1.0942 sec/batch\n",
      "Epoch 13/20  Iteration 4711/7380 Training loss: 4.9949 1.0924 sec/batch\n",
      "Epoch 13/20  Iteration 4712/7380 Training loss: 4.9949 1.0866 sec/batch\n",
      "Epoch 13/20  Iteration 4713/7380 Training loss: 4.9954 1.0897 sec/batch\n",
      "Epoch 13/20  Iteration 4714/7380 Training loss: 4.9951 1.0949 sec/batch\n",
      "Epoch 13/20  Iteration 4715/7380 Training loss: 4.9948 1.0949 sec/batch\n",
      "Epoch 13/20  Iteration 4716/7380 Training loss: 4.9946 1.0874 sec/batch\n",
      "Epoch 13/20  Iteration 4717/7380 Training loss: 4.9944 1.0846 sec/batch\n",
      "Epoch 13/20  Iteration 4718/7380 Training loss: 4.9939 1.0861 sec/batch\n",
      "Epoch 13/20  Iteration 4719/7380 Training loss: 4.9936 1.0880 sec/batch\n",
      "Epoch 13/20  Iteration 4720/7380 Training loss: 4.9934 1.0915 sec/batch\n",
      "Epoch 13/20  Iteration 4721/7380 Training loss: 4.9934 1.0975 sec/batch\n",
      "Epoch 13/20  Iteration 4722/7380 Training loss: 4.9934 1.0906 sec/batch\n",
      "Epoch 13/20  Iteration 4723/7380 Training loss: 4.9935 1.0848 sec/batch\n",
      "Epoch 13/20  Iteration 4724/7380 Training loss: 4.9933 1.0949 sec/batch\n",
      "Epoch 13/20  Iteration 4725/7380 Training loss: 4.9932 1.0874 sec/batch\n",
      "Epoch 13/20  Iteration 4726/7380 Training loss: 4.9930 1.1129 sec/batch\n",
      "Epoch 13/20  Iteration 4727/7380 Training loss: 4.9933 1.0865 sec/batch\n",
      "Epoch 13/20  Iteration 4728/7380 Training loss: 4.9938 1.0951 sec/batch\n",
      "Epoch 13/20  Iteration 4729/7380 Training loss: 4.9937 1.0947 sec/batch\n",
      "Epoch 13/20  Iteration 4730/7380 Training loss: 4.9940 1.0980 sec/batch\n",
      "Epoch 13/20  Iteration 4731/7380 Training loss: 4.9940 1.1307 sec/batch\n",
      "Epoch 13/20  Iteration 4732/7380 Training loss: 4.9945 1.0914 sec/batch\n",
      "Epoch 13/20  Iteration 4733/7380 Training loss: 4.9946 1.0910 sec/batch\n",
      "Epoch 13/20  Iteration 4734/7380 Training loss: 4.9945 1.0867 sec/batch\n",
      "Epoch 13/20  Iteration 4735/7380 Training loss: 4.9942 1.0994 sec/batch\n",
      "Epoch 13/20  Iteration 4736/7380 Training loss: 4.9938 1.0845 sec/batch\n",
      "Epoch 13/20  Iteration 4737/7380 Training loss: 4.9936 1.0888 sec/batch\n",
      "Epoch 13/20  Iteration 4738/7380 Training loss: 4.9932 1.0889 sec/batch\n",
      "Epoch 13/20  Iteration 4739/7380 Training loss: 4.9932 1.0850 sec/batch\n",
      "Epoch 13/20  Iteration 4740/7380 Training loss: 4.9930 1.0947 sec/batch\n",
      "Epoch 13/20  Iteration 4741/7380 Training loss: 4.9928 1.0833 sec/batch\n",
      "Epoch 13/20  Iteration 4742/7380 Training loss: 4.9927 1.0831 sec/batch\n",
      "Epoch 13/20  Iteration 4743/7380 Training loss: 4.9927 1.1486 sec/batch\n",
      "Epoch 13/20  Iteration 4744/7380 Training loss: 4.9929 1.0905 sec/batch\n",
      "Epoch 13/20  Iteration 4745/7380 Training loss: 4.9931 1.0845 sec/batch\n",
      "Epoch 13/20  Iteration 4746/7380 Training loss: 4.9933 1.0969 sec/batch\n",
      "Epoch 13/20  Iteration 4747/7380 Training loss: 4.9939 1.0927 sec/batch\n",
      "Epoch 13/20  Iteration 4748/7380 Training loss: 4.9944 1.0863 sec/batch\n",
      "Epoch 13/20  Iteration 4749/7380 Training loss: 4.9941 1.0878 sec/batch\n",
      "Epoch 13/20  Iteration 4750/7380 Training loss: 4.9938 1.0863 sec/batch\n",
      "Validation loss: 5.05688 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 4751/7380 Training loss: 4.9942 1.0940 sec/batch\n",
      "Epoch 13/20  Iteration 4752/7380 Training loss: 4.9940 1.0949 sec/batch\n",
      "Epoch 13/20  Iteration 4753/7380 Training loss: 4.9941 1.0810 sec/batch\n",
      "Epoch 13/20  Iteration 4754/7380 Training loss: 4.9946 1.0871 sec/batch\n",
      "Epoch 13/20  Iteration 4755/7380 Training loss: 4.9943 1.0815 sec/batch\n",
      "Epoch 13/20  Iteration 4756/7380 Training loss: 4.9940 1.0843 sec/batch\n",
      "Epoch 13/20  Iteration 4757/7380 Training loss: 4.9936 1.1519 sec/batch\n",
      "Epoch 13/20  Iteration 4758/7380 Training loss: 4.9934 1.0940 sec/batch\n",
      "Epoch 13/20  Iteration 4759/7380 Training loss: 4.9931 1.1032 sec/batch\n",
      "Epoch 13/20  Iteration 4760/7380 Training loss: 4.9934 1.1075 sec/batch\n",
      "Epoch 13/20  Iteration 4761/7380 Training loss: 4.9931 1.0889 sec/batch\n",
      "Epoch 13/20  Iteration 4762/7380 Training loss: 4.9933 1.0884 sec/batch\n",
      "Epoch 13/20  Iteration 4763/7380 Training loss: 4.9933 1.0934 sec/batch\n",
      "Epoch 13/20  Iteration 4764/7380 Training loss: 4.9934 1.0837 sec/batch\n",
      "Epoch 13/20  Iteration 4765/7380 Training loss: 4.9936 1.0839 sec/batch\n",
      "Epoch 13/20  Iteration 4766/7380 Training loss: 4.9935 1.0865 sec/batch\n",
      "Epoch 13/20  Iteration 4767/7380 Training loss: 4.9937 1.0843 sec/batch\n",
      "Epoch 13/20  Iteration 4768/7380 Training loss: 4.9933 1.0845 sec/batch\n",
      "Epoch 13/20  Iteration 4769/7380 Training loss: 4.9931 1.0886 sec/batch\n",
      "Epoch 13/20  Iteration 4770/7380 Training loss: 4.9929 1.0858 sec/batch\n",
      "Epoch 13/20  Iteration 4771/7380 Training loss: 4.9930 1.0976 sec/batch\n",
      "Epoch 13/20  Iteration 4772/7380 Training loss: 4.9929 1.0931 sec/batch\n",
      "Epoch 13/20  Iteration 4773/7380 Training loss: 4.9928 1.1035 sec/batch\n",
      "Epoch 13/20  Iteration 4774/7380 Training loss: 4.9926 1.0873 sec/batch\n",
      "Epoch 13/20  Iteration 4775/7380 Training loss: 4.9929 1.0848 sec/batch\n",
      "Epoch 13/20  Iteration 4776/7380 Training loss: 4.9930 1.0830 sec/batch\n",
      "Epoch 13/20  Iteration 4777/7380 Training loss: 4.9927 1.0994 sec/batch\n",
      "Epoch 13/20  Iteration 4778/7380 Training loss: 4.9927 1.0922 sec/batch\n",
      "Epoch 13/20  Iteration 4779/7380 Training loss: 4.9931 1.1044 sec/batch\n",
      "Epoch 13/20  Iteration 4780/7380 Training loss: 4.9928 1.0891 sec/batch\n",
      "Epoch 13/20  Iteration 4781/7380 Training loss: 4.9930 1.0913 sec/batch\n",
      "Epoch 13/20  Iteration 4782/7380 Training loss: 4.9927 1.0857 sec/batch\n",
      "Epoch 13/20  Iteration 4783/7380 Training loss: 4.9924 1.0850 sec/batch\n",
      "Epoch 13/20  Iteration 4784/7380 Training loss: 4.9923 1.0905 sec/batch\n",
      "Epoch 13/20  Iteration 4785/7380 Training loss: 4.9925 1.0864 sec/batch\n",
      "Epoch 13/20  Iteration 4786/7380 Training loss: 4.9924 1.0883 sec/batch\n",
      "Epoch 13/20  Iteration 4787/7380 Training loss: 4.9921 1.0818 sec/batch\n",
      "Epoch 13/20  Iteration 4788/7380 Training loss: 4.9918 1.1302 sec/batch\n",
      "Epoch 13/20  Iteration 4789/7380 Training loss: 4.9918 1.1188 sec/batch\n",
      "Epoch 13/20  Iteration 4790/7380 Training loss: 4.9919 1.0939 sec/batch\n",
      "Epoch 13/20  Iteration 4791/7380 Training loss: 4.9918 1.0839 sec/batch\n",
      "Epoch 13/20  Iteration 4792/7380 Training loss: 4.9915 1.0902 sec/batch\n",
      "Epoch 13/20  Iteration 4793/7380 Training loss: 4.9914 1.0874 sec/batch\n",
      "Epoch 13/20  Iteration 4794/7380 Training loss: 4.9911 1.0886 sec/batch\n",
      "Epoch 13/20  Iteration 4795/7380 Training loss: 4.9909 1.0872 sec/batch\n",
      "Epoch 13/20  Iteration 4796/7380 Training loss: 4.9910 1.1427 sec/batch\n",
      "Epoch 13/20  Iteration 4797/7380 Training loss: 4.9908 1.0887 sec/batch\n",
      "Epoch 14/20  Iteration 4798/7380 Training loss: 4.9998 1.0821 sec/batch\n",
      "Epoch 14/20  Iteration 4799/7380 Training loss: 4.9974 1.0986 sec/batch\n",
      "Epoch 14/20  Iteration 4800/7380 Training loss: 4.9719 1.1007 sec/batch\n",
      "Validation loss: 5.05574 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 4801/7380 Training loss: 4.9984 1.1052 sec/batch\n",
      "Epoch 14/20  Iteration 4802/7380 Training loss: 4.9884 1.0905 sec/batch\n",
      "Epoch 14/20  Iteration 4803/7380 Training loss: 4.9912 1.0806 sec/batch\n",
      "Epoch 14/20  Iteration 4804/7380 Training loss: 5.0124 1.0880 sec/batch\n",
      "Epoch 14/20  Iteration 4805/7380 Training loss: 4.9965 1.0884 sec/batch\n",
      "Epoch 14/20  Iteration 4806/7380 Training loss: 4.9941 1.0906 sec/batch\n",
      "Epoch 14/20  Iteration 4807/7380 Training loss: 5.0002 1.0964 sec/batch\n",
      "Epoch 14/20  Iteration 4808/7380 Training loss: 4.9964 1.0914 sec/batch\n",
      "Epoch 14/20  Iteration 4809/7380 Training loss: 4.9869 1.0828 sec/batch\n",
      "Epoch 14/20  Iteration 4810/7380 Training loss: 4.9808 1.0860 sec/batch\n",
      "Epoch 14/20  Iteration 4811/7380 Training loss: 4.9879 1.0857 sec/batch\n",
      "Epoch 14/20  Iteration 4812/7380 Training loss: 4.9920 1.0842 sec/batch\n",
      "Epoch 14/20  Iteration 4813/7380 Training loss: 4.9910 1.0861 sec/batch\n",
      "Epoch 14/20  Iteration 4814/7380 Training loss: 4.9866 1.0851 sec/batch\n",
      "Epoch 14/20  Iteration 4815/7380 Training loss: 4.9732 1.0911 sec/batch\n",
      "Epoch 14/20  Iteration 4816/7380 Training loss: 4.9667 1.0891 sec/batch\n",
      "Epoch 14/20  Iteration 4817/7380 Training loss: 4.9650 1.0828 sec/batch\n",
      "Epoch 14/20  Iteration 4818/7380 Training loss: 4.9608 1.0977 sec/batch\n",
      "Epoch 14/20  Iteration 4819/7380 Training loss: 4.9656 1.0870 sec/batch\n",
      "Epoch 14/20  Iteration 4820/7380 Training loss: 4.9653 1.0951 sec/batch\n",
      "Epoch 14/20  Iteration 4821/7380 Training loss: 4.9714 1.0936 sec/batch\n",
      "Epoch 14/20  Iteration 4822/7380 Training loss: 4.9736 1.0875 sec/batch\n",
      "Epoch 14/20  Iteration 4823/7380 Training loss: 4.9752 1.0823 sec/batch\n",
      "Epoch 14/20  Iteration 4824/7380 Training loss: 4.9699 1.0905 sec/batch\n",
      "Epoch 14/20  Iteration 4825/7380 Training loss: 4.9752 1.0871 sec/batch\n",
      "Epoch 14/20  Iteration 4826/7380 Training loss: 4.9750 1.0931 sec/batch\n",
      "Epoch 14/20  Iteration 4827/7380 Training loss: 4.9755 1.0848 sec/batch\n",
      "Epoch 14/20  Iteration 4828/7380 Training loss: 4.9742 1.0836 sec/batch\n",
      "Epoch 14/20  Iteration 4829/7380 Training loss: 4.9761 1.0839 sec/batch\n",
      "Epoch 14/20  Iteration 4830/7380 Training loss: 4.9764 1.0861 sec/batch\n",
      "Epoch 14/20  Iteration 4831/7380 Training loss: 4.9784 1.0938 sec/batch\n",
      "Epoch 14/20  Iteration 4832/7380 Training loss: 4.9815 1.0964 sec/batch\n",
      "Epoch 14/20  Iteration 4833/7380 Training loss: 4.9797 1.0870 sec/batch\n",
      "Epoch 14/20  Iteration 4834/7380 Training loss: 4.9796 1.0878 sec/batch\n",
      "Epoch 14/20  Iteration 4835/7380 Training loss: 4.9773 1.0867 sec/batch\n",
      "Epoch 14/20  Iteration 4836/7380 Training loss: 4.9795 1.0876 sec/batch\n",
      "Epoch 14/20  Iteration 4837/7380 Training loss: 4.9797 1.0907 sec/batch\n",
      "Epoch 14/20  Iteration 4838/7380 Training loss: 4.9802 1.0872 sec/batch\n",
      "Epoch 14/20  Iteration 4839/7380 Training loss: 4.9790 1.0860 sec/batch\n",
      "Epoch 14/20  Iteration 4840/7380 Training loss: 4.9794 1.0882 sec/batch\n",
      "Epoch 14/20  Iteration 4841/7380 Training loss: 4.9800 1.0889 sec/batch\n",
      "Epoch 14/20  Iteration 4842/7380 Training loss: 4.9799 1.0912 sec/batch\n",
      "Epoch 14/20  Iteration 4843/7380 Training loss: 4.9777 1.0829 sec/batch\n",
      "Epoch 14/20  Iteration 4844/7380 Training loss: 4.9783 1.0940 sec/batch\n",
      "Epoch 14/20  Iteration 4845/7380 Training loss: 4.9754 1.0960 sec/batch\n",
      "Epoch 14/20  Iteration 4846/7380 Training loss: 4.9762 1.1034 sec/batch\n",
      "Epoch 14/20  Iteration 4847/7380 Training loss: 4.9753 1.0837 sec/batch\n",
      "Epoch 14/20  Iteration 4848/7380 Training loss: 4.9740 1.1101 sec/batch\n",
      "Epoch 14/20  Iteration 4849/7380 Training loss: 4.9730 1.0860 sec/batch\n",
      "Epoch 14/20  Iteration 4850/7380 Training loss: 4.9714 1.0925 sec/batch\n",
      "Validation loss: 5.0626 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 4851/7380 Training loss: 4.9727 1.0996 sec/batch\n",
      "Epoch 14/20  Iteration 4852/7380 Training loss: 4.9721 1.1145 sec/batch\n",
      "Epoch 14/20  Iteration 4853/7380 Training loss: 4.9697 1.0894 sec/batch\n",
      "Epoch 14/20  Iteration 4854/7380 Training loss: 4.9691 1.0832 sec/batch\n",
      "Epoch 14/20  Iteration 4855/7380 Training loss: 4.9700 1.0880 sec/batch\n",
      "Epoch 14/20  Iteration 4856/7380 Training loss: 4.9692 1.0920 sec/batch\n",
      "Epoch 14/20  Iteration 4857/7380 Training loss: 4.9703 1.0851 sec/batch\n",
      "Epoch 14/20  Iteration 4858/7380 Training loss: 4.9682 1.0868 sec/batch\n",
      "Epoch 14/20  Iteration 4859/7380 Training loss: 4.9689 1.1333 sec/batch\n",
      "Epoch 14/20  Iteration 4860/7380 Training loss: 4.9677 1.0877 sec/batch\n",
      "Epoch 14/20  Iteration 4861/7380 Training loss: 4.9662 1.0891 sec/batch\n",
      "Epoch 14/20  Iteration 4862/7380 Training loss: 4.9663 1.1026 sec/batch\n",
      "Epoch 14/20  Iteration 4863/7380 Training loss: 4.9658 1.1010 sec/batch\n",
      "Epoch 14/20  Iteration 4864/7380 Training loss: 4.9667 1.0813 sec/batch\n",
      "Epoch 14/20  Iteration 4865/7380 Training loss: 4.9670 1.0970 sec/batch\n",
      "Epoch 14/20  Iteration 4866/7380 Training loss: 4.9688 1.0910 sec/batch\n",
      "Epoch 14/20  Iteration 4867/7380 Training loss: 4.9678 1.1044 sec/batch\n",
      "Epoch 14/20  Iteration 4868/7380 Training loss: 4.9663 1.0860 sec/batch\n",
      "Epoch 14/20  Iteration 4869/7380 Training loss: 4.9675 1.0913 sec/batch\n",
      "Epoch 14/20  Iteration 4870/7380 Training loss: 4.9662 1.0867 sec/batch\n",
      "Epoch 14/20  Iteration 4871/7380 Training loss: 4.9674 1.0838 sec/batch\n",
      "Epoch 14/20  Iteration 4872/7380 Training loss: 4.9689 1.0879 sec/batch\n",
      "Epoch 14/20  Iteration 4873/7380 Training loss: 4.9680 1.0952 sec/batch\n",
      "Epoch 14/20  Iteration 4874/7380 Training loss: 4.9680 1.1022 sec/batch\n",
      "Epoch 14/20  Iteration 4875/7380 Training loss: 4.9674 1.0963 sec/batch\n",
      "Epoch 14/20  Iteration 4876/7380 Training loss: 4.9683 1.0878 sec/batch\n",
      "Epoch 14/20  Iteration 4877/7380 Training loss: 4.9693 1.0987 sec/batch\n",
      "Epoch 14/20  Iteration 4878/7380 Training loss: 4.9692 1.1039 sec/batch\n",
      "Epoch 14/20  Iteration 4879/7380 Training loss: 4.9686 1.0861 sec/batch\n",
      "Epoch 14/20  Iteration 4880/7380 Training loss: 4.9675 1.1076 sec/batch\n",
      "Epoch 14/20  Iteration 4881/7380 Training loss: 4.9669 1.0998 sec/batch\n",
      "Epoch 14/20  Iteration 4882/7380 Training loss: 4.9671 1.0949 sec/batch\n",
      "Epoch 14/20  Iteration 4883/7380 Training loss: 4.9668 1.0974 sec/batch\n",
      "Epoch 14/20  Iteration 4884/7380 Training loss: 4.9666 1.0840 sec/batch\n",
      "Epoch 14/20  Iteration 4885/7380 Training loss: 4.9667 1.0909 sec/batch\n",
      "Epoch 14/20  Iteration 4886/7380 Training loss: 4.9644 1.1156 sec/batch\n",
      "Epoch 14/20  Iteration 4887/7380 Training loss: 4.9641 1.0864 sec/batch\n",
      "Epoch 14/20  Iteration 4888/7380 Training loss: 4.9650 1.0987 sec/batch\n",
      "Epoch 14/20  Iteration 4889/7380 Training loss: 4.9668 1.0878 sec/batch\n",
      "Epoch 14/20  Iteration 4890/7380 Training loss: 4.9666 1.1250 sec/batch\n",
      "Epoch 14/20  Iteration 4891/7380 Training loss: 4.9662 1.0886 sec/batch\n",
      "Epoch 14/20  Iteration 4892/7380 Training loss: 4.9658 1.0872 sec/batch\n",
      "Epoch 14/20  Iteration 4893/7380 Training loss: 4.9655 1.0894 sec/batch\n",
      "Epoch 14/20  Iteration 4894/7380 Training loss: 4.9653 1.0866 sec/batch\n",
      "Epoch 14/20  Iteration 4895/7380 Training loss: 4.9662 1.0911 sec/batch\n",
      "Epoch 14/20  Iteration 4896/7380 Training loss: 4.9662 1.0863 sec/batch\n",
      "Epoch 14/20  Iteration 4897/7380 Training loss: 4.9658 1.1050 sec/batch\n",
      "Epoch 14/20  Iteration 4898/7380 Training loss: 4.9668 1.0814 sec/batch\n",
      "Epoch 14/20  Iteration 4899/7380 Training loss: 4.9673 1.0905 sec/batch\n",
      "Epoch 14/20  Iteration 4900/7380 Training loss: 4.9675 1.0891 sec/batch\n",
      "Validation loss: 5.04057 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 4901/7380 Training loss: 4.9681 1.0966 sec/batch\n",
      "Epoch 14/20  Iteration 4902/7380 Training loss: 4.9684 1.0991 sec/batch\n",
      "Epoch 14/20  Iteration 4903/7380 Training loss: 4.9686 1.0867 sec/batch\n",
      "Epoch 14/20  Iteration 4904/7380 Training loss: 4.9681 1.0920 sec/batch\n",
      "Epoch 14/20  Iteration 4905/7380 Training loss: 4.9680 1.0860 sec/batch\n",
      "Epoch 14/20  Iteration 4906/7380 Training loss: 4.9666 1.0897 sec/batch\n",
      "Epoch 14/20  Iteration 4907/7380 Training loss: 4.9663 1.1083 sec/batch\n",
      "Epoch 14/20  Iteration 4908/7380 Training loss: 4.9658 1.0877 sec/batch\n",
      "Epoch 14/20  Iteration 4909/7380 Training loss: 4.9659 1.0962 sec/batch\n",
      "Epoch 14/20  Iteration 4910/7380 Training loss: 4.9650 1.1354 sec/batch\n",
      "Epoch 14/20  Iteration 4911/7380 Training loss: 4.9653 1.0858 sec/batch\n",
      "Epoch 14/20  Iteration 4912/7380 Training loss: 4.9653 1.0946 sec/batch\n",
      "Epoch 14/20  Iteration 4913/7380 Training loss: 4.9649 1.0938 sec/batch\n",
      "Epoch 14/20  Iteration 4914/7380 Training loss: 4.9640 1.0978 sec/batch\n",
      "Epoch 14/20  Iteration 4915/7380 Training loss: 4.9636 1.0876 sec/batch\n",
      "Epoch 14/20  Iteration 4916/7380 Training loss: 4.9642 1.0864 sec/batch\n",
      "Epoch 14/20  Iteration 4917/7380 Training loss: 4.9639 1.0995 sec/batch\n",
      "Epoch 14/20  Iteration 4918/7380 Training loss: 4.9644 1.0873 sec/batch\n",
      "Epoch 14/20  Iteration 4919/7380 Training loss: 4.9657 1.0918 sec/batch\n",
      "Epoch 14/20  Iteration 4920/7380 Training loss: 4.9658 1.0942 sec/batch\n",
      "Epoch 14/20  Iteration 4921/7380 Training loss: 4.9658 1.0931 sec/batch\n",
      "Epoch 14/20  Iteration 4922/7380 Training loss: 4.9656 1.0880 sec/batch\n",
      "Epoch 14/20  Iteration 4923/7380 Training loss: 4.9648 1.0911 sec/batch\n",
      "Epoch 14/20  Iteration 4924/7380 Training loss: 4.9633 1.0894 sec/batch\n",
      "Epoch 14/20  Iteration 4925/7380 Training loss: 4.9632 1.0896 sec/batch\n",
      "Epoch 14/20  Iteration 4926/7380 Training loss: 4.9625 1.0908 sec/batch\n",
      "Epoch 14/20  Iteration 4927/7380 Training loss: 4.9624 1.0881 sec/batch\n",
      "Epoch 14/20  Iteration 4928/7380 Training loss: 4.9628 1.0856 sec/batch\n",
      "Epoch 14/20  Iteration 4929/7380 Training loss: 4.9628 1.0871 sec/batch\n",
      "Epoch 14/20  Iteration 4930/7380 Training loss: 4.9632 1.0940 sec/batch\n",
      "Epoch 14/20  Iteration 4931/7380 Training loss: 4.9622 1.0872 sec/batch\n",
      "Epoch 14/20  Iteration 4932/7380 Training loss: 4.9617 1.0919 sec/batch\n",
      "Epoch 14/20  Iteration 4933/7380 Training loss: 4.9618 1.0972 sec/batch\n",
      "Epoch 14/20  Iteration 4934/7380 Training loss: 4.9613 1.0821 sec/batch\n",
      "Epoch 14/20  Iteration 4935/7380 Training loss: 4.9613 1.0947 sec/batch\n",
      "Epoch 14/20  Iteration 4936/7380 Training loss: 4.9605 1.0880 sec/batch\n",
      "Epoch 14/20  Iteration 4937/7380 Training loss: 4.9596 1.0970 sec/batch\n",
      "Epoch 14/20  Iteration 4938/7380 Training loss: 4.9611 1.1039 sec/batch\n",
      "Epoch 14/20  Iteration 4939/7380 Training loss: 4.9612 1.0974 sec/batch\n",
      "Epoch 14/20  Iteration 4940/7380 Training loss: 4.9615 1.1056 sec/batch\n",
      "Epoch 14/20  Iteration 4941/7380 Training loss: 4.9615 1.0955 sec/batch\n",
      "Epoch 14/20  Iteration 4942/7380 Training loss: 4.9615 1.0843 sec/batch\n",
      "Epoch 14/20  Iteration 4943/7380 Training loss: 4.9615 1.0852 sec/batch\n",
      "Epoch 14/20  Iteration 4944/7380 Training loss: 4.9611 1.0925 sec/batch\n",
      "Epoch 14/20  Iteration 4945/7380 Training loss: 4.9612 1.0886 sec/batch\n",
      "Epoch 14/20  Iteration 4946/7380 Training loss: 4.9606 1.1031 sec/batch\n",
      "Epoch 14/20  Iteration 4947/7380 Training loss: 4.9610 1.0972 sec/batch\n",
      "Epoch 14/20  Iteration 4948/7380 Training loss: 4.9603 1.1120 sec/batch\n",
      "Epoch 14/20  Iteration 4949/7380 Training loss: 4.9609 1.0906 sec/batch\n",
      "Epoch 14/20  Iteration 4950/7380 Training loss: 4.9611 1.0859 sec/batch\n",
      "Validation loss: 5.04588 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 4951/7380 Training loss: 4.9624 1.0884 sec/batch\n",
      "Epoch 14/20  Iteration 4952/7380 Training loss: 4.9623 1.0901 sec/batch\n",
      "Epoch 14/20  Iteration 4953/7380 Training loss: 4.9618 1.0880 sec/batch\n",
      "Epoch 14/20  Iteration 4954/7380 Training loss: 4.9613 1.0904 sec/batch\n",
      "Epoch 14/20  Iteration 4955/7380 Training loss: 4.9621 1.1027 sec/batch\n",
      "Epoch 14/20  Iteration 4956/7380 Training loss: 4.9618 1.1031 sec/batch\n",
      "Epoch 14/20  Iteration 4957/7380 Training loss: 4.9619 1.0884 sec/batch\n",
      "Epoch 14/20  Iteration 4958/7380 Training loss: 4.9626 1.1049 sec/batch\n",
      "Epoch 14/20  Iteration 4959/7380 Training loss: 4.9630 1.0874 sec/batch\n",
      "Epoch 14/20  Iteration 4960/7380 Training loss: 4.9634 1.0913 sec/batch\n",
      "Epoch 14/20  Iteration 4961/7380 Training loss: 4.9638 1.0979 sec/batch\n",
      "Epoch 14/20  Iteration 4962/7380 Training loss: 4.9641 1.0912 sec/batch\n",
      "Epoch 14/20  Iteration 4963/7380 Training loss: 4.9645 1.0923 sec/batch\n",
      "Epoch 14/20  Iteration 4964/7380 Training loss: 4.9647 1.0960 sec/batch\n",
      "Epoch 14/20  Iteration 4965/7380 Training loss: 4.9651 1.0940 sec/batch\n",
      "Epoch 14/20  Iteration 4966/7380 Training loss: 4.9646 1.0885 sec/batch\n",
      "Epoch 14/20  Iteration 4967/7380 Training loss: 4.9642 1.1046 sec/batch\n",
      "Epoch 14/20  Iteration 4968/7380 Training loss: 4.9648 1.0903 sec/batch\n",
      "Epoch 14/20  Iteration 4969/7380 Training loss: 4.9653 1.0878 sec/batch\n",
      "Epoch 14/20  Iteration 4970/7380 Training loss: 4.9654 1.0988 sec/batch\n",
      "Epoch 14/20  Iteration 4971/7380 Training loss: 4.9649 1.1019 sec/batch\n",
      "Epoch 14/20  Iteration 4972/7380 Training loss: 4.9648 1.0934 sec/batch\n",
      "Epoch 14/20  Iteration 4973/7380 Training loss: 4.9643 1.0825 sec/batch\n",
      "Epoch 14/20  Iteration 4974/7380 Training loss: 4.9642 1.1006 sec/batch\n",
      "Epoch 14/20  Iteration 4975/7380 Training loss: 4.9647 1.0954 sec/batch\n",
      "Epoch 14/20  Iteration 4976/7380 Training loss: 4.9641 1.0856 sec/batch\n",
      "Epoch 14/20  Iteration 4977/7380 Training loss: 4.9635 1.0989 sec/batch\n",
      "Epoch 14/20  Iteration 4978/7380 Training loss: 4.9635 1.0877 sec/batch\n",
      "Epoch 14/20  Iteration 4979/7380 Training loss: 4.9635 1.0834 sec/batch\n",
      "Epoch 14/20  Iteration 4980/7380 Training loss: 4.9640 1.0983 sec/batch\n",
      "Epoch 14/20  Iteration 4981/7380 Training loss: 4.9638 1.0877 sec/batch\n",
      "Epoch 14/20  Iteration 4982/7380 Training loss: 4.9639 1.0863 sec/batch\n",
      "Epoch 14/20  Iteration 4983/7380 Training loss: 4.9640 1.0911 sec/batch\n",
      "Epoch 14/20  Iteration 4984/7380 Training loss: 4.9636 1.0963 sec/batch\n",
      "Epoch 14/20  Iteration 4985/7380 Training loss: 4.9634 1.1035 sec/batch\n",
      "Epoch 14/20  Iteration 4986/7380 Training loss: 4.9630 1.1013 sec/batch\n",
      "Epoch 14/20  Iteration 4987/7380 Training loss: 4.9631 1.0865 sec/batch\n",
      "Epoch 14/20  Iteration 4988/7380 Training loss: 4.9622 1.1159 sec/batch\n",
      "Epoch 14/20  Iteration 4989/7380 Training loss: 4.9627 1.0971 sec/batch\n",
      "Epoch 14/20  Iteration 4990/7380 Training loss: 4.9632 1.0911 sec/batch\n",
      "Epoch 14/20  Iteration 4991/7380 Training loss: 4.9626 1.1056 sec/batch\n",
      "Epoch 14/20  Iteration 4992/7380 Training loss: 4.9631 1.0907 sec/batch\n",
      "Epoch 14/20  Iteration 4993/7380 Training loss: 4.9633 1.1013 sec/batch\n",
      "Epoch 14/20  Iteration 4994/7380 Training loss: 4.9633 1.1024 sec/batch\n",
      "Epoch 14/20  Iteration 4995/7380 Training loss: 4.9638 1.1413 sec/batch\n",
      "Epoch 14/20  Iteration 4996/7380 Training loss: 4.9636 1.0890 sec/batch\n",
      "Epoch 14/20  Iteration 4997/7380 Training loss: 4.9637 1.0900 sec/batch\n",
      "Epoch 14/20  Iteration 4998/7380 Training loss: 4.9640 1.1026 sec/batch\n",
      "Epoch 14/20  Iteration 4999/7380 Training loss: 4.9644 1.0998 sec/batch\n",
      "Epoch 14/20  Iteration 5000/7380 Training loss: 4.9649 1.0898 sec/batch\n",
      "Validation loss: 5.05234 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 5001/7380 Training loss: 4.9654 1.1126 sec/batch\n",
      "Epoch 14/20  Iteration 5002/7380 Training loss: 4.9658 1.1001 sec/batch\n",
      "Epoch 14/20  Iteration 5003/7380 Training loss: 4.9662 1.0948 sec/batch\n",
      "Epoch 14/20  Iteration 5004/7380 Training loss: 4.9653 1.0847 sec/batch\n",
      "Epoch 14/20  Iteration 5005/7380 Training loss: 4.9649 1.0872 sec/batch\n",
      "Epoch 14/20  Iteration 5006/7380 Training loss: 4.9645 1.0892 sec/batch\n",
      "Epoch 14/20  Iteration 5007/7380 Training loss: 4.9638 1.0882 sec/batch\n",
      "Epoch 14/20  Iteration 5008/7380 Training loss: 4.9636 1.1059 sec/batch\n",
      "Epoch 14/20  Iteration 5009/7380 Training loss: 4.9633 1.0933 sec/batch\n",
      "Epoch 14/20  Iteration 5010/7380 Training loss: 4.9633 1.0952 sec/batch\n",
      "Epoch 14/20  Iteration 5011/7380 Training loss: 4.9632 1.1424 sec/batch\n",
      "Epoch 14/20  Iteration 5012/7380 Training loss: 4.9628 1.0986 sec/batch\n",
      "Epoch 14/20  Iteration 5013/7380 Training loss: 4.9624 1.1005 sec/batch\n",
      "Epoch 14/20  Iteration 5014/7380 Training loss: 4.9618 1.0986 sec/batch\n",
      "Epoch 14/20  Iteration 5015/7380 Training loss: 4.9621 1.0935 sec/batch\n",
      "Epoch 14/20  Iteration 5016/7380 Training loss: 4.9625 1.0839 sec/batch\n",
      "Epoch 14/20  Iteration 5017/7380 Training loss: 4.9625 1.0889 sec/batch\n",
      "Epoch 14/20  Iteration 5018/7380 Training loss: 4.9623 1.0885 sec/batch\n",
      "Epoch 14/20  Iteration 5019/7380 Training loss: 4.9625 1.0889 sec/batch\n",
      "Epoch 14/20  Iteration 5020/7380 Training loss: 4.9625 1.0912 sec/batch\n",
      "Epoch 14/20  Iteration 5021/7380 Training loss: 4.9621 1.0960 sec/batch\n",
      "Epoch 14/20  Iteration 5022/7380 Training loss: 4.9622 1.1114 sec/batch\n",
      "Epoch 14/20  Iteration 5023/7380 Training loss: 4.9619 1.1032 sec/batch\n",
      "Epoch 14/20  Iteration 5024/7380 Training loss: 4.9619 1.0873 sec/batch\n",
      "Epoch 14/20  Iteration 5025/7380 Training loss: 4.9613 1.0922 sec/batch\n",
      "Epoch 14/20  Iteration 5026/7380 Training loss: 4.9613 1.0918 sec/batch\n",
      "Epoch 14/20  Iteration 5027/7380 Training loss: 4.9618 1.0887 sec/batch\n",
      "Epoch 14/20  Iteration 5028/7380 Training loss: 4.9613 1.0971 sec/batch\n",
      "Epoch 14/20  Iteration 5029/7380 Training loss: 4.9611 1.0912 sec/batch\n",
      "Epoch 14/20  Iteration 5030/7380 Training loss: 4.9606 1.0889 sec/batch\n",
      "Epoch 14/20  Iteration 5031/7380 Training loss: 4.9604 1.0920 sec/batch\n",
      "Epoch 14/20  Iteration 5032/7380 Training loss: 4.9601 1.0894 sec/batch\n",
      "Epoch 14/20  Iteration 5033/7380 Training loss: 4.9596 1.0944 sec/batch\n",
      "Epoch 14/20  Iteration 5034/7380 Training loss: 4.9593 1.1019 sec/batch\n",
      "Epoch 14/20  Iteration 5035/7380 Training loss: 4.9585 1.1117 sec/batch\n",
      "Epoch 14/20  Iteration 5036/7380 Training loss: 4.9573 1.1061 sec/batch\n",
      "Epoch 14/20  Iteration 5037/7380 Training loss: 4.9568 1.1034 sec/batch\n",
      "Epoch 14/20  Iteration 5038/7380 Training loss: 4.9569 1.0942 sec/batch\n",
      "Epoch 14/20  Iteration 5039/7380 Training loss: 4.9575 1.0878 sec/batch\n",
      "Epoch 14/20  Iteration 5040/7380 Training loss: 4.9571 1.0966 sec/batch\n",
      "Epoch 14/20  Iteration 5041/7380 Training loss: 4.9571 1.0902 sec/batch\n",
      "Epoch 14/20  Iteration 5042/7380 Training loss: 4.9572 1.0968 sec/batch\n",
      "Epoch 14/20  Iteration 5043/7380 Training loss: 4.9573 1.0944 sec/batch\n",
      "Epoch 14/20  Iteration 5044/7380 Training loss: 4.9572 1.0918 sec/batch\n",
      "Epoch 14/20  Iteration 5045/7380 Training loss: 4.9571 1.1045 sec/batch\n",
      "Epoch 14/20  Iteration 5046/7380 Training loss: 4.9570 1.0874 sec/batch\n",
      "Epoch 14/20  Iteration 5047/7380 Training loss: 4.9570 1.0918 sec/batch\n",
      "Epoch 14/20  Iteration 5048/7380 Training loss: 4.9566 1.0919 sec/batch\n",
      "Epoch 14/20  Iteration 5049/7380 Training loss: 4.9565 1.1081 sec/batch\n",
      "Epoch 14/20  Iteration 5050/7380 Training loss: 4.9563 1.0900 sec/batch\n",
      "Validation loss: 5.04185 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 5051/7380 Training loss: 4.9563 1.0990 sec/batch\n",
      "Epoch 14/20  Iteration 5052/7380 Training loss: 4.9565 1.0919 sec/batch\n",
      "Epoch 14/20  Iteration 5053/7380 Training loss: 4.9565 1.1005 sec/batch\n",
      "Epoch 14/20  Iteration 5054/7380 Training loss: 4.9568 1.0845 sec/batch\n",
      "Epoch 14/20  Iteration 5055/7380 Training loss: 4.9565 1.0910 sec/batch\n",
      "Epoch 14/20  Iteration 5056/7380 Training loss: 4.9567 1.1054 sec/batch\n",
      "Epoch 14/20  Iteration 5057/7380 Training loss: 4.9562 1.0927 sec/batch\n",
      "Epoch 14/20  Iteration 5058/7380 Training loss: 4.9561 1.1005 sec/batch\n",
      "Epoch 14/20  Iteration 5059/7380 Training loss: 4.9561 1.0945 sec/batch\n",
      "Epoch 14/20  Iteration 5060/7380 Training loss: 4.9563 1.1080 sec/batch\n",
      "Epoch 14/20  Iteration 5061/7380 Training loss: 4.9560 1.0926 sec/batch\n",
      "Epoch 14/20  Iteration 5062/7380 Training loss: 4.9556 1.1049 sec/batch\n",
      "Epoch 14/20  Iteration 5063/7380 Training loss: 4.9554 1.0892 sec/batch\n",
      "Epoch 14/20  Iteration 5064/7380 Training loss: 4.9555 1.1007 sec/batch\n",
      "Epoch 14/20  Iteration 5065/7380 Training loss: 4.9555 1.0972 sec/batch\n",
      "Epoch 14/20  Iteration 5066/7380 Training loss: 4.9549 1.1048 sec/batch\n",
      "Epoch 14/20  Iteration 5067/7380 Training loss: 4.9553 1.1002 sec/batch\n",
      "Epoch 14/20  Iteration 5068/7380 Training loss: 4.9554 1.1036 sec/batch\n",
      "Epoch 14/20  Iteration 5069/7380 Training loss: 4.9550 1.1035 sec/batch\n",
      "Epoch 14/20  Iteration 5070/7380 Training loss: 4.9552 1.0997 sec/batch\n",
      "Epoch 14/20  Iteration 5071/7380 Training loss: 4.9555 1.0895 sec/batch\n",
      "Epoch 14/20  Iteration 5072/7380 Training loss: 4.9554 1.0922 sec/batch\n",
      "Epoch 14/20  Iteration 5073/7380 Training loss: 4.9551 1.0879 sec/batch\n",
      "Epoch 14/20  Iteration 5074/7380 Training loss: 4.9558 1.0929 sec/batch\n",
      "Epoch 14/20  Iteration 5075/7380 Training loss: 4.9557 1.0911 sec/batch\n",
      "Epoch 14/20  Iteration 5076/7380 Training loss: 4.9558 1.0862 sec/batch\n",
      "Epoch 14/20  Iteration 5077/7380 Training loss: 4.9556 1.1021 sec/batch\n",
      "Epoch 14/20  Iteration 5078/7380 Training loss: 4.9557 1.0937 sec/batch\n",
      "Epoch 14/20  Iteration 5079/7380 Training loss: 4.9558 1.0914 sec/batch\n",
      "Epoch 14/20  Iteration 5080/7380 Training loss: 4.9559 1.1018 sec/batch\n",
      "Epoch 14/20  Iteration 5081/7380 Training loss: 4.9560 1.0973 sec/batch\n",
      "Epoch 14/20  Iteration 5082/7380 Training loss: 4.9563 1.1067 sec/batch\n",
      "Epoch 14/20  Iteration 5083/7380 Training loss: 4.9560 1.1027 sec/batch\n",
      "Epoch 14/20  Iteration 5084/7380 Training loss: 4.9555 1.1028 sec/batch\n",
      "Epoch 14/20  Iteration 5085/7380 Training loss: 4.9554 1.1004 sec/batch\n",
      "Epoch 14/20  Iteration 5086/7380 Training loss: 4.9551 1.0923 sec/batch\n",
      "Epoch 14/20  Iteration 5087/7380 Training loss: 4.9547 1.0905 sec/batch\n",
      "Epoch 14/20  Iteration 5088/7380 Training loss: 4.9543 1.0853 sec/batch\n",
      "Epoch 14/20  Iteration 5089/7380 Training loss: 4.9542 1.0881 sec/batch\n",
      "Epoch 14/20  Iteration 5090/7380 Training loss: 4.9541 1.0927 sec/batch\n",
      "Epoch 14/20  Iteration 5091/7380 Training loss: 4.9543 1.0833 sec/batch\n",
      "Epoch 14/20  Iteration 5092/7380 Training loss: 4.9543 1.0945 sec/batch\n",
      "Epoch 14/20  Iteration 5093/7380 Training loss: 4.9541 1.1106 sec/batch\n",
      "Epoch 14/20  Iteration 5094/7380 Training loss: 4.9540 1.0897 sec/batch\n",
      "Epoch 14/20  Iteration 5095/7380 Training loss: 4.9539 1.1026 sec/batch\n",
      "Epoch 14/20  Iteration 5096/7380 Training loss: 4.9540 1.0894 sec/batch\n",
      "Epoch 14/20  Iteration 5097/7380 Training loss: 4.9547 1.0997 sec/batch\n",
      "Epoch 14/20  Iteration 5098/7380 Training loss: 4.9545 1.1029 sec/batch\n",
      "Epoch 14/20  Iteration 5099/7380 Training loss: 4.9547 1.0999 sec/batch\n",
      "Epoch 14/20  Iteration 5100/7380 Training loss: 4.9547 1.0932 sec/batch\n",
      "Validation loss: 5.05097 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 5101/7380 Training loss: 4.9557 1.0991 sec/batch\n",
      "Epoch 14/20  Iteration 5102/7380 Training loss: 4.9559 1.0942 sec/batch\n",
      "Epoch 14/20  Iteration 5103/7380 Training loss: 4.9560 1.1023 sec/batch\n",
      "Epoch 14/20  Iteration 5104/7380 Training loss: 4.9557 1.0853 sec/batch\n",
      "Epoch 14/20  Iteration 5105/7380 Training loss: 4.9553 1.0918 sec/batch\n",
      "Epoch 14/20  Iteration 5106/7380 Training loss: 4.9552 1.0957 sec/batch\n",
      "Epoch 14/20  Iteration 5107/7380 Training loss: 4.9549 1.0908 sec/batch\n",
      "Epoch 14/20  Iteration 5108/7380 Training loss: 4.9548 1.0955 sec/batch\n",
      "Epoch 14/20  Iteration 5109/7380 Training loss: 4.9546 1.0933 sec/batch\n",
      "Epoch 14/20  Iteration 5110/7380 Training loss: 4.9544 1.1000 sec/batch\n",
      "Epoch 14/20  Iteration 5111/7380 Training loss: 4.9545 1.0923 sec/batch\n",
      "Epoch 14/20  Iteration 5112/7380 Training loss: 4.9545 1.1031 sec/batch\n",
      "Epoch 14/20  Iteration 5113/7380 Training loss: 4.9547 1.0955 sec/batch\n",
      "Epoch 14/20  Iteration 5114/7380 Training loss: 4.9549 1.0913 sec/batch\n",
      "Epoch 14/20  Iteration 5115/7380 Training loss: 4.9550 1.0894 sec/batch\n",
      "Epoch 14/20  Iteration 5116/7380 Training loss: 4.9555 1.0942 sec/batch\n",
      "Epoch 14/20  Iteration 5117/7380 Training loss: 4.9559 1.0948 sec/batch\n",
      "Epoch 14/20  Iteration 5118/7380 Training loss: 4.9556 1.1020 sec/batch\n",
      "Epoch 14/20  Iteration 5119/7380 Training loss: 4.9554 1.0935 sec/batch\n",
      "Epoch 14/20  Iteration 5120/7380 Training loss: 4.9554 1.1012 sec/batch\n",
      "Epoch 14/20  Iteration 5121/7380 Training loss: 4.9553 1.1045 sec/batch\n",
      "Epoch 14/20  Iteration 5122/7380 Training loss: 4.9553 1.0940 sec/batch\n",
      "Epoch 14/20  Iteration 5123/7380 Training loss: 4.9559 1.0948 sec/batch\n",
      "Epoch 14/20  Iteration 5124/7380 Training loss: 4.9557 1.1258 sec/batch\n",
      "Epoch 14/20  Iteration 5125/7380 Training loss: 4.9554 1.0962 sec/batch\n",
      "Epoch 14/20  Iteration 5126/7380 Training loss: 4.9552 1.0945 sec/batch\n",
      "Epoch 14/20  Iteration 5127/7380 Training loss: 4.9549 1.0945 sec/batch\n",
      "Epoch 14/20  Iteration 5128/7380 Training loss: 4.9544 1.0992 sec/batch\n",
      "Epoch 14/20  Iteration 5129/7380 Training loss: 4.9545 1.1035 sec/batch\n",
      "Epoch 14/20  Iteration 5130/7380 Training loss: 4.9543 1.0944 sec/batch\n",
      "Epoch 14/20  Iteration 5131/7380 Training loss: 4.9544 1.0976 sec/batch\n",
      "Epoch 14/20  Iteration 5132/7380 Training loss: 4.9545 1.0953 sec/batch\n",
      "Epoch 14/20  Iteration 5133/7380 Training loss: 4.9547 1.1123 sec/batch\n",
      "Epoch 14/20  Iteration 5134/7380 Training loss: 4.9549 1.1047 sec/batch\n",
      "Epoch 14/20  Iteration 5135/7380 Training loss: 4.9548 1.0999 sec/batch\n",
      "Epoch 14/20  Iteration 5136/7380 Training loss: 4.9548 1.0919 sec/batch\n",
      "Epoch 14/20  Iteration 5137/7380 Training loss: 4.9545 1.1011 sec/batch\n",
      "Epoch 14/20  Iteration 5138/7380 Training loss: 4.9543 1.0983 sec/batch\n",
      "Epoch 14/20  Iteration 5139/7380 Training loss: 4.9540 1.0989 sec/batch\n",
      "Epoch 14/20  Iteration 5140/7380 Training loss: 4.9539 1.0948 sec/batch\n",
      "Epoch 14/20  Iteration 5141/7380 Training loss: 4.9539 1.0965 sec/batch\n",
      "Epoch 14/20  Iteration 5142/7380 Training loss: 4.9537 1.0954 sec/batch\n",
      "Epoch 14/20  Iteration 5143/7380 Training loss: 4.9536 1.0873 sec/batch\n",
      "Epoch 14/20  Iteration 5144/7380 Training loss: 4.9539 1.1038 sec/batch\n",
      "Epoch 14/20  Iteration 5145/7380 Training loss: 4.9539 1.0932 sec/batch\n",
      "Epoch 14/20  Iteration 5146/7380 Training loss: 4.9537 1.0962 sec/batch\n",
      "Epoch 14/20  Iteration 5147/7380 Training loss: 4.9536 1.1076 sec/batch\n",
      "Epoch 14/20  Iteration 5148/7380 Training loss: 4.9540 1.0986 sec/batch\n",
      "Epoch 14/20  Iteration 5149/7380 Training loss: 4.9538 1.1034 sec/batch\n",
      "Epoch 14/20  Iteration 5150/7380 Training loss: 4.9539 1.1027 sec/batch\n",
      "Validation loss: 5.04356 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 5151/7380 Training loss: 4.9539 1.1010 sec/batch\n",
      "Epoch 14/20  Iteration 5152/7380 Training loss: 4.9537 1.1072 sec/batch\n",
      "Epoch 14/20  Iteration 5153/7380 Training loss: 4.9537 1.0967 sec/batch\n",
      "Epoch 14/20  Iteration 5154/7380 Training loss: 4.9539 1.0994 sec/batch\n",
      "Epoch 14/20  Iteration 5155/7380 Training loss: 4.9539 1.1029 sec/batch\n",
      "Epoch 14/20  Iteration 5156/7380 Training loss: 4.9537 1.1109 sec/batch\n",
      "Epoch 14/20  Iteration 5157/7380 Training loss: 4.9536 1.0941 sec/batch\n",
      "Epoch 14/20  Iteration 5158/7380 Training loss: 4.9535 1.1090 sec/batch\n",
      "Epoch 14/20  Iteration 5159/7380 Training loss: 4.9535 1.0959 sec/batch\n",
      "Epoch 14/20  Iteration 5160/7380 Training loss: 4.9534 1.1004 sec/batch\n",
      "Epoch 14/20  Iteration 5161/7380 Training loss: 4.9531 1.0971 sec/batch\n",
      "Epoch 14/20  Iteration 5162/7380 Training loss: 4.9530 1.1087 sec/batch\n",
      "Epoch 14/20  Iteration 5163/7380 Training loss: 4.9528 1.1232 sec/batch\n",
      "Epoch 14/20  Iteration 5164/7380 Training loss: 4.9525 1.1181 sec/batch\n",
      "Epoch 14/20  Iteration 5165/7380 Training loss: 4.9526 1.0965 sec/batch\n",
      "Epoch 14/20  Iteration 5166/7380 Training loss: 4.9524 1.0996 sec/batch\n",
      "Epoch 15/20  Iteration 5167/7380 Training loss: 4.9705 1.0931 sec/batch\n",
      "Epoch 15/20  Iteration 5168/7380 Training loss: 4.9666 1.0934 sec/batch\n",
      "Epoch 15/20  Iteration 5169/7380 Training loss: 4.9432 1.0930 sec/batch\n",
      "Epoch 15/20  Iteration 5170/7380 Training loss: 4.9395 1.0972 sec/batch\n",
      "Epoch 15/20  Iteration 5171/7380 Training loss: 4.9294 1.0944 sec/batch\n",
      "Epoch 15/20  Iteration 5172/7380 Training loss: 4.9355 1.0984 sec/batch\n",
      "Epoch 15/20  Iteration 5173/7380 Training loss: 4.9567 1.1018 sec/batch\n",
      "Epoch 15/20  Iteration 5174/7380 Training loss: 4.9442 1.0998 sec/batch\n",
      "Epoch 15/20  Iteration 5175/7380 Training loss: 4.9439 1.1010 sec/batch\n",
      "Epoch 15/20  Iteration 5176/7380 Training loss: 4.9565 1.0966 sec/batch\n",
      "Epoch 15/20  Iteration 5177/7380 Training loss: 4.9533 1.0917 sec/batch\n",
      "Epoch 15/20  Iteration 5178/7380 Training loss: 4.9419 1.1000 sec/batch\n",
      "Epoch 15/20  Iteration 5179/7380 Training loss: 4.9380 1.1058 sec/batch\n",
      "Epoch 15/20  Iteration 5180/7380 Training loss: 4.9473 1.0975 sec/batch\n",
      "Epoch 15/20  Iteration 5181/7380 Training loss: 4.9514 1.1078 sec/batch\n",
      "Epoch 15/20  Iteration 5182/7380 Training loss: 4.9513 1.0985 sec/batch\n",
      "Epoch 15/20  Iteration 5183/7380 Training loss: 4.9471 1.1045 sec/batch\n",
      "Epoch 15/20  Iteration 5184/7380 Training loss: 4.9341 1.0974 sec/batch\n",
      "Epoch 15/20  Iteration 5185/7380 Training loss: 4.9277 1.1016 sec/batch\n",
      "Epoch 15/20  Iteration 5186/7380 Training loss: 4.9263 1.1034 sec/batch\n",
      "Epoch 15/20  Iteration 5187/7380 Training loss: 4.9228 1.1535 sec/batch\n",
      "Epoch 15/20  Iteration 5188/7380 Training loss: 4.9299 1.1025 sec/batch\n",
      "Epoch 15/20  Iteration 5189/7380 Training loss: 4.9290 1.1058 sec/batch\n",
      "Epoch 15/20  Iteration 5190/7380 Training loss: 4.9360 1.1003 sec/batch\n",
      "Epoch 15/20  Iteration 5191/7380 Training loss: 4.9387 1.0963 sec/batch\n",
      "Epoch 15/20  Iteration 5192/7380 Training loss: 4.9411 1.1068 sec/batch\n",
      "Epoch 15/20  Iteration 5193/7380 Training loss: 4.9352 1.1063 sec/batch\n",
      "Epoch 15/20  Iteration 5194/7380 Training loss: 4.9417 1.1044 sec/batch\n",
      "Epoch 15/20  Iteration 5195/7380 Training loss: 4.9409 1.1090 sec/batch\n",
      "Epoch 15/20  Iteration 5196/7380 Training loss: 4.9415 1.0971 sec/batch\n",
      "Epoch 15/20  Iteration 5197/7380 Training loss: 4.9398 1.1058 sec/batch\n",
      "Epoch 15/20  Iteration 5198/7380 Training loss: 4.9405 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5199/7380 Training loss: 4.9398 1.0963 sec/batch\n",
      "Epoch 15/20  Iteration 5200/7380 Training loss: 4.9409 1.0976 sec/batch\n",
      "Validation loss: 5.03732 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 5201/7380 Training loss: 4.9478 1.0944 sec/batch\n",
      "Epoch 15/20  Iteration 5202/7380 Training loss: 4.9472 1.0991 sec/batch\n",
      "Epoch 15/20  Iteration 5203/7380 Training loss: 4.9471 1.1065 sec/batch\n",
      "Epoch 15/20  Iteration 5204/7380 Training loss: 4.9453 1.0926 sec/batch\n",
      "Epoch 15/20  Iteration 5205/7380 Training loss: 4.9478 1.1003 sec/batch\n",
      "Epoch 15/20  Iteration 5206/7380 Training loss: 4.9473 1.0963 sec/batch\n",
      "Epoch 15/20  Iteration 5207/7380 Training loss: 4.9463 1.0900 sec/batch\n",
      "Epoch 15/20  Iteration 5208/7380 Training loss: 4.9450 1.1310 sec/batch\n",
      "Epoch 15/20  Iteration 5209/7380 Training loss: 4.9455 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5210/7380 Training loss: 4.9474 1.0956 sec/batch\n",
      "Epoch 15/20  Iteration 5211/7380 Training loss: 4.9480 1.0979 sec/batch\n",
      "Epoch 15/20  Iteration 5212/7380 Training loss: 4.9464 1.1190 sec/batch\n",
      "Epoch 15/20  Iteration 5213/7380 Training loss: 4.9473 1.0961 sec/batch\n",
      "Epoch 15/20  Iteration 5214/7380 Training loss: 4.9446 1.0867 sec/batch\n",
      "Epoch 15/20  Iteration 5215/7380 Training loss: 4.9452 1.0977 sec/batch\n",
      "Epoch 15/20  Iteration 5216/7380 Training loss: 4.9445 1.1040 sec/batch\n",
      "Epoch 15/20  Iteration 5217/7380 Training loss: 4.9435 1.1080 sec/batch\n",
      "Epoch 15/20  Iteration 5218/7380 Training loss: 4.9421 1.1027 sec/batch\n",
      "Epoch 15/20  Iteration 5219/7380 Training loss: 4.9407 1.0982 sec/batch\n",
      "Epoch 15/20  Iteration 5220/7380 Training loss: 4.9398 1.0981 sec/batch\n",
      "Epoch 15/20  Iteration 5221/7380 Training loss: 4.9389 1.1131 sec/batch\n",
      "Epoch 15/20  Iteration 5222/7380 Training loss: 4.9364 1.1014 sec/batch\n",
      "Epoch 15/20  Iteration 5223/7380 Training loss: 4.9363 1.1005 sec/batch\n",
      "Epoch 15/20  Iteration 5224/7380 Training loss: 4.9372 1.1011 sec/batch\n",
      "Epoch 15/20  Iteration 5225/7380 Training loss: 4.9371 1.0946 sec/batch\n",
      "Epoch 15/20  Iteration 5226/7380 Training loss: 4.9383 1.1047 sec/batch\n",
      "Epoch 15/20  Iteration 5227/7380 Training loss: 4.9368 1.0969 sec/batch\n",
      "Epoch 15/20  Iteration 5228/7380 Training loss: 4.9373 1.1051 sec/batch\n",
      "Epoch 15/20  Iteration 5229/7380 Training loss: 4.9359 1.1028 sec/batch\n",
      "Epoch 15/20  Iteration 5230/7380 Training loss: 4.9341 1.1117 sec/batch\n",
      "Epoch 15/20  Iteration 5231/7380 Training loss: 4.9346 1.1149 sec/batch\n",
      "Epoch 15/20  Iteration 5232/7380 Training loss: 4.9339 1.1039 sec/batch\n",
      "Epoch 15/20  Iteration 5233/7380 Training loss: 4.9344 1.1053 sec/batch\n",
      "Epoch 15/20  Iteration 5234/7380 Training loss: 4.9350 1.1105 sec/batch\n",
      "Epoch 15/20  Iteration 5235/7380 Training loss: 4.9366 1.0983 sec/batch\n",
      "Epoch 15/20  Iteration 5236/7380 Training loss: 4.9358 1.0896 sec/batch\n",
      "Epoch 15/20  Iteration 5237/7380 Training loss: 4.9345 1.1072 sec/batch\n",
      "Epoch 15/20  Iteration 5238/7380 Training loss: 4.9354 1.0962 sec/batch\n",
      "Epoch 15/20  Iteration 5239/7380 Training loss: 4.9334 1.1060 sec/batch\n",
      "Epoch 15/20  Iteration 5240/7380 Training loss: 4.9350 1.1048 sec/batch\n",
      "Epoch 15/20  Iteration 5241/7380 Training loss: 4.9359 1.0944 sec/batch\n",
      "Epoch 15/20  Iteration 5242/7380 Training loss: 4.9351 1.1054 sec/batch\n",
      "Epoch 15/20  Iteration 5243/7380 Training loss: 4.9350 1.1045 sec/batch\n",
      "Epoch 15/20  Iteration 5244/7380 Training loss: 4.9346 1.0946 sec/batch\n",
      "Epoch 15/20  Iteration 5245/7380 Training loss: 4.9351 1.1003 sec/batch\n",
      "Epoch 15/20  Iteration 5246/7380 Training loss: 4.9359 1.0957 sec/batch\n",
      "Epoch 15/20  Iteration 5247/7380 Training loss: 4.9351 1.0870 sec/batch\n",
      "Epoch 15/20  Iteration 5248/7380 Training loss: 4.9348 1.0978 sec/batch\n",
      "Epoch 15/20  Iteration 5249/7380 Training loss: 4.9335 1.0974 sec/batch\n",
      "Epoch 15/20  Iteration 5250/7380 Training loss: 4.9331 1.0893 sec/batch\n",
      "Validation loss: 5.04729 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 5251/7380 Training loss: 4.9351 1.0998 sec/batch\n",
      "Epoch 15/20  Iteration 5252/7380 Training loss: 4.9343 1.1023 sec/batch\n",
      "Epoch 15/20  Iteration 5253/7380 Training loss: 4.9344 1.0955 sec/batch\n",
      "Epoch 15/20  Iteration 5254/7380 Training loss: 4.9344 1.1021 sec/batch\n",
      "Epoch 15/20  Iteration 5255/7380 Training loss: 4.9325 1.1037 sec/batch\n",
      "Epoch 15/20  Iteration 5256/7380 Training loss: 4.9320 1.0882 sec/batch\n",
      "Epoch 15/20  Iteration 5257/7380 Training loss: 4.9328 1.1069 sec/batch\n",
      "Epoch 15/20  Iteration 5258/7380 Training loss: 4.9344 1.1200 sec/batch\n",
      "Epoch 15/20  Iteration 5259/7380 Training loss: 4.9339 1.0949 sec/batch\n",
      "Epoch 15/20  Iteration 5260/7380 Training loss: 4.9337 1.1100 sec/batch\n",
      "Epoch 15/20  Iteration 5261/7380 Training loss: 4.9332 1.1095 sec/batch\n",
      "Epoch 15/20  Iteration 5262/7380 Training loss: 4.9332 1.0987 sec/batch\n",
      "Epoch 15/20  Iteration 5263/7380 Training loss: 4.9331 1.1032 sec/batch\n",
      "Epoch 15/20  Iteration 5264/7380 Training loss: 4.9341 1.1064 sec/batch\n",
      "Epoch 15/20  Iteration 5265/7380 Training loss: 4.9341 1.1056 sec/batch\n",
      "Epoch 15/20  Iteration 5266/7380 Training loss: 4.9338 1.0980 sec/batch\n",
      "Epoch 15/20  Iteration 5267/7380 Training loss: 4.9349 1.0992 sec/batch\n",
      "Epoch 15/20  Iteration 5268/7380 Training loss: 4.9354 1.0933 sec/batch\n",
      "Epoch 15/20  Iteration 5269/7380 Training loss: 4.9355 1.0953 sec/batch\n",
      "Epoch 15/20  Iteration 5270/7380 Training loss: 4.9352 1.0928 sec/batch\n",
      "Epoch 15/20  Iteration 5271/7380 Training loss: 4.9355 1.0937 sec/batch\n",
      "Epoch 15/20  Iteration 5272/7380 Training loss: 4.9354 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5273/7380 Training loss: 4.9350 1.0989 sec/batch\n",
      "Epoch 15/20  Iteration 5274/7380 Training loss: 4.9352 1.1037 sec/batch\n",
      "Epoch 15/20  Iteration 5275/7380 Training loss: 4.9338 1.1040 sec/batch\n",
      "Epoch 15/20  Iteration 5276/7380 Training loss: 4.9334 1.0964 sec/batch\n",
      "Epoch 15/20  Iteration 5277/7380 Training loss: 4.9331 1.0978 sec/batch\n",
      "Epoch 15/20  Iteration 5278/7380 Training loss: 4.9333 1.0938 sec/batch\n",
      "Epoch 15/20  Iteration 5279/7380 Training loss: 4.9325 1.0955 sec/batch\n",
      "Epoch 15/20  Iteration 5280/7380 Training loss: 4.9331 1.1039 sec/batch\n",
      "Epoch 15/20  Iteration 5281/7380 Training loss: 4.9331 1.0986 sec/batch\n",
      "Epoch 15/20  Iteration 5282/7380 Training loss: 4.9327 1.0991 sec/batch\n",
      "Epoch 15/20  Iteration 5283/7380 Training loss: 4.9321 1.1096 sec/batch\n",
      "Epoch 15/20  Iteration 5284/7380 Training loss: 4.9317 1.0940 sec/batch\n",
      "Epoch 15/20  Iteration 5285/7380 Training loss: 4.9323 1.1103 sec/batch\n",
      "Epoch 15/20  Iteration 5286/7380 Training loss: 4.9321 1.0948 sec/batch\n",
      "Epoch 15/20  Iteration 5287/7380 Training loss: 4.9326 1.1022 sec/batch\n",
      "Epoch 15/20  Iteration 5288/7380 Training loss: 4.9336 1.0938 sec/batch\n",
      "Epoch 15/20  Iteration 5289/7380 Training loss: 4.9336 1.0866 sec/batch\n",
      "Epoch 15/20  Iteration 5290/7380 Training loss: 4.9335 1.1029 sec/batch\n",
      "Epoch 15/20  Iteration 5291/7380 Training loss: 4.9336 1.0927 sec/batch\n",
      "Epoch 15/20  Iteration 5292/7380 Training loss: 4.9327 1.1046 sec/batch\n",
      "Epoch 15/20  Iteration 5293/7380 Training loss: 4.9314 1.0927 sec/batch\n",
      "Epoch 15/20  Iteration 5294/7380 Training loss: 4.9313 1.0987 sec/batch\n",
      "Epoch 15/20  Iteration 5295/7380 Training loss: 4.9307 1.0994 sec/batch\n",
      "Epoch 15/20  Iteration 5296/7380 Training loss: 4.9306 1.1012 sec/batch\n",
      "Epoch 15/20  Iteration 5297/7380 Training loss: 4.9308 1.0893 sec/batch\n",
      "Epoch 15/20  Iteration 5298/7380 Training loss: 4.9309 1.1005 sec/batch\n",
      "Epoch 15/20  Iteration 5299/7380 Training loss: 4.9310 1.1023 sec/batch\n",
      "Epoch 15/20  Iteration 5300/7380 Training loss: 4.9302 1.0955 sec/batch\n",
      "Validation loss: 5.03887 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 5301/7380 Training loss: 4.9305 1.0994 sec/batch\n",
      "Epoch 15/20  Iteration 5302/7380 Training loss: 4.9306 1.1046 sec/batch\n",
      "Epoch 15/20  Iteration 5303/7380 Training loss: 4.9304 1.1118 sec/batch\n",
      "Epoch 15/20  Iteration 5304/7380 Training loss: 4.9303 1.1248 sec/batch\n",
      "Epoch 15/20  Iteration 5305/7380 Training loss: 4.9296 1.1125 sec/batch\n",
      "Epoch 15/20  Iteration 5306/7380 Training loss: 4.9289 1.1143 sec/batch\n",
      "Epoch 15/20  Iteration 5307/7380 Training loss: 4.9303 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5308/7380 Training loss: 4.9303 1.0911 sec/batch\n",
      "Epoch 15/20  Iteration 5309/7380 Training loss: 4.9307 1.0961 sec/batch\n",
      "Epoch 15/20  Iteration 5310/7380 Training loss: 4.9307 1.0975 sec/batch\n",
      "Epoch 15/20  Iteration 5311/7380 Training loss: 4.9309 1.1101 sec/batch\n",
      "Epoch 15/20  Iteration 5312/7380 Training loss: 4.9308 1.0912 sec/batch\n",
      "Epoch 15/20  Iteration 5313/7380 Training loss: 4.9304 1.1194 sec/batch\n",
      "Epoch 15/20  Iteration 5314/7380 Training loss: 4.9305 1.0969 sec/batch\n",
      "Epoch 15/20  Iteration 5315/7380 Training loss: 4.9301 1.1010 sec/batch\n",
      "Epoch 15/20  Iteration 5316/7380 Training loss: 4.9304 1.0914 sec/batch\n",
      "Epoch 15/20  Iteration 5317/7380 Training loss: 4.9299 1.1185 sec/batch\n",
      "Epoch 15/20  Iteration 5318/7380 Training loss: 4.9303 1.1000 sec/batch\n",
      "Epoch 15/20  Iteration 5319/7380 Training loss: 4.9303 1.1145 sec/batch\n",
      "Epoch 15/20  Iteration 5320/7380 Training loss: 4.9308 1.0954 sec/batch\n",
      "Epoch 15/20  Iteration 5321/7380 Training loss: 4.9304 1.0929 sec/batch\n",
      "Epoch 15/20  Iteration 5322/7380 Training loss: 4.9300 1.1046 sec/batch\n",
      "Epoch 15/20  Iteration 5323/7380 Training loss: 4.9295 1.0939 sec/batch\n",
      "Epoch 15/20  Iteration 5324/7380 Training loss: 4.9304 1.0975 sec/batch\n",
      "Epoch 15/20  Iteration 5325/7380 Training loss: 4.9300 1.1417 sec/batch\n",
      "Epoch 15/20  Iteration 5326/7380 Training loss: 4.9298 1.0913 sec/batch\n",
      "Epoch 15/20  Iteration 5327/7380 Training loss: 4.9305 1.1060 sec/batch\n",
      "Epoch 15/20  Iteration 5328/7380 Training loss: 4.9307 1.0920 sec/batch\n",
      "Epoch 15/20  Iteration 5329/7380 Training loss: 4.9310 1.0934 sec/batch\n",
      "Epoch 15/20  Iteration 5330/7380 Training loss: 4.9313 1.0988 sec/batch\n",
      "Epoch 15/20  Iteration 5331/7380 Training loss: 4.9317 1.0977 sec/batch\n",
      "Epoch 15/20  Iteration 5332/7380 Training loss: 4.9320 1.0967 sec/batch\n",
      "Epoch 15/20  Iteration 5333/7380 Training loss: 4.9322 1.1016 sec/batch\n",
      "Epoch 15/20  Iteration 5334/7380 Training loss: 4.9325 1.0970 sec/batch\n",
      "Epoch 15/20  Iteration 5335/7380 Training loss: 4.9321 1.1130 sec/batch\n",
      "Epoch 15/20  Iteration 5336/7380 Training loss: 4.9316 1.1036 sec/batch\n",
      "Epoch 15/20  Iteration 5337/7380 Training loss: 4.9317 1.1408 sec/batch\n",
      "Epoch 15/20  Iteration 5338/7380 Training loss: 4.9323 1.0974 sec/batch\n",
      "Epoch 15/20  Iteration 5339/7380 Training loss: 4.9324 1.1058 sec/batch\n",
      "Epoch 15/20  Iteration 5340/7380 Training loss: 4.9318 1.0962 sec/batch\n",
      "Epoch 15/20  Iteration 5341/7380 Training loss: 4.9314 1.0972 sec/batch\n",
      "Epoch 15/20  Iteration 5342/7380 Training loss: 4.9308 1.0944 sec/batch\n",
      "Epoch 15/20  Iteration 5343/7380 Training loss: 4.9307 1.1007 sec/batch\n",
      "Epoch 15/20  Iteration 5344/7380 Training loss: 4.9312 1.1069 sec/batch\n",
      "Epoch 15/20  Iteration 5345/7380 Training loss: 4.9306 1.1009 sec/batch\n",
      "Epoch 15/20  Iteration 5346/7380 Training loss: 4.9302 1.1021 sec/batch\n",
      "Epoch 15/20  Iteration 5347/7380 Training loss: 4.9298 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5348/7380 Training loss: 4.9300 1.1347 sec/batch\n",
      "Epoch 15/20  Iteration 5349/7380 Training loss: 4.9306 1.1047 sec/batch\n",
      "Epoch 15/20  Iteration 5350/7380 Training loss: 4.9303 1.1011 sec/batch\n",
      "Validation loss: 5.04048 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 5351/7380 Training loss: 4.9312 1.1460 sec/batch\n",
      "Epoch 15/20  Iteration 5352/7380 Training loss: 4.9314 1.0908 sec/batch\n",
      "Epoch 15/20  Iteration 5353/7380 Training loss: 4.9310 1.1097 sec/batch\n",
      "Epoch 15/20  Iteration 5354/7380 Training loss: 4.9307 1.1091 sec/batch\n",
      "Epoch 15/20  Iteration 5355/7380 Training loss: 4.9305 1.1046 sec/batch\n",
      "Epoch 15/20  Iteration 5356/7380 Training loss: 4.9306 1.1053 sec/batch\n",
      "Epoch 15/20  Iteration 5357/7380 Training loss: 4.9298 1.0991 sec/batch\n",
      "Epoch 15/20  Iteration 5358/7380 Training loss: 4.9301 1.0940 sec/batch\n",
      "Epoch 15/20  Iteration 5359/7380 Training loss: 4.9304 1.1000 sec/batch\n",
      "Epoch 15/20  Iteration 5360/7380 Training loss: 4.9297 1.1088 sec/batch\n",
      "Epoch 15/20  Iteration 5361/7380 Training loss: 4.9301 1.1004 sec/batch\n",
      "Epoch 15/20  Iteration 5362/7380 Training loss: 4.9303 1.1383 sec/batch\n",
      "Epoch 15/20  Iteration 5363/7380 Training loss: 4.9303 1.1062 sec/batch\n",
      "Epoch 15/20  Iteration 5364/7380 Training loss: 4.9305 1.0972 sec/batch\n",
      "Epoch 15/20  Iteration 5365/7380 Training loss: 4.9303 1.1002 sec/batch\n",
      "Epoch 15/20  Iteration 5366/7380 Training loss: 4.9305 1.0988 sec/batch\n",
      "Epoch 15/20  Iteration 5367/7380 Training loss: 4.9309 1.1039 sec/batch\n",
      "Epoch 15/20  Iteration 5368/7380 Training loss: 4.9311 1.1037 sec/batch\n",
      "Epoch 15/20  Iteration 5369/7380 Training loss: 4.9316 1.0973 sec/batch\n",
      "Epoch 15/20  Iteration 5370/7380 Training loss: 4.9317 1.1089 sec/batch\n",
      "Epoch 15/20  Iteration 5371/7380 Training loss: 4.9320 1.0994 sec/batch\n",
      "Epoch 15/20  Iteration 5372/7380 Training loss: 4.9323 1.0988 sec/batch\n",
      "Epoch 15/20  Iteration 5373/7380 Training loss: 4.9314 1.1051 sec/batch\n",
      "Epoch 15/20  Iteration 5374/7380 Training loss: 4.9310 1.1045 sec/batch\n",
      "Epoch 15/20  Iteration 5375/7380 Training loss: 4.9307 1.1056 sec/batch\n",
      "Epoch 15/20  Iteration 5376/7380 Training loss: 4.9300 1.1093 sec/batch\n",
      "Epoch 15/20  Iteration 5377/7380 Training loss: 4.9297 1.1124 sec/batch\n",
      "Epoch 15/20  Iteration 5378/7380 Training loss: 4.9295 1.1121 sec/batch\n",
      "Epoch 15/20  Iteration 5379/7380 Training loss: 4.9294 1.1106 sec/batch\n",
      "Epoch 15/20  Iteration 5380/7380 Training loss: 4.9293 1.1161 sec/batch\n",
      "Epoch 15/20  Iteration 5381/7380 Training loss: 4.9289 1.0960 sec/batch\n",
      "Epoch 15/20  Iteration 5382/7380 Training loss: 4.9285 1.1084 sec/batch\n",
      "Epoch 15/20  Iteration 5383/7380 Training loss: 4.9280 1.1157 sec/batch\n",
      "Epoch 15/20  Iteration 5384/7380 Training loss: 4.9281 1.0913 sec/batch\n",
      "Epoch 15/20  Iteration 5385/7380 Training loss: 4.9286 1.1071 sec/batch\n",
      "Epoch 15/20  Iteration 5386/7380 Training loss: 4.9285 1.1027 sec/batch\n",
      "Epoch 15/20  Iteration 5387/7380 Training loss: 4.9283 1.1073 sec/batch\n",
      "Epoch 15/20  Iteration 5388/7380 Training loss: 4.9287 1.1000 sec/batch\n",
      "Epoch 15/20  Iteration 5389/7380 Training loss: 4.9289 1.0987 sec/batch\n",
      "Epoch 15/20  Iteration 5390/7380 Training loss: 4.9284 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5391/7380 Training loss: 4.9284 1.1007 sec/batch\n",
      "Epoch 15/20  Iteration 5392/7380 Training loss: 4.9282 1.0987 sec/batch\n",
      "Epoch 15/20  Iteration 5393/7380 Training loss: 4.9282 1.1139 sec/batch\n",
      "Epoch 15/20  Iteration 5394/7380 Training loss: 4.9276 1.1164 sec/batch\n",
      "Epoch 15/20  Iteration 5395/7380 Training loss: 4.9276 1.1130 sec/batch\n",
      "Epoch 15/20  Iteration 5396/7380 Training loss: 4.9282 1.1042 sec/batch\n",
      "Epoch 15/20  Iteration 5397/7380 Training loss: 4.9279 1.1007 sec/batch\n",
      "Epoch 15/20  Iteration 5398/7380 Training loss: 4.9278 1.0942 sec/batch\n",
      "Epoch 15/20  Iteration 5399/7380 Training loss: 4.9273 1.1018 sec/batch\n",
      "Epoch 15/20  Iteration 5400/7380 Training loss: 4.9272 1.1031 sec/batch\n",
      "Validation loss: 5.03389 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 5401/7380 Training loss: 4.9273 1.1032 sec/batch\n",
      "Epoch 15/20  Iteration 5402/7380 Training loss: 4.9269 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5403/7380 Training loss: 4.9267 1.1092 sec/batch\n",
      "Epoch 15/20  Iteration 5404/7380 Training loss: 4.9261 1.1033 sec/batch\n",
      "Epoch 15/20  Iteration 5405/7380 Training loss: 4.9250 1.0960 sec/batch\n",
      "Epoch 15/20  Iteration 5406/7380 Training loss: 4.9243 1.1056 sec/batch\n",
      "Epoch 15/20  Iteration 5407/7380 Training loss: 4.9244 1.1119 sec/batch\n",
      "Epoch 15/20  Iteration 5408/7380 Training loss: 4.9250 1.0953 sec/batch\n",
      "Epoch 15/20  Iteration 5409/7380 Training loss: 4.9247 1.0968 sec/batch\n",
      "Epoch 15/20  Iteration 5410/7380 Training loss: 4.9246 1.1210 sec/batch\n",
      "Epoch 15/20  Iteration 5411/7380 Training loss: 4.9248 1.1023 sec/batch\n",
      "Epoch 15/20  Iteration 5412/7380 Training loss: 4.9250 1.1033 sec/batch\n",
      "Epoch 15/20  Iteration 5413/7380 Training loss: 4.9250 1.1033 sec/batch\n",
      "Epoch 15/20  Iteration 5414/7380 Training loss: 4.9249 1.0964 sec/batch\n",
      "Epoch 15/20  Iteration 5415/7380 Training loss: 4.9248 1.1091 sec/batch\n",
      "Epoch 15/20  Iteration 5416/7380 Training loss: 4.9250 1.1123 sec/batch\n",
      "Epoch 15/20  Iteration 5417/7380 Training loss: 4.9246 1.1088 sec/batch\n",
      "Epoch 15/20  Iteration 5418/7380 Training loss: 4.9244 1.1049 sec/batch\n",
      "Epoch 15/20  Iteration 5419/7380 Training loss: 4.9244 1.1017 sec/batch\n",
      "Epoch 15/20  Iteration 5420/7380 Training loss: 4.9239 1.1091 sec/batch\n",
      "Epoch 15/20  Iteration 5421/7380 Training loss: 4.9241 1.1035 sec/batch\n",
      "Epoch 15/20  Iteration 5422/7380 Training loss: 4.9241 1.1011 sec/batch\n",
      "Epoch 15/20  Iteration 5423/7380 Training loss: 4.9243 1.0981 sec/batch\n",
      "Epoch 15/20  Iteration 5424/7380 Training loss: 4.9240 1.0995 sec/batch\n",
      "Epoch 15/20  Iteration 5425/7380 Training loss: 4.9241 1.1061 sec/batch\n",
      "Epoch 15/20  Iteration 5426/7380 Training loss: 4.9237 1.1029 sec/batch\n",
      "Epoch 15/20  Iteration 5427/7380 Training loss: 4.9235 1.1040 sec/batch\n",
      "Epoch 15/20  Iteration 5428/7380 Training loss: 4.9236 1.1316 sec/batch\n",
      "Epoch 15/20  Iteration 5429/7380 Training loss: 4.9237 1.1034 sec/batch\n",
      "Epoch 15/20  Iteration 5430/7380 Training loss: 4.9234 1.1060 sec/batch\n",
      "Epoch 15/20  Iteration 5431/7380 Training loss: 4.9232 1.1029 sec/batch\n",
      "Epoch 15/20  Iteration 5432/7380 Training loss: 4.9230 1.0938 sec/batch\n",
      "Epoch 15/20  Iteration 5433/7380 Training loss: 4.9230 1.1008 sec/batch\n",
      "Epoch 15/20  Iteration 5434/7380 Training loss: 4.9230 1.0891 sec/batch\n",
      "Epoch 15/20  Iteration 5435/7380 Training loss: 4.9225 1.1393 sec/batch\n",
      "Epoch 15/20  Iteration 5436/7380 Training loss: 4.9229 1.1109 sec/batch\n",
      "Epoch 15/20  Iteration 5437/7380 Training loss: 4.9230 1.1044 sec/batch\n",
      "Epoch 15/20  Iteration 5438/7380 Training loss: 4.9225 1.1103 sec/batch\n",
      "Epoch 15/20  Iteration 5439/7380 Training loss: 4.9227 1.1093 sec/batch\n",
      "Epoch 15/20  Iteration 5440/7380 Training loss: 4.9228 1.0907 sec/batch\n",
      "Epoch 15/20  Iteration 5441/7380 Training loss: 4.9228 1.1036 sec/batch\n",
      "Epoch 15/20  Iteration 5442/7380 Training loss: 4.9224 1.1018 sec/batch\n",
      "Epoch 15/20  Iteration 5443/7380 Training loss: 4.9231 1.0967 sec/batch\n",
      "Epoch 15/20  Iteration 5444/7380 Training loss: 4.9231 1.1026 sec/batch\n",
      "Epoch 15/20  Iteration 5445/7380 Training loss: 4.9234 1.1006 sec/batch\n",
      "Epoch 15/20  Iteration 5446/7380 Training loss: 4.9232 1.0962 sec/batch\n",
      "Epoch 15/20  Iteration 5447/7380 Training loss: 4.9231 1.1047 sec/batch\n",
      "Epoch 15/20  Iteration 5448/7380 Training loss: 4.9231 1.1035 sec/batch\n",
      "Epoch 15/20  Iteration 5449/7380 Training loss: 4.9232 1.0902 sec/batch\n",
      "Epoch 15/20  Iteration 5450/7380 Training loss: 4.9233 1.1080 sec/batch\n",
      "Validation loss: 5.03103 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 5451/7380 Training loss: 4.9242 1.1082 sec/batch\n",
      "Epoch 15/20  Iteration 5452/7380 Training loss: 4.9239 1.1004 sec/batch\n",
      "Epoch 15/20  Iteration 5453/7380 Training loss: 4.9235 1.1109 sec/batch\n",
      "Epoch 15/20  Iteration 5454/7380 Training loss: 4.9233 1.1033 sec/batch\n",
      "Epoch 15/20  Iteration 5455/7380 Training loss: 4.9230 1.1087 sec/batch\n",
      "Epoch 15/20  Iteration 5456/7380 Training loss: 4.9226 1.1025 sec/batch\n",
      "Epoch 15/20  Iteration 5457/7380 Training loss: 4.9223 1.0956 sec/batch\n",
      "Epoch 15/20  Iteration 5458/7380 Training loss: 4.9221 1.1074 sec/batch\n",
      "Epoch 15/20  Iteration 5459/7380 Training loss: 4.9222 1.0979 sec/batch\n",
      "Epoch 15/20  Iteration 5460/7380 Training loss: 4.9222 1.0974 sec/batch\n",
      "Epoch 15/20  Iteration 5461/7380 Training loss: 4.9223 1.1096 sec/batch\n",
      "Epoch 15/20  Iteration 5462/7380 Training loss: 4.9221 1.1007 sec/batch\n",
      "Epoch 15/20  Iteration 5463/7380 Training loss: 4.9218 1.1086 sec/batch\n",
      "Epoch 15/20  Iteration 5464/7380 Training loss: 4.9216 1.0998 sec/batch\n",
      "Epoch 15/20  Iteration 5465/7380 Training loss: 4.9219 1.0974 sec/batch\n",
      "Epoch 15/20  Iteration 5466/7380 Training loss: 4.9224 1.0951 sec/batch\n",
      "Epoch 15/20  Iteration 5467/7380 Training loss: 4.9224 1.1005 sec/batch\n",
      "Epoch 15/20  Iteration 5468/7380 Training loss: 4.9226 1.1007 sec/batch\n",
      "Epoch 15/20  Iteration 5469/7380 Training loss: 4.9227 1.1125 sec/batch\n",
      "Epoch 15/20  Iteration 5470/7380 Training loss: 4.9233 1.1114 sec/batch\n",
      "Epoch 15/20  Iteration 5471/7380 Training loss: 4.9235 1.1076 sec/batch\n",
      "Epoch 15/20  Iteration 5472/7380 Training loss: 4.9235 1.1043 sec/batch\n",
      "Epoch 15/20  Iteration 5473/7380 Training loss: 4.9231 1.1055 sec/batch\n",
      "Epoch 15/20  Iteration 5474/7380 Training loss: 4.9227 1.0971 sec/batch\n",
      "Epoch 15/20  Iteration 5475/7380 Training loss: 4.9225 1.1050 sec/batch\n",
      "Epoch 15/20  Iteration 5476/7380 Training loss: 4.9221 1.1239 sec/batch\n",
      "Epoch 15/20  Iteration 5477/7380 Training loss: 4.9221 1.0986 sec/batch\n",
      "Epoch 15/20  Iteration 5478/7380 Training loss: 4.9219 1.1085 sec/batch\n",
      "Epoch 15/20  Iteration 5479/7380 Training loss: 4.9217 1.1015 sec/batch\n",
      "Epoch 15/20  Iteration 5480/7380 Training loss: 4.9217 1.0937 sec/batch\n",
      "Epoch 15/20  Iteration 5481/7380 Training loss: 4.9219 1.1078 sec/batch\n",
      "Epoch 15/20  Iteration 5482/7380 Training loss: 4.9221 1.1033 sec/batch\n",
      "Epoch 15/20  Iteration 5483/7380 Training loss: 4.9223 1.1011 sec/batch\n",
      "Epoch 15/20  Iteration 5484/7380 Training loss: 4.9225 1.1003 sec/batch\n",
      "Epoch 15/20  Iteration 5485/7380 Training loss: 4.9231 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5486/7380 Training loss: 4.9234 1.0997 sec/batch\n",
      "Epoch 15/20  Iteration 5487/7380 Training loss: 4.9231 1.1000 sec/batch\n",
      "Epoch 15/20  Iteration 5488/7380 Training loss: 4.9228 1.1015 sec/batch\n",
      "Epoch 15/20  Iteration 5489/7380 Training loss: 4.9228 1.1178 sec/batch\n",
      "Epoch 15/20  Iteration 5490/7380 Training loss: 4.9227 1.1031 sec/batch\n",
      "Epoch 15/20  Iteration 5491/7380 Training loss: 4.9227 1.1037 sec/batch\n",
      "Epoch 15/20  Iteration 5492/7380 Training loss: 4.9233 1.1100 sec/batch\n",
      "Epoch 15/20  Iteration 5493/7380 Training loss: 4.9230 1.1012 sec/batch\n",
      "Epoch 15/20  Iteration 5494/7380 Training loss: 4.9227 1.0997 sec/batch\n",
      "Epoch 15/20  Iteration 5495/7380 Training loss: 4.9225 1.1044 sec/batch\n",
      "Epoch 15/20  Iteration 5496/7380 Training loss: 4.9223 1.0936 sec/batch\n",
      "Epoch 15/20  Iteration 5497/7380 Training loss: 4.9220 1.0986 sec/batch\n",
      "Epoch 15/20  Iteration 5498/7380 Training loss: 4.9221 1.1041 sec/batch\n",
      "Epoch 15/20  Iteration 5499/7380 Training loss: 4.9218 1.0963 sec/batch\n",
      "Epoch 15/20  Iteration 5500/7380 Training loss: 4.9220 1.1107 sec/batch\n",
      "Validation loss: 5.03663 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 5501/7380 Training loss: 4.9223 1.1022 sec/batch\n",
      "Epoch 15/20  Iteration 5502/7380 Training loss: 4.9225 1.1063 sec/batch\n",
      "Epoch 15/20  Iteration 5503/7380 Training loss: 4.9227 1.0968 sec/batch\n",
      "Epoch 15/20  Iteration 5504/7380 Training loss: 4.9225 1.1003 sec/batch\n",
      "Epoch 15/20  Iteration 5505/7380 Training loss: 4.9226 1.0962 sec/batch\n",
      "Epoch 15/20  Iteration 5506/7380 Training loss: 4.9224 1.1052 sec/batch\n",
      "Epoch 15/20  Iteration 5507/7380 Training loss: 4.9221 1.0986 sec/batch\n",
      "Epoch 15/20  Iteration 5508/7380 Training loss: 4.9219 1.1197 sec/batch\n",
      "Epoch 15/20  Iteration 5509/7380 Training loss: 4.9220 1.1027 sec/batch\n",
      "Epoch 15/20  Iteration 5510/7380 Training loss: 4.9219 1.1086 sec/batch\n",
      "Epoch 15/20  Iteration 5511/7380 Training loss: 4.9217 1.0981 sec/batch\n",
      "Epoch 15/20  Iteration 5512/7380 Training loss: 4.9216 1.1054 sec/batch\n",
      "Epoch 15/20  Iteration 5513/7380 Training loss: 4.9219 1.1000 sec/batch\n",
      "Epoch 15/20  Iteration 5514/7380 Training loss: 4.9218 1.0996 sec/batch\n",
      "Epoch 15/20  Iteration 5515/7380 Training loss: 4.9216 1.1079 sec/batch\n",
      "Epoch 15/20  Iteration 5516/7380 Training loss: 4.9216 1.1024 sec/batch\n",
      "Epoch 15/20  Iteration 5517/7380 Training loss: 4.9219 1.1639 sec/batch\n",
      "Epoch 15/20  Iteration 5518/7380 Training loss: 4.9216 1.1030 sec/batch\n",
      "Epoch 15/20  Iteration 5519/7380 Training loss: 4.9218 1.1040 sec/batch\n",
      "Epoch 15/20  Iteration 5520/7380 Training loss: 4.9215 1.0965 sec/batch\n",
      "Epoch 15/20  Iteration 5521/7380 Training loss: 4.9213 1.1132 sec/batch\n",
      "Epoch 15/20  Iteration 5522/7380 Training loss: 4.9212 1.1005 sec/batch\n",
      "Epoch 15/20  Iteration 5523/7380 Training loss: 4.9214 1.1001 sec/batch\n",
      "Epoch 15/20  Iteration 5524/7380 Training loss: 4.9214 1.1187 sec/batch\n",
      "Epoch 15/20  Iteration 5525/7380 Training loss: 4.9211 1.1002 sec/batch\n",
      "Epoch 15/20  Iteration 5526/7380 Training loss: 4.9210 1.1098 sec/batch\n",
      "Epoch 15/20  Iteration 5527/7380 Training loss: 4.9210 1.1134 sec/batch\n",
      "Epoch 15/20  Iteration 5528/7380 Training loss: 4.9210 1.0999 sec/batch\n",
      "Epoch 15/20  Iteration 5529/7380 Training loss: 4.9209 1.1323 sec/batch\n",
      "Epoch 15/20  Iteration 5530/7380 Training loss: 4.9205 1.0994 sec/batch\n",
      "Epoch 15/20  Iteration 5531/7380 Training loss: 4.9204 1.1219 sec/batch\n",
      "Epoch 15/20  Iteration 5532/7380 Training loss: 4.9201 1.1123 sec/batch\n",
      "Epoch 15/20  Iteration 5533/7380 Training loss: 4.9199 1.1082 sec/batch\n",
      "Epoch 15/20  Iteration 5534/7380 Training loss: 4.9201 1.1145 sec/batch\n",
      "Epoch 15/20  Iteration 5535/7380 Training loss: 4.9198 1.1073 sec/batch\n",
      "Epoch 16/20  Iteration 5536/7380 Training loss: 4.9608 1.1067 sec/batch\n",
      "Epoch 16/20  Iteration 5537/7380 Training loss: 4.9426 1.1015 sec/batch\n",
      "Epoch 16/20  Iteration 5538/7380 Training loss: 4.9198 1.0966 sec/batch\n",
      "Epoch 16/20  Iteration 5539/7380 Training loss: 4.9071 1.1019 sec/batch\n",
      "Epoch 16/20  Iteration 5540/7380 Training loss: 4.8997 1.1098 sec/batch\n",
      "Epoch 16/20  Iteration 5541/7380 Training loss: 4.9076 1.0968 sec/batch\n",
      "Epoch 16/20  Iteration 5542/7380 Training loss: 4.9275 1.1032 sec/batch\n",
      "Epoch 16/20  Iteration 5543/7380 Training loss: 4.9148 1.1009 sec/batch\n",
      "Epoch 16/20  Iteration 5544/7380 Training loss: 4.9124 1.1024 sec/batch\n",
      "Epoch 16/20  Iteration 5545/7380 Training loss: 4.9218 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5546/7380 Training loss: 4.9194 1.0987 sec/batch\n",
      "Epoch 16/20  Iteration 5547/7380 Training loss: 4.9099 1.1064 sec/batch\n",
      "Epoch 16/20  Iteration 5548/7380 Training loss: 4.9071 1.1078 sec/batch\n",
      "Epoch 16/20  Iteration 5549/7380 Training loss: 4.9133 1.1029 sec/batch\n",
      "Epoch 16/20  Iteration 5550/7380 Training loss: 4.9186 1.1039 sec/batch\n",
      "Validation loss: 5.03753 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5551/7380 Training loss: 4.9265 1.0996 sec/batch\n",
      "Epoch 16/20  Iteration 5552/7380 Training loss: 4.9220 1.1004 sec/batch\n",
      "Epoch 16/20  Iteration 5553/7380 Training loss: 4.9089 1.1125 sec/batch\n",
      "Epoch 16/20  Iteration 5554/7380 Training loss: 4.9037 1.1015 sec/batch\n",
      "Epoch 16/20  Iteration 5555/7380 Training loss: 4.9020 1.1113 sec/batch\n",
      "Epoch 16/20  Iteration 5556/7380 Training loss: 4.8990 1.1091 sec/batch\n",
      "Epoch 16/20  Iteration 5557/7380 Training loss: 4.9050 1.1019 sec/batch\n",
      "Epoch 16/20  Iteration 5558/7380 Training loss: 4.9025 1.1192 sec/batch\n",
      "Epoch 16/20  Iteration 5559/7380 Training loss: 4.9099 1.1004 sec/batch\n",
      "Epoch 16/20  Iteration 5560/7380 Training loss: 4.9123 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5561/7380 Training loss: 4.9129 1.1041 sec/batch\n",
      "Epoch 16/20  Iteration 5562/7380 Training loss: 4.9070 1.1109 sec/batch\n",
      "Epoch 16/20  Iteration 5563/7380 Training loss: 4.9126 1.1474 sec/batch\n",
      "Epoch 16/20  Iteration 5564/7380 Training loss: 4.9117 1.1234 sec/batch\n",
      "Epoch 16/20  Iteration 5565/7380 Training loss: 4.9109 1.1041 sec/batch\n",
      "Epoch 16/20  Iteration 5566/7380 Training loss: 4.9088 1.0914 sec/batch\n",
      "Epoch 16/20  Iteration 5567/7380 Training loss: 4.9091 1.1499 sec/batch\n",
      "Epoch 16/20  Iteration 5568/7380 Training loss: 4.9090 1.0986 sec/batch\n",
      "Epoch 16/20  Iteration 5569/7380 Training loss: 4.9106 1.1031 sec/batch\n",
      "Epoch 16/20  Iteration 5570/7380 Training loss: 4.9130 1.0986 sec/batch\n",
      "Epoch 16/20  Iteration 5571/7380 Training loss: 4.9115 1.1049 sec/batch\n",
      "Epoch 16/20  Iteration 5572/7380 Training loss: 4.9133 1.1038 sec/batch\n",
      "Epoch 16/20  Iteration 5573/7380 Training loss: 4.9119 1.1233 sec/batch\n",
      "Epoch 16/20  Iteration 5574/7380 Training loss: 4.9135 1.1027 sec/batch\n",
      "Epoch 16/20  Iteration 5575/7380 Training loss: 4.9132 1.1046 sec/batch\n",
      "Epoch 16/20  Iteration 5576/7380 Training loss: 4.9129 1.1027 sec/batch\n",
      "Epoch 16/20  Iteration 5577/7380 Training loss: 4.9116 1.1024 sec/batch\n",
      "Epoch 16/20  Iteration 5578/7380 Training loss: 4.9124 1.1035 sec/batch\n",
      "Epoch 16/20  Iteration 5579/7380 Training loss: 4.9129 1.0984 sec/batch\n",
      "Epoch 16/20  Iteration 5580/7380 Training loss: 4.9131 1.1046 sec/batch\n",
      "Epoch 16/20  Iteration 5581/7380 Training loss: 4.9106 1.1075 sec/batch\n",
      "Epoch 16/20  Iteration 5582/7380 Training loss: 4.9110 1.1051 sec/batch\n",
      "Epoch 16/20  Iteration 5583/7380 Training loss: 4.9079 1.1189 sec/batch\n",
      "Epoch 16/20  Iteration 5584/7380 Training loss: 4.9083 1.1137 sec/batch\n",
      "Epoch 16/20  Iteration 5585/7380 Training loss: 4.9069 1.1013 sec/batch\n",
      "Epoch 16/20  Iteration 5586/7380 Training loss: 4.9058 1.0961 sec/batch\n",
      "Epoch 16/20  Iteration 5587/7380 Training loss: 4.9044 1.1125 sec/batch\n",
      "Epoch 16/20  Iteration 5588/7380 Training loss: 4.9035 1.1203 sec/batch\n",
      "Epoch 16/20  Iteration 5589/7380 Training loss: 4.9035 1.1108 sec/batch\n",
      "Epoch 16/20  Iteration 5590/7380 Training loss: 4.9023 1.1104 sec/batch\n",
      "Epoch 16/20  Iteration 5591/7380 Training loss: 4.8998 1.1041 sec/batch\n",
      "Epoch 16/20  Iteration 5592/7380 Training loss: 4.8995 1.1033 sec/batch\n",
      "Epoch 16/20  Iteration 5593/7380 Training loss: 4.8997 1.1113 sec/batch\n",
      "Epoch 16/20  Iteration 5594/7380 Training loss: 4.8994 1.0963 sec/batch\n",
      "Epoch 16/20  Iteration 5595/7380 Training loss: 4.9010 1.0966 sec/batch\n",
      "Epoch 16/20  Iteration 5596/7380 Training loss: 4.8998 1.1006 sec/batch\n",
      "Epoch 16/20  Iteration 5597/7380 Training loss: 4.9005 1.1024 sec/batch\n",
      "Epoch 16/20  Iteration 5598/7380 Training loss: 4.8996 1.0990 sec/batch\n",
      "Epoch 16/20  Iteration 5599/7380 Training loss: 4.8974 1.1124 sec/batch\n",
      "Epoch 16/20  Iteration 5600/7380 Training loss: 4.8980 1.0986 sec/batch\n",
      "Validation loss: 5.04834 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5601/7380 Training loss: 4.9001 1.1150 sec/batch\n",
      "Epoch 16/20  Iteration 5602/7380 Training loss: 4.9004 1.1058 sec/batch\n",
      "Epoch 16/20  Iteration 5603/7380 Training loss: 4.9004 1.1117 sec/batch\n",
      "Epoch 16/20  Iteration 5604/7380 Training loss: 4.9019 1.1016 sec/batch\n",
      "Epoch 16/20  Iteration 5605/7380 Training loss: 4.9008 1.1090 sec/batch\n",
      "Epoch 16/20  Iteration 5606/7380 Training loss: 4.8995 1.1012 sec/batch\n",
      "Epoch 16/20  Iteration 5607/7380 Training loss: 4.9006 1.1237 sec/batch\n",
      "Epoch 16/20  Iteration 5608/7380 Training loss: 4.8992 1.0988 sec/batch\n",
      "Epoch 16/20  Iteration 5609/7380 Training loss: 4.9004 1.1339 sec/batch\n",
      "Epoch 16/20  Iteration 5610/7380 Training loss: 4.9016 1.1037 sec/batch\n",
      "Epoch 16/20  Iteration 5611/7380 Training loss: 4.9011 1.1092 sec/batch\n",
      "Epoch 16/20  Iteration 5612/7380 Training loss: 4.9011 1.1031 sec/batch\n",
      "Epoch 16/20  Iteration 5613/7380 Training loss: 4.9009 1.1068 sec/batch\n",
      "Epoch 16/20  Iteration 5614/7380 Training loss: 4.9016 1.1010 sec/batch\n",
      "Epoch 16/20  Iteration 5615/7380 Training loss: 4.9021 1.1027 sec/batch\n",
      "Epoch 16/20  Iteration 5616/7380 Training loss: 4.9010 1.0951 sec/batch\n",
      "Epoch 16/20  Iteration 5617/7380 Training loss: 4.9007 1.0936 sec/batch\n",
      "Epoch 16/20  Iteration 5618/7380 Training loss: 4.8993 1.0945 sec/batch\n",
      "Epoch 16/20  Iteration 5619/7380 Training loss: 4.8984 1.0942 sec/batch\n",
      "Epoch 16/20  Iteration 5620/7380 Training loss: 4.8984 1.0924 sec/batch\n",
      "Epoch 16/20  Iteration 5621/7380 Training loss: 4.8979 1.1019 sec/batch\n",
      "Epoch 16/20  Iteration 5622/7380 Training loss: 4.8979 1.0991 sec/batch\n",
      "Epoch 16/20  Iteration 5623/7380 Training loss: 4.8981 1.1080 sec/batch\n",
      "Epoch 16/20  Iteration 5624/7380 Training loss: 4.8962 1.1052 sec/batch\n",
      "Epoch 16/20  Iteration 5625/7380 Training loss: 4.8958 1.1046 sec/batch\n",
      "Epoch 16/20  Iteration 5626/7380 Training loss: 4.8965 1.1112 sec/batch\n",
      "Epoch 16/20  Iteration 5627/7380 Training loss: 4.8982 1.1274 sec/batch\n",
      "Epoch 16/20  Iteration 5628/7380 Training loss: 4.8977 1.1003 sec/batch\n",
      "Epoch 16/20  Iteration 5629/7380 Training loss: 4.8969 1.1086 sec/batch\n",
      "Epoch 16/20  Iteration 5630/7380 Training loss: 4.8964 1.1012 sec/batch\n",
      "Epoch 16/20  Iteration 5631/7380 Training loss: 4.8964 1.1173 sec/batch\n",
      "Epoch 16/20  Iteration 5632/7380 Training loss: 4.8962 1.1044 sec/batch\n",
      "Epoch 16/20  Iteration 5633/7380 Training loss: 4.8968 1.0995 sec/batch\n",
      "Epoch 16/20  Iteration 5634/7380 Training loss: 4.8967 1.1085 sec/batch\n",
      "Epoch 16/20  Iteration 5635/7380 Training loss: 4.8963 1.1024 sec/batch\n",
      "Epoch 16/20  Iteration 5636/7380 Training loss: 4.8977 1.1011 sec/batch\n",
      "Epoch 16/20  Iteration 5637/7380 Training loss: 4.8982 1.0985 sec/batch\n",
      "Epoch 16/20  Iteration 5638/7380 Training loss: 4.8985 1.1067 sec/batch\n",
      "Epoch 16/20  Iteration 5639/7380 Training loss: 4.8980 1.1056 sec/batch\n",
      "Epoch 16/20  Iteration 5640/7380 Training loss: 4.8983 1.1015 sec/batch\n",
      "Epoch 16/20  Iteration 5641/7380 Training loss: 4.8981 1.0999 sec/batch\n",
      "Epoch 16/20  Iteration 5642/7380 Training loss: 4.8978 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5643/7380 Training loss: 4.8977 1.0941 sec/batch\n",
      "Epoch 16/20  Iteration 5644/7380 Training loss: 4.8960 1.1178 sec/batch\n",
      "Epoch 16/20  Iteration 5645/7380 Training loss: 4.8958 1.1103 sec/batch\n",
      "Epoch 16/20  Iteration 5646/7380 Training loss: 4.8954 1.1070 sec/batch\n",
      "Epoch 16/20  Iteration 5647/7380 Training loss: 4.8952 1.1047 sec/batch\n",
      "Epoch 16/20  Iteration 5648/7380 Training loss: 4.8945 1.0916 sec/batch\n",
      "Epoch 16/20  Iteration 5649/7380 Training loss: 4.8946 1.1080 sec/batch\n",
      "Epoch 16/20  Iteration 5650/7380 Training loss: 4.8945 1.1007 sec/batch\n",
      "Validation loss: 5.02022 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5651/7380 Training loss: 4.8951 1.1114 sec/batch\n",
      "Epoch 16/20  Iteration 5652/7380 Training loss: 4.8942 1.1094 sec/batch\n",
      "Epoch 16/20  Iteration 5653/7380 Training loss: 4.8940 1.1061 sec/batch\n",
      "Epoch 16/20  Iteration 5654/7380 Training loss: 4.8946 1.1040 sec/batch\n",
      "Epoch 16/20  Iteration 5655/7380 Training loss: 4.8942 1.1109 sec/batch\n",
      "Epoch 16/20  Iteration 5656/7380 Training loss: 4.8946 1.0977 sec/batch\n",
      "Epoch 16/20  Iteration 5657/7380 Training loss: 4.8957 1.1133 sec/batch\n",
      "Epoch 16/20  Iteration 5658/7380 Training loss: 4.8957 1.1404 sec/batch\n",
      "Epoch 16/20  Iteration 5659/7380 Training loss: 4.8955 1.1059 sec/batch\n",
      "Epoch 16/20  Iteration 5660/7380 Training loss: 4.8957 1.0974 sec/batch\n",
      "Epoch 16/20  Iteration 5661/7380 Training loss: 4.8948 1.1030 sec/batch\n",
      "Epoch 16/20  Iteration 5662/7380 Training loss: 4.8934 1.1052 sec/batch\n",
      "Epoch 16/20  Iteration 5663/7380 Training loss: 4.8932 1.0990 sec/batch\n",
      "Epoch 16/20  Iteration 5664/7380 Training loss: 4.8926 1.1076 sec/batch\n",
      "Epoch 16/20  Iteration 5665/7380 Training loss: 4.8926 1.1107 sec/batch\n",
      "Epoch 16/20  Iteration 5666/7380 Training loss: 4.8932 1.1038 sec/batch\n",
      "Epoch 16/20  Iteration 5667/7380 Training loss: 4.8932 1.1011 sec/batch\n",
      "Epoch 16/20  Iteration 5668/7380 Training loss: 4.8932 1.1240 sec/batch\n",
      "Epoch 16/20  Iteration 5669/7380 Training loss: 4.8923 1.1067 sec/batch\n",
      "Epoch 16/20  Iteration 5670/7380 Training loss: 4.8919 1.0968 sec/batch\n",
      "Epoch 16/20  Iteration 5671/7380 Training loss: 4.8920 1.1126 sec/batch\n",
      "Epoch 16/20  Iteration 5672/7380 Training loss: 4.8915 1.1069 sec/batch\n",
      "Epoch 16/20  Iteration 5673/7380 Training loss: 4.8915 1.1109 sec/batch\n",
      "Epoch 16/20  Iteration 5674/7380 Training loss: 4.8908 1.0967 sec/batch\n",
      "Epoch 16/20  Iteration 5675/7380 Training loss: 4.8901 1.1254 sec/batch\n",
      "Epoch 16/20  Iteration 5676/7380 Training loss: 4.8913 1.1039 sec/batch\n",
      "Epoch 16/20  Iteration 5677/7380 Training loss: 4.8914 1.1007 sec/batch\n",
      "Epoch 16/20  Iteration 5678/7380 Training loss: 4.8919 1.1010 sec/batch\n",
      "Epoch 16/20  Iteration 5679/7380 Training loss: 4.8918 1.1137 sec/batch\n",
      "Epoch 16/20  Iteration 5680/7380 Training loss: 4.8919 1.1012 sec/batch\n",
      "Epoch 16/20  Iteration 5681/7380 Training loss: 4.8918 1.1126 sec/batch\n",
      "Epoch 16/20  Iteration 5682/7380 Training loss: 4.8919 1.1176 sec/batch\n",
      "Epoch 16/20  Iteration 5683/7380 Training loss: 4.8920 1.1059 sec/batch\n",
      "Epoch 16/20  Iteration 5684/7380 Training loss: 4.8913 1.1264 sec/batch\n",
      "Epoch 16/20  Iteration 5685/7380 Training loss: 4.8917 1.1155 sec/batch\n",
      "Epoch 16/20  Iteration 5686/7380 Training loss: 4.8911 1.0993 sec/batch\n",
      "Epoch 16/20  Iteration 5687/7380 Training loss: 4.8916 1.1152 sec/batch\n",
      "Epoch 16/20  Iteration 5688/7380 Training loss: 4.8917 1.1176 sec/batch\n",
      "Epoch 16/20  Iteration 5689/7380 Training loss: 4.8924 1.1091 sec/batch\n",
      "Epoch 16/20  Iteration 5690/7380 Training loss: 4.8922 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5691/7380 Training loss: 4.8918 1.1006 sec/batch\n",
      "Epoch 16/20  Iteration 5692/7380 Training loss: 4.8913 1.0947 sec/batch\n",
      "Epoch 16/20  Iteration 5693/7380 Training loss: 4.8919 1.1227 sec/batch\n",
      "Epoch 16/20  Iteration 5694/7380 Training loss: 4.8916 1.1087 sec/batch\n",
      "Epoch 16/20  Iteration 5695/7380 Training loss: 4.8916 1.1000 sec/batch\n",
      "Epoch 16/20  Iteration 5696/7380 Training loss: 4.8924 1.1033 sec/batch\n",
      "Epoch 16/20  Iteration 5697/7380 Training loss: 4.8930 1.1037 sec/batch\n",
      "Epoch 16/20  Iteration 5698/7380 Training loss: 4.8934 1.1050 sec/batch\n",
      "Epoch 16/20  Iteration 5699/7380 Training loss: 4.8937 1.1004 sec/batch\n",
      "Epoch 16/20  Iteration 5700/7380 Training loss: 4.8939 1.1036 sec/batch\n",
      "Validation loss: 5.01875 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5701/7380 Training loss: 4.8951 1.1146 sec/batch\n",
      "Epoch 16/20  Iteration 5702/7380 Training loss: 4.8953 1.1156 sec/batch\n",
      "Epoch 16/20  Iteration 5703/7380 Training loss: 4.8956 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5704/7380 Training loss: 4.8950 1.1014 sec/batch\n",
      "Epoch 16/20  Iteration 5705/7380 Training loss: 4.8945 1.1042 sec/batch\n",
      "Epoch 16/20  Iteration 5706/7380 Training loss: 4.8946 1.0975 sec/batch\n",
      "Epoch 16/20  Iteration 5707/7380 Training loss: 4.8952 1.1019 sec/batch\n",
      "Epoch 16/20  Iteration 5708/7380 Training loss: 4.8953 1.1082 sec/batch\n",
      "Epoch 16/20  Iteration 5709/7380 Training loss: 4.8950 1.1061 sec/batch\n",
      "Epoch 16/20  Iteration 5710/7380 Training loss: 4.8948 1.1145 sec/batch\n",
      "Epoch 16/20  Iteration 5711/7380 Training loss: 4.8944 1.1052 sec/batch\n",
      "Epoch 16/20  Iteration 5712/7380 Training loss: 4.8944 1.1624 sec/batch\n",
      "Epoch 16/20  Iteration 5713/7380 Training loss: 4.8948 1.1054 sec/batch\n",
      "Epoch 16/20  Iteration 5714/7380 Training loss: 4.8941 1.1356 sec/batch\n",
      "Epoch 16/20  Iteration 5715/7380 Training loss: 4.8937 1.1154 sec/batch\n",
      "Epoch 16/20  Iteration 5716/7380 Training loss: 4.8934 1.0977 sec/batch\n",
      "Epoch 16/20  Iteration 5717/7380 Training loss: 4.8936 1.1079 sec/batch\n",
      "Epoch 16/20  Iteration 5718/7380 Training loss: 4.8944 1.1021 sec/batch\n",
      "Epoch 16/20  Iteration 5719/7380 Training loss: 4.8941 1.1188 sec/batch\n",
      "Epoch 16/20  Iteration 5720/7380 Training loss: 4.8941 1.1091 sec/batch\n",
      "Epoch 16/20  Iteration 5721/7380 Training loss: 4.8941 1.1150 sec/batch\n",
      "Epoch 16/20  Iteration 5722/7380 Training loss: 4.8938 1.1018 sec/batch\n",
      "Epoch 16/20  Iteration 5723/7380 Training loss: 4.8935 1.1048 sec/batch\n",
      "Epoch 16/20  Iteration 5724/7380 Training loss: 4.8934 1.1018 sec/batch\n",
      "Epoch 16/20  Iteration 5725/7380 Training loss: 4.8935 1.1006 sec/batch\n",
      "Epoch 16/20  Iteration 5726/7380 Training loss: 4.8927 1.1002 sec/batch\n",
      "Epoch 16/20  Iteration 5727/7380 Training loss: 4.8931 1.1057 sec/batch\n",
      "Epoch 16/20  Iteration 5728/7380 Training loss: 4.8936 1.1028 sec/batch\n",
      "Epoch 16/20  Iteration 5729/7380 Training loss: 4.8930 1.1049 sec/batch\n",
      "Epoch 16/20  Iteration 5730/7380 Training loss: 4.8935 1.1082 sec/batch\n",
      "Epoch 16/20  Iteration 5731/7380 Training loss: 4.8937 1.1158 sec/batch\n",
      "Epoch 16/20  Iteration 5732/7380 Training loss: 4.8940 1.1098 sec/batch\n",
      "Epoch 16/20  Iteration 5733/7380 Training loss: 4.8945 1.1059 sec/batch\n",
      "Epoch 16/20  Iteration 5734/7380 Training loss: 4.8943 1.1059 sec/batch\n",
      "Epoch 16/20  Iteration 5735/7380 Training loss: 4.8945 1.1038 sec/batch\n",
      "Epoch 16/20  Iteration 5736/7380 Training loss: 4.8949 1.0917 sec/batch\n",
      "Epoch 16/20  Iteration 5737/7380 Training loss: 4.8953 1.1033 sec/batch\n",
      "Epoch 16/20  Iteration 5738/7380 Training loss: 4.8960 1.1034 sec/batch\n",
      "Epoch 16/20  Iteration 5739/7380 Training loss: 4.8961 1.1017 sec/batch\n",
      "Epoch 16/20  Iteration 5740/7380 Training loss: 4.8964 1.1112 sec/batch\n",
      "Epoch 16/20  Iteration 5741/7380 Training loss: 4.8967 1.1018 sec/batch\n",
      "Epoch 16/20  Iteration 5742/7380 Training loss: 4.8960 1.1001 sec/batch\n",
      "Epoch 16/20  Iteration 5743/7380 Training loss: 4.8955 1.1050 sec/batch\n",
      "Epoch 16/20  Iteration 5744/7380 Training loss: 4.8952 1.1049 sec/batch\n",
      "Epoch 16/20  Iteration 5745/7380 Training loss: 4.8946 1.1116 sec/batch\n",
      "Epoch 16/20  Iteration 5746/7380 Training loss: 4.8945 1.1094 sec/batch\n",
      "Epoch 16/20  Iteration 5747/7380 Training loss: 4.8944 1.1129 sec/batch\n",
      "Epoch 16/20  Iteration 5748/7380 Training loss: 4.8945 1.1033 sec/batch\n",
      "Epoch 16/20  Iteration 5749/7380 Training loss: 4.8945 1.1161 sec/batch\n",
      "Epoch 16/20  Iteration 5750/7380 Training loss: 4.8941 1.1051 sec/batch\n",
      "Validation loss: 5.03653 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5751/7380 Training loss: 4.8943 1.0971 sec/batch\n",
      "Epoch 16/20  Iteration 5752/7380 Training loss: 4.8939 1.0977 sec/batch\n",
      "Epoch 16/20  Iteration 5753/7380 Training loss: 4.8943 1.1151 sec/batch\n",
      "Epoch 16/20  Iteration 5754/7380 Training loss: 4.8945 1.1031 sec/batch\n",
      "Epoch 16/20  Iteration 5755/7380 Training loss: 4.8946 1.1103 sec/batch\n",
      "Epoch 16/20  Iteration 5756/7380 Training loss: 4.8942 1.1123 sec/batch\n",
      "Epoch 16/20  Iteration 5757/7380 Training loss: 4.8946 1.1090 sec/batch\n",
      "Epoch 16/20  Iteration 5758/7380 Training loss: 4.8947 1.1224 sec/batch\n",
      "Epoch 16/20  Iteration 5759/7380 Training loss: 4.8942 1.1061 sec/batch\n",
      "Epoch 16/20  Iteration 5760/7380 Training loss: 4.8944 1.1153 sec/batch\n",
      "Epoch 16/20  Iteration 5761/7380 Training loss: 4.8941 1.1058 sec/batch\n",
      "Epoch 16/20  Iteration 5762/7380 Training loss: 4.8941 1.1103 sec/batch\n",
      "Epoch 16/20  Iteration 5763/7380 Training loss: 4.8935 1.1062 sec/batch\n",
      "Epoch 16/20  Iteration 5764/7380 Training loss: 4.8935 1.1065 sec/batch\n",
      "Epoch 16/20  Iteration 5765/7380 Training loss: 4.8940 1.1088 sec/batch\n",
      "Epoch 16/20  Iteration 5766/7380 Training loss: 4.8936 1.1005 sec/batch\n",
      "Epoch 16/20  Iteration 5767/7380 Training loss: 4.8934 1.1050 sec/batch\n",
      "Epoch 16/20  Iteration 5768/7380 Training loss: 4.8928 1.1094 sec/batch\n",
      "Epoch 16/20  Iteration 5769/7380 Training loss: 4.8927 1.1066 sec/batch\n",
      "Epoch 16/20  Iteration 5770/7380 Training loss: 4.8924 1.1048 sec/batch\n",
      "Epoch 16/20  Iteration 5771/7380 Training loss: 4.8919 1.1129 sec/batch\n",
      "Epoch 16/20  Iteration 5772/7380 Training loss: 4.8916 1.1048 sec/batch\n",
      "Epoch 16/20  Iteration 5773/7380 Training loss: 4.8909 1.1062 sec/batch\n",
      "Epoch 16/20  Iteration 5774/7380 Training loss: 4.8899 1.1067 sec/batch\n",
      "Epoch 16/20  Iteration 5775/7380 Training loss: 4.8894 1.1003 sec/batch\n",
      "Epoch 16/20  Iteration 5776/7380 Training loss: 4.8896 1.0973 sec/batch\n",
      "Epoch 16/20  Iteration 5777/7380 Training loss: 4.8900 1.1062 sec/batch\n",
      "Epoch 16/20  Iteration 5778/7380 Training loss: 4.8897 1.1075 sec/batch\n",
      "Epoch 16/20  Iteration 5779/7380 Training loss: 4.8897 1.1065 sec/batch\n",
      "Epoch 16/20  Iteration 5780/7380 Training loss: 4.8899 1.1063 sec/batch\n",
      "Epoch 16/20  Iteration 5781/7380 Training loss: 4.8901 1.1131 sec/batch\n",
      "Epoch 16/20  Iteration 5782/7380 Training loss: 4.8900 1.1053 sec/batch\n",
      "Epoch 16/20  Iteration 5783/7380 Training loss: 4.8900 1.1154 sec/batch\n",
      "Epoch 16/20  Iteration 5784/7380 Training loss: 4.8899 1.1126 sec/batch\n",
      "Epoch 16/20  Iteration 5785/7380 Training loss: 4.8898 1.1026 sec/batch\n",
      "Epoch 16/20  Iteration 5786/7380 Training loss: 4.8895 1.1125 sec/batch\n",
      "Epoch 16/20  Iteration 5787/7380 Training loss: 4.8895 1.1020 sec/batch\n",
      "Epoch 16/20  Iteration 5788/7380 Training loss: 4.8894 1.1116 sec/batch\n",
      "Epoch 16/20  Iteration 5789/7380 Training loss: 4.8889 1.1092 sec/batch\n",
      "Epoch 16/20  Iteration 5790/7380 Training loss: 4.8891 1.1019 sec/batch\n",
      "Epoch 16/20  Iteration 5791/7380 Training loss: 4.8890 1.1042 sec/batch\n",
      "Epoch 16/20  Iteration 5792/7380 Training loss: 4.8893 1.1043 sec/batch\n",
      "Epoch 16/20  Iteration 5793/7380 Training loss: 4.8890 1.0986 sec/batch\n",
      "Epoch 16/20  Iteration 5794/7380 Training loss: 4.8893 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5795/7380 Training loss: 4.8886 1.1153 sec/batch\n",
      "Epoch 16/20  Iteration 5796/7380 Training loss: 4.8884 1.0967 sec/batch\n",
      "Epoch 16/20  Iteration 5797/7380 Training loss: 4.8885 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5798/7380 Training loss: 4.8887 1.1044 sec/batch\n",
      "Epoch 16/20  Iteration 5799/7380 Training loss: 4.8884 1.1075 sec/batch\n",
      "Epoch 16/20  Iteration 5800/7380 Training loss: 4.8881 1.0961 sec/batch\n",
      "Validation loss: 5.01343 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5801/7380 Training loss: 4.8884 1.1158 sec/batch\n",
      "Epoch 16/20  Iteration 5802/7380 Training loss: 4.8885 1.1046 sec/batch\n",
      "Epoch 16/20  Iteration 5803/7380 Training loss: 4.8886 1.1304 sec/batch\n",
      "Epoch 16/20  Iteration 5804/7380 Training loss: 4.8881 1.1051 sec/batch\n",
      "Epoch 16/20  Iteration 5805/7380 Training loss: 4.8884 1.1047 sec/batch\n",
      "Epoch 16/20  Iteration 5806/7380 Training loss: 4.8886 1.1433 sec/batch\n",
      "Epoch 16/20  Iteration 5807/7380 Training loss: 4.8880 1.1021 sec/batch\n",
      "Epoch 16/20  Iteration 5808/7380 Training loss: 4.8883 1.1129 sec/batch\n",
      "Epoch 16/20  Iteration 5809/7380 Training loss: 4.8886 1.1104 sec/batch\n",
      "Epoch 16/20  Iteration 5810/7380 Training loss: 4.8885 1.1104 sec/batch\n",
      "Epoch 16/20  Iteration 5811/7380 Training loss: 4.8882 1.1173 sec/batch\n",
      "Epoch 16/20  Iteration 5812/7380 Training loss: 4.8889 1.1014 sec/batch\n",
      "Epoch 16/20  Iteration 5813/7380 Training loss: 4.8888 1.1054 sec/batch\n",
      "Epoch 16/20  Iteration 5814/7380 Training loss: 4.8890 1.1029 sec/batch\n",
      "Epoch 16/20  Iteration 5815/7380 Training loss: 4.8889 1.0939 sec/batch\n",
      "Epoch 16/20  Iteration 5816/7380 Training loss: 4.8891 1.1070 sec/batch\n",
      "Epoch 16/20  Iteration 5817/7380 Training loss: 4.8892 1.1056 sec/batch\n",
      "Epoch 16/20  Iteration 5818/7380 Training loss: 4.8893 1.0993 sec/batch\n",
      "Epoch 16/20  Iteration 5819/7380 Training loss: 4.8893 1.0995 sec/batch\n",
      "Epoch 16/20  Iteration 5820/7380 Training loss: 4.8896 1.1037 sec/batch\n",
      "Epoch 16/20  Iteration 5821/7380 Training loss: 4.8892 1.1036 sec/batch\n",
      "Epoch 16/20  Iteration 5822/7380 Training loss: 4.8888 1.1079 sec/batch\n",
      "Epoch 16/20  Iteration 5823/7380 Training loss: 4.8887 1.1150 sec/batch\n",
      "Epoch 16/20  Iteration 5824/7380 Training loss: 4.8885 1.1378 sec/batch\n",
      "Epoch 16/20  Iteration 5825/7380 Training loss: 4.8882 1.1121 sec/batch\n",
      "Epoch 16/20  Iteration 5826/7380 Training loss: 4.8878 1.1192 sec/batch\n",
      "Epoch 16/20  Iteration 5827/7380 Training loss: 4.8876 1.1156 sec/batch\n",
      "Epoch 16/20  Iteration 5828/7380 Training loss: 4.8875 1.1100 sec/batch\n",
      "Epoch 16/20  Iteration 5829/7380 Training loss: 4.8876 1.1192 sec/batch\n",
      "Epoch 16/20  Iteration 5830/7380 Training loss: 4.8877 1.1142 sec/batch\n",
      "Epoch 16/20  Iteration 5831/7380 Training loss: 4.8875 1.1283 sec/batch\n",
      "Epoch 16/20  Iteration 5832/7380 Training loss: 4.8872 1.1147 sec/batch\n",
      "Epoch 16/20  Iteration 5833/7380 Training loss: 4.8870 1.1181 sec/batch\n",
      "Epoch 16/20  Iteration 5834/7380 Training loss: 4.8872 1.1091 sec/batch\n",
      "Epoch 16/20  Iteration 5835/7380 Training loss: 4.8878 1.1149 sec/batch\n",
      "Epoch 16/20  Iteration 5836/7380 Training loss: 4.8878 1.1085 sec/batch\n",
      "Epoch 16/20  Iteration 5837/7380 Training loss: 4.8879 1.1183 sec/batch\n",
      "Epoch 16/20  Iteration 5838/7380 Training loss: 4.8879 1.1054 sec/batch\n",
      "Epoch 16/20  Iteration 5839/7380 Training loss: 4.8885 1.1035 sec/batch\n",
      "Epoch 16/20  Iteration 5840/7380 Training loss: 4.8886 1.1193 sec/batch\n",
      "Epoch 16/20  Iteration 5841/7380 Training loss: 4.8886 1.1070 sec/batch\n",
      "Epoch 16/20  Iteration 5842/7380 Training loss: 4.8883 1.1038 sec/batch\n",
      "Epoch 16/20  Iteration 5843/7380 Training loss: 4.8881 1.1238 sec/batch\n",
      "Epoch 16/20  Iteration 5844/7380 Training loss: 4.8878 1.1120 sec/batch\n",
      "Epoch 16/20  Iteration 5845/7380 Training loss: 4.8875 1.1135 sec/batch\n",
      "Epoch 16/20  Iteration 5846/7380 Training loss: 4.8873 1.1059 sec/batch\n",
      "Epoch 16/20  Iteration 5847/7380 Training loss: 4.8872 1.1157 sec/batch\n",
      "Epoch 16/20  Iteration 5848/7380 Training loss: 4.8869 1.1084 sec/batch\n",
      "Epoch 16/20  Iteration 5849/7380 Training loss: 4.8868 1.1012 sec/batch\n",
      "Epoch 16/20  Iteration 5850/7380 Training loss: 4.8868 1.1035 sec/batch\n",
      "Validation loss: 5.02624 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5851/7380 Training loss: 4.8875 1.1137 sec/batch\n",
      "Epoch 16/20  Iteration 5852/7380 Training loss: 4.8879 1.1053 sec/batch\n",
      "Epoch 16/20  Iteration 5853/7380 Training loss: 4.8881 1.1123 sec/batch\n",
      "Epoch 16/20  Iteration 5854/7380 Training loss: 4.8886 1.0977 sec/batch\n",
      "Epoch 16/20  Iteration 5855/7380 Training loss: 4.8889 1.1146 sec/batch\n",
      "Epoch 16/20  Iteration 5856/7380 Training loss: 4.8886 1.1156 sec/batch\n",
      "Epoch 16/20  Iteration 5857/7380 Training loss: 4.8884 1.1043 sec/batch\n",
      "Epoch 16/20  Iteration 5858/7380 Training loss: 4.8884 1.1035 sec/batch\n",
      "Epoch 16/20  Iteration 5859/7380 Training loss: 4.8884 1.1062 sec/batch\n",
      "Epoch 16/20  Iteration 5860/7380 Training loss: 4.8884 1.1051 sec/batch\n",
      "Epoch 16/20  Iteration 5861/7380 Training loss: 4.8888 1.1089 sec/batch\n",
      "Epoch 16/20  Iteration 5862/7380 Training loss: 4.8886 1.1169 sec/batch\n",
      "Epoch 16/20  Iteration 5863/7380 Training loss: 4.8884 1.1072 sec/batch\n",
      "Epoch 16/20  Iteration 5864/7380 Training loss: 4.8882 1.1143 sec/batch\n",
      "Epoch 16/20  Iteration 5865/7380 Training loss: 4.8881 1.1107 sec/batch\n",
      "Epoch 16/20  Iteration 5866/7380 Training loss: 4.8876 1.1101 sec/batch\n",
      "Epoch 16/20  Iteration 5867/7380 Training loss: 4.8878 1.1033 sec/batch\n",
      "Epoch 16/20  Iteration 5868/7380 Training loss: 4.8875 1.1035 sec/batch\n",
      "Epoch 16/20  Iteration 5869/7380 Training loss: 4.8877 1.1108 sec/batch\n",
      "Epoch 16/20  Iteration 5870/7380 Training loss: 4.8878 1.1176 sec/batch\n",
      "Epoch 16/20  Iteration 5871/7380 Training loss: 4.8879 1.0987 sec/batch\n",
      "Epoch 16/20  Iteration 5872/7380 Training loss: 4.8881 1.1049 sec/batch\n",
      "Epoch 16/20  Iteration 5873/7380 Training loss: 4.8880 1.1635 sec/batch\n",
      "Epoch 16/20  Iteration 5874/7380 Training loss: 4.8881 1.0987 sec/batch\n",
      "Epoch 16/20  Iteration 5875/7380 Training loss: 4.8877 1.1088 sec/batch\n",
      "Epoch 16/20  Iteration 5876/7380 Training loss: 4.8874 1.1032 sec/batch\n",
      "Epoch 16/20  Iteration 5877/7380 Training loss: 4.8873 1.0965 sec/batch\n",
      "Epoch 16/20  Iteration 5878/7380 Training loss: 4.8873 1.1292 sec/batch\n",
      "Epoch 16/20  Iteration 5879/7380 Training loss: 4.8873 1.1082 sec/batch\n",
      "Epoch 16/20  Iteration 5880/7380 Training loss: 4.8872 1.1038 sec/batch\n",
      "Epoch 16/20  Iteration 5881/7380 Training loss: 4.8871 1.1199 sec/batch\n",
      "Epoch 16/20  Iteration 5882/7380 Training loss: 4.8873 1.1154 sec/batch\n",
      "Epoch 16/20  Iteration 5883/7380 Training loss: 4.8875 1.1098 sec/batch\n",
      "Epoch 16/20  Iteration 5884/7380 Training loss: 4.8873 1.1110 sec/batch\n",
      "Epoch 16/20  Iteration 5885/7380 Training loss: 4.8873 1.1181 sec/batch\n",
      "Epoch 16/20  Iteration 5886/7380 Training loss: 4.8877 1.1047 sec/batch\n",
      "Epoch 16/20  Iteration 5887/7380 Training loss: 4.8874 1.1078 sec/batch\n",
      "Epoch 16/20  Iteration 5888/7380 Training loss: 4.8875 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5889/7380 Training loss: 4.8872 1.1083 sec/batch\n",
      "Epoch 16/20  Iteration 5890/7380 Training loss: 4.8870 1.1109 sec/batch\n",
      "Epoch 16/20  Iteration 5891/7380 Training loss: 4.8870 1.1144 sec/batch\n",
      "Epoch 16/20  Iteration 5892/7380 Training loss: 4.8873 1.1045 sec/batch\n",
      "Epoch 16/20  Iteration 5893/7380 Training loss: 4.8873 1.1055 sec/batch\n",
      "Epoch 16/20  Iteration 5894/7380 Training loss: 4.8870 1.1174 sec/batch\n",
      "Epoch 16/20  Iteration 5895/7380 Training loss: 4.8868 1.1332 sec/batch\n",
      "Epoch 16/20  Iteration 5896/7380 Training loss: 4.8868 1.1128 sec/batch\n",
      "Epoch 16/20  Iteration 5897/7380 Training loss: 4.8868 1.1192 sec/batch\n",
      "Epoch 16/20  Iteration 5898/7380 Training loss: 4.8866 1.1164 sec/batch\n",
      "Epoch 16/20  Iteration 5899/7380 Training loss: 4.8865 1.1157 sec/batch\n",
      "Epoch 16/20  Iteration 5900/7380 Training loss: 4.8864 1.1194 sec/batch\n",
      "Validation loss: 5.02694 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 5901/7380 Training loss: 4.8866 1.1113 sec/batch\n",
      "Epoch 16/20  Iteration 5902/7380 Training loss: 4.8864 1.1204 sec/batch\n",
      "Epoch 16/20  Iteration 5903/7380 Training loss: 4.8865 1.1138 sec/batch\n",
      "Epoch 16/20  Iteration 5904/7380 Training loss: 4.8863 1.1073 sec/batch\n",
      "Epoch 17/20  Iteration 5905/7380 Training loss: 4.9047 1.0997 sec/batch\n",
      "Epoch 17/20  Iteration 5906/7380 Training loss: 4.9027 1.1136 sec/batch\n",
      "Epoch 17/20  Iteration 5907/7380 Training loss: 4.8694 1.1053 sec/batch\n",
      "Epoch 17/20  Iteration 5908/7380 Training loss: 4.8663 1.1155 sec/batch\n",
      "Epoch 17/20  Iteration 5909/7380 Training loss: 4.8594 1.1097 sec/batch\n",
      "Epoch 17/20  Iteration 5910/7380 Training loss: 4.8680 1.1110 sec/batch\n",
      "Epoch 17/20  Iteration 5911/7380 Training loss: 4.8879 1.0972 sec/batch\n",
      "Epoch 17/20  Iteration 5912/7380 Training loss: 4.8727 1.1104 sec/batch\n",
      "Epoch 17/20  Iteration 5913/7380 Training loss: 4.8721 1.1315 sec/batch\n",
      "Epoch 17/20  Iteration 5914/7380 Training loss: 4.8820 1.1092 sec/batch\n",
      "Epoch 17/20  Iteration 5915/7380 Training loss: 4.8783 1.1092 sec/batch\n",
      "Epoch 17/20  Iteration 5916/7380 Training loss: 4.8701 1.1136 sec/batch\n",
      "Epoch 17/20  Iteration 5917/7380 Training loss: 4.8698 1.1059 sec/batch\n",
      "Epoch 17/20  Iteration 5918/7380 Training loss: 4.8772 1.1050 sec/batch\n",
      "Epoch 17/20  Iteration 5919/7380 Training loss: 4.8809 1.1039 sec/batch\n",
      "Epoch 17/20  Iteration 5920/7380 Training loss: 4.8819 1.1052 sec/batch\n",
      "Epoch 17/20  Iteration 5921/7380 Training loss: 4.8800 1.1243 sec/batch\n",
      "Epoch 17/20  Iteration 5922/7380 Training loss: 4.8657 1.1050 sec/batch\n",
      "Epoch 17/20  Iteration 5923/7380 Training loss: 4.8609 1.1257 sec/batch\n",
      "Epoch 17/20  Iteration 5924/7380 Training loss: 4.8596 1.1121 sec/batch\n",
      "Epoch 17/20  Iteration 5925/7380 Training loss: 4.8568 1.1085 sec/batch\n",
      "Epoch 17/20  Iteration 5926/7380 Training loss: 4.8618 1.1038 sec/batch\n",
      "Epoch 17/20  Iteration 5927/7380 Training loss: 4.8603 1.1061 sec/batch\n",
      "Epoch 17/20  Iteration 5928/7380 Training loss: 4.8689 1.1180 sec/batch\n",
      "Epoch 17/20  Iteration 5929/7380 Training loss: 4.8711 1.0957 sec/batch\n",
      "Epoch 17/20  Iteration 5930/7380 Training loss: 4.8717 1.1081 sec/batch\n",
      "Epoch 17/20  Iteration 5931/7380 Training loss: 4.8663 1.1174 sec/batch\n",
      "Epoch 17/20  Iteration 5932/7380 Training loss: 4.8713 1.1077 sec/batch\n",
      "Epoch 17/20  Iteration 5933/7380 Training loss: 4.8691 1.1016 sec/batch\n",
      "Epoch 17/20  Iteration 5934/7380 Training loss: 4.8698 1.1079 sec/batch\n",
      "Epoch 17/20  Iteration 5935/7380 Training loss: 4.8691 1.1142 sec/batch\n",
      "Epoch 17/20  Iteration 5936/7380 Training loss: 4.8702 1.1104 sec/batch\n",
      "Epoch 17/20  Iteration 5937/7380 Training loss: 4.8710 1.1134 sec/batch\n",
      "Epoch 17/20  Iteration 5938/7380 Training loss: 4.8725 1.1196 sec/batch\n",
      "Epoch 17/20  Iteration 5939/7380 Training loss: 4.8759 1.1079 sec/batch\n",
      "Epoch 17/20  Iteration 5940/7380 Training loss: 4.8747 1.1025 sec/batch\n",
      "Epoch 17/20  Iteration 5941/7380 Training loss: 4.8757 1.1108 sec/batch\n",
      "Epoch 17/20  Iteration 5942/7380 Training loss: 4.8746 1.1093 sec/batch\n",
      "Epoch 17/20  Iteration 5943/7380 Training loss: 4.8761 1.1303 sec/batch\n",
      "Epoch 17/20  Iteration 5944/7380 Training loss: 4.8763 1.1605 sec/batch\n",
      "Epoch 17/20  Iteration 5945/7380 Training loss: 4.8759 1.1115 sec/batch\n",
      "Epoch 17/20  Iteration 5946/7380 Training loss: 4.8738 1.1861 sec/batch\n",
      "Epoch 17/20  Iteration 5947/7380 Training loss: 4.8745 1.1116 sec/batch\n",
      "Epoch 17/20  Iteration 5948/7380 Training loss: 4.8759 1.1099 sec/batch\n",
      "Epoch 17/20  Iteration 5949/7380 Training loss: 4.8766 1.1064 sec/batch\n",
      "Epoch 17/20  Iteration 5950/7380 Training loss: 4.8743 1.1059 sec/batch\n",
      "Validation loss: 5.01464 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 5951/7380 Training loss: 4.8785 1.1028 sec/batch\n",
      "Epoch 17/20  Iteration 5952/7380 Training loss: 4.8759 1.1175 sec/batch\n",
      "Epoch 17/20  Iteration 5953/7380 Training loss: 4.8769 1.1193 sec/batch\n",
      "Epoch 17/20  Iteration 5954/7380 Training loss: 4.8764 1.1130 sec/batch\n",
      "Epoch 17/20  Iteration 5955/7380 Training loss: 4.8749 1.0983 sec/batch\n",
      "Epoch 17/20  Iteration 5956/7380 Training loss: 4.8737 1.1187 sec/batch\n",
      "Epoch 17/20  Iteration 5957/7380 Training loss: 4.8723 1.1204 sec/batch\n",
      "Epoch 17/20  Iteration 5958/7380 Training loss: 4.8715 1.1154 sec/batch\n",
      "Epoch 17/20  Iteration 5959/7380 Training loss: 4.8704 1.1088 sec/batch\n",
      "Epoch 17/20  Iteration 5960/7380 Training loss: 4.8675 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 5961/7380 Training loss: 4.8666 1.1064 sec/batch\n",
      "Epoch 17/20  Iteration 5962/7380 Training loss: 4.8672 1.1110 sec/batch\n",
      "Epoch 17/20  Iteration 5963/7380 Training loss: 4.8669 1.1164 sec/batch\n",
      "Epoch 17/20  Iteration 5964/7380 Training loss: 4.8679 1.0989 sec/batch\n",
      "Epoch 17/20  Iteration 5965/7380 Training loss: 4.8665 1.1076 sec/batch\n",
      "Epoch 17/20  Iteration 5966/7380 Training loss: 4.8675 1.1078 sec/batch\n",
      "Epoch 17/20  Iteration 5967/7380 Training loss: 4.8664 1.0997 sec/batch\n",
      "Epoch 17/20  Iteration 5968/7380 Training loss: 4.8646 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 5969/7380 Training loss: 4.8649 1.1185 sec/batch\n",
      "Epoch 17/20  Iteration 5970/7380 Training loss: 4.8642 1.1200 sec/batch\n",
      "Epoch 17/20  Iteration 5971/7380 Training loss: 4.8650 1.1129 sec/batch\n",
      "Epoch 17/20  Iteration 5972/7380 Training loss: 4.8656 1.1170 sec/batch\n",
      "Epoch 17/20  Iteration 5973/7380 Training loss: 4.8669 1.1149 sec/batch\n",
      "Epoch 17/20  Iteration 5974/7380 Training loss: 4.8659 1.1088 sec/batch\n",
      "Epoch 17/20  Iteration 5975/7380 Training loss: 4.8648 1.1069 sec/batch\n",
      "Epoch 17/20  Iteration 5976/7380 Training loss: 4.8659 1.0985 sec/batch\n",
      "Epoch 17/20  Iteration 5977/7380 Training loss: 4.8645 1.1044 sec/batch\n",
      "Epoch 17/20  Iteration 5978/7380 Training loss: 4.8658 1.1188 sec/batch\n",
      "Epoch 17/20  Iteration 5979/7380 Training loss: 4.8670 1.1049 sec/batch\n",
      "Epoch 17/20  Iteration 5980/7380 Training loss: 4.8665 1.1028 sec/batch\n",
      "Epoch 17/20  Iteration 5981/7380 Training loss: 4.8668 1.1142 sec/batch\n",
      "Epoch 17/20  Iteration 5982/7380 Training loss: 4.8663 1.1165 sec/batch\n",
      "Epoch 17/20  Iteration 5983/7380 Training loss: 4.8666 1.1075 sec/batch\n",
      "Epoch 17/20  Iteration 5984/7380 Training loss: 4.8673 1.1212 sec/batch\n",
      "Epoch 17/20  Iteration 5985/7380 Training loss: 4.8670 1.1079 sec/batch\n",
      "Epoch 17/20  Iteration 5986/7380 Training loss: 4.8666 1.1105 sec/batch\n",
      "Epoch 17/20  Iteration 5987/7380 Training loss: 4.8655 1.1110 sec/batch\n",
      "Epoch 17/20  Iteration 5988/7380 Training loss: 4.8651 1.1149 sec/batch\n",
      "Epoch 17/20  Iteration 5989/7380 Training loss: 4.8659 1.1113 sec/batch\n",
      "Epoch 17/20  Iteration 5990/7380 Training loss: 4.8650 1.1145 sec/batch\n",
      "Epoch 17/20  Iteration 5991/7380 Training loss: 4.8651 1.1292 sec/batch\n",
      "Epoch 17/20  Iteration 5992/7380 Training loss: 4.8648 1.1115 sec/batch\n",
      "Epoch 17/20  Iteration 5993/7380 Training loss: 4.8632 1.1209 sec/batch\n",
      "Epoch 17/20  Iteration 5994/7380 Training loss: 4.8629 1.0987 sec/batch\n",
      "Epoch 17/20  Iteration 5995/7380 Training loss: 4.8637 1.1103 sec/batch\n",
      "Epoch 17/20  Iteration 5996/7380 Training loss: 4.8654 1.1177 sec/batch\n",
      "Epoch 17/20  Iteration 5997/7380 Training loss: 4.8648 1.1235 sec/batch\n",
      "Epoch 17/20  Iteration 5998/7380 Training loss: 4.8645 1.1197 sec/batch\n",
      "Epoch 17/20  Iteration 5999/7380 Training loss: 4.8643 1.1060 sec/batch\n",
      "Epoch 17/20  Iteration 6000/7380 Training loss: 4.8642 1.1096 sec/batch\n",
      "Validation loss: 5.02257 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 6001/7380 Training loss: 4.8659 1.1079 sec/batch\n",
      "Epoch 17/20  Iteration 6002/7380 Training loss: 4.8667 1.1203 sec/batch\n",
      "Epoch 17/20  Iteration 6003/7380 Training loss: 4.8668 1.1116 sec/batch\n",
      "Epoch 17/20  Iteration 6004/7380 Training loss: 4.8665 1.1050 sec/batch\n",
      "Epoch 17/20  Iteration 6005/7380 Training loss: 4.8676 1.1074 sec/batch\n",
      "Epoch 17/20  Iteration 6006/7380 Training loss: 4.8681 1.1060 sec/batch\n",
      "Epoch 17/20  Iteration 6007/7380 Training loss: 4.8686 1.1125 sec/batch\n",
      "Epoch 17/20  Iteration 6008/7380 Training loss: 4.8684 1.1145 sec/batch\n",
      "Epoch 17/20  Iteration 6009/7380 Training loss: 4.8688 1.1144 sec/batch\n",
      "Epoch 17/20  Iteration 6010/7380 Training loss: 4.8688 1.1158 sec/batch\n",
      "Epoch 17/20  Iteration 6011/7380 Training loss: 4.8686 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 6012/7380 Training loss: 4.8686 1.1092 sec/batch\n",
      "Epoch 17/20  Iteration 6013/7380 Training loss: 4.8673 1.1173 sec/batch\n",
      "Epoch 17/20  Iteration 6014/7380 Training loss: 4.8670 1.1228 sec/batch\n",
      "Epoch 17/20  Iteration 6015/7380 Training loss: 4.8666 1.1100 sec/batch\n",
      "Epoch 17/20  Iteration 6016/7380 Training loss: 4.8670 1.1086 sec/batch\n",
      "Epoch 17/20  Iteration 6017/7380 Training loss: 4.8659 1.1102 sec/batch\n",
      "Epoch 17/20  Iteration 6018/7380 Training loss: 4.8663 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 6019/7380 Training loss: 4.8663 1.1176 sec/batch\n",
      "Epoch 17/20  Iteration 6020/7380 Training loss: 4.8660 1.1057 sec/batch\n",
      "Epoch 17/20  Iteration 6021/7380 Training loss: 4.8651 1.1173 sec/batch\n",
      "Epoch 17/20  Iteration 6022/7380 Training loss: 4.8652 1.1051 sec/batch\n",
      "Epoch 17/20  Iteration 6023/7380 Training loss: 4.8658 1.1174 sec/batch\n",
      "Epoch 17/20  Iteration 6024/7380 Training loss: 4.8654 1.1107 sec/batch\n",
      "Epoch 17/20  Iteration 6025/7380 Training loss: 4.8657 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 6026/7380 Training loss: 4.8666 1.1064 sec/batch\n",
      "Epoch 17/20  Iteration 6027/7380 Training loss: 4.8667 1.1055 sec/batch\n",
      "Epoch 17/20  Iteration 6028/7380 Training loss: 4.8667 1.1053 sec/batch\n",
      "Epoch 17/20  Iteration 6029/7380 Training loss: 4.8666 1.1081 sec/batch\n",
      "Epoch 17/20  Iteration 6030/7380 Training loss: 4.8657 1.1053 sec/batch\n",
      "Epoch 17/20  Iteration 6031/7380 Training loss: 4.8642 1.1073 sec/batch\n",
      "Epoch 17/20  Iteration 6032/7380 Training loss: 4.8639 1.1280 sec/batch\n",
      "Epoch 17/20  Iteration 6033/7380 Training loss: 4.8634 1.1108 sec/batch\n",
      "Epoch 17/20  Iteration 6034/7380 Training loss: 4.8634 1.1102 sec/batch\n",
      "Epoch 17/20  Iteration 6035/7380 Training loss: 4.8638 1.1053 sec/batch\n",
      "Epoch 17/20  Iteration 6036/7380 Training loss: 4.8640 1.1096 sec/batch\n",
      "Epoch 17/20  Iteration 6037/7380 Training loss: 4.8642 1.1105 sec/batch\n",
      "Epoch 17/20  Iteration 6038/7380 Training loss: 4.8634 1.1088 sec/batch\n",
      "Epoch 17/20  Iteration 6039/7380 Training loss: 4.8629 1.1073 sec/batch\n",
      "Epoch 17/20  Iteration 6040/7380 Training loss: 4.8632 1.1075 sec/batch\n",
      "Epoch 17/20  Iteration 6041/7380 Training loss: 4.8629 1.1100 sec/batch\n",
      "Epoch 17/20  Iteration 6042/7380 Training loss: 4.8628 1.1103 sec/batch\n",
      "Epoch 17/20  Iteration 6043/7380 Training loss: 4.8621 1.1080 sec/batch\n",
      "Epoch 17/20  Iteration 6044/7380 Training loss: 4.8614 1.1554 sec/batch\n",
      "Epoch 17/20  Iteration 6045/7380 Training loss: 4.8628 1.1041 sec/batch\n",
      "Epoch 17/20  Iteration 6046/7380 Training loss: 4.8629 1.1142 sec/batch\n",
      "Epoch 17/20  Iteration 6047/7380 Training loss: 4.8635 1.1097 sec/batch\n",
      "Epoch 17/20  Iteration 6048/7380 Training loss: 4.8633 1.1426 sec/batch\n",
      "Epoch 17/20  Iteration 6049/7380 Training loss: 4.8634 1.1112 sec/batch\n",
      "Epoch 17/20  Iteration 6050/7380 Training loss: 4.8634 1.1090 sec/batch\n",
      "Validation loss: 5.00898 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 6051/7380 Training loss: 4.8643 1.1101 sec/batch\n",
      "Epoch 17/20  Iteration 6052/7380 Training loss: 4.8645 1.1040 sec/batch\n",
      "Epoch 17/20  Iteration 6053/7380 Training loss: 4.8639 1.1132 sec/batch\n",
      "Epoch 17/20  Iteration 6054/7380 Training loss: 4.8645 1.1098 sec/batch\n",
      "Epoch 17/20  Iteration 6055/7380 Training loss: 4.8638 1.1181 sec/batch\n",
      "Epoch 17/20  Iteration 6056/7380 Training loss: 4.8642 1.1077 sec/batch\n",
      "Epoch 17/20  Iteration 6057/7380 Training loss: 4.8646 1.1148 sec/batch\n",
      "Epoch 17/20  Iteration 6058/7380 Training loss: 4.8650 1.1103 sec/batch\n",
      "Epoch 17/20  Iteration 6059/7380 Training loss: 4.8647 1.1048 sec/batch\n",
      "Epoch 17/20  Iteration 6060/7380 Training loss: 4.8643 1.1123 sec/batch\n",
      "Epoch 17/20  Iteration 6061/7380 Training loss: 4.8638 1.1084 sec/batch\n",
      "Epoch 17/20  Iteration 6062/7380 Training loss: 4.8649 1.1053 sec/batch\n",
      "Epoch 17/20  Iteration 6063/7380 Training loss: 4.8644 1.1324 sec/batch\n",
      "Epoch 17/20  Iteration 6064/7380 Training loss: 4.8643 1.1080 sec/batch\n",
      "Epoch 17/20  Iteration 6065/7380 Training loss: 4.8651 1.1160 sec/batch\n",
      "Epoch 17/20  Iteration 6066/7380 Training loss: 4.8654 1.1099 sec/batch\n",
      "Epoch 17/20  Iteration 6067/7380 Training loss: 4.8657 1.1076 sec/batch\n",
      "Epoch 17/20  Iteration 6068/7380 Training loss: 4.8661 1.1063 sec/batch\n",
      "Epoch 17/20  Iteration 6069/7380 Training loss: 4.8664 1.1250 sec/batch\n",
      "Epoch 17/20  Iteration 6070/7380 Training loss: 4.8667 1.1201 sec/batch\n",
      "Epoch 17/20  Iteration 6071/7380 Training loss: 4.8668 1.1130 sec/batch\n",
      "Epoch 17/20  Iteration 6072/7380 Training loss: 4.8669 1.1081 sec/batch\n",
      "Epoch 17/20  Iteration 6073/7380 Training loss: 4.8663 1.1069 sec/batch\n",
      "Epoch 17/20  Iteration 6074/7380 Training loss: 4.8659 1.1607 sec/batch\n",
      "Epoch 17/20  Iteration 6075/7380 Training loss: 4.8662 1.1151 sec/batch\n",
      "Epoch 17/20  Iteration 6076/7380 Training loss: 4.8667 1.1006 sec/batch\n",
      "Epoch 17/20  Iteration 6077/7380 Training loss: 4.8667 1.1105 sec/batch\n",
      "Epoch 17/20  Iteration 6078/7380 Training loss: 4.8662 1.1062 sec/batch\n",
      "Epoch 17/20  Iteration 6079/7380 Training loss: 4.8659 1.1064 sec/batch\n",
      "Epoch 17/20  Iteration 6080/7380 Training loss: 4.8655 1.1058 sec/batch\n",
      "Epoch 17/20  Iteration 6081/7380 Training loss: 4.8654 1.1163 sec/batch\n",
      "Epoch 17/20  Iteration 6082/7380 Training loss: 4.8659 1.1222 sec/batch\n",
      "Epoch 17/20  Iteration 6083/7380 Training loss: 4.8653 1.1066 sec/batch\n",
      "Epoch 17/20  Iteration 6084/7380 Training loss: 4.8649 1.1090 sec/batch\n",
      "Epoch 17/20  Iteration 6085/7380 Training loss: 4.8646 1.1086 sec/batch\n",
      "Epoch 17/20  Iteration 6086/7380 Training loss: 4.8646 1.1107 sec/batch\n",
      "Epoch 17/20  Iteration 6087/7380 Training loss: 4.8653 1.1098 sec/batch\n",
      "Epoch 17/20  Iteration 6088/7380 Training loss: 4.8651 1.1080 sec/batch\n",
      "Epoch 17/20  Iteration 6089/7380 Training loss: 4.8650 1.1566 sec/batch\n",
      "Epoch 17/20  Iteration 6090/7380 Training loss: 4.8650 1.1107 sec/batch\n",
      "Epoch 17/20  Iteration 6091/7380 Training loss: 4.8645 1.1170 sec/batch\n",
      "Epoch 17/20  Iteration 6092/7380 Training loss: 4.8644 1.1243 sec/batch\n",
      "Epoch 17/20  Iteration 6093/7380 Training loss: 4.8641 1.1055 sec/batch\n",
      "Epoch 17/20  Iteration 6094/7380 Training loss: 4.8642 1.1064 sec/batch\n",
      "Epoch 17/20  Iteration 6095/7380 Training loss: 4.8633 1.1336 sec/batch\n",
      "Epoch 17/20  Iteration 6096/7380 Training loss: 4.8635 1.1074 sec/batch\n",
      "Epoch 17/20  Iteration 6097/7380 Training loss: 4.8639 1.1105 sec/batch\n",
      "Epoch 17/20  Iteration 6098/7380 Training loss: 4.8633 1.1131 sec/batch\n",
      "Epoch 17/20  Iteration 6099/7380 Training loss: 4.8638 1.1093 sec/batch\n",
      "Epoch 17/20  Iteration 6100/7380 Training loss: 4.8640 1.1068 sec/batch\n",
      "Validation loss: 5.01409 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 6101/7380 Training loss: 4.8648 1.1113 sec/batch\n",
      "Epoch 17/20  Iteration 6102/7380 Training loss: 4.8652 1.1093 sec/batch\n",
      "Epoch 17/20  Iteration 6103/7380 Training loss: 4.8650 1.1137 sec/batch\n",
      "Epoch 17/20  Iteration 6104/7380 Training loss: 4.8651 1.1059 sec/batch\n",
      "Epoch 17/20  Iteration 6105/7380 Training loss: 4.8656 1.1144 sec/batch\n",
      "Epoch 17/20  Iteration 6106/7380 Training loss: 4.8659 1.1175 sec/batch\n",
      "Epoch 17/20  Iteration 6107/7380 Training loss: 4.8666 1.1196 sec/batch\n",
      "Epoch 17/20  Iteration 6108/7380 Training loss: 4.8666 1.1202 sec/batch\n",
      "Epoch 17/20  Iteration 6109/7380 Training loss: 4.8669 1.1095 sec/batch\n",
      "Epoch 17/20  Iteration 6110/7380 Training loss: 4.8672 1.1187 sec/batch\n",
      "Epoch 17/20  Iteration 6111/7380 Training loss: 4.8665 1.1200 sec/batch\n",
      "Epoch 17/20  Iteration 6112/7380 Training loss: 4.8660 1.1237 sec/batch\n",
      "Epoch 17/20  Iteration 6113/7380 Training loss: 4.8658 1.1033 sec/batch\n",
      "Epoch 17/20  Iteration 6114/7380 Training loss: 4.8653 1.1072 sec/batch\n",
      "Epoch 17/20  Iteration 6115/7380 Training loss: 4.8652 1.1058 sec/batch\n",
      "Epoch 17/20  Iteration 6116/7380 Training loss: 4.8650 1.1191 sec/batch\n",
      "Epoch 17/20  Iteration 6117/7380 Training loss: 4.8650 1.1162 sec/batch\n",
      "Epoch 17/20  Iteration 6118/7380 Training loss: 4.8650 1.1064 sec/batch\n",
      "Epoch 17/20  Iteration 6119/7380 Training loss: 4.8646 1.1075 sec/batch\n",
      "Epoch 17/20  Iteration 6120/7380 Training loss: 4.8641 1.1095 sec/batch\n",
      "Epoch 17/20  Iteration 6121/7380 Training loss: 4.8636 1.1118 sec/batch\n",
      "Epoch 17/20  Iteration 6122/7380 Training loss: 4.8637 1.1144 sec/batch\n",
      "Epoch 17/20  Iteration 6123/7380 Training loss: 4.8640 1.1264 sec/batch\n",
      "Epoch 17/20  Iteration 6124/7380 Training loss: 4.8640 1.1107 sec/batch\n",
      "Epoch 17/20  Iteration 6125/7380 Training loss: 4.8638 1.1135 sec/batch\n",
      "Epoch 17/20  Iteration 6126/7380 Training loss: 4.8642 1.1113 sec/batch\n",
      "Epoch 17/20  Iteration 6127/7380 Training loss: 4.8643 1.1197 sec/batch\n",
      "Epoch 17/20  Iteration 6128/7380 Training loss: 4.8636 1.1089 sec/batch\n",
      "Epoch 17/20  Iteration 6129/7380 Training loss: 4.8638 1.2008 sec/batch\n",
      "Epoch 17/20  Iteration 6130/7380 Training loss: 4.8637 1.1315 sec/batch\n",
      "Epoch 17/20  Iteration 6131/7380 Training loss: 4.8636 1.1082 sec/batch\n",
      "Epoch 17/20  Iteration 6132/7380 Training loss: 4.8631 1.1079 sec/batch\n",
      "Epoch 17/20  Iteration 6133/7380 Training loss: 4.8630 1.1072 sec/batch\n",
      "Epoch 17/20  Iteration 6134/7380 Training loss: 4.8635 1.1064 sec/batch\n",
      "Epoch 17/20  Iteration 6135/7380 Training loss: 4.8632 1.1073 sec/batch\n",
      "Epoch 17/20  Iteration 6136/7380 Training loss: 4.8630 1.1074 sec/batch\n",
      "Epoch 17/20  Iteration 6137/7380 Training loss: 4.8626 1.1146 sec/batch\n",
      "Epoch 17/20  Iteration 6138/7380 Training loss: 4.8625 1.1086 sec/batch\n",
      "Epoch 17/20  Iteration 6139/7380 Training loss: 4.8621 1.1179 sec/batch\n",
      "Epoch 17/20  Iteration 6140/7380 Training loss: 4.8617 1.1026 sec/batch\n",
      "Epoch 17/20  Iteration 6141/7380 Training loss: 4.8614 1.1121 sec/batch\n",
      "Epoch 17/20  Iteration 6142/7380 Training loss: 4.8607 1.1125 sec/batch\n",
      "Epoch 17/20  Iteration 6143/7380 Training loss: 4.8597 1.1115 sec/batch\n",
      "Epoch 17/20  Iteration 6144/7380 Training loss: 4.8592 1.1150 sec/batch\n",
      "Epoch 17/20  Iteration 6145/7380 Training loss: 4.8594 1.1290 sec/batch\n",
      "Epoch 17/20  Iteration 6146/7380 Training loss: 4.8600 1.1158 sec/batch\n",
      "Epoch 17/20  Iteration 6147/7380 Training loss: 4.8597 1.1247 sec/batch\n",
      "Epoch 17/20  Iteration 6148/7380 Training loss: 4.8597 1.1107 sec/batch\n",
      "Epoch 17/20  Iteration 6149/7380 Training loss: 4.8599 1.1073 sec/batch\n",
      "Epoch 17/20  Iteration 6150/7380 Training loss: 4.8601 1.1267 sec/batch\n",
      "Validation loss: 5.01199 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 6151/7380 Training loss: 4.8607 1.1199 sec/batch\n",
      "Epoch 17/20  Iteration 6152/7380 Training loss: 4.8608 1.1195 sec/batch\n",
      "Epoch 17/20  Iteration 6153/7380 Training loss: 4.8607 1.1168 sec/batch\n",
      "Epoch 17/20  Iteration 6154/7380 Training loss: 4.8608 1.1254 sec/batch\n",
      "Epoch 17/20  Iteration 6155/7380 Training loss: 4.8605 1.1044 sec/batch\n",
      "Epoch 17/20  Iteration 6156/7380 Training loss: 4.8603 1.1080 sec/batch\n",
      "Epoch 17/20  Iteration 6157/7380 Training loss: 4.8601 1.1230 sec/batch\n",
      "Epoch 17/20  Iteration 6158/7380 Training loss: 4.8597 1.1078 sec/batch\n",
      "Epoch 17/20  Iteration 6159/7380 Training loss: 4.8599 1.1134 sec/batch\n",
      "Epoch 17/20  Iteration 6160/7380 Training loss: 4.8599 1.1331 sec/batch\n",
      "Epoch 17/20  Iteration 6161/7380 Training loss: 4.8601 1.1121 sec/batch\n",
      "Epoch 17/20  Iteration 6162/7380 Training loss: 4.8600 1.1160 sec/batch\n",
      "Epoch 17/20  Iteration 6163/7380 Training loss: 4.8602 1.1114 sec/batch\n",
      "Epoch 17/20  Iteration 6164/7380 Training loss: 4.8596 1.1298 sec/batch\n",
      "Epoch 17/20  Iteration 6165/7380 Training loss: 4.8595 1.1102 sec/batch\n",
      "Epoch 17/20  Iteration 6166/7380 Training loss: 4.8595 1.1075 sec/batch\n",
      "Epoch 17/20  Iteration 6167/7380 Training loss: 4.8598 1.1078 sec/batch\n",
      "Epoch 17/20  Iteration 6168/7380 Training loss: 4.8594 1.1019 sec/batch\n",
      "Epoch 17/20  Iteration 6169/7380 Training loss: 4.8592 1.1174 sec/batch\n",
      "Epoch 17/20  Iteration 6170/7380 Training loss: 4.8589 1.1070 sec/batch\n",
      "Epoch 17/20  Iteration 6171/7380 Training loss: 4.8589 1.1080 sec/batch\n",
      "Epoch 17/20  Iteration 6172/7380 Training loss: 4.8589 1.1548 sec/batch\n",
      "Epoch 17/20  Iteration 6173/7380 Training loss: 4.8584 1.1095 sec/batch\n",
      "Epoch 17/20  Iteration 6174/7380 Training loss: 4.8587 1.1141 sec/batch\n",
      "Epoch 17/20  Iteration 6175/7380 Training loss: 4.8589 1.1074 sec/batch\n",
      "Epoch 17/20  Iteration 6176/7380 Training loss: 4.8585 1.1166 sec/batch\n",
      "Epoch 17/20  Iteration 6177/7380 Training loss: 4.8587 1.1078 sec/batch\n",
      "Epoch 17/20  Iteration 6178/7380 Training loss: 4.8588 1.1107 sec/batch\n",
      "Epoch 17/20  Iteration 6179/7380 Training loss: 4.8588 1.1167 sec/batch\n",
      "Epoch 17/20  Iteration 6180/7380 Training loss: 4.8585 1.1099 sec/batch\n",
      "Epoch 17/20  Iteration 6181/7380 Training loss: 4.8591 1.1085 sec/batch\n",
      "Epoch 17/20  Iteration 6182/7380 Training loss: 4.8591 1.1156 sec/batch\n",
      "Epoch 17/20  Iteration 6183/7380 Training loss: 4.8594 1.1164 sec/batch\n",
      "Epoch 17/20  Iteration 6184/7380 Training loss: 4.8591 1.1096 sec/batch\n",
      "Epoch 17/20  Iteration 6185/7380 Training loss: 4.8591 1.1141 sec/batch\n",
      "Epoch 17/20  Iteration 6186/7380 Training loss: 4.8591 1.1092 sec/batch\n",
      "Epoch 17/20  Iteration 6187/7380 Training loss: 4.8592 1.1148 sec/batch\n",
      "Epoch 17/20  Iteration 6188/7380 Training loss: 4.8593 1.1140 sec/batch\n",
      "Epoch 17/20  Iteration 6189/7380 Training loss: 4.8597 1.1203 sec/batch\n",
      "Epoch 17/20  Iteration 6190/7380 Training loss: 4.8594 1.1102 sec/batch\n",
      "Epoch 17/20  Iteration 6191/7380 Training loss: 4.8590 1.1077 sec/batch\n",
      "Epoch 17/20  Iteration 6192/7380 Training loss: 4.8589 1.1160 sec/batch\n",
      "Epoch 17/20  Iteration 6193/7380 Training loss: 4.8586 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 6194/7380 Training loss: 4.8583 1.1012 sec/batch\n",
      "Epoch 17/20  Iteration 6195/7380 Training loss: 4.8579 1.1063 sec/batch\n",
      "Epoch 17/20  Iteration 6196/7380 Training loss: 4.8577 1.1193 sec/batch\n",
      "Epoch 17/20  Iteration 6197/7380 Training loss: 4.8575 1.1130 sec/batch\n",
      "Epoch 17/20  Iteration 6198/7380 Training loss: 4.8576 1.1080 sec/batch\n",
      "Epoch 17/20  Iteration 6199/7380 Training loss: 4.8577 1.1112 sec/batch\n",
      "Epoch 17/20  Iteration 6200/7380 Training loss: 4.8575 1.1101 sec/batch\n",
      "Validation loss: 5.02519 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 6201/7380 Training loss: 4.8578 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 6202/7380 Training loss: 4.8576 1.1157 sec/batch\n",
      "Epoch 17/20  Iteration 6203/7380 Training loss: 4.8579 1.1232 sec/batch\n",
      "Epoch 17/20  Iteration 6204/7380 Training loss: 4.8583 1.1151 sec/batch\n",
      "Epoch 17/20  Iteration 6205/7380 Training loss: 4.8583 1.1183 sec/batch\n",
      "Epoch 17/20  Iteration 6206/7380 Training loss: 4.8586 1.1155 sec/batch\n",
      "Epoch 17/20  Iteration 6207/7380 Training loss: 4.8587 1.1141 sec/batch\n",
      "Epoch 17/20  Iteration 6208/7380 Training loss: 4.8593 1.1133 sec/batch\n",
      "Epoch 17/20  Iteration 6209/7380 Training loss: 4.8596 1.1155 sec/batch\n",
      "Epoch 17/20  Iteration 6210/7380 Training loss: 4.8596 1.1117 sec/batch\n",
      "Epoch 17/20  Iteration 6211/7380 Training loss: 4.8593 1.1154 sec/batch\n",
      "Epoch 17/20  Iteration 6212/7380 Training loss: 4.8590 1.1089 sec/batch\n",
      "Epoch 17/20  Iteration 6213/7380 Training loss: 4.8587 1.1095 sec/batch\n",
      "Epoch 17/20  Iteration 6214/7380 Training loss: 4.8584 1.1136 sec/batch\n",
      "Epoch 17/20  Iteration 6215/7380 Training loss: 4.8583 1.1292 sec/batch\n",
      "Epoch 17/20  Iteration 6216/7380 Training loss: 4.8581 1.1148 sec/batch\n",
      "Epoch 17/20  Iteration 6217/7380 Training loss: 4.8579 1.1061 sec/batch\n",
      "Epoch 17/20  Iteration 6218/7380 Training loss: 4.8579 1.1203 sec/batch\n",
      "Epoch 17/20  Iteration 6219/7380 Training loss: 4.8580 1.1100 sec/batch\n",
      "Epoch 17/20  Iteration 6220/7380 Training loss: 4.8581 1.1105 sec/batch\n",
      "Epoch 17/20  Iteration 6221/7380 Training loss: 4.8583 1.1081 sec/batch\n",
      "Epoch 17/20  Iteration 6222/7380 Training loss: 4.8585 1.1111 sec/batch\n",
      "Epoch 17/20  Iteration 6223/7380 Training loss: 4.8589 1.1095 sec/batch\n",
      "Epoch 17/20  Iteration 6224/7380 Training loss: 4.8592 1.1098 sec/batch\n",
      "Epoch 17/20  Iteration 6225/7380 Training loss: 4.8589 1.1439 sec/batch\n",
      "Epoch 17/20  Iteration 6226/7380 Training loss: 4.8586 1.1077 sec/batch\n",
      "Epoch 17/20  Iteration 6227/7380 Training loss: 4.8585 1.1316 sec/batch\n",
      "Epoch 17/20  Iteration 6228/7380 Training loss: 4.8584 1.1095 sec/batch\n",
      "Epoch 17/20  Iteration 6229/7380 Training loss: 4.8584 1.1086 sec/batch\n",
      "Epoch 17/20  Iteration 6230/7380 Training loss: 4.8591 1.1146 sec/batch\n",
      "Epoch 17/20  Iteration 6231/7380 Training loss: 4.8588 1.1138 sec/batch\n",
      "Epoch 17/20  Iteration 6232/7380 Training loss: 4.8584 1.1117 sec/batch\n",
      "Epoch 17/20  Iteration 6233/7380 Training loss: 4.8583 1.1108 sec/batch\n",
      "Epoch 17/20  Iteration 6234/7380 Training loss: 4.8582 1.1129 sec/batch\n",
      "Epoch 17/20  Iteration 6235/7380 Training loss: 4.8579 1.1170 sec/batch\n",
      "Epoch 17/20  Iteration 6236/7380 Training loss: 4.8581 1.1178 sec/batch\n",
      "Epoch 17/20  Iteration 6237/7380 Training loss: 4.8578 1.1017 sec/batch\n",
      "Epoch 17/20  Iteration 6238/7380 Training loss: 4.8580 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 6239/7380 Training loss: 4.8581 1.1337 sec/batch\n",
      "Epoch 17/20  Iteration 6240/7380 Training loss: 4.8583 1.1009 sec/batch\n",
      "Epoch 17/20  Iteration 6241/7380 Training loss: 4.8585 1.1055 sec/batch\n",
      "Epoch 17/20  Iteration 6242/7380 Training loss: 4.8584 1.1072 sec/batch\n",
      "Epoch 17/20  Iteration 6243/7380 Training loss: 4.8585 1.1188 sec/batch\n",
      "Epoch 17/20  Iteration 6244/7380 Training loss: 4.8582 1.1215 sec/batch\n",
      "Epoch 17/20  Iteration 6245/7380 Training loss: 4.8578 1.1036 sec/batch\n",
      "Epoch 17/20  Iteration 6246/7380 Training loss: 4.8576 1.1163 sec/batch\n",
      "Epoch 17/20  Iteration 6247/7380 Training loss: 4.8577 1.1200 sec/batch\n",
      "Epoch 17/20  Iteration 6248/7380 Training loss: 4.8577 1.1224 sec/batch\n",
      "Epoch 17/20  Iteration 6249/7380 Training loss: 4.8575 1.1098 sec/batch\n",
      "Epoch 17/20  Iteration 6250/7380 Training loss: 4.8575 1.1124 sec/batch\n",
      "Validation loss: 5.02528 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 6251/7380 Training loss: 4.8580 1.1122 sec/batch\n",
      "Epoch 17/20  Iteration 6252/7380 Training loss: 4.8582 1.1043 sec/batch\n",
      "Epoch 17/20  Iteration 6253/7380 Training loss: 4.8580 1.1078 sec/batch\n",
      "Epoch 17/20  Iteration 6254/7380 Training loss: 4.8580 1.1066 sec/batch\n",
      "Epoch 17/20  Iteration 6255/7380 Training loss: 4.8585 1.1154 sec/batch\n",
      "Epoch 17/20  Iteration 6256/7380 Training loss: 4.8582 1.1195 sec/batch\n",
      "Epoch 17/20  Iteration 6257/7380 Training loss: 4.8584 1.1115 sec/batch\n",
      "Epoch 17/20  Iteration 6258/7380 Training loss: 4.8582 1.1081 sec/batch\n",
      "Epoch 17/20  Iteration 6259/7380 Training loss: 4.8580 1.1180 sec/batch\n",
      "Epoch 17/20  Iteration 6260/7380 Training loss: 4.8578 1.1203 sec/batch\n",
      "Epoch 17/20  Iteration 6261/7380 Training loss: 4.8580 1.1121 sec/batch\n",
      "Epoch 17/20  Iteration 6262/7380 Training loss: 4.8580 1.1114 sec/batch\n",
      "Epoch 17/20  Iteration 6263/7380 Training loss: 4.8577 1.1111 sec/batch\n",
      "Epoch 17/20  Iteration 6264/7380 Training loss: 4.8576 1.1055 sec/batch\n",
      "Epoch 17/20  Iteration 6265/7380 Training loss: 4.8575 1.1198 sec/batch\n",
      "Epoch 17/20  Iteration 6266/7380 Training loss: 4.8575 1.1244 sec/batch\n",
      "Epoch 17/20  Iteration 6267/7380 Training loss: 4.8573 1.1037 sec/batch\n",
      "Epoch 17/20  Iteration 6268/7380 Training loss: 4.8571 1.1145 sec/batch\n",
      "Epoch 17/20  Iteration 6269/7380 Training loss: 4.8570 1.1100 sec/batch\n",
      "Epoch 17/20  Iteration 6270/7380 Training loss: 4.8568 1.1101 sec/batch\n",
      "Epoch 17/20  Iteration 6271/7380 Training loss: 4.8566 1.1226 sec/batch\n",
      "Epoch 17/20  Iteration 6272/7380 Training loss: 4.8567 1.1045 sec/batch\n",
      "Epoch 17/20  Iteration 6273/7380 Training loss: 4.8566 1.1104 sec/batch\n",
      "Epoch 18/20  Iteration 6274/7380 Training loss: 4.8584 1.1304 sec/batch\n",
      "Epoch 18/20  Iteration 6275/7380 Training loss: 4.8555 1.1463 sec/batch\n",
      "Epoch 18/20  Iteration 6276/7380 Training loss: 4.8507 1.1207 sec/batch\n",
      "Epoch 18/20  Iteration 6277/7380 Training loss: 4.8322 1.1271 sec/batch\n",
      "Epoch 18/20  Iteration 6278/7380 Training loss: 4.8244 1.1220 sec/batch\n",
      "Epoch 18/20  Iteration 6279/7380 Training loss: 4.8326 1.1114 sec/batch\n",
      "Epoch 18/20  Iteration 6280/7380 Training loss: 4.8511 1.1188 sec/batch\n",
      "Epoch 18/20  Iteration 6281/7380 Training loss: 4.8398 1.1600 sec/batch\n",
      "Epoch 18/20  Iteration 6282/7380 Training loss: 4.8387 1.1044 sec/batch\n",
      "Epoch 18/20  Iteration 6283/7380 Training loss: 4.8478 1.1045 sec/batch\n",
      "Epoch 18/20  Iteration 6284/7380 Training loss: 4.8453 1.1087 sec/batch\n",
      "Epoch 18/20  Iteration 6285/7380 Training loss: 4.8380 1.1107 sec/batch\n",
      "Epoch 18/20  Iteration 6286/7380 Training loss: 4.8342 1.1161 sec/batch\n",
      "Epoch 18/20  Iteration 6287/7380 Training loss: 4.8417 1.1085 sec/batch\n",
      "Epoch 18/20  Iteration 6288/7380 Training loss: 4.8454 1.1033 sec/batch\n",
      "Epoch 18/20  Iteration 6289/7380 Training loss: 4.8462 1.1120 sec/batch\n",
      "Epoch 18/20  Iteration 6290/7380 Training loss: 4.8420 1.1166 sec/batch\n",
      "Epoch 18/20  Iteration 6291/7380 Training loss: 4.8286 1.1050 sec/batch\n",
      "Epoch 18/20  Iteration 6292/7380 Training loss: 4.8240 1.1048 sec/batch\n",
      "Epoch 18/20  Iteration 6293/7380 Training loss: 4.8239 1.1204 sec/batch\n",
      "Epoch 18/20  Iteration 6294/7380 Training loss: 4.8208 1.1142 sec/batch\n",
      "Epoch 18/20  Iteration 6295/7380 Training loss: 4.8269 1.1136 sec/batch\n",
      "Epoch 18/20  Iteration 6296/7380 Training loss: 4.8271 1.1444 sec/batch\n",
      "Epoch 18/20  Iteration 6297/7380 Training loss: 4.8341 1.1093 sec/batch\n",
      "Epoch 18/20  Iteration 6298/7380 Training loss: 4.8365 1.1245 sec/batch\n",
      "Epoch 18/20  Iteration 6299/7380 Training loss: 4.8386 1.1037 sec/batch\n",
      "Epoch 18/20  Iteration 6300/7380 Training loss: 4.8336 1.1108 sec/batch\n",
      "Validation loss: 5.00984 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 6301/7380 Training loss: 4.8445 1.1170 sec/batch\n",
      "Epoch 18/20  Iteration 6302/7380 Training loss: 4.8438 1.1120 sec/batch\n",
      "Epoch 18/20  Iteration 6303/7380 Training loss: 4.8453 1.1180 sec/batch\n",
      "Epoch 18/20  Iteration 6304/7380 Training loss: 4.8442 1.1128 sec/batch\n",
      "Epoch 18/20  Iteration 6305/7380 Training loss: 4.8458 1.1219 sec/batch\n",
      "Epoch 18/20  Iteration 6306/7380 Training loss: 4.8464 1.1049 sec/batch\n",
      "Epoch 18/20  Iteration 6307/7380 Training loss: 4.8479 1.1231 sec/batch\n",
      "Epoch 18/20  Iteration 6308/7380 Training loss: 4.8507 1.1176 sec/batch\n",
      "Epoch 18/20  Iteration 6309/7380 Training loss: 4.8507 1.1245 sec/batch\n",
      "Epoch 18/20  Iteration 6310/7380 Training loss: 4.8517 1.1209 sec/batch\n",
      "Epoch 18/20  Iteration 6311/7380 Training loss: 4.8496 1.1194 sec/batch\n",
      "Epoch 18/20  Iteration 6312/7380 Training loss: 4.8511 1.1111 sec/batch\n",
      "Epoch 18/20  Iteration 6313/7380 Training loss: 4.8514 1.1223 sec/batch\n",
      "Epoch 18/20  Iteration 6314/7380 Training loss: 4.8505 1.1179 sec/batch\n",
      "Epoch 18/20  Iteration 6315/7380 Training loss: 4.8489 1.1221 sec/batch\n",
      "Epoch 18/20  Iteration 6316/7380 Training loss: 4.8494 1.1138 sec/batch\n",
      "Epoch 18/20  Iteration 6317/7380 Training loss: 4.8497 1.1204 sec/batch\n",
      "Epoch 18/20  Iteration 6318/7380 Training loss: 4.8497 1.1153 sec/batch\n",
      "Epoch 18/20  Iteration 6319/7380 Training loss: 4.8477 1.1149 sec/batch\n",
      "Epoch 18/20  Iteration 6320/7380 Training loss: 4.8482 1.1124 sec/batch\n",
      "Epoch 18/20  Iteration 6321/7380 Training loss: 4.8454 1.1133 sec/batch\n",
      "Epoch 18/20  Iteration 6322/7380 Training loss: 4.8465 1.1122 sec/batch\n",
      "Epoch 18/20  Iteration 6323/7380 Training loss: 4.8456 1.1244 sec/batch\n",
      "Epoch 18/20  Iteration 6324/7380 Training loss: 4.8446 1.1100 sec/batch\n",
      "Epoch 18/20  Iteration 6325/7380 Training loss: 4.8431 1.1132 sec/batch\n",
      "Epoch 18/20  Iteration 6326/7380 Training loss: 4.8414 1.1162 sec/batch\n",
      "Epoch 18/20  Iteration 6327/7380 Training loss: 4.8406 1.1213 sec/batch\n",
      "Epoch 18/20  Iteration 6328/7380 Training loss: 4.8393 1.1313 sec/batch\n",
      "Epoch 18/20  Iteration 6329/7380 Training loss: 4.8370 1.1101 sec/batch\n",
      "Epoch 18/20  Iteration 6330/7380 Training loss: 4.8367 1.1246 sec/batch\n",
      "Epoch 18/20  Iteration 6331/7380 Training loss: 4.8377 1.1182 sec/batch\n",
      "Epoch 18/20  Iteration 6332/7380 Training loss: 4.8377 1.1346 sec/batch\n",
      "Epoch 18/20  Iteration 6333/7380 Training loss: 4.8390 1.1110 sec/batch\n",
      "Epoch 18/20  Iteration 6334/7380 Training loss: 4.8378 1.1022 sec/batch\n",
      "Epoch 18/20  Iteration 6335/7380 Training loss: 4.8383 1.1116 sec/batch\n",
      "Epoch 18/20  Iteration 6336/7380 Training loss: 4.8378 1.1103 sec/batch\n",
      "Epoch 18/20  Iteration 6337/7380 Training loss: 4.8355 1.1093 sec/batch\n",
      "Epoch 18/20  Iteration 6338/7380 Training loss: 4.8357 1.1142 sec/batch\n",
      "Epoch 18/20  Iteration 6339/7380 Training loss: 4.8350 1.1215 sec/batch\n",
      "Epoch 18/20  Iteration 6340/7380 Training loss: 4.8355 1.1159 sec/batch\n",
      "Epoch 18/20  Iteration 6341/7380 Training loss: 4.8357 1.1151 sec/batch\n",
      "Epoch 18/20  Iteration 6342/7380 Training loss: 4.8376 1.1182 sec/batch\n",
      "Epoch 18/20  Iteration 6343/7380 Training loss: 4.8366 1.1159 sec/batch\n",
      "Epoch 18/20  Iteration 6344/7380 Training loss: 4.8354 1.1105 sec/batch\n",
      "Epoch 18/20  Iteration 6345/7380 Training loss: 4.8363 1.1198 sec/batch\n",
      "Epoch 18/20  Iteration 6346/7380 Training loss: 4.8351 1.1183 sec/batch\n",
      "Epoch 18/20  Iteration 6347/7380 Training loss: 4.8365 1.1101 sec/batch\n",
      "Epoch 18/20  Iteration 6348/7380 Training loss: 4.8375 1.1009 sec/batch\n",
      "Epoch 18/20  Iteration 6349/7380 Training loss: 4.8370 1.1093 sec/batch\n",
      "Epoch 18/20  Iteration 6350/7380 Training loss: 4.8372 1.1283 sec/batch\n",
      "Validation loss: 5.01289 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 6351/7380 Training loss: 4.8387 1.1241 sec/batch\n",
      "Epoch 18/20  Iteration 6352/7380 Training loss: 4.8390 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6353/7380 Training loss: 4.8397 1.1202 sec/batch\n",
      "Epoch 18/20  Iteration 6354/7380 Training loss: 4.8390 1.1358 sec/batch\n",
      "Epoch 18/20  Iteration 6355/7380 Training loss: 4.8383 1.1056 sec/batch\n",
      "Epoch 18/20  Iteration 6356/7380 Training loss: 4.8370 1.1084 sec/batch\n",
      "Epoch 18/20  Iteration 6357/7380 Training loss: 4.8367 1.1194 sec/batch\n",
      "Epoch 18/20  Iteration 6358/7380 Training loss: 4.8371 1.1159 sec/batch\n",
      "Epoch 18/20  Iteration 6359/7380 Training loss: 4.8364 1.1046 sec/batch\n",
      "Epoch 18/20  Iteration 6360/7380 Training loss: 4.8362 1.1165 sec/batch\n",
      "Epoch 18/20  Iteration 6361/7380 Training loss: 4.8363 1.1027 sec/batch\n",
      "Epoch 18/20  Iteration 6362/7380 Training loss: 4.8345 1.1267 sec/batch\n",
      "Epoch 18/20  Iteration 6363/7380 Training loss: 4.8341 1.1091 sec/batch\n",
      "Epoch 18/20  Iteration 6364/7380 Training loss: 4.8350 1.1394 sec/batch\n",
      "Epoch 18/20  Iteration 6365/7380 Training loss: 4.8370 1.1325 sec/batch\n",
      "Epoch 18/20  Iteration 6366/7380 Training loss: 4.8365 1.1132 sec/batch\n",
      "Epoch 18/20  Iteration 6367/7380 Training loss: 4.8363 1.1078 sec/batch\n",
      "Epoch 18/20  Iteration 6368/7380 Training loss: 4.8358 1.1354 sec/batch\n",
      "Epoch 18/20  Iteration 6369/7380 Training loss: 4.8357 1.1120 sec/batch\n",
      "Epoch 18/20  Iteration 6370/7380 Training loss: 4.8356 1.1253 sec/batch\n",
      "Epoch 18/20  Iteration 6371/7380 Training loss: 4.8363 1.1064 sec/batch\n",
      "Epoch 18/20  Iteration 6372/7380 Training loss: 4.8362 1.1124 sec/batch\n",
      "Epoch 18/20  Iteration 6373/7380 Training loss: 4.8360 1.1099 sec/batch\n",
      "Epoch 18/20  Iteration 6374/7380 Training loss: 4.8370 1.1091 sec/batch\n",
      "Epoch 18/20  Iteration 6375/7380 Training loss: 4.8374 1.1097 sec/batch\n",
      "Epoch 18/20  Iteration 6376/7380 Training loss: 4.8378 1.1049 sec/batch\n",
      "Epoch 18/20  Iteration 6377/7380 Training loss: 4.8375 1.1194 sec/batch\n",
      "Epoch 18/20  Iteration 6378/7380 Training loss: 4.8381 1.1076 sec/batch\n",
      "Epoch 18/20  Iteration 6379/7380 Training loss: 4.8382 1.0977 sec/batch\n",
      "Epoch 18/20  Iteration 6380/7380 Training loss: 4.8382 1.1183 sec/batch\n",
      "Epoch 18/20  Iteration 6381/7380 Training loss: 4.8379 1.1035 sec/batch\n",
      "Epoch 18/20  Iteration 6382/7380 Training loss: 4.8365 1.1193 sec/batch\n",
      "Epoch 18/20  Iteration 6383/7380 Training loss: 4.8363 1.1013 sec/batch\n",
      "Epoch 18/20  Iteration 6384/7380 Training loss: 4.8358 1.1143 sec/batch\n",
      "Epoch 18/20  Iteration 6385/7380 Training loss: 4.8361 1.1058 sec/batch\n",
      "Epoch 18/20  Iteration 6386/7380 Training loss: 4.8354 1.1332 sec/batch\n",
      "Epoch 18/20  Iteration 6387/7380 Training loss: 4.8358 1.1148 sec/batch\n",
      "Epoch 18/20  Iteration 6388/7380 Training loss: 4.8358 1.1211 sec/batch\n",
      "Epoch 18/20  Iteration 6389/7380 Training loss: 4.8355 1.1186 sec/batch\n",
      "Epoch 18/20  Iteration 6390/7380 Training loss: 4.8347 1.1122 sec/batch\n",
      "Epoch 18/20  Iteration 6391/7380 Training loss: 4.8343 1.1101 sec/batch\n",
      "Epoch 18/20  Iteration 6392/7380 Training loss: 4.8350 1.1101 sec/batch\n",
      "Epoch 18/20  Iteration 6393/7380 Training loss: 4.8346 1.1458 sec/batch\n",
      "Epoch 18/20  Iteration 6394/7380 Training loss: 4.8352 1.1139 sec/batch\n",
      "Epoch 18/20  Iteration 6395/7380 Training loss: 4.8360 1.1123 sec/batch\n",
      "Epoch 18/20  Iteration 6396/7380 Training loss: 4.8361 1.1019 sec/batch\n",
      "Epoch 18/20  Iteration 6397/7380 Training loss: 4.8361 1.1152 sec/batch\n",
      "Epoch 18/20  Iteration 6398/7380 Training loss: 4.8363 1.1156 sec/batch\n",
      "Epoch 18/20  Iteration 6399/7380 Training loss: 4.8350 1.1058 sec/batch\n",
      "Epoch 18/20  Iteration 6400/7380 Training loss: 4.8335 1.1126 sec/batch\n",
      "Validation loss: 5.00801 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 6401/7380 Training loss: 4.8349 1.1116 sec/batch\n",
      "Epoch 18/20  Iteration 6402/7380 Training loss: 4.8344 1.1139 sec/batch\n",
      "Epoch 18/20  Iteration 6403/7380 Training loss: 4.8343 1.1062 sec/batch\n",
      "Epoch 18/20  Iteration 6404/7380 Training loss: 4.8344 1.1507 sec/batch\n",
      "Epoch 18/20  Iteration 6405/7380 Training loss: 4.8340 1.1108 sec/batch\n",
      "Epoch 18/20  Iteration 6406/7380 Training loss: 4.8341 1.1098 sec/batch\n",
      "Epoch 18/20  Iteration 6407/7380 Training loss: 4.8332 1.1093 sec/batch\n",
      "Epoch 18/20  Iteration 6408/7380 Training loss: 4.8325 1.1077 sec/batch\n",
      "Epoch 18/20  Iteration 6409/7380 Training loss: 4.8327 1.1111 sec/batch\n",
      "Epoch 18/20  Iteration 6410/7380 Training loss: 4.8326 1.1353 sec/batch\n",
      "Epoch 18/20  Iteration 6411/7380 Training loss: 4.8325 1.1114 sec/batch\n",
      "Epoch 18/20  Iteration 6412/7380 Training loss: 4.8316 1.1169 sec/batch\n",
      "Epoch 18/20  Iteration 6413/7380 Training loss: 4.8308 1.1160 sec/batch\n",
      "Epoch 18/20  Iteration 6414/7380 Training loss: 4.8323 1.1308 sec/batch\n",
      "Epoch 18/20  Iteration 6415/7380 Training loss: 4.8324 1.1115 sec/batch\n",
      "Epoch 18/20  Iteration 6416/7380 Training loss: 4.8327 1.1093 sec/batch\n",
      "Epoch 18/20  Iteration 6417/7380 Training loss: 4.8327 1.1063 sec/batch\n",
      "Epoch 18/20  Iteration 6418/7380 Training loss: 4.8325 1.1373 sec/batch\n",
      "Epoch 18/20  Iteration 6419/7380 Training loss: 4.8326 1.1124 sec/batch\n",
      "Epoch 18/20  Iteration 6420/7380 Training loss: 4.8326 1.1122 sec/batch\n",
      "Epoch 18/20  Iteration 6421/7380 Training loss: 4.8326 1.1156 sec/batch\n",
      "Epoch 18/20  Iteration 6422/7380 Training loss: 4.8319 1.1123 sec/batch\n",
      "Epoch 18/20  Iteration 6423/7380 Training loss: 4.8323 1.1106 sec/batch\n",
      "Epoch 18/20  Iteration 6424/7380 Training loss: 4.8318 1.1164 sec/batch\n",
      "Epoch 18/20  Iteration 6425/7380 Training loss: 4.8322 1.1161 sec/batch\n",
      "Epoch 18/20  Iteration 6426/7380 Training loss: 4.8326 1.1082 sec/batch\n",
      "Epoch 18/20  Iteration 6427/7380 Training loss: 4.8333 1.1152 sec/batch\n",
      "Epoch 18/20  Iteration 6428/7380 Training loss: 4.8333 1.1163 sec/batch\n",
      "Epoch 18/20  Iteration 6429/7380 Training loss: 4.8328 1.1279 sec/batch\n",
      "Epoch 18/20  Iteration 6430/7380 Training loss: 4.8325 1.1124 sec/batch\n",
      "Epoch 18/20  Iteration 6431/7380 Training loss: 4.8334 1.1000 sec/batch\n",
      "Epoch 18/20  Iteration 6432/7380 Training loss: 4.8329 1.1195 sec/batch\n",
      "Epoch 18/20  Iteration 6433/7380 Training loss: 4.8328 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6434/7380 Training loss: 4.8335 1.1087 sec/batch\n",
      "Epoch 18/20  Iteration 6435/7380 Training loss: 4.8340 1.1239 sec/batch\n",
      "Epoch 18/20  Iteration 6436/7380 Training loss: 4.8344 1.1217 sec/batch\n",
      "Epoch 18/20  Iteration 6437/7380 Training loss: 4.8349 1.1097 sec/batch\n",
      "Epoch 18/20  Iteration 6438/7380 Training loss: 4.8353 1.1134 sec/batch\n",
      "Epoch 18/20  Iteration 6439/7380 Training loss: 4.8357 1.1252 sec/batch\n",
      "Epoch 18/20  Iteration 6440/7380 Training loss: 4.8357 1.1246 sec/batch\n",
      "Epoch 18/20  Iteration 6441/7380 Training loss: 4.8361 1.1123 sec/batch\n",
      "Epoch 18/20  Iteration 6442/7380 Training loss: 4.8355 1.1114 sec/batch\n",
      "Epoch 18/20  Iteration 6443/7380 Training loss: 4.8352 1.1171 sec/batch\n",
      "Epoch 18/20  Iteration 6444/7380 Training loss: 4.8353 1.1222 sec/batch\n",
      "Epoch 18/20  Iteration 6445/7380 Training loss: 4.8360 1.1099 sec/batch\n",
      "Epoch 18/20  Iteration 6446/7380 Training loss: 4.8360 1.1153 sec/batch\n",
      "Epoch 18/20  Iteration 6447/7380 Training loss: 4.8356 1.1087 sec/batch\n",
      "Epoch 18/20  Iteration 6448/7380 Training loss: 4.8355 1.1080 sec/batch\n",
      "Epoch 18/20  Iteration 6449/7380 Training loss: 4.8352 1.1148 sec/batch\n",
      "Epoch 18/20  Iteration 6450/7380 Training loss: 4.8352 1.1287 sec/batch\n",
      "Validation loss: 5.01223 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 6451/7380 Training loss: 4.8364 1.1171 sec/batch\n",
      "Epoch 18/20  Iteration 6452/7380 Training loss: 4.8358 1.1139 sec/batch\n",
      "Epoch 18/20  Iteration 6453/7380 Training loss: 4.8354 1.1162 sec/batch\n",
      "Epoch 18/20  Iteration 6454/7380 Training loss: 4.8351 1.1215 sec/batch\n",
      "Epoch 18/20  Iteration 6455/7380 Training loss: 4.8352 1.1060 sec/batch\n",
      "Epoch 18/20  Iteration 6456/7380 Training loss: 4.8358 1.1094 sec/batch\n",
      "Epoch 18/20  Iteration 6457/7380 Training loss: 4.8356 1.1162 sec/batch\n",
      "Epoch 18/20  Iteration 6458/7380 Training loss: 4.8357 1.1222 sec/batch\n",
      "Epoch 18/20  Iteration 6459/7380 Training loss: 4.8358 1.1144 sec/batch\n",
      "Epoch 18/20  Iteration 6460/7380 Training loss: 4.8353 1.1229 sec/batch\n",
      "Epoch 18/20  Iteration 6461/7380 Training loss: 4.8351 1.1291 sec/batch\n",
      "Epoch 18/20  Iteration 6462/7380 Training loss: 4.8348 1.1162 sec/batch\n",
      "Epoch 18/20  Iteration 6463/7380 Training loss: 4.8348 1.1265 sec/batch\n",
      "Epoch 18/20  Iteration 6464/7380 Training loss: 4.8338 1.1185 sec/batch\n",
      "Epoch 18/20  Iteration 6465/7380 Training loss: 4.8341 1.1244 sec/batch\n",
      "Epoch 18/20  Iteration 6466/7380 Training loss: 4.8346 1.1151 sec/batch\n",
      "Epoch 18/20  Iteration 6467/7380 Training loss: 4.8340 1.1260 sec/batch\n",
      "Epoch 18/20  Iteration 6468/7380 Training loss: 4.8343 1.1173 sec/batch\n",
      "Epoch 18/20  Iteration 6469/7380 Training loss: 4.8343 1.1125 sec/batch\n",
      "Epoch 18/20  Iteration 6470/7380 Training loss: 4.8344 1.1271 sec/batch\n",
      "Epoch 18/20  Iteration 6471/7380 Training loss: 4.8345 1.1164 sec/batch\n",
      "Epoch 18/20  Iteration 6472/7380 Training loss: 4.8341 1.1235 sec/batch\n",
      "Epoch 18/20  Iteration 6473/7380 Training loss: 4.8342 1.1127 sec/batch\n",
      "Epoch 18/20  Iteration 6474/7380 Training loss: 4.8345 1.1105 sec/batch\n",
      "Epoch 18/20  Iteration 6475/7380 Training loss: 4.8349 1.1435 sec/batch\n",
      "Epoch 18/20  Iteration 6476/7380 Training loss: 4.8355 1.1133 sec/batch\n",
      "Epoch 18/20  Iteration 6477/7380 Training loss: 4.8354 1.1120 sec/batch\n",
      "Epoch 18/20  Iteration 6478/7380 Training loss: 4.8356 1.1261 sec/batch\n",
      "Epoch 18/20  Iteration 6479/7380 Training loss: 4.8358 1.1224 sec/batch\n",
      "Epoch 18/20  Iteration 6480/7380 Training loss: 4.8350 1.1247 sec/batch\n",
      "Epoch 18/20  Iteration 6481/7380 Training loss: 4.8347 1.1147 sec/batch\n",
      "Epoch 18/20  Iteration 6482/7380 Training loss: 4.8344 1.1086 sec/batch\n",
      "Epoch 18/20  Iteration 6483/7380 Training loss: 4.8338 1.1162 sec/batch\n",
      "Epoch 18/20  Iteration 6484/7380 Training loss: 4.8337 1.1174 sec/batch\n",
      "Epoch 18/20  Iteration 6485/7380 Training loss: 4.8336 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6486/7380 Training loss: 4.8337 1.1111 sec/batch\n",
      "Epoch 18/20  Iteration 6487/7380 Training loss: 4.8337 1.1078 sec/batch\n",
      "Epoch 18/20  Iteration 6488/7380 Training loss: 4.8331 1.1077 sec/batch\n",
      "Epoch 18/20  Iteration 6489/7380 Training loss: 4.8326 1.1125 sec/batch\n",
      "Epoch 18/20  Iteration 6490/7380 Training loss: 4.8322 1.1086 sec/batch\n",
      "Epoch 18/20  Iteration 6491/7380 Training loss: 4.8325 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6492/7380 Training loss: 4.8328 1.1098 sec/batch\n",
      "Epoch 18/20  Iteration 6493/7380 Training loss: 4.8329 1.1025 sec/batch\n",
      "Epoch 18/20  Iteration 6494/7380 Training loss: 4.8325 1.1119 sec/batch\n",
      "Epoch 18/20  Iteration 6495/7380 Training loss: 4.8329 1.1276 sec/batch\n",
      "Epoch 18/20  Iteration 6496/7380 Training loss: 4.8330 1.1219 sec/batch\n",
      "Epoch 18/20  Iteration 6497/7380 Training loss: 4.8325 1.1141 sec/batch\n",
      "Epoch 18/20  Iteration 6498/7380 Training loss: 4.8326 1.1345 sec/batch\n",
      "Epoch 18/20  Iteration 6499/7380 Training loss: 4.8323 1.1155 sec/batch\n",
      "Epoch 18/20  Iteration 6500/7380 Training loss: 4.8323 1.1198 sec/batch\n",
      "Validation loss: 5.01465 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 6501/7380 Training loss: 4.8325 1.1136 sec/batch\n",
      "Epoch 18/20  Iteration 6502/7380 Training loss: 4.8325 1.1236 sec/batch\n",
      "Epoch 18/20  Iteration 6503/7380 Training loss: 4.8331 1.1059 sec/batch\n",
      "Epoch 18/20  Iteration 6504/7380 Training loss: 4.8327 1.1390 sec/batch\n",
      "Epoch 18/20  Iteration 6505/7380 Training loss: 4.8324 1.1201 sec/batch\n",
      "Epoch 18/20  Iteration 6506/7380 Training loss: 4.8320 1.1227 sec/batch\n",
      "Epoch 18/20  Iteration 6507/7380 Training loss: 4.8318 1.1094 sec/batch\n",
      "Epoch 18/20  Iteration 6508/7380 Training loss: 4.8314 1.1239 sec/batch\n",
      "Epoch 18/20  Iteration 6509/7380 Training loss: 4.8311 1.1089 sec/batch\n",
      "Epoch 18/20  Iteration 6510/7380 Training loss: 4.8310 1.1077 sec/batch\n",
      "Epoch 18/20  Iteration 6511/7380 Training loss: 4.8303 1.1120 sec/batch\n",
      "Epoch 18/20  Iteration 6512/7380 Training loss: 4.8292 1.1216 sec/batch\n",
      "Epoch 18/20  Iteration 6513/7380 Training loss: 4.8288 1.1111 sec/batch\n",
      "Epoch 18/20  Iteration 6514/7380 Training loss: 4.8290 1.1152 sec/batch\n",
      "Epoch 18/20  Iteration 6515/7380 Training loss: 4.8296 1.1218 sec/batch\n",
      "Epoch 18/20  Iteration 6516/7380 Training loss: 4.8293 1.1200 sec/batch\n",
      "Epoch 18/20  Iteration 6517/7380 Training loss: 4.8293 1.1072 sec/batch\n",
      "Epoch 18/20  Iteration 6518/7380 Training loss: 4.8295 1.1225 sec/batch\n",
      "Epoch 18/20  Iteration 6519/7380 Training loss: 4.8298 1.1222 sec/batch\n",
      "Epoch 18/20  Iteration 6520/7380 Training loss: 4.8298 1.1128 sec/batch\n",
      "Epoch 18/20  Iteration 6521/7380 Training loss: 4.8299 1.1251 sec/batch\n",
      "Epoch 18/20  Iteration 6522/7380 Training loss: 4.8299 1.1114 sec/batch\n",
      "Epoch 18/20  Iteration 6523/7380 Training loss: 4.8299 1.1117 sec/batch\n",
      "Epoch 18/20  Iteration 6524/7380 Training loss: 4.8297 1.1215 sec/batch\n",
      "Epoch 18/20  Iteration 6525/7380 Training loss: 4.8296 1.1242 sec/batch\n",
      "Epoch 18/20  Iteration 6526/7380 Training loss: 4.8295 1.1446 sec/batch\n",
      "Epoch 18/20  Iteration 6527/7380 Training loss: 4.8290 1.1154 sec/batch\n",
      "Epoch 18/20  Iteration 6528/7380 Training loss: 4.8293 1.1159 sec/batch\n",
      "Epoch 18/20  Iteration 6529/7380 Training loss: 4.8294 1.1277 sec/batch\n",
      "Epoch 18/20  Iteration 6530/7380 Training loss: 4.8296 1.1247 sec/batch\n",
      "Epoch 18/20  Iteration 6531/7380 Training loss: 4.8295 1.1176 sec/batch\n",
      "Epoch 18/20  Iteration 6532/7380 Training loss: 4.8298 1.1158 sec/batch\n",
      "Epoch 18/20  Iteration 6533/7380 Training loss: 4.8292 1.1153 sec/batch\n",
      "Epoch 18/20  Iteration 6534/7380 Training loss: 4.8289 1.1138 sec/batch\n",
      "Epoch 18/20  Iteration 6535/7380 Training loss: 4.8290 1.1116 sec/batch\n",
      "Epoch 18/20  Iteration 6536/7380 Training loss: 4.8292 1.1117 sec/batch\n",
      "Epoch 18/20  Iteration 6537/7380 Training loss: 4.8289 1.0988 sec/batch\n",
      "Epoch 18/20  Iteration 6538/7380 Training loss: 4.8286 1.1140 sec/batch\n",
      "Epoch 18/20  Iteration 6539/7380 Training loss: 4.8284 1.1045 sec/batch\n",
      "Epoch 18/20  Iteration 6540/7380 Training loss: 4.8286 1.1038 sec/batch\n",
      "Epoch 18/20  Iteration 6541/7380 Training loss: 4.8285 1.1284 sec/batch\n",
      "Epoch 18/20  Iteration 6542/7380 Training loss: 4.8279 1.1119 sec/batch\n",
      "Epoch 18/20  Iteration 6543/7380 Training loss: 4.8282 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6544/7380 Training loss: 4.8283 1.1117 sec/batch\n",
      "Epoch 18/20  Iteration 6545/7380 Training loss: 4.8280 1.1166 sec/batch\n",
      "Epoch 18/20  Iteration 6546/7380 Training loss: 4.8282 1.1405 sec/batch\n",
      "Epoch 18/20  Iteration 6547/7380 Training loss: 4.8284 1.1102 sec/batch\n",
      "Epoch 18/20  Iteration 6548/7380 Training loss: 4.8283 1.1165 sec/batch\n",
      "Epoch 18/20  Iteration 6549/7380 Training loss: 4.8280 1.1323 sec/batch\n",
      "Epoch 18/20  Iteration 6550/7380 Training loss: 4.8287 1.1416 sec/batch\n",
      "Validation loss: 5.01404 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 6551/7380 Training loss: 4.8293 1.1122 sec/batch\n",
      "Epoch 18/20  Iteration 6552/7380 Training loss: 4.8295 1.1120 sec/batch\n",
      "Epoch 18/20  Iteration 6553/7380 Training loss: 4.8293 1.1201 sec/batch\n",
      "Epoch 18/20  Iteration 6554/7380 Training loss: 4.8295 1.1118 sec/batch\n",
      "Epoch 18/20  Iteration 6555/7380 Training loss: 4.8295 1.1084 sec/batch\n",
      "Epoch 18/20  Iteration 6556/7380 Training loss: 4.8296 1.1139 sec/batch\n",
      "Epoch 18/20  Iteration 6557/7380 Training loss: 4.8295 1.1147 sec/batch\n",
      "Epoch 18/20  Iteration 6558/7380 Training loss: 4.8298 1.1239 sec/batch\n",
      "Epoch 18/20  Iteration 6559/7380 Training loss: 4.8295 1.1375 sec/batch\n",
      "Epoch 18/20  Iteration 6560/7380 Training loss: 4.8291 1.1192 sec/batch\n",
      "Epoch 18/20  Iteration 6561/7380 Training loss: 4.8290 1.1301 sec/batch\n",
      "Epoch 18/20  Iteration 6562/7380 Training loss: 4.8287 1.1143 sec/batch\n",
      "Epoch 18/20  Iteration 6563/7380 Training loss: 4.8283 1.1248 sec/batch\n",
      "Epoch 18/20  Iteration 6564/7380 Training loss: 4.8280 1.1163 sec/batch\n",
      "Epoch 18/20  Iteration 6565/7380 Training loss: 4.8279 1.1128 sec/batch\n",
      "Epoch 18/20  Iteration 6566/7380 Training loss: 4.8277 1.1132 sec/batch\n",
      "Epoch 18/20  Iteration 6567/7380 Training loss: 4.8278 1.1070 sec/batch\n",
      "Epoch 18/20  Iteration 6568/7380 Training loss: 4.8279 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6569/7380 Training loss: 4.8277 1.1126 sec/batch\n",
      "Epoch 18/20  Iteration 6570/7380 Training loss: 4.8276 1.1131 sec/batch\n",
      "Epoch 18/20  Iteration 6571/7380 Training loss: 4.8273 1.1138 sec/batch\n",
      "Epoch 18/20  Iteration 6572/7380 Training loss: 4.8276 1.1129 sec/batch\n",
      "Epoch 18/20  Iteration 6573/7380 Training loss: 4.8283 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6574/7380 Training loss: 4.8282 1.1255 sec/batch\n",
      "Epoch 18/20  Iteration 6575/7380 Training loss: 4.8284 1.1195 sec/batch\n",
      "Epoch 18/20  Iteration 6576/7380 Training loss: 4.8285 1.1061 sec/batch\n",
      "Epoch 18/20  Iteration 6577/7380 Training loss: 4.8291 1.1260 sec/batch\n",
      "Epoch 18/20  Iteration 6578/7380 Training loss: 4.8292 1.2329 sec/batch\n",
      "Epoch 18/20  Iteration 6579/7380 Training loss: 4.8291 1.1129 sec/batch\n",
      "Epoch 18/20  Iteration 6580/7380 Training loss: 4.8288 1.1057 sec/batch\n",
      "Epoch 18/20  Iteration 6581/7380 Training loss: 4.8285 1.1188 sec/batch\n",
      "Epoch 18/20  Iteration 6582/7380 Training loss: 4.8283 1.1083 sec/batch\n",
      "Epoch 18/20  Iteration 6583/7380 Training loss: 4.8280 1.1206 sec/batch\n",
      "Epoch 18/20  Iteration 6584/7380 Training loss: 4.8281 1.1220 sec/batch\n",
      "Epoch 18/20  Iteration 6585/7380 Training loss: 4.8279 1.1215 sec/batch\n",
      "Epoch 18/20  Iteration 6586/7380 Training loss: 4.8276 1.1072 sec/batch\n",
      "Epoch 18/20  Iteration 6587/7380 Training loss: 4.8276 1.1122 sec/batch\n",
      "Epoch 18/20  Iteration 6588/7380 Training loss: 4.8277 1.1153 sec/batch\n",
      "Epoch 18/20  Iteration 6589/7380 Training loss: 4.8279 1.1140 sec/batch\n",
      "Epoch 18/20  Iteration 6590/7380 Training loss: 4.8280 1.1137 sec/batch\n",
      "Epoch 18/20  Iteration 6591/7380 Training loss: 4.8283 1.1196 sec/batch\n",
      "Epoch 18/20  Iteration 6592/7380 Training loss: 4.8289 1.1150 sec/batch\n",
      "Epoch 18/20  Iteration 6593/7380 Training loss: 4.8292 1.1039 sec/batch\n",
      "Epoch 18/20  Iteration 6594/7380 Training loss: 4.8289 1.1106 sec/batch\n",
      "Epoch 18/20  Iteration 6595/7380 Training loss: 4.8287 1.1264 sec/batch\n",
      "Epoch 18/20  Iteration 6596/7380 Training loss: 4.8288 1.1090 sec/batch\n",
      "Epoch 18/20  Iteration 6597/7380 Training loss: 4.8288 1.1130 sec/batch\n",
      "Epoch 18/20  Iteration 6598/7380 Training loss: 4.8288 1.1169 sec/batch\n",
      "Epoch 18/20  Iteration 6599/7380 Training loss: 4.8293 1.1144 sec/batch\n",
      "Epoch 18/20  Iteration 6600/7380 Training loss: 4.8292 1.1217 sec/batch\n",
      "Validation loss: 5.00793 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 6601/7380 Training loss: 4.8293 1.1141 sec/batch\n",
      "Epoch 18/20  Iteration 6602/7380 Training loss: 4.8292 1.1276 sec/batch\n",
      "Epoch 18/20  Iteration 6603/7380 Training loss: 4.8291 1.1322 sec/batch\n",
      "Epoch 18/20  Iteration 6604/7380 Training loss: 4.8287 1.1126 sec/batch\n",
      "Epoch 18/20  Iteration 6605/7380 Training loss: 4.8289 1.1067 sec/batch\n",
      "Epoch 18/20  Iteration 6606/7380 Training loss: 4.8285 1.1194 sec/batch\n",
      "Epoch 18/20  Iteration 6607/7380 Training loss: 4.8288 1.1186 sec/batch\n",
      "Epoch 18/20  Iteration 6608/7380 Training loss: 4.8289 1.1136 sec/batch\n",
      "Epoch 18/20  Iteration 6609/7380 Training loss: 4.8290 1.1181 sec/batch\n",
      "Epoch 18/20  Iteration 6610/7380 Training loss: 4.8292 1.1142 sec/batch\n",
      "Epoch 18/20  Iteration 6611/7380 Training loss: 4.8291 1.1157 sec/batch\n",
      "Epoch 18/20  Iteration 6612/7380 Training loss: 4.8292 1.1045 sec/batch\n",
      "Epoch 18/20  Iteration 6613/7380 Training loss: 4.8289 1.1203 sec/batch\n",
      "Epoch 18/20  Iteration 6614/7380 Training loss: 4.8287 1.1259 sec/batch\n",
      "Epoch 18/20  Iteration 6615/7380 Training loss: 4.8284 1.1177 sec/batch\n",
      "Epoch 18/20  Iteration 6616/7380 Training loss: 4.8284 1.1156 sec/batch\n",
      "Epoch 18/20  Iteration 6617/7380 Training loss: 4.8284 1.1158 sec/batch\n",
      "Epoch 18/20  Iteration 6618/7380 Training loss: 4.8283 1.1167 sec/batch\n",
      "Epoch 18/20  Iteration 6619/7380 Training loss: 4.8282 1.1208 sec/batch\n",
      "Epoch 18/20  Iteration 6620/7380 Training loss: 4.8284 1.1135 sec/batch\n",
      "Epoch 18/20  Iteration 6621/7380 Training loss: 4.8285 1.1305 sec/batch\n",
      "Epoch 18/20  Iteration 6622/7380 Training loss: 4.8283 1.1149 sec/batch\n",
      "Epoch 18/20  Iteration 6623/7380 Training loss: 4.8282 1.1259 sec/batch\n",
      "Epoch 18/20  Iteration 6624/7380 Training loss: 4.8286 1.1148 sec/batch\n",
      "Epoch 18/20  Iteration 6625/7380 Training loss: 4.8283 1.1068 sec/batch\n",
      "Epoch 18/20  Iteration 6626/7380 Training loss: 4.8286 1.1152 sec/batch\n",
      "Epoch 18/20  Iteration 6627/7380 Training loss: 4.8284 1.1207 sec/batch\n",
      "Epoch 18/20  Iteration 6628/7380 Training loss: 4.8282 1.1248 sec/batch\n",
      "Epoch 18/20  Iteration 6629/7380 Training loss: 4.8281 1.1240 sec/batch\n",
      "Epoch 18/20  Iteration 6630/7380 Training loss: 4.8283 1.1139 sec/batch\n",
      "Epoch 18/20  Iteration 6631/7380 Training loss: 4.8283 1.1156 sec/batch\n",
      "Epoch 18/20  Iteration 6632/7380 Training loss: 4.8280 1.1591 sec/batch\n",
      "Epoch 18/20  Iteration 6633/7380 Training loss: 4.8277 1.1160 sec/batch\n",
      "Epoch 18/20  Iteration 6634/7380 Training loss: 4.8277 1.1216 sec/batch\n",
      "Epoch 18/20  Iteration 6635/7380 Training loss: 4.8278 1.1192 sec/batch\n",
      "Epoch 18/20  Iteration 6636/7380 Training loss: 4.8277 1.1145 sec/batch\n",
      "Epoch 18/20  Iteration 6637/7380 Training loss: 4.8275 1.1040 sec/batch\n",
      "Epoch 18/20  Iteration 6638/7380 Training loss: 4.8274 1.1145 sec/batch\n",
      "Epoch 18/20  Iteration 6639/7380 Training loss: 4.8273 1.1136 sec/batch\n",
      "Epoch 18/20  Iteration 6640/7380 Training loss: 4.8271 1.1137 sec/batch\n",
      "Epoch 18/20  Iteration 6641/7380 Training loss: 4.8273 1.1206 sec/batch\n",
      "Epoch 18/20  Iteration 6642/7380 Training loss: 4.8270 1.1171 sec/batch\n",
      "Epoch 19/20  Iteration 6643/7380 Training loss: 4.8420 1.1092 sec/batch\n",
      "Epoch 19/20  Iteration 6644/7380 Training loss: 4.8213 1.1153 sec/batch\n",
      "Epoch 19/20  Iteration 6645/7380 Training loss: 4.8161 1.1327 sec/batch\n",
      "Epoch 19/20  Iteration 6646/7380 Training loss: 4.8087 1.1133 sec/batch\n",
      "Epoch 19/20  Iteration 6647/7380 Training loss: 4.8023 1.1144 sec/batch\n",
      "Epoch 19/20  Iteration 6648/7380 Training loss: 4.8085 1.1267 sec/batch\n",
      "Epoch 19/20  Iteration 6649/7380 Training loss: 4.8266 1.1198 sec/batch\n",
      "Epoch 19/20  Iteration 6650/7380 Training loss: 4.8139 1.1230 sec/batch\n",
      "Validation loss: 5.01015 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 6651/7380 Training loss: 4.8320 1.1302 sec/batch\n",
      "Epoch 19/20  Iteration 6652/7380 Training loss: 4.8427 1.1222 sec/batch\n",
      "Epoch 19/20  Iteration 6653/7380 Training loss: 4.8388 1.1311 sec/batch\n",
      "Epoch 19/20  Iteration 6654/7380 Training loss: 4.8287 1.1049 sec/batch\n",
      "Epoch 19/20  Iteration 6655/7380 Training loss: 4.8260 1.1186 sec/batch\n",
      "Epoch 19/20  Iteration 6656/7380 Training loss: 4.8321 1.1184 sec/batch\n",
      "Epoch 19/20  Iteration 6657/7380 Training loss: 4.8352 1.1144 sec/batch\n",
      "Epoch 19/20  Iteration 6658/7380 Training loss: 4.8350 1.1225 sec/batch\n",
      "Epoch 19/20  Iteration 6659/7380 Training loss: 4.8301 1.1134 sec/batch\n",
      "Epoch 19/20  Iteration 6660/7380 Training loss: 4.8165 1.1114 sec/batch\n",
      "Epoch 19/20  Iteration 6661/7380 Training loss: 4.8109 1.1155 sec/batch\n",
      "Epoch 19/20  Iteration 6662/7380 Training loss: 4.8087 1.1178 sec/batch\n",
      "Epoch 19/20  Iteration 6663/7380 Training loss: 4.8048 1.1200 sec/batch\n",
      "Epoch 19/20  Iteration 6664/7380 Training loss: 4.8109 1.1141 sec/batch\n",
      "Epoch 19/20  Iteration 6665/7380 Training loss: 4.8109 1.1209 sec/batch\n",
      "Epoch 19/20  Iteration 6666/7380 Training loss: 4.8162 1.1137 sec/batch\n",
      "Epoch 19/20  Iteration 6667/7380 Training loss: 4.8197 1.1189 sec/batch\n",
      "Epoch 19/20  Iteration 6668/7380 Training loss: 4.8207 1.1270 sec/batch\n",
      "Epoch 19/20  Iteration 6669/7380 Training loss: 4.8146 1.1242 sec/batch\n",
      "Epoch 19/20  Iteration 6670/7380 Training loss: 4.8193 1.1119 sec/batch\n",
      "Epoch 19/20  Iteration 6671/7380 Training loss: 4.8181 1.1136 sec/batch\n",
      "Epoch 19/20  Iteration 6672/7380 Training loss: 4.8179 1.1097 sec/batch\n",
      "Epoch 19/20  Iteration 6673/7380 Training loss: 4.8167 1.1277 sec/batch\n",
      "Epoch 19/20  Iteration 6674/7380 Training loss: 4.8178 1.1193 sec/batch\n",
      "Epoch 19/20  Iteration 6675/7380 Training loss: 4.8178 1.1187 sec/batch\n",
      "Epoch 19/20  Iteration 6676/7380 Training loss: 4.8194 1.1433 sec/batch\n",
      "Epoch 19/20  Iteration 6677/7380 Training loss: 4.8221 1.1240 sec/batch\n",
      "Epoch 19/20  Iteration 6678/7380 Training loss: 4.8209 1.1159 sec/batch\n",
      "Epoch 19/20  Iteration 6679/7380 Training loss: 4.8213 1.1144 sec/batch\n",
      "Epoch 19/20  Iteration 6680/7380 Training loss: 4.8198 1.1201 sec/batch\n",
      "Epoch 19/20  Iteration 6681/7380 Training loss: 4.8213 1.1119 sec/batch\n",
      "Epoch 19/20  Iteration 6682/7380 Training loss: 4.8212 1.1550 sec/batch\n",
      "Epoch 19/20  Iteration 6683/7380 Training loss: 4.8217 1.1258 sec/batch\n",
      "Epoch 19/20  Iteration 6684/7380 Training loss: 4.8211 1.1487 sec/batch\n",
      "Epoch 19/20  Iteration 6685/7380 Training loss: 4.8213 1.1165 sec/batch\n",
      "Epoch 19/20  Iteration 6686/7380 Training loss: 4.8217 1.1234 sec/batch\n",
      "Epoch 19/20  Iteration 6687/7380 Training loss: 4.8229 1.1116 sec/batch\n",
      "Epoch 19/20  Iteration 6688/7380 Training loss: 4.8212 1.1148 sec/batch\n",
      "Epoch 19/20  Iteration 6689/7380 Training loss: 4.8222 1.1202 sec/batch\n",
      "Epoch 19/20  Iteration 6690/7380 Training loss: 4.8198 1.1100 sec/batch\n",
      "Epoch 19/20  Iteration 6691/7380 Training loss: 4.8208 1.1278 sec/batch\n",
      "Epoch 19/20  Iteration 6692/7380 Training loss: 4.8204 1.1242 sec/batch\n",
      "Epoch 19/20  Iteration 6693/7380 Training loss: 4.8197 1.1133 sec/batch\n",
      "Epoch 19/20  Iteration 6694/7380 Training loss: 4.8185 1.1131 sec/batch\n",
      "Epoch 19/20  Iteration 6695/7380 Training loss: 4.8164 1.1144 sec/batch\n",
      "Epoch 19/20  Iteration 6696/7380 Training loss: 4.8155 1.1209 sec/batch\n",
      "Epoch 19/20  Iteration 6697/7380 Training loss: 4.8142 1.1166 sec/batch\n",
      "Epoch 19/20  Iteration 6698/7380 Training loss: 4.8118 1.1053 sec/batch\n",
      "Epoch 19/20  Iteration 6699/7380 Training loss: 4.8113 1.1161 sec/batch\n",
      "Epoch 19/20  Iteration 6700/7380 Training loss: 4.8120 1.1114 sec/batch\n",
      "Validation loss: 5.01475 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 6701/7380 Training loss: 4.8139 1.1177 sec/batch\n",
      "Epoch 19/20  Iteration 6702/7380 Training loss: 4.8155 1.1199 sec/batch\n",
      "Epoch 19/20  Iteration 6703/7380 Training loss: 4.8138 1.1258 sec/batch\n",
      "Epoch 19/20  Iteration 6704/7380 Training loss: 4.8145 1.1201 sec/batch\n",
      "Epoch 19/20  Iteration 6705/7380 Training loss: 4.8136 1.1167 sec/batch\n",
      "Epoch 19/20  Iteration 6706/7380 Training loss: 4.8117 1.1110 sec/batch\n",
      "Epoch 19/20  Iteration 6707/7380 Training loss: 4.8122 1.1131 sec/batch\n",
      "Epoch 19/20  Iteration 6708/7380 Training loss: 4.8113 1.1183 sec/batch\n",
      "Epoch 19/20  Iteration 6709/7380 Training loss: 4.8113 1.1195 sec/batch\n",
      "Epoch 19/20  Iteration 6710/7380 Training loss: 4.8112 1.1120 sec/batch\n",
      "Epoch 19/20  Iteration 6711/7380 Training loss: 4.8127 1.1109 sec/batch\n",
      "Epoch 19/20  Iteration 6712/7380 Training loss: 4.8119 1.1202 sec/batch\n",
      "Epoch 19/20  Iteration 6713/7380 Training loss: 4.8103 1.1205 sec/batch\n",
      "Epoch 19/20  Iteration 6714/7380 Training loss: 4.8115 1.1290 sec/batch\n",
      "Epoch 19/20  Iteration 6715/7380 Training loss: 4.8098 1.1085 sec/batch\n",
      "Epoch 19/20  Iteration 6716/7380 Training loss: 4.8112 1.1136 sec/batch\n",
      "Epoch 19/20  Iteration 6717/7380 Training loss: 4.8128 1.1145 sec/batch\n",
      "Epoch 19/20  Iteration 6718/7380 Training loss: 4.8126 1.1057 sec/batch\n",
      "Epoch 19/20  Iteration 6719/7380 Training loss: 4.8123 1.1174 sec/batch\n",
      "Epoch 19/20  Iteration 6720/7380 Training loss: 4.8120 1.1148 sec/batch\n",
      "Epoch 19/20  Iteration 6721/7380 Training loss: 4.8127 1.1128 sec/batch\n",
      "Epoch 19/20  Iteration 6722/7380 Training loss: 4.8132 1.1235 sec/batch\n",
      "Epoch 19/20  Iteration 6723/7380 Training loss: 4.8126 1.1167 sec/batch\n",
      "Epoch 19/20  Iteration 6724/7380 Training loss: 4.8123 1.1131 sec/batch\n",
      "Epoch 19/20  Iteration 6725/7380 Training loss: 4.8111 1.1158 sec/batch\n",
      "Epoch 19/20  Iteration 6726/7380 Training loss: 4.8109 1.1336 sec/batch\n",
      "Epoch 19/20  Iteration 6727/7380 Training loss: 4.8113 1.1127 sec/batch\n",
      "Epoch 19/20  Iteration 6728/7380 Training loss: 4.8105 1.1241 sec/batch\n",
      "Epoch 19/20  Iteration 6729/7380 Training loss: 4.8101 1.1072 sec/batch\n",
      "Epoch 19/20  Iteration 6730/7380 Training loss: 4.8098 1.1226 sec/batch\n",
      "Epoch 19/20  Iteration 6731/7380 Training loss: 4.8082 1.1214 sec/batch\n",
      "Epoch 19/20  Iteration 6732/7380 Training loss: 4.8078 1.1239 sec/batch\n",
      "Epoch 19/20  Iteration 6733/7380 Training loss: 4.8091 1.1281 sec/batch\n",
      "Epoch 19/20  Iteration 6734/7380 Training loss: 4.8107 1.1150 sec/batch\n",
      "Epoch 19/20  Iteration 6735/7380 Training loss: 4.8103 1.1110 sec/batch\n",
      "Epoch 19/20  Iteration 6736/7380 Training loss: 4.8099 1.1120 sec/batch\n",
      "Epoch 19/20  Iteration 6737/7380 Training loss: 4.8095 1.1137 sec/batch\n",
      "Epoch 19/20  Iteration 6738/7380 Training loss: 4.8091 1.1194 sec/batch\n",
      "Epoch 19/20  Iteration 6739/7380 Training loss: 4.8090 1.1125 sec/batch\n",
      "Epoch 19/20  Iteration 6740/7380 Training loss: 4.8095 1.1151 sec/batch\n",
      "Epoch 19/20  Iteration 6741/7380 Training loss: 4.8097 1.1144 sec/batch\n",
      "Epoch 19/20  Iteration 6742/7380 Training loss: 4.8092 1.1185 sec/batch\n",
      "Epoch 19/20  Iteration 6743/7380 Training loss: 4.8101 1.1179 sec/batch\n",
      "Epoch 19/20  Iteration 6744/7380 Training loss: 4.8106 1.1145 sec/batch\n",
      "Epoch 19/20  Iteration 6745/7380 Training loss: 4.8109 1.1178 sec/batch\n",
      "Epoch 19/20  Iteration 6746/7380 Training loss: 4.8107 1.1223 sec/batch\n",
      "Epoch 19/20  Iteration 6747/7380 Training loss: 4.8109 1.1208 sec/batch\n",
      "Epoch 19/20  Iteration 6748/7380 Training loss: 4.8111 1.1269 sec/batch\n",
      "Epoch 19/20  Iteration 6749/7380 Training loss: 4.8109 1.1164 sec/batch\n",
      "Epoch 19/20  Iteration 6750/7380 Training loss: 4.8108 1.1353 sec/batch\n",
      "Validation loss: 4.99229 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 6751/7380 Training loss: 4.8105 1.1125 sec/batch\n",
      "Epoch 19/20  Iteration 6752/7380 Training loss: 4.8105 1.1164 sec/batch\n",
      "Epoch 19/20  Iteration 6753/7380 Training loss: 4.8099 1.1131 sec/batch\n",
      "Epoch 19/20  Iteration 6754/7380 Training loss: 4.8104 1.1055 sec/batch\n",
      "Epoch 19/20  Iteration 6755/7380 Training loss: 4.8096 1.1156 sec/batch\n",
      "Epoch 19/20  Iteration 6756/7380 Training loss: 4.8100 1.1321 sec/batch\n",
      "Epoch 19/20  Iteration 6757/7380 Training loss: 4.8100 1.1107 sec/batch\n",
      "Epoch 19/20  Iteration 6758/7380 Training loss: 4.8097 1.1166 sec/batch\n",
      "Epoch 19/20  Iteration 6759/7380 Training loss: 4.8090 1.1488 sec/batch\n",
      "Epoch 19/20  Iteration 6760/7380 Training loss: 4.8086 1.1090 sec/batch\n",
      "Epoch 19/20  Iteration 6761/7380 Training loss: 4.8092 1.1079 sec/batch\n",
      "Epoch 19/20  Iteration 6762/7380 Training loss: 4.8090 1.1286 sec/batch\n",
      "Epoch 19/20  Iteration 6763/7380 Training loss: 4.8093 1.1071 sec/batch\n",
      "Epoch 19/20  Iteration 6764/7380 Training loss: 4.8101 1.1311 sec/batch\n",
      "Epoch 19/20  Iteration 6765/7380 Training loss: 4.8104 1.1149 sec/batch\n",
      "Epoch 19/20  Iteration 6766/7380 Training loss: 4.8103 1.1201 sec/batch\n",
      "Epoch 19/20  Iteration 6767/7380 Training loss: 4.8104 1.1085 sec/batch\n",
      "Epoch 19/20  Iteration 6768/7380 Training loss: 4.8090 1.1194 sec/batch\n",
      "Epoch 19/20  Iteration 6769/7380 Training loss: 4.8077 1.1398 sec/batch\n",
      "Epoch 19/20  Iteration 6770/7380 Training loss: 4.8076 1.1254 sec/batch\n",
      "Epoch 19/20  Iteration 6771/7380 Training loss: 4.8069 1.1266 sec/batch\n",
      "Epoch 19/20  Iteration 6772/7380 Training loss: 4.8071 1.1189 sec/batch\n",
      "Epoch 19/20  Iteration 6773/7380 Training loss: 4.8074 1.1185 sec/batch\n",
      "Epoch 19/20  Iteration 6774/7380 Training loss: 4.8075 1.1190 sec/batch\n",
      "Epoch 19/20  Iteration 6775/7380 Training loss: 4.8076 1.1162 sec/batch\n",
      "Epoch 19/20  Iteration 6776/7380 Training loss: 4.8068 1.1258 sec/batch\n",
      "Epoch 19/20  Iteration 6777/7380 Training loss: 4.8063 1.1086 sec/batch\n",
      "Epoch 19/20  Iteration 6778/7380 Training loss: 4.8064 1.1209 sec/batch\n",
      "Epoch 19/20  Iteration 6779/7380 Training loss: 4.8062 1.1154 sec/batch\n",
      "Epoch 19/20  Iteration 6780/7380 Training loss: 4.8062 1.1068 sec/batch\n",
      "Epoch 19/20  Iteration 6781/7380 Training loss: 4.8055 1.1284 sec/batch\n",
      "Epoch 19/20  Iteration 6782/7380 Training loss: 4.8046 1.1278 sec/batch\n",
      "Epoch 19/20  Iteration 6783/7380 Training loss: 4.8059 1.1301 sec/batch\n",
      "Epoch 19/20  Iteration 6784/7380 Training loss: 4.8059 1.1060 sec/batch\n",
      "Epoch 19/20  Iteration 6785/7380 Training loss: 4.8065 1.1122 sec/batch\n",
      "Epoch 19/20  Iteration 6786/7380 Training loss: 4.8064 1.1060 sec/batch\n",
      "Epoch 19/20  Iteration 6787/7380 Training loss: 4.8067 1.1173 sec/batch\n",
      "Epoch 19/20  Iteration 6788/7380 Training loss: 4.8065 1.1130 sec/batch\n",
      "Epoch 19/20  Iteration 6789/7380 Training loss: 4.8065 1.1047 sec/batch\n",
      "Epoch 19/20  Iteration 6790/7380 Training loss: 4.8063 1.1168 sec/batch\n",
      "Epoch 19/20  Iteration 6791/7380 Training loss: 4.8058 1.1093 sec/batch\n",
      "Epoch 19/20  Iteration 6792/7380 Training loss: 4.8062 1.1136 sec/batch\n",
      "Epoch 19/20  Iteration 6793/7380 Training loss: 4.8055 1.1167 sec/batch\n",
      "Epoch 19/20  Iteration 6794/7380 Training loss: 4.8057 1.1199 sec/batch\n",
      "Epoch 19/20  Iteration 6795/7380 Training loss: 4.8059 1.1270 sec/batch\n",
      "Epoch 19/20  Iteration 6796/7380 Training loss: 4.8065 1.1220 sec/batch\n",
      "Epoch 19/20  Iteration 6797/7380 Training loss: 4.8063 1.1142 sec/batch\n",
      "Epoch 19/20  Iteration 6798/7380 Training loss: 4.8059 1.1117 sec/batch\n",
      "Epoch 19/20  Iteration 6799/7380 Training loss: 4.8057 1.1159 sec/batch\n",
      "Epoch 19/20  Iteration 6800/7380 Training loss: 4.8067 1.1142 sec/batch\n",
      "Validation loss: 4.99745 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 6801/7380 Training loss: 4.8072 1.1287 sec/batch\n",
      "Epoch 19/20  Iteration 6802/7380 Training loss: 4.8071 1.1142 sec/batch\n",
      "Epoch 19/20  Iteration 6803/7380 Training loss: 4.8078 1.1250 sec/batch\n",
      "Epoch 19/20  Iteration 6804/7380 Training loss: 4.8082 1.1107 sec/batch\n",
      "Epoch 19/20  Iteration 6805/7380 Training loss: 4.8086 1.1268 sec/batch\n",
      "Epoch 19/20  Iteration 6806/7380 Training loss: 4.8091 1.1133 sec/batch\n",
      "Epoch 19/20  Iteration 6807/7380 Training loss: 4.8096 1.1117 sec/batch\n",
      "Epoch 19/20  Iteration 6808/7380 Training loss: 4.8102 1.1286 sec/batch\n",
      "Epoch 19/20  Iteration 6809/7380 Training loss: 4.8103 1.1061 sec/batch\n",
      "Epoch 19/20  Iteration 6810/7380 Training loss: 4.8103 1.1278 sec/batch\n",
      "Epoch 19/20  Iteration 6811/7380 Training loss: 4.8098 1.1162 sec/batch\n",
      "Epoch 19/20  Iteration 6812/7380 Training loss: 4.8093 1.1152 sec/batch\n",
      "Epoch 19/20  Iteration 6813/7380 Training loss: 4.8096 1.1545 sec/batch\n",
      "Epoch 19/20  Iteration 6814/7380 Training loss: 4.8104 1.1217 sec/batch\n",
      "Epoch 19/20  Iteration 6815/7380 Training loss: 4.8104 1.1238 sec/batch\n",
      "Epoch 19/20  Iteration 6816/7380 Training loss: 4.8101 1.1149 sec/batch\n",
      "Epoch 19/20  Iteration 6817/7380 Training loss: 4.8101 1.1189 sec/batch\n",
      "Epoch 19/20  Iteration 6818/7380 Training loss: 4.8097 1.1182 sec/batch\n",
      "Epoch 19/20  Iteration 6819/7380 Training loss: 4.8098 1.1302 sec/batch\n",
      "Epoch 19/20  Iteration 6820/7380 Training loss: 4.8103 1.1243 sec/batch\n",
      "Epoch 19/20  Iteration 6821/7380 Training loss: 4.8098 1.1183 sec/batch\n",
      "Epoch 19/20  Iteration 6822/7380 Training loss: 4.8094 1.1137 sec/batch\n",
      "Epoch 19/20  Iteration 6823/7380 Training loss: 4.8091 1.1181 sec/batch\n",
      "Epoch 19/20  Iteration 6824/7380 Training loss: 4.8089 1.1318 sec/batch\n",
      "Epoch 19/20  Iteration 6825/7380 Training loss: 4.8098 1.1204 sec/batch\n",
      "Epoch 19/20  Iteration 6826/7380 Training loss: 4.8096 1.1105 sec/batch\n",
      "Epoch 19/20  Iteration 6827/7380 Training loss: 4.8095 1.1173 sec/batch\n",
      "Epoch 19/20  Iteration 6828/7380 Training loss: 4.8096 1.1259 sec/batch\n",
      "Epoch 19/20  Iteration 6829/7380 Training loss: 4.8091 1.1196 sec/batch\n",
      "Epoch 19/20  Iteration 6830/7380 Training loss: 4.8089 1.1298 sec/batch\n",
      "Epoch 19/20  Iteration 6831/7380 Training loss: 4.8087 1.1168 sec/batch\n",
      "Epoch 19/20  Iteration 6832/7380 Training loss: 4.8087 1.1334 sec/batch\n",
      "Epoch 19/20  Iteration 6833/7380 Training loss: 4.8076 1.1255 sec/batch\n",
      "Epoch 19/20  Iteration 6834/7380 Training loss: 4.8080 1.1154 sec/batch\n",
      "Epoch 19/20  Iteration 6835/7380 Training loss: 4.8085 1.1320 sec/batch\n",
      "Epoch 19/20  Iteration 6836/7380 Training loss: 4.8079 1.1306 sec/batch\n",
      "Epoch 19/20  Iteration 6837/7380 Training loss: 4.8082 1.1112 sec/batch\n",
      "Epoch 19/20  Iteration 6838/7380 Training loss: 4.8086 1.1102 sec/batch\n",
      "Epoch 19/20  Iteration 6839/7380 Training loss: 4.8086 1.1286 sec/batch\n",
      "Epoch 19/20  Iteration 6840/7380 Training loss: 4.8089 1.1087 sec/batch\n",
      "Epoch 19/20  Iteration 6841/7380 Training loss: 4.8086 1.1057 sec/batch\n",
      "Epoch 19/20  Iteration 6842/7380 Training loss: 4.8084 1.1183 sec/batch\n",
      "Epoch 19/20  Iteration 6843/7380 Training loss: 4.8088 1.1149 sec/batch\n",
      "Epoch 19/20  Iteration 6844/7380 Training loss: 4.8091 1.1186 sec/batch\n",
      "Epoch 19/20  Iteration 6845/7380 Training loss: 4.8096 1.1180 sec/batch\n",
      "Epoch 19/20  Iteration 6846/7380 Training loss: 4.8096 1.1322 sec/batch\n",
      "Epoch 19/20  Iteration 6847/7380 Training loss: 4.8099 1.1089 sec/batch\n",
      "Epoch 19/20  Iteration 6848/7380 Training loss: 4.8103 1.1133 sec/batch\n",
      "Epoch 19/20  Iteration 6849/7380 Training loss: 4.8096 1.1396 sec/batch\n",
      "Epoch 19/20  Iteration 6850/7380 Training loss: 4.8093 1.1209 sec/batch\n",
      "Validation loss: 5.00008 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 6851/7380 Training loss: 4.8096 1.1311 sec/batch\n",
      "Epoch 19/20  Iteration 6852/7380 Training loss: 4.8090 1.1248 sec/batch\n",
      "Epoch 19/20  Iteration 6853/7380 Training loss: 4.8089 1.1182 sec/batch\n",
      "Epoch 19/20  Iteration 6854/7380 Training loss: 4.8087 1.1156 sec/batch\n",
      "Epoch 19/20  Iteration 6855/7380 Training loss: 4.8088 1.1228 sec/batch\n",
      "Epoch 19/20  Iteration 6856/7380 Training loss: 4.8088 1.1223 sec/batch\n",
      "Epoch 19/20  Iteration 6857/7380 Training loss: 4.8084 1.1188 sec/batch\n",
      "Epoch 19/20  Iteration 6858/7380 Training loss: 4.8078 1.1178 sec/batch\n",
      "Epoch 19/20  Iteration 6859/7380 Training loss: 4.8074 1.1573 sec/batch\n",
      "Epoch 19/20  Iteration 6860/7380 Training loss: 4.8076 1.1105 sec/batch\n",
      "Epoch 19/20  Iteration 6861/7380 Training loss: 4.8080 1.1141 sec/batch\n",
      "Epoch 19/20  Iteration 6862/7380 Training loss: 4.8081 1.1112 sec/batch\n",
      "Epoch 19/20  Iteration 6863/7380 Training loss: 4.8078 1.1233 sec/batch\n",
      "Epoch 19/20  Iteration 6864/7380 Training loss: 4.8082 1.1117 sec/batch\n",
      "Epoch 19/20  Iteration 6865/7380 Training loss: 4.8082 1.1178 sec/batch\n",
      "Epoch 19/20  Iteration 6866/7380 Training loss: 4.8077 1.1230 sec/batch\n",
      "Epoch 19/20  Iteration 6867/7380 Training loss: 4.8079 1.1148 sec/batch\n",
      "Epoch 19/20  Iteration 6868/7380 Training loss: 4.8077 1.1135 sec/batch\n",
      "Epoch 19/20  Iteration 6869/7380 Training loss: 4.8076 1.1170 sec/batch\n",
      "Epoch 19/20  Iteration 6870/7380 Training loss: 4.8071 1.1116 sec/batch\n",
      "Epoch 19/20  Iteration 6871/7380 Training loss: 4.8069 1.1256 sec/batch\n",
      "Epoch 19/20  Iteration 6872/7380 Training loss: 4.8074 1.1168 sec/batch\n",
      "Epoch 19/20  Iteration 6873/7380 Training loss: 4.8072 1.1168 sec/batch\n",
      "Epoch 19/20  Iteration 6874/7380 Training loss: 4.8069 1.1150 sec/batch\n",
      "Epoch 19/20  Iteration 6875/7380 Training loss: 4.8064 1.1118 sec/batch\n",
      "Epoch 19/20  Iteration 6876/7380 Training loss: 4.8062 1.1128 sec/batch\n",
      "Epoch 19/20  Iteration 6877/7380 Training loss: 4.8060 1.1184 sec/batch\n",
      "Epoch 19/20  Iteration 6878/7380 Training loss: 4.8056 1.1159 sec/batch\n",
      "Epoch 19/20  Iteration 6879/7380 Training loss: 4.8055 1.1184 sec/batch\n",
      "Epoch 19/20  Iteration 6880/7380 Training loss: 4.8047 1.1201 sec/batch\n",
      "Epoch 19/20  Iteration 6881/7380 Training loss: 4.8035 1.1092 sec/batch\n",
      "Epoch 19/20  Iteration 6882/7380 Training loss: 4.8030 1.1241 sec/batch\n",
      "Epoch 19/20  Iteration 6883/7380 Training loss: 4.8031 1.1190 sec/batch\n",
      "Epoch 19/20  Iteration 6884/7380 Training loss: 4.8037 1.1026 sec/batch\n",
      "Epoch 19/20  Iteration 6885/7380 Training loss: 4.8034 1.1167 sec/batch\n",
      "Epoch 19/20  Iteration 6886/7380 Training loss: 4.8033 1.1296 sec/batch\n",
      "Epoch 19/20  Iteration 6887/7380 Training loss: 4.8036 1.1236 sec/batch\n",
      "Epoch 19/20  Iteration 6888/7380 Training loss: 4.8038 1.1236 sec/batch\n",
      "Epoch 19/20  Iteration 6889/7380 Training loss: 4.8038 1.1305 sec/batch\n",
      "Epoch 19/20  Iteration 6890/7380 Training loss: 4.8037 1.1178 sec/batch\n",
      "Epoch 19/20  Iteration 6891/7380 Training loss: 4.8035 1.1173 sec/batch\n",
      "Epoch 19/20  Iteration 6892/7380 Training loss: 4.8035 1.1133 sec/batch\n",
      "Epoch 19/20  Iteration 6893/7380 Training loss: 4.8033 1.1214 sec/batch\n",
      "Epoch 19/20  Iteration 6894/7380 Training loss: 4.8032 1.1164 sec/batch\n",
      "Epoch 19/20  Iteration 6895/7380 Training loss: 4.8030 1.1202 sec/batch\n",
      "Epoch 19/20  Iteration 6896/7380 Training loss: 4.8024 1.1166 sec/batch\n",
      "Epoch 19/20  Iteration 6897/7380 Training loss: 4.8025 1.1240 sec/batch\n",
      "Epoch 19/20  Iteration 6898/7380 Training loss: 4.8025 1.1206 sec/batch\n",
      "Epoch 19/20  Iteration 6899/7380 Training loss: 4.8029 1.1215 sec/batch\n",
      "Epoch 19/20  Iteration 6900/7380 Training loss: 4.8028 1.1195 sec/batch\n",
      "Validation loss: 5.00355 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 6901/7380 Training loss: 4.8037 1.1325 sec/batch\n",
      "Epoch 19/20  Iteration 6902/7380 Training loss: 4.8032 1.1217 sec/batch\n",
      "Epoch 19/20  Iteration 6903/7380 Training loss: 4.8029 1.1130 sec/batch\n",
      "Epoch 19/20  Iteration 6904/7380 Training loss: 4.8030 1.1220 sec/batch\n",
      "Epoch 19/20  Iteration 6905/7380 Training loss: 4.8033 1.1173 sec/batch\n",
      "Epoch 19/20  Iteration 6906/7380 Training loss: 4.8030 1.1083 sec/batch\n",
      "Epoch 19/20  Iteration 6907/7380 Training loss: 4.8027 1.1165 sec/batch\n",
      "Epoch 19/20  Iteration 6908/7380 Training loss: 4.8025 1.1184 sec/batch\n",
      "Epoch 19/20  Iteration 6909/7380 Training loss: 4.8026 1.1277 sec/batch\n",
      "Epoch 19/20  Iteration 6910/7380 Training loss: 4.8026 1.1224 sec/batch\n",
      "Epoch 19/20  Iteration 6911/7380 Training loss: 4.8022 1.1212 sec/batch\n",
      "Epoch 19/20  Iteration 6912/7380 Training loss: 4.8025 1.1184 sec/batch\n",
      "Epoch 19/20  Iteration 6913/7380 Training loss: 4.8027 1.1154 sec/batch\n",
      "Epoch 19/20  Iteration 6914/7380 Training loss: 4.8023 1.1403 sec/batch\n",
      "Epoch 19/20  Iteration 6915/7380 Training loss: 4.8026 1.1205 sec/batch\n",
      "Epoch 19/20  Iteration 6916/7380 Training loss: 4.8028 1.1196 sec/batch\n",
      "Epoch 19/20  Iteration 6917/7380 Training loss: 4.8027 1.1199 sec/batch\n",
      "Epoch 19/20  Iteration 6918/7380 Training loss: 4.8024 1.1297 sec/batch\n",
      "Epoch 19/20  Iteration 6919/7380 Training loss: 4.8030 1.1217 sec/batch\n",
      "Epoch 19/20  Iteration 6920/7380 Training loss: 4.8029 1.1188 sec/batch\n",
      "Epoch 19/20  Iteration 6921/7380 Training loss: 4.8032 1.1199 sec/batch\n",
      "Epoch 19/20  Iteration 6922/7380 Training loss: 4.8030 1.1123 sec/batch\n",
      "Epoch 19/20  Iteration 6923/7380 Training loss: 4.8031 1.1193 sec/batch\n",
      "Epoch 19/20  Iteration 6924/7380 Training loss: 4.8032 1.1221 sec/batch\n",
      "Epoch 19/20  Iteration 6925/7380 Training loss: 4.8035 1.1263 sec/batch\n",
      "Epoch 19/20  Iteration 6926/7380 Training loss: 4.8034 1.1157 sec/batch\n",
      "Epoch 19/20  Iteration 6927/7380 Training loss: 4.8037 1.1190 sec/batch\n",
      "Epoch 19/20  Iteration 6928/7380 Training loss: 4.8035 1.1172 sec/batch\n",
      "Epoch 19/20  Iteration 6929/7380 Training loss: 4.8032 1.1165 sec/batch\n",
      "Epoch 19/20  Iteration 6930/7380 Training loss: 4.8031 1.1189 sec/batch\n",
      "Epoch 19/20  Iteration 6931/7380 Training loss: 4.8027 1.1315 sec/batch\n",
      "Epoch 19/20  Iteration 6932/7380 Training loss: 4.8022 1.1107 sec/batch\n",
      "Epoch 19/20  Iteration 6933/7380 Training loss: 4.8019 1.1162 sec/batch\n",
      "Epoch 19/20  Iteration 6934/7380 Training loss: 4.8019 1.1168 sec/batch\n",
      "Epoch 19/20  Iteration 6935/7380 Training loss: 4.8019 1.1170 sec/batch\n",
      "Epoch 19/20  Iteration 6936/7380 Training loss: 4.8019 1.1093 sec/batch\n",
      "Epoch 19/20  Iteration 6937/7380 Training loss: 4.8020 1.1179 sec/batch\n",
      "Epoch 19/20  Iteration 6938/7380 Training loss: 4.8018 1.1132 sec/batch\n",
      "Epoch 19/20  Iteration 6939/7380 Training loss: 4.8016 1.1143 sec/batch\n",
      "Epoch 19/20  Iteration 6940/7380 Training loss: 4.8016 1.1293 sec/batch\n",
      "Epoch 19/20  Iteration 6941/7380 Training loss: 4.8017 1.1186 sec/batch\n",
      "Epoch 19/20  Iteration 6942/7380 Training loss: 4.8022 1.1187 sec/batch\n",
      "Epoch 19/20  Iteration 6943/7380 Training loss: 4.8022 1.1285 sec/batch\n",
      "Epoch 19/20  Iteration 6944/7380 Training loss: 4.8024 1.1111 sec/batch\n",
      "Epoch 19/20  Iteration 6945/7380 Training loss: 4.8025 1.1174 sec/batch\n",
      "Epoch 19/20  Iteration 6946/7380 Training loss: 4.8031 1.1102 sec/batch\n",
      "Epoch 19/20  Iteration 6947/7380 Training loss: 4.8033 1.1295 sec/batch\n",
      "Epoch 19/20  Iteration 6948/7380 Training loss: 4.8034 1.1251 sec/batch\n",
      "Epoch 19/20  Iteration 6949/7380 Training loss: 4.8031 1.1157 sec/batch\n",
      "Epoch 19/20  Iteration 6950/7380 Training loss: 4.8028 1.1179 sec/batch\n",
      "Validation loss: 5.01773 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 6951/7380 Training loss: 4.8030 1.1234 sec/batch\n",
      "Epoch 19/20  Iteration 6952/7380 Training loss: 4.8028 1.1303 sec/batch\n",
      "Epoch 19/20  Iteration 6953/7380 Training loss: 4.8029 1.1320 sec/batch\n",
      "Epoch 19/20  Iteration 6954/7380 Training loss: 4.8027 1.1181 sec/batch\n",
      "Epoch 19/20  Iteration 6955/7380 Training loss: 4.8024 1.1223 sec/batch\n",
      "Epoch 19/20  Iteration 6956/7380 Training loss: 4.8024 1.1169 sec/batch\n",
      "Epoch 19/20  Iteration 6957/7380 Training loss: 4.8025 1.1138 sec/batch\n",
      "Epoch 19/20  Iteration 6958/7380 Training loss: 4.8027 1.1209 sec/batch\n",
      "Epoch 19/20  Iteration 6959/7380 Training loss: 4.8030 1.1224 sec/batch\n",
      "Epoch 19/20  Iteration 6960/7380 Training loss: 4.8033 1.1148 sec/batch\n",
      "Epoch 19/20  Iteration 6961/7380 Training loss: 4.8038 1.1378 sec/batch\n",
      "Epoch 19/20  Iteration 6962/7380 Training loss: 4.8042 1.1101 sec/batch\n",
      "Epoch 19/20  Iteration 6963/7380 Training loss: 4.8039 1.1113 sec/batch\n",
      "Epoch 19/20  Iteration 6964/7380 Training loss: 4.8035 1.1251 sec/batch\n",
      "Epoch 19/20  Iteration 6965/7380 Training loss: 4.8035 1.1113 sec/batch\n",
      "Epoch 19/20  Iteration 6966/7380 Training loss: 4.8034 1.1131 sec/batch\n",
      "Epoch 19/20  Iteration 6967/7380 Training loss: 4.8034 1.1320 sec/batch\n",
      "Epoch 19/20  Iteration 6968/7380 Training loss: 4.8038 1.1159 sec/batch\n",
      "Epoch 19/20  Iteration 6969/7380 Training loss: 4.8037 1.1345 sec/batch\n",
      "Epoch 19/20  Iteration 6970/7380 Training loss: 4.8035 1.1098 sec/batch\n",
      "Epoch 19/20  Iteration 6971/7380 Training loss: 4.8032 1.1483 sec/batch\n",
      "Epoch 19/20  Iteration 6972/7380 Training loss: 4.8030 1.1156 sec/batch\n",
      "Epoch 19/20  Iteration 6973/7380 Training loss: 4.8027 1.1157 sec/batch\n",
      "Epoch 19/20  Iteration 6974/7380 Training loss: 4.8028 1.1222 sec/batch\n",
      "Epoch 19/20  Iteration 6975/7380 Training loss: 4.8024 1.1165 sec/batch\n",
      "Epoch 19/20  Iteration 6976/7380 Training loss: 4.8025 1.1222 sec/batch\n",
      "Epoch 19/20  Iteration 6977/7380 Training loss: 4.8027 1.1262 sec/batch\n",
      "Epoch 19/20  Iteration 6978/7380 Training loss: 4.8028 1.1119 sec/batch\n",
      "Epoch 19/20  Iteration 6979/7380 Training loss: 4.8030 1.1325 sec/batch\n",
      "Epoch 19/20  Iteration 6980/7380 Training loss: 4.8030 1.1133 sec/batch\n",
      "Epoch 19/20  Iteration 6981/7380 Training loss: 4.8031 1.1221 sec/batch\n",
      "Epoch 19/20  Iteration 6982/7380 Training loss: 4.8027 1.1148 sec/batch\n",
      "Epoch 19/20  Iteration 6983/7380 Training loss: 4.8025 1.1193 sec/batch\n",
      "Epoch 19/20  Iteration 6984/7380 Training loss: 4.8023 1.1349 sec/batch\n",
      "Epoch 19/20  Iteration 6985/7380 Training loss: 4.8024 1.1192 sec/batch\n",
      "Epoch 19/20  Iteration 6986/7380 Training loss: 4.8024 1.1175 sec/batch\n",
      "Epoch 19/20  Iteration 6987/7380 Training loss: 4.8023 1.1174 sec/batch\n",
      "Epoch 19/20  Iteration 6988/7380 Training loss: 4.8023 1.1305 sec/batch\n",
      "Epoch 19/20  Iteration 6989/7380 Training loss: 4.8026 1.1165 sec/batch\n",
      "Epoch 19/20  Iteration 6990/7380 Training loss: 4.8027 1.1237 sec/batch\n",
      "Epoch 19/20  Iteration 6991/7380 Training loss: 4.8025 1.1273 sec/batch\n",
      "Epoch 19/20  Iteration 6992/7380 Training loss: 4.8025 1.1198 sec/batch\n",
      "Epoch 19/20  Iteration 6993/7380 Training loss: 4.8028 1.1089 sec/batch\n",
      "Epoch 19/20  Iteration 6994/7380 Training loss: 4.8027 1.1081 sec/batch\n",
      "Epoch 19/20  Iteration 6995/7380 Training loss: 4.8029 1.1178 sec/batch\n",
      "Epoch 19/20  Iteration 6996/7380 Training loss: 4.8025 1.1183 sec/batch\n",
      "Epoch 19/20  Iteration 6997/7380 Training loss: 4.8024 1.1177 sec/batch\n",
      "Epoch 19/20  Iteration 6998/7380 Training loss: 4.8022 1.1221 sec/batch\n",
      "Epoch 19/20  Iteration 6999/7380 Training loss: 4.8026 1.1247 sec/batch\n",
      "Epoch 19/20  Iteration 7000/7380 Training loss: 4.8026 1.1274 sec/batch\n",
      "Validation loss: 5.00174 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 7001/7380 Training loss: 4.8028 1.1227 sec/batch\n",
      "Epoch 19/20  Iteration 7002/7380 Training loss: 4.8025 1.1211 sec/batch\n",
      "Epoch 19/20  Iteration 7003/7380 Training loss: 4.8024 1.1181 sec/batch\n",
      "Epoch 19/20  Iteration 7004/7380 Training loss: 4.8024 1.1246 sec/batch\n",
      "Epoch 19/20  Iteration 7005/7380 Training loss: 4.8024 1.1192 sec/batch\n",
      "Epoch 19/20  Iteration 7006/7380 Training loss: 4.8023 1.1298 sec/batch\n",
      "Epoch 19/20  Iteration 7007/7380 Training loss: 4.8021 1.1217 sec/batch\n",
      "Epoch 19/20  Iteration 7008/7380 Training loss: 4.8019 1.1235 sec/batch\n",
      "Epoch 19/20  Iteration 7009/7380 Training loss: 4.8016 1.1181 sec/batch\n",
      "Epoch 19/20  Iteration 7010/7380 Training loss: 4.8017 1.1361 sec/batch\n",
      "Epoch 19/20  Iteration 7011/7380 Training loss: 4.8015 1.1288 sec/batch\n",
      "Epoch 20/20  Iteration 7012/7380 Training loss: 4.8075 1.1049 sec/batch\n",
      "Epoch 20/20  Iteration 7013/7380 Training loss: 4.8084 1.1316 sec/batch\n",
      "Epoch 20/20  Iteration 7014/7380 Training loss: 4.7983 1.1236 sec/batch\n",
      "Epoch 20/20  Iteration 7015/7380 Training loss: 4.7883 1.1157 sec/batch\n",
      "Epoch 20/20  Iteration 7016/7380 Training loss: 4.7753 1.1253 sec/batch\n",
      "Epoch 20/20  Iteration 7017/7380 Training loss: 4.7833 1.1162 sec/batch\n",
      "Epoch 20/20  Iteration 7018/7380 Training loss: 4.8006 1.1179 sec/batch\n",
      "Epoch 20/20  Iteration 7019/7380 Training loss: 4.7852 1.1242 sec/batch\n",
      "Epoch 20/20  Iteration 7020/7380 Training loss: 4.7874 1.1143 sec/batch\n",
      "Epoch 20/20  Iteration 7021/7380 Training loss: 4.7972 1.1148 sec/batch\n",
      "Epoch 20/20  Iteration 7022/7380 Training loss: 4.7943 1.1257 sec/batch\n",
      "Epoch 20/20  Iteration 7023/7380 Training loss: 4.7833 1.1118 sec/batch\n",
      "Epoch 20/20  Iteration 7024/7380 Training loss: 4.7801 1.1194 sec/batch\n",
      "Epoch 20/20  Iteration 7025/7380 Training loss: 4.7866 1.1235 sec/batch\n",
      "Epoch 20/20  Iteration 7026/7380 Training loss: 4.7905 1.1082 sec/batch\n",
      "Epoch 20/20  Iteration 7027/7380 Training loss: 4.7914 1.1235 sec/batch\n",
      "Epoch 20/20  Iteration 7028/7380 Training loss: 4.7891 1.1185 sec/batch\n",
      "Epoch 20/20  Iteration 7029/7380 Training loss: 4.7755 1.1176 sec/batch\n",
      "Epoch 20/20  Iteration 7030/7380 Training loss: 4.7702 1.1179 sec/batch\n",
      "Epoch 20/20  Iteration 7031/7380 Training loss: 4.7693 1.1145 sec/batch\n",
      "Epoch 20/20  Iteration 7032/7380 Training loss: 4.7670 1.1325 sec/batch\n",
      "Epoch 20/20  Iteration 7033/7380 Training loss: 4.7731 1.1212 sec/batch\n",
      "Epoch 20/20  Iteration 7034/7380 Training loss: 4.7716 1.1142 sec/batch\n",
      "Epoch 20/20  Iteration 7035/7380 Training loss: 4.7783 1.1129 sec/batch\n",
      "Epoch 20/20  Iteration 7036/7380 Training loss: 4.7807 1.1126 sec/batch\n",
      "Epoch 20/20  Iteration 7037/7380 Training loss: 4.7824 1.1230 sec/batch\n",
      "Epoch 20/20  Iteration 7038/7380 Training loss: 4.7773 1.1401 sec/batch\n",
      "Epoch 20/20  Iteration 7039/7380 Training loss: 4.7835 1.1197 sec/batch\n",
      "Epoch 20/20  Iteration 7040/7380 Training loss: 4.7837 1.1142 sec/batch\n",
      "Epoch 20/20  Iteration 7041/7380 Training loss: 4.7855 1.1272 sec/batch\n",
      "Epoch 20/20  Iteration 7042/7380 Training loss: 4.7839 1.1241 sec/batch\n",
      "Epoch 20/20  Iteration 7043/7380 Training loss: 4.7848 1.1112 sec/batch\n",
      "Epoch 20/20  Iteration 7044/7380 Training loss: 4.7841 1.1344 sec/batch\n",
      "Epoch 20/20  Iteration 7045/7380 Training loss: 4.7861 1.1240 sec/batch\n",
      "Epoch 20/20  Iteration 7046/7380 Training loss: 4.7888 1.1132 sec/batch\n",
      "Epoch 20/20  Iteration 7047/7380 Training loss: 4.7881 1.1296 sec/batch\n",
      "Epoch 20/20  Iteration 7048/7380 Training loss: 4.7882 1.1223 sec/batch\n",
      "Epoch 20/20  Iteration 7049/7380 Training loss: 4.7870 1.1232 sec/batch\n",
      "Epoch 20/20  Iteration 7050/7380 Training loss: 4.7883 1.1294 sec/batch\n",
      "Validation loss: 4.99501 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 7051/7380 Training loss: 4.7925 1.1314 sec/batch\n",
      "Epoch 20/20  Iteration 7052/7380 Training loss: 4.7929 1.1259 sec/batch\n",
      "Epoch 20/20  Iteration 7053/7380 Training loss: 4.7916 1.1359 sec/batch\n",
      "Epoch 20/20  Iteration 7054/7380 Training loss: 4.7928 1.1136 sec/batch\n",
      "Epoch 20/20  Iteration 7055/7380 Training loss: 4.7937 1.1208 sec/batch\n",
      "Epoch 20/20  Iteration 7056/7380 Training loss: 4.7938 1.1503 sec/batch\n",
      "Epoch 20/20  Iteration 7057/7380 Training loss: 4.7918 1.1324 sec/batch\n",
      "Epoch 20/20  Iteration 7058/7380 Training loss: 4.7927 1.1183 sec/batch\n",
      "Epoch 20/20  Iteration 7059/7380 Training loss: 4.7900 1.1241 sec/batch\n",
      "Epoch 20/20  Iteration 7060/7380 Training loss: 4.7911 1.1176 sec/batch\n",
      "Epoch 20/20  Iteration 7061/7380 Training loss: 4.7903 1.1157 sec/batch\n",
      "Epoch 20/20  Iteration 7062/7380 Training loss: 4.7890 1.1194 sec/batch\n",
      "Epoch 20/20  Iteration 7063/7380 Training loss: 4.7872 1.1211 sec/batch\n",
      "Epoch 20/20  Iteration 7064/7380 Training loss: 4.7857 1.1227 sec/batch\n",
      "Epoch 20/20  Iteration 7065/7380 Training loss: 4.7853 1.1085 sec/batch\n",
      "Epoch 20/20  Iteration 7066/7380 Training loss: 4.7845 1.1450 sec/batch\n",
      "Epoch 20/20  Iteration 7067/7380 Training loss: 4.7824 1.1286 sec/batch\n",
      "Epoch 20/20  Iteration 7068/7380 Training loss: 4.7817 1.1234 sec/batch\n",
      "Epoch 20/20  Iteration 7069/7380 Training loss: 4.7825 1.1224 sec/batch\n",
      "Epoch 20/20  Iteration 7070/7380 Training loss: 4.7830 1.1159 sec/batch\n",
      "Epoch 20/20  Iteration 7071/7380 Training loss: 4.7848 1.1220 sec/batch\n",
      "Epoch 20/20  Iteration 7072/7380 Training loss: 4.7840 1.1219 sec/batch\n",
      "Epoch 20/20  Iteration 7073/7380 Training loss: 4.7848 1.1244 sec/batch\n",
      "Epoch 20/20  Iteration 7074/7380 Training loss: 4.7844 1.1208 sec/batch\n",
      "Epoch 20/20  Iteration 7075/7380 Training loss: 4.7825 1.1259 sec/batch\n",
      "Epoch 20/20  Iteration 7076/7380 Training loss: 4.7828 1.1231 sec/batch\n",
      "Epoch 20/20  Iteration 7077/7380 Training loss: 4.7825 1.1154 sec/batch\n",
      "Epoch 20/20  Iteration 7078/7380 Training loss: 4.7831 1.1161 sec/batch\n",
      "Epoch 20/20  Iteration 7079/7380 Training loss: 4.7831 1.1125 sec/batch\n",
      "Epoch 20/20  Iteration 7080/7380 Training loss: 4.7846 1.1168 sec/batch\n",
      "Epoch 20/20  Iteration 7081/7380 Training loss: 4.7835 1.1247 sec/batch\n",
      "Epoch 20/20  Iteration 7082/7380 Training loss: 4.7820 1.1263 sec/batch\n",
      "Epoch 20/20  Iteration 7083/7380 Training loss: 4.7827 1.1302 sec/batch\n",
      "Epoch 20/20  Iteration 7084/7380 Training loss: 4.7812 1.1253 sec/batch\n",
      "Epoch 20/20  Iteration 7085/7380 Training loss: 4.7822 1.1154 sec/batch\n",
      "Epoch 20/20  Iteration 7086/7380 Training loss: 4.7833 1.1139 sec/batch\n",
      "Epoch 20/20  Iteration 7087/7380 Training loss: 4.7827 1.1156 sec/batch\n",
      "Epoch 20/20  Iteration 7088/7380 Training loss: 4.7827 1.1094 sec/batch\n",
      "Epoch 20/20  Iteration 7089/7380 Training loss: 4.7820 1.1257 sec/batch\n",
      "Epoch 20/20  Iteration 7090/7380 Training loss: 4.7820 1.1244 sec/batch\n",
      "Epoch 20/20  Iteration 7091/7380 Training loss: 4.7824 1.1199 sec/batch\n",
      "Epoch 20/20  Iteration 7092/7380 Training loss: 4.7820 1.1187 sec/batch\n",
      "Epoch 20/20  Iteration 7093/7380 Training loss: 4.7817 1.1148 sec/batch\n",
      "Epoch 20/20  Iteration 7094/7380 Training loss: 4.7800 1.1229 sec/batch\n",
      "Epoch 20/20  Iteration 7095/7380 Training loss: 4.7794 1.1033 sec/batch\n",
      "Epoch 20/20  Iteration 7096/7380 Training loss: 4.7796 1.1119 sec/batch\n",
      "Epoch 20/20  Iteration 7097/7380 Training loss: 4.7791 1.1128 sec/batch\n",
      "Epoch 20/20  Iteration 7098/7380 Training loss: 4.7788 1.1229 sec/batch\n",
      "Epoch 20/20  Iteration 7099/7380 Training loss: 4.7787 1.1164 sec/batch\n",
      "Epoch 20/20  Iteration 7100/7380 Training loss: 4.7766 1.1471 sec/batch\n",
      "Validation loss: 4.99852 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 7101/7380 Training loss: 4.7778 1.1199 sec/batch\n",
      "Epoch 20/20  Iteration 7102/7380 Training loss: 4.7786 1.1305 sec/batch\n",
      "Epoch 20/20  Iteration 7103/7380 Training loss: 4.7802 1.1205 sec/batch\n",
      "Epoch 20/20  Iteration 7104/7380 Training loss: 4.7798 1.1159 sec/batch\n",
      "Epoch 20/20  Iteration 7105/7380 Training loss: 4.7796 1.1138 sec/batch\n",
      "Epoch 20/20  Iteration 7106/7380 Training loss: 4.7792 1.1151 sec/batch\n",
      "Epoch 20/20  Iteration 7107/7380 Training loss: 4.7791 1.1048 sec/batch\n",
      "Epoch 20/20  Iteration 7108/7380 Training loss: 4.7787 1.1468 sec/batch\n",
      "Epoch 20/20  Iteration 7109/7380 Training loss: 4.7793 1.1626 sec/batch\n",
      "Epoch 20/20  Iteration 7110/7380 Training loss: 4.7792 1.1139 sec/batch\n",
      "Epoch 20/20  Iteration 7111/7380 Training loss: 4.7787 1.1132 sec/batch\n",
      "Epoch 20/20  Iteration 7112/7380 Training loss: 4.7797 1.1073 sec/batch\n",
      "Epoch 20/20  Iteration 7113/7380 Training loss: 4.7805 1.1145 sec/batch\n",
      "Epoch 20/20  Iteration 7114/7380 Training loss: 4.7807 1.1159 sec/batch\n",
      "Epoch 20/20  Iteration 7115/7380 Training loss: 4.7800 1.1134 sec/batch\n",
      "Epoch 20/20  Iteration 7116/7380 Training loss: 4.7801 1.1217 sec/batch\n",
      "Epoch 20/20  Iteration 7117/7380 Training loss: 4.7799 1.1425 sec/batch\n",
      "Epoch 20/20  Iteration 7118/7380 Training loss: 4.7802 1.1422 sec/batch\n",
      "Epoch 20/20  Iteration 7119/7380 Training loss: 4.7802 1.1149 sec/batch\n",
      "Epoch 20/20  Iteration 7120/7380 Training loss: 4.7788 1.1195 sec/batch\n",
      "Epoch 20/20  Iteration 7121/7380 Training loss: 4.7791 1.1557 sec/batch\n",
      "Epoch 20/20  Iteration 7122/7380 Training loss: 4.7787 1.1094 sec/batch\n",
      "Epoch 20/20  Iteration 7123/7380 Training loss: 4.7788 1.1594 sec/batch\n",
      "Epoch 20/20  Iteration 7124/7380 Training loss: 4.7782 1.1188 sec/batch\n",
      "Epoch 20/20  Iteration 7125/7380 Training loss: 4.7784 1.1275 sec/batch\n",
      "Epoch 20/20  Iteration 7126/7380 Training loss: 4.7784 1.1697 sec/batch\n",
      "Epoch 20/20  Iteration 7127/7380 Training loss: 4.7781 1.1091 sec/batch\n",
      "Epoch 20/20  Iteration 7128/7380 Training loss: 4.7770 1.1173 sec/batch\n",
      "Epoch 20/20  Iteration 7129/7380 Training loss: 4.7769 1.1064 sec/batch\n",
      "Epoch 20/20  Iteration 7130/7380 Training loss: 4.7778 1.1228 sec/batch\n",
      "Epoch 20/20  Iteration 7131/7380 Training loss: 4.7775 1.1093 sec/batch\n",
      "Epoch 20/20  Iteration 7132/7380 Training loss: 4.7781 1.1194 sec/batch\n",
      "Epoch 20/20  Iteration 7133/7380 Training loss: 4.7790 1.1059 sec/batch\n",
      "Epoch 20/20  Iteration 7134/7380 Training loss: 4.7790 1.1261 sec/batch\n",
      "Epoch 20/20  Iteration 7135/7380 Training loss: 4.7788 1.1130 sec/batch\n",
      "Epoch 20/20  Iteration 7136/7380 Training loss: 4.7791 1.1184 sec/batch\n",
      "Epoch 20/20  Iteration 7137/7380 Training loss: 4.7783 1.1187 sec/batch\n",
      "Epoch 20/20  Iteration 7138/7380 Training loss: 4.7769 1.1169 sec/batch\n",
      "Epoch 20/20  Iteration 7139/7380 Training loss: 4.7766 1.1245 sec/batch\n",
      "Epoch 20/20  Iteration 7140/7380 Training loss: 4.7758 1.1214 sec/batch\n",
      "Epoch 20/20  Iteration 7141/7380 Training loss: 4.7760 1.1267 sec/batch\n",
      "Epoch 20/20  Iteration 7142/7380 Training loss: 4.7763 1.1125 sec/batch\n",
      "Epoch 20/20  Iteration 7143/7380 Training loss: 4.7764 1.1155 sec/batch\n",
      "Epoch 20/20  Iteration 7144/7380 Training loss: 4.7766 1.1077 sec/batch\n",
      "Epoch 20/20  Iteration 7145/7380 Training loss: 4.7758 1.1155 sec/batch\n",
      "Epoch 20/20  Iteration 7146/7380 Training loss: 4.7753 1.1192 sec/batch\n",
      "Epoch 20/20  Iteration 7147/7380 Training loss: 4.7755 1.1205 sec/batch\n",
      "Epoch 20/20  Iteration 7148/7380 Training loss: 4.7752 1.1148 sec/batch\n",
      "Epoch 20/20  Iteration 7149/7380 Training loss: 4.7753 1.1185 sec/batch\n",
      "Epoch 20/20  Iteration 7150/7380 Training loss: 4.7746 1.1519 sec/batch\n",
      "Validation loss: 4.99007 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 7151/7380 Training loss: 4.7750 1.1190 sec/batch\n",
      "Epoch 20/20  Iteration 7152/7380 Training loss: 4.7764 1.1104 sec/batch\n",
      "Epoch 20/20  Iteration 7153/7380 Training loss: 4.7764 1.1214 sec/batch\n",
      "Epoch 20/20  Iteration 7154/7380 Training loss: 4.7768 1.1229 sec/batch\n",
      "Epoch 20/20  Iteration 7155/7380 Training loss: 4.7767 1.1319 sec/batch\n",
      "Epoch 20/20  Iteration 7156/7380 Training loss: 4.7770 1.1279 sec/batch\n",
      "Epoch 20/20  Iteration 7157/7380 Training loss: 4.7769 1.1211 sec/batch\n",
      "Epoch 20/20  Iteration 7158/7380 Training loss: 4.7765 1.1166 sec/batch\n",
      "Epoch 20/20  Iteration 7159/7380 Training loss: 4.7766 1.1229 sec/batch\n",
      "Epoch 20/20  Iteration 7160/7380 Training loss: 4.7760 1.1526 sec/batch\n",
      "Epoch 20/20  Iteration 7161/7380 Training loss: 4.7764 1.1150 sec/batch\n",
      "Epoch 20/20  Iteration 7162/7380 Training loss: 4.7758 1.1429 sec/batch\n",
      "Epoch 20/20  Iteration 7163/7380 Training loss: 4.7762 1.1161 sec/batch\n",
      "Epoch 20/20  Iteration 7164/7380 Training loss: 4.7764 1.1309 sec/batch\n",
      "Epoch 20/20  Iteration 7165/7380 Training loss: 4.7769 1.1209 sec/batch\n",
      "Epoch 20/20  Iteration 7166/7380 Training loss: 4.7766 1.1101 sec/batch\n",
      "Epoch 20/20  Iteration 7167/7380 Training loss: 4.7761 1.1276 sec/batch\n",
      "Epoch 20/20  Iteration 7168/7380 Training loss: 4.7757 1.1125 sec/batch\n",
      "Epoch 20/20  Iteration 7169/7380 Training loss: 4.7764 1.1163 sec/batch\n",
      "Epoch 20/20  Iteration 7170/7380 Training loss: 4.7758 1.1245 sec/batch\n",
      "Epoch 20/20  Iteration 7171/7380 Training loss: 4.7757 1.1104 sec/batch\n",
      "Epoch 20/20  Iteration 7172/7380 Training loss: 4.7766 1.1210 sec/batch\n",
      "Epoch 20/20  Iteration 7173/7380 Training loss: 4.7770 1.1329 sec/batch\n",
      "Epoch 20/20  Iteration 7174/7380 Training loss: 4.7773 1.1172 sec/batch\n",
      "Epoch 20/20  Iteration 7175/7380 Training loss: 4.7778 1.1416 sec/batch\n",
      "Epoch 20/20  Iteration 7176/7380 Training loss: 4.7782 1.1167 sec/batch\n",
      "Epoch 20/20  Iteration 7177/7380 Training loss: 4.7787 1.1130 sec/batch\n",
      "Epoch 20/20  Iteration 7178/7380 Training loss: 4.7791 1.1153 sec/batch\n",
      "Epoch 20/20  Iteration 7179/7380 Training loss: 4.7793 1.1084 sec/batch\n",
      "Epoch 20/20  Iteration 7180/7380 Training loss: 4.7787 1.1153 sec/batch\n",
      "Epoch 20/20  Iteration 7181/7380 Training loss: 4.7783 1.1105 sec/batch\n",
      "Epoch 20/20  Iteration 7182/7380 Training loss: 4.7786 1.1182 sec/batch\n",
      "Epoch 20/20  Iteration 7183/7380 Training loss: 4.7792 1.1104 sec/batch\n",
      "Epoch 20/20  Iteration 7184/7380 Training loss: 4.7795 1.1151 sec/batch\n",
      "Epoch 20/20  Iteration 7185/7380 Training loss: 4.7792 1.1099 sec/batch\n",
      "Epoch 20/20  Iteration 7186/7380 Training loss: 4.7791 1.1271 sec/batch\n",
      "Epoch 20/20  Iteration 7187/7380 Training loss: 4.7789 1.1178 sec/batch\n",
      "Epoch 20/20  Iteration 7188/7380 Training loss: 4.7792 1.1302 sec/batch\n",
      "Epoch 20/20  Iteration 7189/7380 Training loss: 4.7798 1.1200 sec/batch\n",
      "Epoch 20/20  Iteration 7190/7380 Training loss: 4.7795 1.1214 sec/batch\n",
      "Epoch 20/20  Iteration 7191/7380 Training loss: 4.7791 1.1265 sec/batch\n",
      "Epoch 20/20  Iteration 7192/7380 Training loss: 4.7791 1.1170 sec/batch\n",
      "Epoch 20/20  Iteration 7193/7380 Training loss: 4.7792 1.1189 sec/batch\n",
      "Epoch 20/20  Iteration 7194/7380 Training loss: 4.7800 1.1182 sec/batch\n",
      "Epoch 20/20  Iteration 7195/7380 Training loss: 4.7799 1.1298 sec/batch\n",
      "Epoch 20/20  Iteration 7196/7380 Training loss: 4.7799 1.1274 sec/batch\n",
      "Epoch 20/20  Iteration 7197/7380 Training loss: 4.7801 1.1187 sec/batch\n",
      "Epoch 20/20  Iteration 7198/7380 Training loss: 4.7798 1.1273 sec/batch\n",
      "Epoch 20/20  Iteration 7199/7380 Training loss: 4.7796 1.1332 sec/batch\n",
      "Epoch 20/20  Iteration 7200/7380 Training loss: 4.7795 1.1166 sec/batch\n",
      "Validation loss: 5.0044 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 7201/7380 Training loss: 4.7804 1.1217 sec/batch\n",
      "Epoch 20/20  Iteration 7202/7380 Training loss: 4.7797 1.1310 sec/batch\n",
      "Epoch 20/20  Iteration 7203/7380 Training loss: 4.7800 1.1205 sec/batch\n",
      "Epoch 20/20  Iteration 7204/7380 Training loss: 4.7807 1.1124 sec/batch\n",
      "Epoch 20/20  Iteration 7205/7380 Training loss: 4.7802 1.1123 sec/batch\n",
      "Epoch 20/20  Iteration 7206/7380 Training loss: 4.7805 1.1427 sec/batch\n",
      "Epoch 20/20  Iteration 7207/7380 Training loss: 4.7808 1.1300 sec/batch\n",
      "Epoch 20/20  Iteration 7208/7380 Training loss: 4.7810 1.1295 sec/batch\n",
      "Epoch 20/20  Iteration 7209/7380 Training loss: 4.7811 1.1284 sec/batch\n",
      "Epoch 20/20  Iteration 7210/7380 Training loss: 4.7809 1.1153 sec/batch\n",
      "Epoch 20/20  Iteration 7211/7380 Training loss: 4.7809 1.1240 sec/batch\n",
      "Epoch 20/20  Iteration 7212/7380 Training loss: 4.7813 1.1183 sec/batch\n",
      "Epoch 20/20  Iteration 7213/7380 Training loss: 4.7817 1.1302 sec/batch\n",
      "Epoch 20/20  Iteration 7214/7380 Training loss: 4.7822 1.1344 sec/batch\n",
      "Epoch 20/20  Iteration 7215/7380 Training loss: 4.7823 1.1235 sec/batch\n",
      "Epoch 20/20  Iteration 7216/7380 Training loss: 4.7826 1.1166 sec/batch\n",
      "Epoch 20/20  Iteration 7217/7380 Training loss: 4.7829 1.1222 sec/batch\n",
      "Epoch 20/20  Iteration 7218/7380 Training loss: 4.7821 1.1286 sec/batch\n",
      "Epoch 20/20  Iteration 7219/7380 Training loss: 4.7817 1.1219 sec/batch\n",
      "Epoch 20/20  Iteration 7220/7380 Training loss: 4.7814 1.1162 sec/batch\n",
      "Epoch 20/20  Iteration 7221/7380 Training loss: 4.7807 1.1165 sec/batch\n",
      "Epoch 20/20  Iteration 7222/7380 Training loss: 4.7807 1.1250 sec/batch\n",
      "Epoch 20/20  Iteration 7223/7380 Training loss: 4.7806 1.1174 sec/batch\n",
      "Epoch 20/20  Iteration 7224/7380 Training loss: 4.7807 1.1140 sec/batch\n",
      "Epoch 20/20  Iteration 7225/7380 Training loss: 4.7806 1.1181 sec/batch\n",
      "Epoch 20/20  Iteration 7226/7380 Training loss: 4.7801 1.1553 sec/batch\n",
      "Epoch 20/20  Iteration 7227/7380 Training loss: 4.7796 1.1241 sec/batch\n",
      "Epoch 20/20  Iteration 7228/7380 Training loss: 4.7791 1.1160 sec/batch\n",
      "Epoch 20/20  Iteration 7229/7380 Training loss: 4.7795 1.1149 sec/batch\n",
      "Epoch 20/20  Iteration 7230/7380 Training loss: 4.7799 1.1274 sec/batch\n",
      "Epoch 20/20  Iteration 7231/7380 Training loss: 4.7800 1.1076 sec/batch\n",
      "Epoch 20/20  Iteration 7232/7380 Training loss: 4.7797 1.1287 sec/batch\n",
      "Epoch 20/20  Iteration 7233/7380 Training loss: 4.7800 1.1147 sec/batch\n",
      "Epoch 20/20  Iteration 7234/7380 Training loss: 4.7800 1.1151 sec/batch\n",
      "Epoch 20/20  Iteration 7235/7380 Training loss: 4.7795 1.1190 sec/batch\n",
      "Epoch 20/20  Iteration 7236/7380 Training loss: 4.7796 1.1123 sec/batch\n",
      "Epoch 20/20  Iteration 7237/7380 Training loss: 4.7794 1.1188 sec/batch\n",
      "Epoch 20/20  Iteration 7238/7380 Training loss: 4.7796 1.1811 sec/batch\n",
      "Epoch 20/20  Iteration 7239/7380 Training loss: 4.7791 1.1302 sec/batch\n",
      "Epoch 20/20  Iteration 7240/7380 Training loss: 4.7790 1.1178 sec/batch\n",
      "Epoch 20/20  Iteration 7241/7380 Training loss: 4.7795 1.1152 sec/batch\n",
      "Epoch 20/20  Iteration 7242/7380 Training loss: 4.7790 1.1069 sec/batch\n",
      "Epoch 20/20  Iteration 7243/7380 Training loss: 4.7787 1.1129 sec/batch\n",
      "Epoch 20/20  Iteration 7244/7380 Training loss: 4.7782 1.1384 sec/batch\n",
      "Epoch 20/20  Iteration 7245/7380 Training loss: 4.7781 1.1112 sec/batch\n",
      "Epoch 20/20  Iteration 7246/7380 Training loss: 4.7777 1.1478 sec/batch\n",
      "Epoch 20/20  Iteration 7247/7380 Training loss: 4.7774 1.1221 sec/batch\n",
      "Epoch 20/20  Iteration 7248/7380 Training loss: 4.7772 1.1160 sec/batch\n",
      "Epoch 20/20  Iteration 7249/7380 Training loss: 4.7766 1.1218 sec/batch\n",
      "Epoch 20/20  Iteration 7250/7380 Training loss: 4.7755 1.1199 sec/batch\n",
      "Validation loss: 4.99309 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 7251/7380 Training loss: 4.7755 1.1206 sec/batch\n",
      "Epoch 20/20  Iteration 7252/7380 Training loss: 4.7757 1.1222 sec/batch\n",
      "Epoch 20/20  Iteration 7253/7380 Training loss: 4.7764 1.1258 sec/batch\n",
      "Epoch 20/20  Iteration 7254/7380 Training loss: 4.7762 1.1199 sec/batch\n",
      "Epoch 20/20  Iteration 7255/7380 Training loss: 4.7763 1.1117 sec/batch\n",
      "Epoch 20/20  Iteration 7256/7380 Training loss: 4.7764 1.1179 sec/batch\n",
      "Epoch 20/20  Iteration 7257/7380 Training loss: 4.7765 1.1245 sec/batch\n",
      "Epoch 20/20  Iteration 7258/7380 Training loss: 4.7767 1.1313 sec/batch\n",
      "Epoch 20/20  Iteration 7259/7380 Training loss: 4.7766 1.1238 sec/batch\n",
      "Epoch 20/20  Iteration 7260/7380 Training loss: 4.7765 1.1177 sec/batch\n",
      "Epoch 20/20  Iteration 7261/7380 Training loss: 4.7765 1.1285 sec/batch\n",
      "Epoch 20/20  Iteration 7262/7380 Training loss: 4.7763 1.1216 sec/batch\n",
      "Epoch 20/20  Iteration 7263/7380 Training loss: 4.7762 1.1287 sec/batch\n",
      "Epoch 20/20  Iteration 7264/7380 Training loss: 4.7760 1.1235 sec/batch\n",
      "Epoch 20/20  Iteration 7265/7380 Training loss: 4.7755 1.1181 sec/batch\n",
      "Epoch 20/20  Iteration 7266/7380 Training loss: 4.7756 1.1270 sec/batch\n",
      "Epoch 20/20  Iteration 7267/7380 Training loss: 4.7757 1.1268 sec/batch\n",
      "Epoch 20/20  Iteration 7268/7380 Training loss: 4.7760 1.1219 sec/batch\n",
      "Epoch 20/20  Iteration 7269/7380 Training loss: 4.7758 1.1189 sec/batch\n",
      "Epoch 20/20  Iteration 7270/7380 Training loss: 4.7761 1.1326 sec/batch\n",
      "Epoch 20/20  Iteration 7271/7380 Training loss: 4.7755 1.1169 sec/batch\n",
      "Epoch 20/20  Iteration 7272/7380 Training loss: 4.7753 1.1154 sec/batch\n",
      "Epoch 20/20  Iteration 7273/7380 Training loss: 4.7755 1.1168 sec/batch\n",
      "Epoch 20/20  Iteration 7274/7380 Training loss: 4.7758 1.1131 sec/batch\n",
      "Epoch 20/20  Iteration 7275/7380 Training loss: 4.7755 1.1236 sec/batch\n",
      "Epoch 20/20  Iteration 7276/7380 Training loss: 4.7751 1.1217 sec/batch\n",
      "Epoch 20/20  Iteration 7277/7380 Training loss: 4.7750 1.1243 sec/batch\n",
      "Epoch 20/20  Iteration 7278/7380 Training loss: 4.7751 1.1175 sec/batch\n",
      "Epoch 20/20  Iteration 7279/7380 Training loss: 4.7751 1.1451 sec/batch\n",
      "Epoch 20/20  Iteration 7280/7380 Training loss: 4.7746 1.1211 sec/batch\n",
      "Epoch 20/20  Iteration 7281/7380 Training loss: 4.7750 1.1333 sec/batch\n",
      "Epoch 20/20  Iteration 7282/7380 Training loss: 4.7752 1.1252 sec/batch\n",
      "Epoch 20/20  Iteration 7283/7380 Training loss: 4.7748 1.1274 sec/batch\n",
      "Epoch 20/20  Iteration 7284/7380 Training loss: 4.7751 1.1190 sec/batch\n",
      "Epoch 20/20  Iteration 7285/7380 Training loss: 4.7754 1.1251 sec/batch\n",
      "Epoch 20/20  Iteration 7286/7380 Training loss: 4.7754 1.1166 sec/batch\n",
      "Epoch 20/20  Iteration 7287/7380 Training loss: 4.7751 1.1139 sec/batch\n",
      "Epoch 20/20  Iteration 7288/7380 Training loss: 4.7757 1.1142 sec/batch\n",
      "Epoch 20/20  Iteration 7289/7380 Training loss: 4.7757 1.1165 sec/batch\n",
      "Epoch 20/20  Iteration 7290/7380 Training loss: 4.7759 1.1304 sec/batch\n",
      "Epoch 20/20  Iteration 7291/7380 Training loss: 4.7756 1.1513 sec/batch\n",
      "Epoch 20/20  Iteration 7292/7380 Training loss: 4.7757 1.1168 sec/batch\n",
      "Epoch 20/20  Iteration 7293/7380 Training loss: 4.7757 1.1287 sec/batch\n",
      "Epoch 20/20  Iteration 7294/7380 Training loss: 4.7759 1.1205 sec/batch\n",
      "Epoch 20/20  Iteration 7295/7380 Training loss: 4.7760 1.1215 sec/batch\n",
      "Epoch 20/20  Iteration 7296/7380 Training loss: 4.7764 1.1249 sec/batch\n",
      "Epoch 20/20  Iteration 7297/7380 Training loss: 4.7761 1.1325 sec/batch\n",
      "Epoch 20/20  Iteration 7298/7380 Training loss: 4.7757 1.1159 sec/batch\n",
      "Epoch 20/20  Iteration 7299/7380 Training loss: 4.7755 1.1184 sec/batch\n",
      "Epoch 20/20  Iteration 7300/7380 Training loss: 4.7751 1.1244 sec/batch\n",
      "Validation loss: 5.00689 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 7301/7380 Training loss: 4.7753 1.1137 sec/batch\n",
      "Epoch 20/20  Iteration 7302/7380 Training loss: 4.7751 1.1264 sec/batch\n",
      "Epoch 20/20  Iteration 7303/7380 Training loss: 4.7749 1.1181 sec/batch\n",
      "Epoch 20/20  Iteration 7304/7380 Training loss: 4.7748 1.1103 sec/batch\n",
      "Epoch 20/20  Iteration 7305/7380 Training loss: 4.7750 1.1277 sec/batch\n",
      "Epoch 20/20  Iteration 7306/7380 Training loss: 4.7750 1.1211 sec/batch\n",
      "Epoch 20/20  Iteration 7307/7380 Training loss: 4.7749 1.1194 sec/batch\n",
      "Epoch 20/20  Iteration 7308/7380 Training loss: 4.7748 1.1260 sec/batch\n",
      "Epoch 20/20  Iteration 7309/7380 Training loss: 4.7746 1.1297 sec/batch\n",
      "Epoch 20/20  Iteration 7310/7380 Training loss: 4.7748 1.1205 sec/batch\n",
      "Epoch 20/20  Iteration 7311/7380 Training loss: 4.7754 1.1240 sec/batch\n",
      "Epoch 20/20  Iteration 7312/7380 Training loss: 4.7753 1.1103 sec/batch\n",
      "Epoch 20/20  Iteration 7313/7380 Training loss: 4.7755 1.1192 sec/batch\n",
      "Epoch 20/20  Iteration 7314/7380 Training loss: 4.7754 1.1224 sec/batch\n",
      "Epoch 20/20  Iteration 7315/7380 Training loss: 4.7760 1.1132 sec/batch\n",
      "Epoch 20/20  Iteration 7316/7380 Training loss: 4.7763 1.1203 sec/batch\n",
      "Epoch 20/20  Iteration 7317/7380 Training loss: 4.7762 1.1194 sec/batch\n",
      "Epoch 20/20  Iteration 7318/7380 Training loss: 4.7759 1.1182 sec/batch\n",
      "Epoch 20/20  Iteration 7319/7380 Training loss: 4.7756 1.1795 sec/batch\n",
      "Epoch 20/20  Iteration 7320/7380 Training loss: 4.7755 1.1249 sec/batch\n",
      "Epoch 20/20  Iteration 7321/7380 Training loss: 4.7753 1.1154 sec/batch\n",
      "Epoch 20/20  Iteration 7322/7380 Training loss: 4.7753 1.1423 sec/batch\n",
      "Epoch 20/20  Iteration 7323/7380 Training loss: 4.7752 1.1152 sec/batch\n",
      "Epoch 20/20  Iteration 7324/7380 Training loss: 4.7750 1.1194 sec/batch\n",
      "Epoch 20/20  Iteration 7325/7380 Training loss: 4.7750 1.1172 sec/batch\n",
      "Epoch 20/20  Iteration 7326/7380 Training loss: 4.7751 1.1309 sec/batch\n",
      "Epoch 20/20  Iteration 7327/7380 Training loss: 4.7754 1.1218 sec/batch\n",
      "Epoch 20/20  Iteration 7328/7380 Training loss: 4.7756 1.1142 sec/batch\n",
      "Epoch 20/20  Iteration 7329/7380 Training loss: 4.7758 1.1285 sec/batch\n",
      "Epoch 20/20  Iteration 7330/7380 Training loss: 4.7764 1.1137 sec/batch\n",
      "Epoch 20/20  Iteration 7331/7380 Training loss: 4.7767 1.1253 sec/batch\n",
      "Epoch 20/20  Iteration 7332/7380 Training loss: 4.7764 1.1152 sec/batch\n",
      "Epoch 20/20  Iteration 7333/7380 Training loss: 4.7761 1.1164 sec/batch\n",
      "Epoch 20/20  Iteration 7334/7380 Training loss: 4.7760 1.1224 sec/batch\n",
      "Epoch 20/20  Iteration 7335/7380 Training loss: 4.7760 1.1164 sec/batch\n",
      "Epoch 20/20  Iteration 7336/7380 Training loss: 4.7760 1.1219 sec/batch\n",
      "Epoch 20/20  Iteration 7337/7380 Training loss: 4.7764 1.1202 sec/batch\n",
      "Epoch 20/20  Iteration 7338/7380 Training loss: 4.7763 1.1166 sec/batch\n",
      "Epoch 20/20  Iteration 7339/7380 Training loss: 4.7760 1.1179 sec/batch\n",
      "Epoch 20/20  Iteration 7340/7380 Training loss: 4.7758 1.1213 sec/batch\n",
      "Epoch 20/20  Iteration 7341/7380 Training loss: 4.7756 1.1251 sec/batch\n",
      "Epoch 20/20  Iteration 7342/7380 Training loss: 4.7753 1.1343 sec/batch\n",
      "Epoch 20/20  Iteration 7343/7380 Training loss: 4.7755 1.1247 sec/batch\n",
      "Epoch 20/20  Iteration 7344/7380 Training loss: 4.7752 1.1252 sec/batch\n",
      "Epoch 20/20  Iteration 7345/7380 Training loss: 4.7754 1.1270 sec/batch\n",
      "Epoch 20/20  Iteration 7346/7380 Training loss: 4.7756 1.1221 sec/batch\n",
      "Epoch 20/20  Iteration 7347/7380 Training loss: 4.7758 1.1207 sec/batch\n",
      "Epoch 20/20  Iteration 7348/7380 Training loss: 4.7760 1.1250 sec/batch\n",
      "Epoch 20/20  Iteration 7349/7380 Training loss: 4.7759 1.1259 sec/batch\n",
      "Epoch 20/20  Iteration 7350/7380 Training loss: 4.7759 1.1188 sec/batch\n",
      "Validation loss: 4.99935 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 7351/7380 Training loss: 4.7760 1.1201 sec/batch\n",
      "Epoch 20/20  Iteration 7352/7380 Training loss: 4.7758 1.1215 sec/batch\n",
      "Epoch 20/20  Iteration 7353/7380 Training loss: 4.7756 1.1170 sec/batch\n",
      "Epoch 20/20  Iteration 7354/7380 Training loss: 4.7757 1.1332 sec/batch\n",
      "Epoch 20/20  Iteration 7355/7380 Training loss: 4.7756 1.1200 sec/batch\n",
      "Epoch 20/20  Iteration 7356/7380 Training loss: 4.7756 1.1293 sec/batch\n",
      "Epoch 20/20  Iteration 7357/7380 Training loss: 4.7755 1.1485 sec/batch\n",
      "Epoch 20/20  Iteration 7358/7380 Training loss: 4.7757 1.1306 sec/batch\n",
      "Epoch 20/20  Iteration 7359/7380 Training loss: 4.7759 1.1262 sec/batch\n",
      "Epoch 20/20  Iteration 7360/7380 Training loss: 4.7759 1.1113 sec/batch\n",
      "Epoch 20/20  Iteration 7361/7380 Training loss: 4.7760 1.1182 sec/batch\n",
      "Epoch 20/20  Iteration 7362/7380 Training loss: 4.7764 1.1218 sec/batch\n",
      "Epoch 20/20  Iteration 7363/7380 Training loss: 4.7762 1.1347 sec/batch\n",
      "Epoch 20/20  Iteration 7364/7380 Training loss: 4.7763 1.1329 sec/batch\n",
      "Epoch 20/20  Iteration 7365/7380 Training loss: 4.7761 1.1250 sec/batch\n",
      "Epoch 20/20  Iteration 7366/7380 Training loss: 4.7760 1.1250 sec/batch\n",
      "Epoch 20/20  Iteration 7367/7380 Training loss: 4.7758 1.1122 sec/batch\n",
      "Epoch 20/20  Iteration 7368/7380 Training loss: 4.7761 1.1231 sec/batch\n",
      "Epoch 20/20  Iteration 7369/7380 Training loss: 4.7761 1.1222 sec/batch\n",
      "Epoch 20/20  Iteration 7370/7380 Training loss: 4.7759 1.1191 sec/batch\n",
      "Epoch 20/20  Iteration 7371/7380 Training loss: 4.7757 1.1307 sec/batch\n",
      "Epoch 20/20  Iteration 7372/7380 Training loss: 4.7757 1.1222 sec/batch\n",
      "Epoch 20/20  Iteration 7373/7380 Training loss: 4.7758 1.1863 sec/batch\n",
      "Epoch 20/20  Iteration 7374/7380 Training loss: 4.7757 1.1279 sec/batch\n",
      "Epoch 20/20  Iteration 7375/7380 Training loss: 4.7755 1.1267 sec/batch\n",
      "Epoch 20/20  Iteration 7376/7380 Training loss: 4.7754 1.1201 sec/batch\n",
      "Epoch 20/20  Iteration 7377/7380 Training loss: 4.7752 1.1330 sec/batch\n",
      "Epoch 20/20  Iteration 7378/7380 Training loss: 4.7751 1.1304 sec/batch\n",
      "Epoch 20/20  Iteration 7379/7380 Training loss: 4.7752 1.1328 sec/batch\n",
      "Epoch 20/20  Iteration 7380/7380 Training loss: 4.7749 1.1516 sec/batch\n",
      "Validation loss: 5.00037 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(words, **params)\n",
    "model = define_rnn_graph(len(dictionary), **params)\n",
    "saver = tf.train.Saver(max_to_keep=200)\n",
    "epoch_start = 0\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # tensorboard 작성을 위한 Filewriter를 만듭니다.\n",
    "    train_writer = tf.summary.FileWriter('./logs/wordRNN/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/wordRNN/test')\n",
    "       \n",
    "    n_batches = int(train_x.shape[1]/params['time_steps'])\n",
    "    iterations = n_batches * epochs\n",
    "     # 기존의 checkpoint를 읽어서 다시 학습\n",
    "    if checkpoint:\n",
    "        try:\n",
    "            saver.restore(sess, checkpoint)\n",
    "            iteration=int(re.search(r'\\bi([\\d]+)_[\\w.]+\\b',checkpoint).group(1))\n",
    "            epoch_start = int(iteration/n_batches)\n",
    "        except:\n",
    "            print('Cannot read the checkpoint. Set it None.')\n",
    "            epoch_start = 0\n",
    "            checkpoint = None\n",
    "            \n",
    "    for e in range(epoch_start, epochs):\n",
    "        # network 학습\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(get_batch([train_x, train_y], params['time_steps']), 1):\n",
    "            iteration = e*n_batches + i\n",
    "            # training 시간을 기록\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state }\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \\\n",
    "                                                          model.final_state, model.optimizer], feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/i),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            # summary추가\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            if (iteration%checkpoint_interval == 0) or (iteration == iterations):\n",
    "                # validation loss 확인. dropout의 값을 1로 설정하여 모든 node가 동작하도록 한다.\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], params['time_steps']):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \\\n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                # summary추가\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/sherlock_word/i{}_l{}_{:.3f}\".format(iteration, params['lstm_size'], np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/sherlock_word/i7380_l128_5.000\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i50_l128_6.307\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i100_l128_6.247\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i150_l128_6.228\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i200_l128_6.222\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i250_l128_6.216\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i300_l128_6.209\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i350_l128_6.161\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i400_l128_6.089\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i450_l128_6.058\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i500_l128_6.016\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i550_l128_5.972\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i600_l128_5.932\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i650_l128_5.896\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i700_l128_5.861\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i750_l128_5.818\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i800_l128_5.792\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i850_l128_5.766\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i900_l128_5.744\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i950_l128_5.718\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1000_l128_5.696\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1050_l128_5.676\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1100_l128_5.657\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1150_l128_5.633\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1200_l128_5.610\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1250_l128_5.581\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1300_l128_5.561\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1350_l128_5.538\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1400_l128_5.519\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1450_l128_5.495\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1500_l128_5.487\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1550_l128_5.459\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1600_l128_5.445\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1650_l128_5.424\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1700_l128_5.410\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1750_l128_5.392\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1800_l128_5.380\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1850_l128_5.377\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1900_l128_5.357\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i1950_l128_5.342\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2000_l128_5.330\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2050_l128_5.319\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2100_l128_5.316\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2150_l128_5.312\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2200_l128_5.285\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2250_l128_5.274\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2300_l128_5.270\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2350_l128_5.269\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2400_l128_5.253\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2450_l128_5.244\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2500_l128_5.234\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2550_l128_5.231\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2600_l128_5.218\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2650_l128_5.212\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2700_l128_5.206\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2750_l128_5.201\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2800_l128_5.207\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2850_l128_5.190\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2900_l128_5.188\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i2950_l128_5.180\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3000_l128_5.172\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3050_l128_5.165\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3100_l128_5.175\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3150_l128_5.161\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3200_l128_5.163\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3250_l128_5.159\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3300_l128_5.146\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3350_l128_5.141\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3400_l128_5.136\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3450_l128_5.133\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3500_l128_5.148\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3550_l128_5.122\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3600_l128_5.124\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3650_l128_5.120\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3700_l128_5.115\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3750_l128_5.111\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3800_l128_5.102\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3850_l128_5.110\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3900_l128_5.109\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i3950_l128_5.097\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4000_l128_5.096\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4050_l128_5.089\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4100_l128_5.091\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4150_l128_5.089\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4200_l128_5.084\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4250_l128_5.087\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4300_l128_5.076\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4350_l128_5.082\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4400_l128_5.090\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4450_l128_5.068\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4500_l128_5.071\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4550_l128_5.062\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4600_l128_5.065\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4650_l128_5.078\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4700_l128_5.058\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4750_l128_5.057\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4800_l128_5.056\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4850_l128_5.063\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4900_l128_5.041\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i4950_l128_5.046\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5000_l128_5.052\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5050_l128_5.042\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5100_l128_5.051\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5150_l128_5.044\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5200_l128_5.037\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5250_l128_5.047\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5300_l128_5.039\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5350_l128_5.040\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5400_l128_5.034\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5450_l128_5.031\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5500_l128_5.037\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5550_l128_5.038\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5600_l128_5.048\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5650_l128_5.020\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5700_l128_5.019\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5750_l128_5.037\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5800_l128_5.013\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5850_l128_5.026\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5900_l128_5.027\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i5950_l128_5.015\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6000_l128_5.023\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6050_l128_5.009\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6100_l128_5.014\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6150_l128_5.012\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6200_l128_5.025\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6250_l128_5.025\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6300_l128_5.010\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6350_l128_5.013\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6400_l128_5.008\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6450_l128_5.012\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6500_l128_5.015\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6550_l128_5.014\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6600_l128_5.008\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6650_l128_5.010\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6700_l128_5.015\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6750_l128_4.992\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6800_l128_4.997\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6850_l128_5.000\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6900_l128_5.004\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i6950_l128_5.018\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7000_l128_5.002\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7050_l128_4.995\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7100_l128_4.999\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7150_l128_4.990\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7200_l128_5.004\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7250_l128_4.993\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7300_l128_5.007\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7350_l128_4.999\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock_word/i7380_l128_5.000\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/sherlock_word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "이제 학습된 모델을 이용하여 문장을 만들어봅시다. 학습된 모델이 문장을 만드는 방법은 이전 글자가 주어졌을때, 다음 글자를 예측을 반복적으로 하면서 이루어집니다. 학습된 모델은 주어진 이전 글자에 대해 다음 글자를 확률 값으로 예측을 하게됩니다. 각각의 확률을 적용하여 Random sampling을 하여 새로운 글자가 추가가 되고, 새로운 글자와 이전 state를 이용하여 다음 글자를 예측합니다. 이 과정을 반복하게되면 문장을 만들 수 있습니다.\n",
    "확률값이 가장 높은 `N`가지중에 하나를 선택하도록 코드를 작성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, dictionary_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(dictionary_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, dictionary_size, prime=[\"The\"]):\n",
    "    samples = list(prime)\n",
    "    model = define_rnn_graph(dictionary_size, **{'lstm_size':lstm_size, 'sampling':True})\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = word_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, dictionary_size)\n",
    "        samples.append(int_to_word[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, dictionary_size)\n",
    "            # dealing with special char not word.\n",
    "            if c in list('.,\\?!\\'\"'):\n",
    "                if samples[-1] in list('.,\\?!\\'\"'):\n",
    "                    samples.append(int_to_word[c])\n",
    "                else:\n",
    "                    # concatnate the c letter to the previous word\n",
    "                    samples[-1] = samples[-1] + c                    \n",
    "            elif c in list('0123456789'):\n",
    "                if samples[-1] in list('0123456789'):\n",
    "                    # concatnate the c letter to the previous word\n",
    "                    samples[-1] = samples[-1] + c\n",
    "                else:\n",
    "                    samples.append(int_to_word[c])\n",
    "            else:\n",
    "                samples.append(int_to_word[c])\n",
    "        \n",
    "    return ' '.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Loss가 가장 작은 모델을 포함한 여러 모델을 이용하여 문장을 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_checkpoints=re.findall(r'\\b([\\w/]+_([\\d.]+))\\b',str(tf.train.get_checkpoint_state('checkpoints/sherlock')),re.IGNORECASE)\n",
    "all_checkpoints_sorted_by_valloss = sorted(all_checkpoints, key=lambda tup: float(tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('checkpoints/sherlock/i1800_l256_1.470', '1.470'),\n",
       " ('checkpoints/sherlock/i1800_l256_1.470', '1.470'),\n",
       " ('checkpoints/sherlock/i1750_l256_1.477', '1.477'),\n",
       " ('checkpoints/sherlock/i1700_l256_1.489', '1.489'),\n",
       " ('checkpoints/sherlock/i1650_l256_1.501', '1.501'),\n",
       " ('checkpoints/sherlock/i1600_l256_1.509', '1.509'),\n",
       " ('checkpoints/sherlock/i1550_l256_1.523', '1.523'),\n",
       " ('checkpoints/sherlock/i1500_l256_1.530', '1.530'),\n",
       " ('checkpoints/sherlock/i1450_l256_1.547', '1.547'),\n",
       " ('checkpoints/sherlock/i1400_l256_1.553', '1.553')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_checkpoints_sorted_by_valloss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10번째 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [20372] rhs shape= [69]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@logits/softmax_b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](logits/softmax_b/Adam, save/RestoreV2_1/_3)]]\n\nCaused by op u'save/Assign_1', defined at:\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-29-0a14c77a2376>\", line 2, in <module>\n    samp = sample(checkpoint, n_samples, params['lstm_size'], len(dictionary), prime=[int_to_word[0]])\n  File \"<ipython-input-25-30e6c462c0b2>\", line 4, in sample\n    saver = tf.train.Saver()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [20372] rhs shape= [69]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@logits/softmax_b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](logits/softmax_b/Adam, save/RestoreV2_1/_3)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0a14c77a2376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_checkpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lstm_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<<{}>>\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-30e6c462c0b2>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, dictionary_size, prime)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [20372] rhs shape= [69]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@logits/softmax_b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](logits/softmax_b/Adam, save/RestoreV2_1/_3)]]\n\nCaused by op u'save/Assign_1', defined at:\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-29-0a14c77a2376>\", line 2, in <module>\n    samp = sample(checkpoint, n_samples, params['lstm_size'], len(dictionary), prime=[int_to_word[0]])\n  File \"<ipython-input-25-30e6c462c0b2>\", line 4, in sample\n    saver = tf.train.Saver()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/wgchang/anaconda3/envs/tensorflow-py2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [20372] rhs shape= [69]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@logits/softmax_b\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](logits/softmax_b/Adam, save/RestoreV2_1/_3)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = all_checkpoints[10][0]\n",
    "samp = sample(checkpoint, n_samples, params['lstm_size'], len(dictionary), prime=[int_to_word[0]])\n",
    "print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation loss가 가장 작은 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:2]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=[int_to_word[0]])\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:1]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=[int_to_word[0]])\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints/sherlock')\n",
    "samp = sample(checkpoint, 1000, params['lstm_size'], len(charset), prime=[int_to_word[0]])\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "log directory를 설정해주고 실행합니다.\n",
    "```bash\n",
    "$ tensorboard --logdir='logs/'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-py2]",
   "language": "python",
   "name": "conda-env-tensorflow-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
