{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for sentence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 단어 단위(Word-level)의 RNN 구현해보자\n",
    "\n",
    "### <학습목표>\n",
    "1. 이번 노트북에서는 단어(word) 단위의 입력값으로 RNN을 학습해 보고, 결과를 Tensorboard를 이용하여 보는 것을 목표로 합니다.\n",
    "2. 학습할 데이터는 Sherlock homes 시리즈 중 The Sign of the Four의 영문책을 이용하여 학습합니다.\n",
    "3. 학습된 모델을 이용하여 새로운 문장을 만들어 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "위에 선언한 함수에서 이제 hyperparameter들을 정합니다. \n",
    "일반적으로 network의 크기가 커질 수록(hidden unit이 많을 수록, layer 수가 많을 수록) 성능이 향상되지만, overfitting(fit to variance)이 되는 현상을 잘 관찰해야 합니다. hyperparameter들이 너무 적을 경우에는 underfitting(fit to bias)되는 현상이 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lstm_size' : 128,\n",
    "    'batch_size': 100,\n",
    "    'time_steps': 15,    \n",
    "    'num_layers' : 3,\n",
    "    'optimizer_params': {'learning_rate': 1e-3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python2 -- python3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import namedtuple\n",
    "from six.moves import urllib\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the data.\n",
    "url = 'http://cvlab.postech.ac.kr/~wgchang/data/others/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        if not os.path.isdir(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        filename, _ = urllib.request.urlretrieve(url + os.path.basename(filename), filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ../data/sherlock.txt\n"
     ]
    }
   ],
   "source": [
    "filename = maybe_download('../data/sherlock.txt', 3377296)\n",
    "# filename = maybe_download('../data/sherlock_short.txt', 609394)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration for prevent whole gpu usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "우선 텍스트 데이터를 불러들인 후 각 단어들을 정수값으로 변환하여 모델이 학습할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_multiple_s(text):\n",
    "    # 여러번 띄어쓰기가 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r' +',r' ',text)\n",
    "    # 여러번 탭이 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\t+',r' ',text)\n",
    "    # 여러번 newline으로된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\n+',r' ',text)\n",
    "    # 특수문자를 제거합니다.\n",
    "    text = re.sub(r'[^A-Za-z0-9.,\\?!\\'\" ]+',r'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    text=f.read()\n",
    "text=remove_multiple_s(text.lower())\n",
    "texts = re.findall(r'([.,\\?!\\'\"0-9]|\\b[a-zA-z]+\\b)',text,re.IGNORECASE)\n",
    "dictionary = set(texts)\n",
    "word_to_int = {c: i for i, c in enumerate(dictionary)}\n",
    "int_to_word = dict(enumerate(dictionary))\n",
    "words = np.array([word_to_int[c] for c in texts], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16387,  9801, 15499, ...,  6579,   848, 13063], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트의 길이와 텍스트가 숫자로 변환되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693263"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20372"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter', 'i', 'mr', '.', 'sherlock', 'holmes', 'in', 'the', 'year', '1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이제 데이터를 training과 validation으로 나누고 각각을 batch로 만들어봅시다. 이번 과제에서는 Test set은 따로 없습니다.\n",
    "문장에서 input과 target의 배열을 만듭니다. 여기서 target은 input과 같은 길이의 글자열이지만 한 글자가 밀려진 글자열입니다.\n",
    "batch 크기를 맞추기 위해서 문장의 뒤에 남는 부분은 버립니다.\n",
    "split_frac은 training과 validation을 나누는 set의 비율을 나타냅니다. 전체 batch갯수중 90%를 training으로, 10%를 validation으로 사용합니다.\n",
    "\n",
    "<img src=\"../resources/dataset.jpeg\" width=\"500\" alt=\"split dataset\">\n",
    "\n",
    "x matrix(행렬)는 (`batch크기 x 글자열 길이`)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(words, **params):\n",
    "    batch_size = params.get('batch_size') or 50\n",
    "    time_steps = params.get('time_steps') or 50\n",
    "    split_frac = params.get('split_frac') or 0.8\n",
    "    \n",
    "    slice_size = batch_size * time_steps\n",
    "    n_batches = int(len(words) / slice_size)\n",
    "    x = words[: n_batches*slice_size]\n",
    "    y = words[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*time_steps], y[:, :split_idx*time_steps]\n",
    "    val_x, val_y = x[:, split_idx*time_steps:], y[:, split_idx*time_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(words, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 나눠졌는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5535)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1395)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16387,  9801, 15499,   848,  4720,  3984, 11012,  6041,  6762,\n",
       "        16845],\n",
       "       [ 6782, 17119, 13681,   848, 10027,  1000,  8628,  5808, 19848,\n",
       "        10465],\n",
       "       [13176,  3632, 11012, 18521, 19091,   848, 13063,  9801,  4258,\n",
       "         4634],\n",
       "       [ 4258,  6724, 10465, 19368, 10465,  4258,  9587,  3881, 10465,\n",
       "         4258],\n",
       "       [  266,  6041, 17112,  8144,   848, 17946,  4258,  2073,  6871,\n",
       "         7560],\n",
       "       [19081,  6464, 19531,  7933,  2453,  7560, 10465,  3944,  7560,\n",
       "         5445],\n",
       "       [ 1281, 10458, 10465,  6230,  2481, 10465,  5439, 10465, 17119,\n",
       "         4061],\n",
       "       [ 2579, 18521, 15527,   848,  5116,  9801,  1000,  2579,  5771,\n",
       "         5439],\n",
       "       [10465, 17119,  2425,  2615, 15378, 11938, 11964, 17920,  1367,\n",
       "        17119],\n",
       "       [ 7777,  9858, 17672, 10138, 18520,   848,  7560, 19961,  3281,\n",
       "        19289],\n",
       "       [  848,  7560,  2138,  6871,  6041, 13234,  7315,  6230,  6871,\n",
       "        19697],\n",
       "       [17431, 10465,    14, 19289,  1779,  2579,  6041, 20080, 17672,\n",
       "         3716],\n",
       "       [17277,  7867,   213, 11012,  6041, 12925, 17672, 11004,   848,\n",
       "        10027],\n",
       "       [16103,  9875,  4757, 18539,   848, 13063, 19081, 19961,  3614,\n",
       "         2495],\n",
       "       [11544, 10465,  4258, 10027,  7771, 10465,  4258, 19697,  7773,\n",
       "         1779],\n",
       "       [15503,  3102,  1000, 18964,  3716,  5841, 19289, 15659, 20080,\n",
       "          848],\n",
       "       [13716,   848,  6041,  9397,  9062,  7867, 12876, 14956,  6724,\n",
       "          848],\n",
       "       [ 1612, 10465,  1073,  6041, 17359, 17119, 15160, 17672,  6041,\n",
       "         9517],\n",
       "       [ 6230, 18457, 11012, 18521, 13118, 19697,  4303, 14211,  1073,\n",
       "         1577],\n",
       "       [ 2425, 10692, 17668,  7867, 13508,   848, 17672,  1612, 10465,\n",
       "         9801],\n",
       "       [  848, 10027,  5943,  6245,  2579, 18521,  9405, 10465, 17119,\n",
       "         5788],\n",
       "       [ 9801, 14476,  6041,  9148,   848, 12206, 15503,  1619, 15309,\n",
       "         4061],\n",
       "       [10465,  9801, 11897, 10465, 12313, 10465,  6871, 19081, 11698,\n",
       "         3956],\n",
       "       [ 6871, 11004,  1000,  6722, 16103, 15994,  5841,  3629,  6041,\n",
       "        14574],\n",
       "       [ 8442,  9801,  3566,  2579, 14476,  6041,  1011, 17672,  6041,\n",
       "        19611],\n",
       "       [11012,  8628,  1577, 10673, 10458, 10465, 17119,  6871,  1637,\n",
       "        15453],\n",
       "       [10465,  2275, 11938, 10465, 17119, 18521,  4088, 10465,  4070,\n",
       "        10465],\n",
       "       [17672,  7867,  4358, 17119, 10677, 14632, 10465,  6230, 10246,\n",
       "        10882],\n",
       "       [18325, 17119,  6041,    65, 17672, 12886, 11975, 10465,  1779,\n",
       "        11993],\n",
       "       [13063, 13063,  9587,  3801,  3656,  6041, 13734,   848, 13063,\n",
       "        13063],\n",
       "       [ 6230,  7867, 16953, 19697, 11002,  7867,   243,  5307,  5116,\n",
       "        15593],\n",
       "       [15453,  6722,  5963,  2967, 11004,  1073,  7867, 13070, 11172,\n",
       "          848],\n",
       "       [ 1022,  6871, 18521, 17831, 11002,  7867,  4724, 10465,  6871,\n",
       "        10027],\n",
       "       [17518, 15033,   848, 16524, 12269,   650, 19289,  8705, 10465,\n",
       "        17119],\n",
       "       [ 4757,  7867,  4498, 13911, 10465, 12035,  2493, 11004, 15453,\n",
       "         6722],\n",
       "       [13063, 13063,  9804, 10465,  9804, 10465,  7560, 11473,  7461,\n",
       "          753],\n",
       "       [ 2825, 12965, 13063, 20051,  8476,  7589,   848, 13063,  7560,\n",
       "         6579],\n",
       "       [17672, 15475, 10465, 17119, 17672, 19360,  7980,  4718,  7867,\n",
       "         7643],\n",
       "       [11436,  7323, 11004, 10465, 15605,  8357, 18521,  9223,  2579,\n",
       "        18521],\n",
       "       [19117, 19697, 13616,  2579,  3614,  2495,  6301, 10465,  6871,\n",
       "         9801],\n",
       "       [15503,  3255, 10465, 16033, 10465, 17119, 11004,  9810,  2260,\n",
       "        15032],\n",
       "       [ 9801,  2490, 19688,  6871, 15503,  5682, 11698,  4505,  7560,\n",
       "        15179],\n",
       "       [17887, 19961, 10465, 17119, 11439, 10465, 17668, 18521, 12994,\n",
       "        11012],\n",
       "       [ 4358,  2579, 15487,  6885,  9801,  6579, 13394,   848, 11018,\n",
       "         9801],\n",
       "       [ 6871, 10518, 17672, 18521,  2172,   848, 18521, 18246, 11002,\n",
       "        10900],\n",
       "       [ 6518,  3629, 16183,  6578, 19697, 10366, 11004,  7867,  1737,\n",
       "         1029],\n",
       "       [13312, 18237, 17231,  2579, 12013, 11002,  6041,  1779, 19697,\n",
       "        11002],\n",
       "       [19697, 15395,  6041,  5873, 16943,   848, 19536, 17672,  4061,\n",
       "        10551],\n",
       "       [13367, 15487,  7867, 12258, 12336,  6724,   848,  4061, 11711,\n",
       "         4258],\n",
       "       [13230,  3791, 13091, 10465, 19697,  6722,  7001, 15925,  2579,\n",
       "        15487],\n",
       "       [ 5963, 17344,  2579,  7560,  4061, 12119,   848,  9801, 18320,\n",
       "         2493],\n",
       "       [ 1741, 10465, 11004,  4258,  2073,  6722, 11012,  7867,  7642,\n",
       "         9871],\n",
       "       [17717,  4258,  2073,  6041,  3335, 17421, 10465, 17119,  6724,\n",
       "         9801],\n",
       "       [ 9801,   650, 15593, 10465, 17119, 10465,  2493,  4961, 11964,\n",
       "        16524],\n",
       "       [13070,    28, 12965, 17946,   616,  9801,  5771,  9927, 13063,\n",
       "        20051],\n",
       "       [ 1133,  9927, 13063, 13063, 16524, 10465, 19368, 10465,  9801,\n",
       "         5963],\n",
       "       [19289,  7461, 20227,  8632,   848, 13063, 13063,  6370, 10465,\n",
       "        15499],\n",
       "       [ 7867, 17486,  3789, 10465, 13063, 10854, 10027,   848, 13063,\n",
       "        15179],\n",
       "       [10465, 16524, 19081, 15486, 13367, 20080,  1916,  2579,  6222,\n",
       "        18539],\n",
       "       [ 9801,  5963, 20286,  3715, 15503, 10692,   848, 15503,  3070,\n",
       "        18571],\n",
       "       [13121,  6871, 10027,  4303,  3566, 18521, 11821,  2453, 12088,\n",
       "          848],\n",
       "       [ 5439, 11004, 11002, 10027, 18667,  7560, 11522,  9927, 13063,\n",
       "        13063],\n",
       "       [  848,  5116,  6041,  2248,  7560, 19961,  3281,  1210,  1073,\n",
       "         6835],\n",
       "       [20141,  2579,  5908, 10465, 15043, 17119, 10119, 15032,  7867,\n",
       "        20015],\n",
       "       [ 4516,  5564, 13874,  5682,  2579,  7560, 10465, 15499,   848,\n",
       "         3984],\n",
       "       [11002,  2425, 16953, 10465, 17119,  6041, 18386, 17672, 11768,\n",
       "         7560],\n",
       "       [ 4303,  6722,  1358, 15487, 11012,  6041,  3354,  6113,   848,\n",
       "         9587],\n",
       "       [16537,   848, 11018,  3984,  1892,  2736, 10027,  6027, 18521,\n",
       "        20227],\n",
       "       [10762,   848, 13063, 13063,  9804, 10465,  5439, 10465, 11052,\n",
       "         6871],\n",
       "       [ 6011, 17119, 10429,  3629,  6041,  4100, 17672,  3591, 14137,\n",
       "          848],\n",
       "       [  848, 13063, 13063,  9801,  6230,    14, 19536,  1073,  5368,\n",
       "        11012],\n",
       "       [ 7251, 17672,  7117, 10465,  4088,  7880, 17668, 12087, 10518,\n",
       "        10465],\n",
       "       [ 7642,   851,  1281,  1073, 11734, 10465, 13063,  2425, 16650,\n",
       "        16578],\n",
       "       [17423,  6871, 10027,  5943,  4061, 11254, 17668,   848, 17946,\n",
       "         5943],\n",
       "       [19368,   848, 19081,  5963, 17633, 18539,  2453,  5116,  6871,\n",
       "        10027],\n",
       "       [17883,  9801,  1862,  6041,  6036,  1035, 16822,  4199,  1073,\n",
       "        13874],\n",
       "       [ 6230, 16524, 15363,  3330,  7080,  6041, 12644, 17119,  6041,\n",
       "        11711],\n",
       "       [10458,  3716,   848, 11004,  6230,  6722,  7867, 11684,  4619,\n",
       "        17119],\n",
       "       [  848, 17119,  6724, 10465, 15503, 11181,  5781, 10465, 19081,\n",
       "         5963],\n",
       "       [11018, 17717,   496,  4258,  7357,  7867,  6248, 11711, 11012,\n",
       "         6041],\n",
       "       [ 6041, 14840, 11012,  5404,  2579, 10366,  6041, 13121, 13101,\n",
       "         1000],\n",
       "       [12965, 13063,  6754,  2453,  1779, 17672, 18521, 16659, 16013,\n",
       "         4641],\n",
       "       [ 9621, 19114,   848, 11004,  5943,  2928,  1044, 18101,  2615,\n",
       "        11012],\n",
       "       [10027, 15623, 11487, 10075,  2495,  6041, 14122, 11012,  6041,\n",
       "         6572],\n",
       "       [13366, 10606, 10465, 17119, 18521, 14449,  2565, 11012,  2453,\n",
       "         6871],\n",
       "       [10027,   818, 10465, 13063, 11698,  9801,  8025, 19707,  5943,\n",
       "        19860],\n",
       "       [10027,  1000, 11915, 16103,   443, 17668, 14211, 17672, 18521,\n",
       "         6353],\n",
       "       [ 9223,   848,  6041,  8067,  1000,  5478,  1073,  7867, 13630,\n",
       "        10465],\n",
       "       [  848,  9801,  4258,  4708,  1000, 10937,  6182, 17672, 11004,\n",
       "          848],\n",
       "       [  848,  4258,  3558,  4258,  6871,  4516,  3614,  7867,  8449,\n",
       "          848],\n",
       "       [11473, 15503, 17366,  2579, 18521, 12722, 10465, 17119,  2799,\n",
       "        15503],\n",
       "       [ 4720,  3984,  2493, 19081, 13550, 15593,  4148, 15864,  6871,\n",
       "         6113],\n",
       "       [17717, 11255, 19531,  5033,   848, 17717, 11002,  1779, 18101,\n",
       "        14226],\n",
       "       [ 6835, 12534,  3715, 11012,  6041, 10673,   848, 13063, 13063,\n",
       "        10060],\n",
       "       [10027, 11002, 13874,  1052, 12789, 17119,  7867, 13312, 16834,\n",
       "        11711],\n",
       "       [  164,   848, 13063,  9380, 12965,  7560, 19961,  7890,   753,\n",
       "         3629],\n",
       "       [13063, 13063,  7560,  1854,  6871,  7560,  5963,  1000, 11004,\n",
       "        20129],\n",
       "       [ 7867, 17887,  4258,  2073,  5955, 17119,  7969,  1043,  8860,\n",
       "        11012],\n",
       "       [16524,  8835,  2579,  3716, 10465, 16033, 10465,  6727, 15453,\n",
       "        19081],\n",
       "       [ 1779, 17672,  7188,  6230, 18521, 19953, 12336, 10465,  8879,\n",
       "         3426]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "학습을 할 때 각각 batch를 순서대로 넣어야 하기때문에 batch하나를 가져오는 함수를 만들어봅시다. 각 batch는 (`batch 크기 X time_steps`)입니다.\n",
    "예를 들면, 우리의 모델이 100개의 문자열에 대해서 학습을 한다면, `time_steps = 100`이 됩니다. 그 다음 batch는 학습한 그 다음 문자열부터 학습됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tensorflow를 이용하여 RNN을 만들어봅시다. tensorflow관련 함수들은 [Tensorflow RNN API](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)를 참조하시면 됩니다.\n",
    "##### 참조 링크\n",
    "- [One-hot vector](https://www.tensorflow.org/api_docs/python/tf/one_hot): \n",
    "<img src='../resources/one_hot.png' width=\"700\" alt=\"one hot encoding\">\n",
    "- [Dropout](https://www.youtube.com/watch?v=NhZVe50QwPM)[참조논문](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf): Dropout은 random하게 특정 node를 0으로 만들어서 back-propagation이 0으로 된 node 이후로 진행되지 않게하여 overfiting을 막아주는 regularization역할을하여 학습을 원할하게합니다. **Advanced Topic: [Batch Normalization](https://arxiv.org/abs/1502.03167)를 추가적으로 공부하시면 overfitting 관련 공부에 도움이 됩니다.**\n",
    "<img src='../resources/dropout.png' width=\"700\" alt=\"dropout\">\n",
    "- [Optimizer](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorBoard에 그래프를 기입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "def define_your_model():\n",
    "    ###\n",
    "    tf.summary.histogram('histogram', histogram)\n",
    "    tf.summary.scalar('scalar', scalar)\n",
    "    ###\n",
    "    merged = tf.summary.merge_all()\n",
    "    ###\n",
    "model = define_your_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ###\n",
    "    file_writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    file_writer.add_summary(summary_to_record, iteration_index)\n",
    "    ###\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def define_rnn_graph(num_classes, **params):\n",
    "    # parameters\n",
    "    lstm_size = params.get('lstm_size') or 128\n",
    "    batch_size = params.get('batch_size') or 50\n",
    "    time_steps = params.get('time_steps') or 50\n",
    "    num_layers = params.get('num_layers') or 2\n",
    "    optimizer_params = params.get('optimizer_params') or {'learning_rate': 1e-3}\n",
    "    grad_clip = params.get('grad_clip') or 10\n",
    "    sampling = params.get('sampling') or False\n",
    "    \n",
    "    if sampling == True:\n",
    "        batch_size, time_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # placeholders를 선언합니다.\n",
    "    # input을 tf.one_hot함수를 이용하여 one_hot vector로 바꿔줍니다.\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, time_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    # target도 비슷한 방식으로 진행합니다\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, time_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "\n",
    "        # Loss를 계산하기위해 one_hot vector들의 matrix를 tf.reshape함수를 이용하여 하나의 긴 vector로 바꾸어줍니다.\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    # Dropout을 위한 확률값을 저장하는 place holder\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # RNN의 한 종류인 LSTM 구현\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            lstm = rnn.BasicLSTMCell(lstm_size)\n",
    "            # rnn.DropoutWrapper를 이용하여 RNN model에 Dropout 추가\n",
    "            drop = rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            # LSTM hidden layer 추가, weight sharing\n",
    "            lstm_layers.append(drop)\n",
    "        cell = rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "    # tf.nn.dynamic_rnn함수를 이용해 RNN을 실행\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # forward propagation\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "\n",
    "    # Output을 Concatenate한 후에 Reshape합니다.\n",
    "    with tf.name_scope('reshaper'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Cost를 계산하기위해 RNN putput을  input으로하는 softmax layer를 제작합니다.\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # weights & bias를 histogram으로 작성\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        # prediction의 확률값을 histogram으로 작성\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        # cost값을 scalar value로 작성\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # 학습을 위한 Optimizer를 정의합니다.\n",
    "    # 대표적인 optimizer로는 SGD(stocastic gradient descent), Adam, RMSprop 등이 있습니다.\n",
    "    # Gradient clipping을 통해 gradient값이 매우 큰 경우는 grad_clip값으로 제한합니다.\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(**optimizer_params)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # summary를 merge합니다.\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 앞에 선언한 노드들을 모두 Graph로 만들어서 결과로 반환합니다.\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer','merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 학습 (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint를 저장할 directory를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('checkpoints/sherlock_word'):\n",
    "    os.makedirs('checkpoints/sherlock_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "    filelist=set([re.search(r'(^i[0-9]+_l[0-9]+_[0-9]+\\.[0-9]+)\\b',l).group(0) \\\n",
    "                  for l in os.listdir(checkpoint_dir) if re.search(r'(^i[0-9]+_l[0-9]+_[0-9]+\\.[0-9]+)\\b',l)])\n",
    "    filelist=list(filelist)\n",
    "    checkpoint_sorted_by_iterations = sorted(filelist, key=lambda pattern:\\\n",
    "                                    int(re.search(r'^i([0-9]+)_l[0-9]+_[0-9]+\\.[0-9]+\\b',pattern).group(1)))\n",
    "    if checkpoint_sorted_by_iterations == []:\n",
    "        return None\n",
    "    else:\n",
    "        return os.path.join(checkpoint_dir,checkpoint_sorted_by_iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "checkpoint_interval = 50\n",
    "checkpoint = None\n",
    "# 기존 checkpoint를 실행하고싶다면 None 대신 checkpoint_path를 넣으면됩니다.\n",
    "checkpoint = load_latest_checkpoint('checkpoints/sherlock_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/7380 Training loss: 9.9219 0.8294 sec/batch\n",
      "Epoch 1/20  Iteration 2/7380 Training loss: 9.9206 0.6049 sec/batch\n",
      "Epoch 1/20  Iteration 3/7380 Training loss: 9.9190 0.5852 sec/batch\n",
      "Epoch 1/20  Iteration 4/7380 Training loss: 9.9168 0.5695 sec/batch\n",
      "Epoch 1/20  Iteration 5/7380 Training loss: 9.9136 0.6527 sec/batch\n",
      "Epoch 1/20  Iteration 6/7380 Training loss: 9.9092 0.6532 sec/batch\n",
      "Epoch 1/20  Iteration 7/7380 Training loss: 9.9023 0.6217 sec/batch\n",
      "Epoch 1/20  Iteration 8/7380 Training loss: 9.8906 0.6690 sec/batch\n",
      "Epoch 1/20  Iteration 9/7380 Training loss: 9.8699 0.7458 sec/batch\n",
      "Epoch 1/20  Iteration 10/7380 Training loss: 9.8386 0.8175 sec/batch\n",
      "Epoch 1/20  Iteration 11/7380 Training loss: 9.7890 0.8779 sec/batch\n",
      "Epoch 1/20  Iteration 12/7380 Training loss: 9.7315 0.9210 sec/batch\n",
      "Epoch 1/20  Iteration 13/7380 Training loss: 9.6735 0.9240 sec/batch\n",
      "Epoch 1/20  Iteration 14/7380 Training loss: 9.6187 0.9306 sec/batch\n",
      "Epoch 1/20  Iteration 15/7380 Training loss: 9.5554 0.9346 sec/batch\n",
      "Epoch 1/20  Iteration 16/7380 Training loss: 9.4903 0.9486 sec/batch\n",
      "Epoch 1/20  Iteration 17/7380 Training loss: 9.4273 1.0072 sec/batch\n",
      "Epoch 1/20  Iteration 18/7380 Training loss: 9.3539 0.9505 sec/batch\n",
      "Epoch 1/20  Iteration 19/7380 Training loss: 9.2823 0.9628 sec/batch\n",
      "Epoch 1/20  Iteration 20/7380 Training loss: 9.2134 0.9749 sec/batch\n",
      "Epoch 1/20  Iteration 21/7380 Training loss: 9.1439 0.9842 sec/batch\n",
      "Epoch 1/20  Iteration 22/7380 Training loss: 9.0784 0.9818 sec/batch\n",
      "Epoch 1/20  Iteration 23/7380 Training loss: 9.0107 0.9849 sec/batch\n",
      "Epoch 1/20  Iteration 24/7380 Training loss: 8.9497 0.9899 sec/batch\n",
      "Epoch 1/20  Iteration 25/7380 Training loss: 8.8868 0.9939 sec/batch\n",
      "Epoch 1/20  Iteration 26/7380 Training loss: 8.8236 0.9891 sec/batch\n",
      "Epoch 1/20  Iteration 27/7380 Training loss: 8.7544 0.9970 sec/batch\n",
      "Epoch 1/20  Iteration 28/7380 Training loss: 8.6984 1.0092 sec/batch\n",
      "Epoch 1/20  Iteration 29/7380 Training loss: 8.6406 1.0022 sec/batch\n",
      "Epoch 1/20  Iteration 30/7380 Training loss: 8.5850 0.9965 sec/batch\n",
      "Epoch 1/20  Iteration 31/7380 Training loss: 8.5303 0.9955 sec/batch\n",
      "Epoch 1/20  Iteration 32/7380 Training loss: 8.4794 0.9965 sec/batch\n",
      "Epoch 1/20  Iteration 33/7380 Training loss: 8.4301 1.0028 sec/batch\n",
      "Epoch 1/20  Iteration 34/7380 Training loss: 8.3838 0.9986 sec/batch\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(words, **params)\n",
    "model = define_rnn_graph(len(dictionary), **params)\n",
    "saver = tf.train.Saver(max_to_keep=200)\n",
    "epoch_start = 0\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # tensorboard 작성을 위한 Filewriter를 만듭니다.\n",
    "    train_writer = tf.summary.FileWriter('./logs/wordRNN/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/wordRNN/test')\n",
    "       \n",
    "    n_batches = int(train_x.shape[1]/params['time_steps'])\n",
    "    iterations = n_batches * epochs\n",
    "     # 기존의 checkpoint를 읽어서 다시 학습\n",
    "    if checkpoint:\n",
    "        try:\n",
    "            saver.restore(sess, checkpoint)\n",
    "            iteration=int(re.search(r'\\bi([\\d]+)_[\\w.]+\\b',checkpoint).group(1))\n",
    "            epoch_start = int(iteration/n_batches)\n",
    "        except:\n",
    "            print('Cannot read the checkpoint. Set it None.')\n",
    "            epoch_start = 0\n",
    "            checkpoint = None\n",
    "            \n",
    "    for e in range(epoch_start, epochs):\n",
    "        # network 학습\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(get_batch([train_x, train_y], params['time_steps']), 1):\n",
    "            iteration = e*n_batches + i\n",
    "            # training 시간을 기록\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state }\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \\\n",
    "                                                          model.final_state, model.optimizer], feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/i),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            # summary추가\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            if (iteration%checkpoint_interval == 0) or (iteration == iterations):\n",
    "                # validation loss 확인. dropout의 값을 1로 설정하여 모든 node가 동작하도록 한다.\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], params['time_steps']):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \\\n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                # summary추가\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/sherlock_word/i{}_l{}_{:.3f}\".format(iteration, params['lstm_size'], np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/sherlock_word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "이제 학습된 모델을 이용하여 문장을 만들어봅시다. 학습된 모델이 문장을 만드는 방법은 이전 글자가 주어졌을때, 다음 글자를 예측을 반복적으로 하면서 이루어집니다. 학습된 모델은 주어진 이전 글자에 대해 다음 글자를 확률 값으로 예측을 하게됩니다. 각각의 확률을 적용하여 Random sampling을 하여 새로운 글자가 추가가 되고, 새로운 글자와 이전 state를 이용하여 다음 글자를 예측합니다. 이 과정을 반복하게되면 문장을 만들 수 있습니다.\n",
    "확률값이 가장 높은 `N`가지중에 하나를 선택하도록 코드를 작성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, dictionary_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(dictionary_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, dictionary_size, prime=[\"The\"]):\n",
    "    samples = list(prime)\n",
    "    model = define_rnn_graph(dictionary_size, **{'lstm_size':lstm_size, 'sampling':True})\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = word_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, dictionary_size)\n",
    "        samples.append(int_to_word[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, dictionary_size)\n",
    "            # dealing with special char not word.\n",
    "            if c in list('.,\\?!\\'\"'):\n",
    "                if samples[-1] in list('.,\\?!\\'\"'):\n",
    "                    samples.append(int_to_word[c])\n",
    "                else:\n",
    "                    # concatnate the c letter to the previous word\n",
    "                    samples[-1] = samples[-1] + c                    \n",
    "            elif c in list('0123456789'):\n",
    "                if samples[-1] in list('0123456789'):\n",
    "                    # concatnate the c letter to the previous word\n",
    "                    samples[-1] = samples[-1] + c\n",
    "                else:\n",
    "                    samples.append(int_to_word[c])\n",
    "            else:\n",
    "                samples.append(int_to_word[c])\n",
    "        \n",
    "    return ' '.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Loss가 가장 작은 모델을 포함한 여러 모델을 이용하여 문장을 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_checkpoints=re.findall(r'\\b([\\w/]+_([\\d.]+))\\b',str(tf.train.get_checkpoint_state('checkpoints/sherlock')),re.IGNORECASE)\n",
    "all_checkpoints_sorted_by_valloss = sorted(all_checkpoints, key=lambda tup: float(tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_checkpoints_sorted_by_valloss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10번째 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = all_checkpoints[10][0]\n",
    "samp = sample(checkpoint, n_samples, params['lstm_size'], len(dictionary), prime=[int_to_word[0]])\n",
    "print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation loss가 가장 작은 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:2]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=[int_to_word[0]])\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:1]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=[int_to_word[0]])\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints/sherlock')\n",
    "samp = sample(checkpoint, 1000, params['lstm_size'], len(charset), prime=[int_to_word[0]])\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "log directory를 설정해주고 실행합니다.\n",
    "```bash\n",
    "$ tensorboard --logdir='logs/'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-py2]",
   "language": "python",
   "name": "conda-env-tensorflow-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
