{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 개요\n",
    "<img src=\"../resources/tensorflow_overview.png\" width=\"1000\">\n",
    "        [이미지 출처](https://www.youtube.com/watch?v=-57Ne86Ia8w&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=3)\n",
    "1. Tensorflow의 operation과 이미 구현된 모델을 이용하여 Graph를 제작한다.\n",
    "2. Input에 해당하는 부분을 placeholder로 만들어, `placeholder`에 넣을 데이터를 `feed_dict`에 넣는다.\n",
    "3. Iteration마다 학습을하고 weights를 갱신한다.\n",
    "4. validation 데이터를 이용하여 학습이 잘되고 있는지 확인한다.\n",
    "5. 나의 모델에 test 데이터를 넣어서 결과를 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 글자 단위(Character-level)의 RNN 구현과 Tensorboard를 배워보자\n",
    "\n",
    "### <학습목표>\n",
    "1. 이번 노트북에서는 글자(character) 단위의 입력값으로 RNN을 학습해 보고, 결과를 Tensorboard를 이용하여 보는 것을 목표로 합니다.\n",
    "2. 학습할 데이터는 Sherlock homes 시리즈 중 The Sign of the Four의 영문책을 이용하여 학습합니다.\n",
    "3. 학습된 모델을 이용하여 새로운 문장을 만들어 봅니다.\n",
    "\n",
    "이 수업의 내용은 Andrej Karpathy의 [블로그 포스트](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)와  [Torch로 구현된 코드](https://github.com/karpathy/char-rnn)에 기반한 Tensorflow 수업입니다. 아래 사진은 일반적인 글자 입력 단위의 RNN의 구조입니다.\n",
    "\n",
    "<img src=\"../resources/charseq.jpeg\" width=\"500\" alt=\"charseq\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python2 -- python3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import namedtuple\n",
    "from six.moves import urllib\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the data.\n",
    "url = 'http://cvlab.postech.ac.kr/~wgchang/data/others/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        if not os.path.isdir(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        filename, _ = urllib.request.urlretrieve(url + os.path.basename(filename), filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = maybe_download('../data/sherlock.txt', 3377296)\n",
    "# filename = maybe_download('../data/sherlock_short.txt', 609394)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration for prevent whole gpu usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "우선 텍스트 데이터를 불러들인 후 각 단어들을 정수값으로 변환하여 모델이 학습할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_multiple_s(text):\n",
    "    # 여러번 띄어쓰기가 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r' +',r' ',text)\n",
    "    # 여러번 탭이 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\t+',r' ',text)\n",
    "    # 여러번 newline으로된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\n+',r' ',text)\n",
    "    # 특수문자를 제거합니다.\n",
    "    text = re.sub(r'[^A-Za-z0-9.,\\?!\\'\" ]+',r'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    text=f.read()\n",
    "text=remove_multiple_s(text)\n",
    "charset = set(text)\n",
    "char_to_int = {c: i for i, c in enumerate(charset)}\n",
    "int_to_char = dict(enumerate(charset))\n",
    "chars = np.array([char_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트의 길이와 텍스트가 숫자로 변환되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이제 데이터를 training과 validation으로 나누고 각각을 batch로 만들어봅시다. 이번 과제에서는 Test set은 따로 없습니다.\n",
    "문장에서 input과 target의 배열을 만듭니다. 여기서 target은 input과 같은 길이의 글자열이지만 한 글자가 밀려진 글자열입니다.\n",
    "batch 크기를 맞추기 위해서 문장의 뒤에 남는 부분은 버립니다.\n",
    "split_frac은 training과 validation을 나누는 set의 비율을 나타냅니다. 전체 batch갯수중 90%를 training으로, 10%를 validation으로 사용합니다.\n",
    "\n",
    "<img src=\"../resources/dataset.jpeg\" width=\"500\" alt=\"split dataset\">\n",
    "\n",
    "x matrix(행렬)는 (`batch크기 x 글자열 길이`)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, **params):\n",
    "    batch_size = params['batch_size']\n",
    "    time_steps = params['time_steps']\n",
    "    split_frac = params.get('split_frac') or 0.9\n",
    "    \n",
    "    slice_size = batch_size * time_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*time_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*time_steps], y[:, :split_idx*time_steps]\n",
    "    val_x, val_y = x[:, split_idx*time_steps:], y[:, split_idx*time_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "학습을 할 때 각각 batch를 순서대로 넣어야 하기때문에 batch하나를 가져오는 함수를 만들어봅시다. 각 batch는 (`batch 크기 X time_steps`)입니다.\n",
    "예를 들면, 우리의 모델이 100개의 문자열에 대해서 학습을 한다면, `time_steps = 100`이 됩니다. 그 다음 batch는 학습한 그 다음 문자열부터 학습됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tensorflow를 이용하여 RNN을 만들어봅시다. tensorflow관련 함수들은 [Tensorflow RNN API](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)를 참조하시면 됩니다.\n",
    "##### 참조 링크\n",
    "- [One-hot vector](https://www.tensorflow.org/api_docs/python/tf/one_hot): \n",
    "<img src='../resources/one_hot.png' width=\"700\" alt=\"one hot encoding\">\n",
    "- [Dropout](https://www.youtube.com/watch?v=NhZVe50QwPM)[참조논문](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf): Dropout은 random하게 특정 node를 0으로 만들어서 back-propagation이 0으로 된 node 이후로 진행되지 않게하여 overfiting을 막아주는 regularization역할을하여 학습을 원할하게합니다. **Advanced Topic: [Batch Normalization](https://arxiv.org/abs/1502.03167)를 추가적으로 공부하시면 overfitting 관련 공부에 도움이 됩니다.**\n",
    "<img src='../resources/dropout.png' width=\"700\" alt=\"dropout\">\n",
    "- [Optimizer](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorBoard에 그래프를 기입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "def define_your_model():\n",
    "    ###\n",
    "    tf.summary.histogram('histogram', histogram)\n",
    "    tf.summary.scalar('scalar', scalar)\n",
    "    ###\n",
    "    merged = tf.summary.merge_all()\n",
    "    ###\n",
    "model = define_your_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ###\n",
    "    file_writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    file_writer.add_summary(summary_to_record, iteration_index)\n",
    "    ###\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def define_rnn_graph(num_classes, **params):\n",
    "    # parameters\n",
    "    lstm_size = params.get('lstm_size') or 128\n",
    "    batch_size = params.get('batch_size') or 50\n",
    "    time_steps = params.get('time_steps') or 50\n",
    "    num_layers = params.get('num_layers') or 2\n",
    "    optimizer_params = params.get('optimizer_params') or {'learning_rate': 1e-3}\n",
    "    grad_clip = params.get('grad_clip') or 10\n",
    "    sampling = params.get('sampling') or False\n",
    "    \n",
    "    if sampling == True:\n",
    "        batch_size, time_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # placeholders를 선언합니다.\n",
    "    # input을 tf.one_hot함수를 이용하여 one_hot vector로 바꿔줍니다.\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, time_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    # target도 비슷한 방식으로 진행합니다\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, time_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "\n",
    "        # Loss를 계산하기위해 one_hot vector들의 matrix를 tf.reshape함수를 이용하여 하나의 긴 vector로 바꾸어줍니다.\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    # Dropout을 위한 확률값을 저장하는 place holder\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # RNN의 한 종류인 LSTM 구현\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            lstm = rnn.BasicLSTMCell(lstm_size)\n",
    "            # rnn.DropoutWrapper를 이용하여 RNN model에 Dropout 추가\n",
    "            drop = rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            # LSTM hidden layer 추가, weight sharing\n",
    "            lstm_layers.append(drop)\n",
    "        cell = rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "    # tf.nn.dynamic_rnn함수를 이용해 RNN을 실행\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # forward propagation\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "\n",
    "    # Output을 Concatenate한 후에 Reshape합니다.\n",
    "    with tf.name_scope('reshaper'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Cost를 계산하기위해 RNN putput을  input으로하는 softmax layer를 제작합니다.\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # weights & bias를 histogram으로 작성\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        # prediction의 확률값을 histogram으로 작성\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        # cost값을 scalar value로 작성\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # 학습을 위한 Optimizer를 정의합니다.\n",
    "    # 대표적인 optimizer로는 SGD(stocastic gradient descent), Adam, RMSprop 등이 있습니다.\n",
    "    # Gradient clipping을 통해 gradient값이 매우 큰 경우는 grad_clip값으로 제한합니다.\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(**optimizer_params)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # summary를 merge합니다.\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 앞에 선언한 노드들을 모두 Graph로 만들어서 결과로 반환합니다.\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer','merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "위에 선언한 함수에서 이제 hyperparameter들을 정합니다. \n",
    "일반적으로 network의 크기가 커질 수록(hidden unit이 많을 수록, layer 수가 많을 수록) 성능이 향상되지만, overfitting(fit to variance)이 되는 현상을 잘 관찰해야 합니다. hyperparameter들이 너무 적을 경우에는 underfitting(fit to bias)되는 현상이 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lstm_size' : 1024,\n",
    "    'batch_size': 100,\n",
    "    'time_steps': 100,    \n",
    "    'num_layers' : 2,\n",
    "    'optimizer_params': {'learning_rate': 1e-3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 나눠졌는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 학습 (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint를 저장할 directory를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('checkpoints/sherlock'):\n",
    "    os.makedirs('checkpoints/sherlock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "    filelist=set([re.search(r'(^i[0-9]+_l[0-9]+_[0-9]+\\.[0-9]+)\\b',l).group(0) \\\n",
    "                  for l in os.listdir(checkpoint_dir) if re.search(r'(^i[0-9]+_l[0-9]+_[0-9]+\\.[0-9]+)\\b',l)])\n",
    "    filelist=list(filelist)\n",
    "    checkpoint_sorted_by_iterations = sorted(filelist, key=lambda pattern:\\\n",
    "                                    int(re.search(r'^i([0-9]+)_l[0-9]+_[0-9]+\\.[0-9]+\\b',pattern).group(1)))\n",
    "    if checkpoint_sorted_by_iterations == []:\n",
    "        return None\n",
    "    else:\n",
    "        return os.path.join(checkpoint_dir,checkpoint_sorted_by_iterations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "checkpoint_interval = 50\n",
    "checkpoint = load_latest_checkpoint('checkpoints/sherlock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, **params)\n",
    "model = define_rnn_graph(len(charset), **params)\n",
    "saver = tf.train.Saver(max_to_keep=200)\n",
    "epoch_start = 0\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # tensorboard 작성을 위한 Filewriter를 만듭니다.\n",
    "    train_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/test')\n",
    "       \n",
    "    n_batches = int(train_x.shape[1]/params['time_steps'])\n",
    "    iterations = n_batches * epochs\n",
    "     # 기존의 checkpoint를 읽어서 다시 학습\n",
    "    if checkpoint:\n",
    "        try:\n",
    "            saver.restore(sess, checkpoint)\n",
    "            iteration=int(re.search(r'\\bi([\\d]+)_[\\w.]+\\b',checkpoint).group(1))\n",
    "            epoch_start = int(iteration/n_batches)\n",
    "        except:\n",
    "            print('Cannot read the checkpoint. Set it None.')\n",
    "            epoch_start = 0\n",
    "            checkpoint = None\n",
    "            \n",
    "    for e in range(epoch_start, epochs):\n",
    "        # network 학습\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(get_batch([train_x, train_y], params['time_steps']), 1):\n",
    "            iteration = e*n_batches + i\n",
    "            # training 시간을 기록\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state }\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \\\n",
    "                                                          model.final_state, model.optimizer], feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/i),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            # summary추가\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            if (iteration%checkpoint_interval == 0) or (iteration == iterations):\n",
    "                # validation loss 확인. dropout의 값을 1로 설정하여 모든 node가 동작하도록 한다.\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], params['time_steps']):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \\\n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                # summary추가\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/sherlock/i{}_l{}_{:.3f}\".format(iteration, params['lstm_size'], np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/sherlock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "이제 학습된 모델을 이용하여 문장을 만들어봅시다. 학습된 모델이 문장을 만드는 방법은 이전 글자가 주어졌을때, 다음 글자를 예측을 반복적으로 하면서 이루어집니다. 학습된 모델은 주어진 이전 글자에 대해 다음 글자를 확률 값으로 예측을 하게됩니다. 각각의 확률을 적용하여 Random sampling을 하여 새로운 글자가 추가가 되고, 새로운 글자와 이전 state를 이용하여 다음 글자를 예측합니다. 이 과정을 반복하게되면 문장을 만들 수 있습니다.\n",
    "확률값이 가장 높은 `N`가지중에 하나를 선택하도록 코드를 작성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, charset_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(charset_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, charset_size, prime=\"The \"):\n",
    "    samples = list(prime)\n",
    "    model = define_rnn_graph(charset_size, **{'lstm_size':lstm_size, 'sampling':True})\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = char_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(charset))\n",
    "        samples.append(int_to_char[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(charset))\n",
    "            samples.append(int_to_char[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Loss가 가장 작은 모델을 포함한 여러 모델을 이용하여 문장을 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_checkpoints=re.findall(r'\\b([\\w/]+_([\\d.]+))\\b',str(tf.train.get_checkpoint_state('checkpoints/sherlock')),re.IGNORECASE)\n",
    "all_checkpoints_sorted_by_valloss = sorted(all_checkpoints, key=lambda tup: float(tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_checkpoints_sorted_by_valloss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10번째 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = all_checkpoints[10][0]\n",
    "samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=\"The \")\n",
    "print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation loss가 가장 작은 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:2]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=\"The \")\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:1]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=\"Mr.\")\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints/sherlock')\n",
    "samp = sample(checkpoint, 1000, params['lstm_size'], len(charset), prime=\"The \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "log directory를 설정해주고 실행합니다.\n",
    "```bash\n",
    "$ tensorboard --logdir='logs/'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-py2]",
   "language": "python",
   "name": "conda-env-tensorflow-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
