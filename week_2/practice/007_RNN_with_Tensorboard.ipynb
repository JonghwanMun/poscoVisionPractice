{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 개요\n",
    "<img src=\"../resources/tensorflow_overview.png\" width=\"1000\">\n",
    "        [이미지 출처](https://www.youtube.com/watch?v=-57Ne86Ia8w&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=3)\n",
    "1. Tensorflow의 operation과 이미 구현된 모델을 이용하여 Graph를 제작한다.\n",
    "2. Input에 해당하는 부분을 placeholder로 만들어, `placeholder`에 넣을 데이터를 `feed_dict`에 넣는다.\n",
    "3. Iteration마다 학습을하고 weights를 갱신한다.\n",
    "4. validation 데이터를 이용하여 학습이 잘되고 있는지 확인한다.\n",
    "5. 나의 모델에 test 데이터를 넣어서 결과를 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 글자 단위(Character-level)의 RNN 구현과 Tensorboard를 배워보자\n",
    "\n",
    "### <학습목표>\n",
    "1. 이번 노트북에서는 글자(character) 단위의 입력값으로 RNN을 학습해 보고, 결과를 Tensorboard를 이용하여 보는 것을 목표로 합니다.\n",
    "2. 학습할 데이터는 Sherlock homes 시리즈 중 The Sign of the Four의 영문책을 이용하여 학습합니다.\n",
    "3. 학습된 모델을 이용하여 새로운 문장을 만들어 봅니다.\n",
    "\n",
    "이 수업의 내용은 Andrej Karpathy의 [블로그 포스트](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)와  [Torch로 구현된 코드](https://github.com/karpathy/char-rnn)에 기반한 Tensorflow 수업입니다. 아래 사진은 일반적인 글자 입력 단위의 RNN의 구조입니다.\n",
    "\n",
    "<img src=\"../resources/charseq.jpeg\" width=\"500\" alt=\"charseq\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# python2 -- python3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import namedtuple\n",
    "from six.moves import urllib\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the data.\n",
    "url = 'http://cvlab.postech.ac.kr/~wgchang/data/others/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        if not os.path.isdir(os.path.dirname(filename)):\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        filename, _ = urllib.request.urlretrieve(url + os.path.basename(filename), filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ../data/sherlock.txt\n"
     ]
    }
   ],
   "source": [
    "filename = maybe_download('../data/sherlock.txt', 3377296)\n",
    "# filename = maybe_download('../data/sherlock_short.txt', 609394)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration for prevent whole gpu usage\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "우선 텍스트 데이터를 불러들인 후 각 단어들을 정수값으로 변환하여 모델이 학습할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_multiple_s(text):\n",
    "    # 여러번 띄어쓰기가 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r' +',r' ',text)\n",
    "    # 여러번 탭이 된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\t+',r' ',text)\n",
    "    # 여러번 newline으로된 부분을 한번으로 수정합니다.\n",
    "    text = re.sub(r'\\n+',r' ',text)\n",
    "    # 특수문자를 제거합니다.\n",
    "    text = re.sub(r'[^A-Za-z0-9.,?!\\'\" ]+',r'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    text=f.read()\n",
    "text=remove_multiple_s(text)\n",
    "charset = set(text)\n",
    "char_to_int = {c: i for i, c in enumerate(charset)}\n",
    "int_to_char = dict(enumerate(charset))\n",
    "chars = np.array([char_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트의 길이와 텍스트가 숫자로 변환되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3140216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' CHAPTER I  Mr. Sherlock Holmes  In the year 1878 I took my degree of Doctor of Medicine of the  Uni'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 18, 25, 17, 33, 37, 20, 35,  1, 24,  1,  1, 28, 61,  5,  1, 34,\n",
       "       51, 46, 61, 55, 56, 44, 52,  1, 25, 56, 55, 54, 46, 60,  1,  1, 24,\n",
       "       57,  1, 63, 51, 46,  1, 66, 46, 43, 61,  1,  6, 15, 12, 15,  1, 24,\n",
       "        1, 63, 56, 56, 52,  1, 54, 66,  1, 47, 46, 48, 61, 46, 46,  1, 56,\n",
       "       49,  1, 21, 56, 44, 63, 56, 61,  1, 56, 49,  1, 28, 46, 47, 50, 44,\n",
       "       50, 57, 46,  1, 56, 49,  1, 63, 51, 46,  1,  1, 36, 57, 50], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이제 데이터를 training과 validation으로 나누고 각각을 batch로 만들어봅시다. 이번 과제에서는 Test set은 따로 없습니다.\n",
    "문장에서 input과 target의 배열을 만듭니다. 여기서 target은 input과 같은 길이의 글자열이지만 한 글자가 밀려진 글자열입니다.\n",
    "batch 크기를 맞추기 위해서 문장의 뒤에 남는 부분은 버립니다.\n",
    "split_frac은 training과 validation을 나누는 set의 비율을 나타냅니다. 전체 batch갯수중 90%를 training으로, 10%를 validation으로 사용합니다.\n",
    "\n",
    "<img src=\"../resources/dataset.jpeg\" width=\"500\" alt=\"split dataset\">\n",
    "\n",
    "x matrix(행렬)는 (`batch크기 x 글자열 길이`)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, **params):\n",
    "    batch_size = params['batch_size']\n",
    "    time_steps = params['time_steps']\n",
    "    split_frac = params.get('split_frac') or 0.9\n",
    "    \n",
    "    slice_size = batch_size * time_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*time_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*time_steps], y[:, :split_idx*time_steps]\n",
    "    val_x, val_y = x[:, split_idx*time_steps:], y[:, split_idx*time_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, **{'batch_size':10, 'time_steps':200, 'split_frac':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 나눠졌는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 251200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 62800)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 18, 25, 17, 33, 37, 20, 35,  1, 24],\n",
       "       [51, 46, 61, 46,  4,  1, 63, 51, 43, 63],\n",
       "       [39, 43, 55, 55, 46, 66,  4,  1, 43, 57],\n",
       "       [60,  1, 56, 49,  1,  1, 55, 43, 66, 50],\n",
       "       [44, 52, 46, 60, 63,  1, 47, 46, 59, 61],\n",
       "       [56, 57, 55, 66,  1,  1, 64, 43, 50, 63],\n",
       "       [57, 47,  1, 24,  1, 44, 56, 62, 55, 47],\n",
       "       [ 4,  1, 60, 56, 54, 46, 63, 51, 50, 57],\n",
       "       [63, 56, 57, 46,  5,  1, 17,  1, 60, 51],\n",
       "       [ 1, 56, 57, 46,  1, 56, 44, 44, 43, 60]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "학습을 할 때 각각 batch를 순서대로 넣어야 하기때문에 batch하나를 가져오는 함수를 만들어봅시다. 각 batch는 (`batch 크기 X time_steps`)입니다.\n",
    "예를 들면, 우리의 모델이 100개의 문자열에 대해서 학습을 한다면, `time_steps = 100`이 됩니다. 그 다음 batch는 학습한 그 다음 문자열부터 학습됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 tensorflow를 이용하여 RNN을 만들어봅시다. tensorflow관련 함수들은 [Tensorflow RNN API](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)를 참조하시면 됩니다.\n",
    "##### 참조 링크\n",
    "- [One-hot vector](https://www.tensorflow.org/api_docs/python/tf/one_hot): \n",
    "<img src='../resources/one_hot.png' width=\"700\" alt=\"one hot encoding\">\n",
    "- [Dropout](https://www.youtube.com/watch?v=NhZVe50QwPM)[참조논문](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf): Dropout은 random하게 특정 node를 0으로 만들어서 back-propagation이 0으로 된 node 이후로 진행되지 않게하여 overfiting을 막아주는 regularization역할을하여 학습을 원할하게합니다. **Advanced Topic: [Batch Normalization](https://arxiv.org/abs/1502.03167)를 추가적으로 공부하시면 overfitting 관련 공부에 도움이 됩니다.**\n",
    "<img src='../resources/dropout.png' width=\"700\" alt=\"dropout\">\n",
    "- [Optimizer](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorBoard에 그래프를 기입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "def define_your_model():\n",
    "    ###\n",
    "    tf.summary.histogram('histogram', histogram)\n",
    "    tf.summary.scalar('scalar', scalar)\n",
    "    ###\n",
    "    merged = tf.summary.merge_all()\n",
    "    ###\n",
    "model = define_your_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ###\n",
    "    file_writer = tf.summary.FileWriter('logs/', sess.graph)\n",
    "    file_writer.add_summary(summary_to_record, iteration_index)\n",
    "    ###\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def define_rnn_graph(num_classes, **params):\n",
    "    # parameters\n",
    "    lstm_size = params.get('lstm_size') or 128\n",
    "    batch_size = params.get('batch_size') or 50\n",
    "    time_steps = params.get('time_steps') or 50\n",
    "    num_layers = params.get('num_layers') or 2\n",
    "    optimizer_params = params.get('optimizer_params') or {'learning_rate': 1e-3}\n",
    "    grad_clip = params.get('grad_clip') or 10\n",
    "    sampling = params.get('sampling') or False\n",
    "    \n",
    "    if sampling == True:\n",
    "        batch_size, time_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # placeholders를 선언합니다.\n",
    "    # input을 tf.one_hot함수를 이용하여 one_hot vector로 바꿔줍니다.\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, time_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    # target도 비슷한 방식으로 진행합니다\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, time_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "\n",
    "        # Loss를 계산하기위해 one_hot vector들의 matrix를 tf.reshape함수를 이용하여 하나의 긴 vector로 바꾸어줍니다.\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    # Dropout을 위한 확률값을 저장하는 place holder\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # RNN의 한 종류인 LSTM 구현\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            lstm = rnn.BasicLSTMCell(lstm_size)\n",
    "            # rnn.DropoutWrapper를 이용하여 RNN model에 Dropout 추가\n",
    "            drop = rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            # LSTM hidden layer 추가, weight sharing\n",
    "            lstm_layers.append(drop)\n",
    "        cell = rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "    # tf.nn.dynamic_rnn함수를 이용해 RNN을 실행\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # forward propagation\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "\n",
    "    # Output을 Concatenate한 후에 Reshape합니다.\n",
    "    with tf.name_scope('reshaper'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Cost를 계산하기위해 RNN putput을  input으로하는 softmax layer를 제작합니다.\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # weights & bias를 histogram으로 작성\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        # prediction의 확률값을 histogram으로 작성\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        # cost값을 scalar value로 작성\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # 학습을 위한 Optimizer를 정의합니다.\n",
    "    # 대표적인 optimizer로는 SGD(stocastic gradient descent), Adam, RMSprop 등이 있습니다.\n",
    "    # Gradient clipping을 통해 gradient값이 매우 큰 경우는 grad_clip값으로 제한합니다.\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(**optimizer_params)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # summary를 merge합니다.\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 앞에 선언한 노드들을 모두 Graph로 만들어서 결과로 반환합니다.\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer','merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "위에 선언한 함수에서 이제 hyperparameter들을 정합니다. \n",
    "일반적으로 network의 크기가 커질 수록(hidden unit이 많을 수록, layer 수가 많을 수록) 성능이 향상되지만, overfitting(fit to variance)이 되는 현상을 잘 관찰해야 합니다. hyperparameter들이 너무 적을 경우에는 underfitting(fit to bias)되는 현상이 있을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lstm_size' : 1024,\n",
    "    'batch_size': 100,\n",
    "    'time_steps': 100,    \n",
    "    'num_layers' : 2,\n",
    "    'optimizer_params': {'learning_rate': 1e-3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 학습 (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint를 저장할 directory를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('checkpoints/sherlock'):\n",
    "    os.makedirs('checkpoints/sherlock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "checkpoint_interval = 50\n",
    "checkpoint = None\n",
    "# 기존 checkpoint를 실행하고싶다면 None 대신 checkpoint_path를 넣으면됩니다.\n",
    "# checkpoint = 'checkpoints/sherlock/i6250_l1024_1.073'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/5640 Training loss: 4.2347 1.4247 sec/batch\n",
      "Epoch 1/20  Iteration 2/5640 Training loss: 4.1081 0.9780 sec/batch\n",
      "Epoch 1/20  Iteration 3/5640 Training loss: 6.5366 0.9816 sec/batch\n",
      "Epoch 1/20  Iteration 4/5640 Training loss: 6.3720 0.9994 sec/batch\n",
      "Epoch 1/20  Iteration 5/5640 Training loss: 5.8561 0.9682 sec/batch\n",
      "Epoch 1/20  Iteration 6/5640 Training loss: 5.5569 0.9761 sec/batch\n",
      "Epoch 1/20  Iteration 7/5640 Training loss: 5.3370 0.9598 sec/batch\n",
      "Epoch 1/20  Iteration 8/5640 Training loss: 5.1355 0.9804 sec/batch\n",
      "Epoch 1/20  Iteration 9/5640 Training loss: 4.9863 0.9615 sec/batch\n",
      "Epoch 1/20  Iteration 10/5640 Training loss: 4.8483 0.9755 sec/batch\n",
      "Epoch 1/20  Iteration 11/5640 Training loss: 4.7166 1.0040 sec/batch\n",
      "Epoch 1/20  Iteration 12/5640 Training loss: 4.6026 0.9690 sec/batch\n",
      "Epoch 1/20  Iteration 13/5640 Training loss: 4.5022 0.9554 sec/batch\n",
      "Epoch 1/20  Iteration 14/5640 Training loss: 4.4120 1.0061 sec/batch\n",
      "Epoch 1/20  Iteration 15/5640 Training loss: 4.3317 0.9957 sec/batch\n",
      "Epoch 1/20  Iteration 16/5640 Training loss: 4.2583 0.9276 sec/batch\n",
      "Epoch 1/20  Iteration 17/5640 Training loss: 4.1930 1.0119 sec/batch\n",
      "Epoch 1/20  Iteration 18/5640 Training loss: 4.1331 0.9868 sec/batch\n",
      "Epoch 1/20  Iteration 19/5640 Training loss: 4.0790 0.9735 sec/batch\n",
      "Epoch 1/20  Iteration 20/5640 Training loss: 4.0307 1.0065 sec/batch\n",
      "Epoch 1/20  Iteration 21/5640 Training loss: 3.9876 0.9833 sec/batch\n",
      "Epoch 1/20  Iteration 22/5640 Training loss: 3.9475 0.9305 sec/batch\n",
      "Epoch 1/20  Iteration 23/5640 Training loss: 3.9099 1.0301 sec/batch\n",
      "Epoch 1/20  Iteration 24/5640 Training loss: 3.8755 1.0104 sec/batch\n",
      "Epoch 1/20  Iteration 25/5640 Training loss: 3.8430 1.0008 sec/batch\n",
      "Epoch 1/20  Iteration 26/5640 Training loss: 3.8135 1.0551 sec/batch\n",
      "Epoch 1/20  Iteration 27/5640 Training loss: 3.7859 0.9631 sec/batch\n",
      "Epoch 1/20  Iteration 28/5640 Training loss: 3.7610 0.9767 sec/batch\n",
      "Epoch 1/20  Iteration 29/5640 Training loss: 3.7381 0.9915 sec/batch\n",
      "Epoch 1/20  Iteration 30/5640 Training loss: 3.7160 0.9591 sec/batch\n",
      "Epoch 1/20  Iteration 31/5640 Training loss: 3.6947 0.9997 sec/batch\n",
      "Epoch 1/20  Iteration 32/5640 Training loss: 3.6748 0.9844 sec/batch\n",
      "Epoch 1/20  Iteration 33/5640 Training loss: 3.6553 0.9375 sec/batch\n",
      "Epoch 1/20  Iteration 34/5640 Training loss: 3.6377 1.0037 sec/batch\n",
      "Epoch 1/20  Iteration 35/5640 Training loss: 3.6209 0.9797 sec/batch\n",
      "Epoch 1/20  Iteration 36/5640 Training loss: 3.6050 0.9352 sec/batch\n",
      "Epoch 1/20  Iteration 37/5640 Training loss: 3.5900 1.0168 sec/batch\n",
      "Epoch 1/20  Iteration 38/5640 Training loss: 3.5749 0.9543 sec/batch\n",
      "Epoch 1/20  Iteration 39/5640 Training loss: 3.5613 0.9500 sec/batch\n",
      "Epoch 1/20  Iteration 40/5640 Training loss: 3.5483 1.0029 sec/batch\n",
      "Epoch 1/20  Iteration 41/5640 Training loss: 3.5360 0.9628 sec/batch\n",
      "Epoch 1/20  Iteration 42/5640 Training loss: 3.5244 0.9613 sec/batch\n",
      "Epoch 1/20  Iteration 43/5640 Training loss: 3.5133 1.0092 sec/batch\n",
      "Epoch 1/20  Iteration 44/5640 Training loss: 3.5018 0.9594 sec/batch\n",
      "Epoch 1/20  Iteration 45/5640 Training loss: 3.4911 0.9965 sec/batch\n",
      "Epoch 1/20  Iteration 46/5640 Training loss: 3.4815 0.9873 sec/batch\n",
      "Epoch 1/20  Iteration 47/5640 Training loss: 3.4720 0.9669 sec/batch\n",
      "Epoch 1/20  Iteration 48/5640 Training loss: 3.4626 1.0047 sec/batch\n",
      "Epoch 1/20  Iteration 49/5640 Training loss: 3.4535 0.9872 sec/batch\n",
      "Epoch 1/20  Iteration 50/5640 Training loss: 3.4447 0.9426 sec/batch\n",
      "Validation loss: 2.98 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 51/5640 Training loss: 3.4365 1.0530 sec/batch\n",
      "Epoch 1/20  Iteration 52/5640 Training loss: 3.4284 0.9349 sec/batch\n",
      "Epoch 1/20  Iteration 53/5640 Training loss: 3.4204 0.9902 sec/batch\n",
      "Epoch 1/20  Iteration 54/5640 Training loss: 3.4128 1.0174 sec/batch\n",
      "Epoch 1/20  Iteration 55/5640 Training loss: 3.4054 0.9881 sec/batch\n",
      "Epoch 1/20  Iteration 56/5640 Training loss: 3.3986 1.0567 sec/batch\n",
      "Epoch 1/20  Iteration 57/5640 Training loss: 3.3916 0.9662 sec/batch\n",
      "Epoch 1/20  Iteration 58/5640 Training loss: 3.3848 0.9795 sec/batch\n",
      "Epoch 1/20  Iteration 59/5640 Training loss: 3.3783 0.9814 sec/batch\n",
      "Epoch 1/20  Iteration 60/5640 Training loss: 3.3721 1.0215 sec/batch\n",
      "Epoch 1/20  Iteration 61/5640 Training loss: 3.3660 1.0461 sec/batch\n",
      "Epoch 1/20  Iteration 62/5640 Training loss: 3.3601 1.1605 sec/batch\n",
      "Epoch 1/20  Iteration 63/5640 Training loss: 3.3544 1.0564 sec/batch\n",
      "Epoch 1/20  Iteration 64/5640 Training loss: 3.3487 0.9516 sec/batch\n",
      "Epoch 1/20  Iteration 65/5640 Training loss: 3.3433 1.0087 sec/batch\n",
      "Epoch 1/20  Iteration 66/5640 Training loss: 3.3379 0.9973 sec/batch\n",
      "Epoch 1/20  Iteration 67/5640 Training loss: 3.3325 0.9908 sec/batch\n",
      "Epoch 1/20  Iteration 68/5640 Training loss: 3.3270 1.0495 sec/batch\n",
      "Epoch 1/20  Iteration 69/5640 Training loss: 3.3221 0.9943 sec/batch\n",
      "Epoch 1/20  Iteration 70/5640 Training loss: 3.3172 0.9638 sec/batch\n",
      "Epoch 1/20  Iteration 71/5640 Training loss: 3.3125 1.0041 sec/batch\n",
      "Epoch 1/20  Iteration 72/5640 Training loss: 3.3076 0.9902 sec/batch\n",
      "Epoch 1/20  Iteration 73/5640 Training loss: 3.3029 0.9598 sec/batch\n",
      "Epoch 1/20  Iteration 74/5640 Training loss: 3.2986 0.9887 sec/batch\n",
      "Epoch 1/20  Iteration 75/5640 Training loss: 3.2983 0.9585 sec/batch\n",
      "Epoch 1/20  Iteration 76/5640 Training loss: 3.2978 0.9676 sec/batch\n",
      "Epoch 1/20  Iteration 77/5640 Training loss: 3.2967 1.0168 sec/batch\n",
      "Epoch 1/20  Iteration 78/5640 Training loss: 3.2950 1.0014 sec/batch\n",
      "Epoch 1/20  Iteration 79/5640 Training loss: 3.2924 1.0253 sec/batch\n",
      "Epoch 1/20  Iteration 80/5640 Training loss: 3.2885 0.9984 sec/batch\n",
      "Epoch 1/20  Iteration 81/5640 Training loss: 3.2846 1.0152 sec/batch\n",
      "Epoch 1/20  Iteration 82/5640 Training loss: 3.2807 1.0589 sec/batch\n",
      "Epoch 1/20  Iteration 83/5640 Training loss: 3.2767 0.9842 sec/batch\n",
      "Epoch 1/20  Iteration 84/5640 Training loss: 3.2729 0.9386 sec/batch\n",
      "Epoch 1/20  Iteration 85/5640 Training loss: 3.2690 1.0058 sec/batch\n",
      "Epoch 1/20  Iteration 86/5640 Training loss: 3.2653 0.9971 sec/batch\n",
      "Epoch 1/20  Iteration 87/5640 Training loss: 3.2617 0.9389 sec/batch\n",
      "Epoch 1/20  Iteration 88/5640 Training loss: 3.2580 0.9770 sec/batch\n",
      "Epoch 1/20  Iteration 89/5640 Training loss: 3.2544 0.9722 sec/batch\n",
      "Epoch 1/20  Iteration 90/5640 Training loss: 3.2505 0.9843 sec/batch\n",
      "Epoch 1/20  Iteration 91/5640 Training loss: 3.2472 1.0032 sec/batch\n",
      "Epoch 1/20  Iteration 92/5640 Training loss: 3.2436 0.9783 sec/batch\n",
      "Epoch 1/20  Iteration 93/5640 Training loss: 3.2400 0.9863 sec/batch\n",
      "Epoch 1/20  Iteration 94/5640 Training loss: 3.2363 1.0096 sec/batch\n",
      "Epoch 1/20  Iteration 95/5640 Training loss: 3.2327 0.9703 sec/batch\n",
      "Epoch 1/20  Iteration 96/5640 Training loss: 3.2291 1.0016 sec/batch\n",
      "Epoch 1/20  Iteration 97/5640 Training loss: 3.2255 0.9857 sec/batch\n",
      "Epoch 1/20  Iteration 98/5640 Training loss: 3.2219 0.9575 sec/batch\n",
      "Epoch 1/20  Iteration 99/5640 Training loss: 3.2185 1.0093 sec/batch\n",
      "Epoch 1/20  Iteration 100/5640 Training loss: 3.2148 1.0025 sec/batch\n",
      "Validation loss: 2.81289 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 101/5640 Training loss: 3.2111 0.9679 sec/batch\n",
      "Epoch 1/20  Iteration 102/5640 Training loss: 3.2073 1.0432 sec/batch\n",
      "Epoch 1/20  Iteration 103/5640 Training loss: 3.2038 1.0233 sec/batch\n",
      "Epoch 1/20  Iteration 104/5640 Training loss: 3.2003 1.0032 sec/batch\n",
      "Epoch 1/20  Iteration 105/5640 Training loss: 3.1969 1.0087 sec/batch\n",
      "Epoch 1/20  Iteration 106/5640 Training loss: 3.1934 1.0391 sec/batch\n",
      "Epoch 1/20  Iteration 107/5640 Training loss: 3.1901 0.9988 sec/batch\n",
      "Epoch 1/20  Iteration 108/5640 Training loss: 3.1867 1.0283 sec/batch\n",
      "Epoch 1/20  Iteration 109/5640 Training loss: 3.1831 1.0453 sec/batch\n",
      "Epoch 1/20  Iteration 110/5640 Training loss: 3.1797 1.0187 sec/batch\n",
      "Epoch 1/20  Iteration 111/5640 Training loss: 3.1761 1.0189 sec/batch\n",
      "Epoch 1/20  Iteration 112/5640 Training loss: 3.1729 1.0134 sec/batch\n",
      "Epoch 1/20  Iteration 113/5640 Training loss: 3.1696 1.0361 sec/batch\n",
      "Epoch 1/20  Iteration 114/5640 Training loss: 3.1660 1.0397 sec/batch\n",
      "Epoch 1/20  Iteration 115/5640 Training loss: 3.1626 1.0132 sec/batch\n",
      "Epoch 1/20  Iteration 116/5640 Training loss: 3.1592 1.0450 sec/batch\n",
      "Epoch 1/20  Iteration 117/5640 Training loss: 3.1559 1.0265 sec/batch\n",
      "Epoch 1/20  Iteration 118/5640 Training loss: 3.1524 1.0176 sec/batch\n",
      "Epoch 1/20  Iteration 119/5640 Training loss: 3.1489 1.0460 sec/batch\n",
      "Epoch 1/20  Iteration 120/5640 Training loss: 3.1454 1.0051 sec/batch\n",
      "Epoch 1/20  Iteration 121/5640 Training loss: 3.1419 1.0433 sec/batch\n",
      "Epoch 1/20  Iteration 122/5640 Training loss: 3.1385 1.0354 sec/batch\n",
      "Epoch 1/20  Iteration 123/5640 Training loss: 3.1351 0.9996 sec/batch\n",
      "Epoch 1/20  Iteration 124/5640 Training loss: 3.1317 1.0263 sec/batch\n",
      "Epoch 1/20  Iteration 125/5640 Training loss: 3.1285 1.0027 sec/batch\n",
      "Epoch 1/20  Iteration 126/5640 Training loss: 3.1251 1.0565 sec/batch\n",
      "Epoch 1/20  Iteration 127/5640 Training loss: 3.1217 1.0226 sec/batch\n",
      "Epoch 1/20  Iteration 128/5640 Training loss: 3.1184 0.9893 sec/batch\n",
      "Epoch 1/20  Iteration 129/5640 Training loss: 3.1150 1.0454 sec/batch\n",
      "Epoch 1/20  Iteration 130/5640 Training loss: 3.1115 1.0321 sec/batch\n",
      "Epoch 1/20  Iteration 131/5640 Training loss: 3.1081 0.9620 sec/batch\n",
      "Epoch 1/20  Iteration 132/5640 Training loss: 3.1048 1.0629 sec/batch\n",
      "Epoch 1/20  Iteration 133/5640 Training loss: 3.1013 1.0006 sec/batch\n",
      "Epoch 1/20  Iteration 134/5640 Training loss: 3.0978 0.9855 sec/batch\n",
      "Epoch 1/20  Iteration 135/5640 Training loss: 3.0944 1.0208 sec/batch\n",
      "Epoch 1/20  Iteration 136/5640 Training loss: 3.0907 1.0325 sec/batch\n",
      "Epoch 1/20  Iteration 137/5640 Training loss: 3.0874 0.9977 sec/batch\n",
      "Epoch 1/20  Iteration 138/5640 Training loss: 3.0839 1.0273 sec/batch\n",
      "Epoch 1/20  Iteration 139/5640 Training loss: 3.0804 1.0183 sec/batch\n",
      "Epoch 1/20  Iteration 140/5640 Training loss: 3.0768 1.0448 sec/batch\n",
      "Epoch 1/20  Iteration 141/5640 Training loss: 3.0732 1.0336 sec/batch\n",
      "Epoch 1/20  Iteration 142/5640 Training loss: 3.0697 0.9998 sec/batch\n",
      "Epoch 1/20  Iteration 143/5640 Training loss: 3.0660 1.0307 sec/batch\n",
      "Epoch 1/20  Iteration 144/5640 Training loss: 3.0624 1.0246 sec/batch\n",
      "Epoch 1/20  Iteration 145/5640 Training loss: 3.0587 1.0045 sec/batch\n",
      "Epoch 1/20  Iteration 146/5640 Training loss: 3.0548 1.0603 sec/batch\n",
      "Epoch 1/20  Iteration 147/5640 Training loss: 3.0515 0.9923 sec/batch\n",
      "Epoch 1/20  Iteration 148/5640 Training loss: 3.0480 1.0039 sec/batch\n",
      "Epoch 1/20  Iteration 149/5640 Training loss: 3.0447 1.0516 sec/batch\n",
      "Epoch 1/20  Iteration 150/5640 Training loss: 3.0413 1.0125 sec/batch\n",
      "Validation loss: 2.45918 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 151/5640 Training loss: 3.0379 1.0007 sec/batch\n",
      "Epoch 1/20  Iteration 152/5640 Training loss: 3.0346 1.0449 sec/batch\n",
      "Epoch 1/20  Iteration 153/5640 Training loss: 3.0312 1.0901 sec/batch\n",
      "Epoch 1/20  Iteration 154/5640 Training loss: 3.0278 1.0026 sec/batch\n",
      "Epoch 1/20  Iteration 155/5640 Training loss: 3.0243 1.0303 sec/batch\n",
      "Epoch 1/20  Iteration 156/5640 Training loss: 3.0209 1.0460 sec/batch\n",
      "Epoch 1/20  Iteration 157/5640 Training loss: 3.0174 0.9978 sec/batch\n",
      "Epoch 1/20  Iteration 158/5640 Training loss: 3.0140 1.0504 sec/batch\n",
      "Epoch 1/20  Iteration 159/5640 Training loss: 3.0105 1.0301 sec/batch\n",
      "Epoch 1/20  Iteration 160/5640 Training loss: 3.0070 0.9727 sec/batch\n",
      "Epoch 1/20  Iteration 161/5640 Training loss: 3.0037 1.0443 sec/batch\n",
      "Epoch 1/20  Iteration 162/5640 Training loss: 3.0002 1.0406 sec/batch\n",
      "Epoch 1/20  Iteration 163/5640 Training loss: 2.9969 0.9747 sec/batch\n",
      "Epoch 1/20  Iteration 164/5640 Training loss: 2.9936 1.0644 sec/batch\n",
      "Epoch 1/20  Iteration 165/5640 Training loss: 2.9903 1.0516 sec/batch\n",
      "Epoch 1/20  Iteration 166/5640 Training loss: 2.9871 0.9878 sec/batch\n",
      "Epoch 1/20  Iteration 167/5640 Training loss: 2.9838 1.0215 sec/batch\n",
      "Epoch 1/20  Iteration 168/5640 Training loss: 2.9805 0.9539 sec/batch\n",
      "Epoch 1/20  Iteration 169/5640 Training loss: 2.9772 1.0180 sec/batch\n",
      "Epoch 1/20  Iteration 170/5640 Training loss: 2.9739 1.0057 sec/batch\n",
      "Epoch 1/20  Iteration 171/5640 Training loss: 2.9706 0.9464 sec/batch\n",
      "Epoch 1/20  Iteration 172/5640 Training loss: 2.9674 1.0374 sec/batch\n",
      "Epoch 1/20  Iteration 173/5640 Training loss: 2.9640 1.0153 sec/batch\n",
      "Epoch 1/20  Iteration 174/5640 Training loss: 2.9608 0.9777 sec/batch\n",
      "Epoch 1/20  Iteration 175/5640 Training loss: 2.9575 1.0084 sec/batch\n",
      "Epoch 1/20  Iteration 176/5640 Training loss: 2.9543 1.0017 sec/batch\n",
      "Epoch 1/20  Iteration 177/5640 Training loss: 2.9513 0.9522 sec/batch\n",
      "Epoch 1/20  Iteration 178/5640 Training loss: 2.9481 1.0198 sec/batch\n",
      "Epoch 1/20  Iteration 179/5640 Training loss: 2.9450 0.9906 sec/batch\n",
      "Epoch 1/20  Iteration 180/5640 Training loss: 2.9420 0.9656 sec/batch\n",
      "Epoch 1/20  Iteration 181/5640 Training loss: 2.9389 1.0070 sec/batch\n",
      "Epoch 1/20  Iteration 182/5640 Training loss: 2.9359 0.9748 sec/batch\n",
      "Epoch 1/20  Iteration 183/5640 Training loss: 2.9330 0.9709 sec/batch\n",
      "Epoch 1/20  Iteration 184/5640 Training loss: 2.9300 0.9849 sec/batch\n",
      "Epoch 1/20  Iteration 185/5640 Training loss: 2.9271 0.9679 sec/batch\n",
      "Epoch 1/20  Iteration 186/5640 Training loss: 2.9241 0.9689 sec/batch\n",
      "Epoch 1/20  Iteration 187/5640 Training loss: 2.9210 0.9990 sec/batch\n",
      "Epoch 1/20  Iteration 188/5640 Training loss: 2.9180 0.9632 sec/batch\n",
      "Epoch 1/20  Iteration 189/5640 Training loss: 2.9150 0.9821 sec/batch\n",
      "Epoch 1/20  Iteration 190/5640 Training loss: 2.9120 0.9805 sec/batch\n",
      "Epoch 1/20  Iteration 191/5640 Training loss: 2.9091 0.9550 sec/batch\n",
      "Epoch 1/20  Iteration 192/5640 Training loss: 2.9061 1.0080 sec/batch\n",
      "Epoch 1/20  Iteration 193/5640 Training loss: 2.9033 0.9882 sec/batch\n",
      "Epoch 1/20  Iteration 194/5640 Training loss: 2.9004 0.9346 sec/batch\n",
      "Epoch 1/20  Iteration 195/5640 Training loss: 2.8977 1.0122 sec/batch\n",
      "Epoch 1/20  Iteration 196/5640 Training loss: 2.8949 0.9806 sec/batch\n",
      "Epoch 1/20  Iteration 197/5640 Training loss: 2.8920 0.9519 sec/batch\n",
      "Epoch 1/20  Iteration 198/5640 Training loss: 2.8892 1.0177 sec/batch\n",
      "Epoch 1/20  Iteration 199/5640 Training loss: 2.8864 0.9521 sec/batch\n",
      "Epoch 1/20  Iteration 200/5640 Training loss: 2.8836 0.9588 sec/batch\n",
      "Validation loss: 2.26422 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 201/5640 Training loss: 2.8808 0.9618 sec/batch\n",
      "Epoch 1/20  Iteration 202/5640 Training loss: 2.8780 0.9981 sec/batch\n",
      "Epoch 1/20  Iteration 203/5640 Training loss: 2.8752 0.9750 sec/batch\n",
      "Epoch 1/20  Iteration 204/5640 Training loss: 2.8725 0.9750 sec/batch\n",
      "Epoch 1/20  Iteration 205/5640 Training loss: 2.8698 1.0105 sec/batch\n",
      "Epoch 1/20  Iteration 206/5640 Training loss: 2.8671 0.9713 sec/batch\n",
      "Epoch 1/20  Iteration 207/5640 Training loss: 2.8642 0.9892 sec/batch\n",
      "Epoch 1/20  Iteration 208/5640 Training loss: 2.8615 1.0141 sec/batch\n",
      "Epoch 1/20  Iteration 209/5640 Training loss: 2.8588 0.9786 sec/batch\n",
      "Epoch 1/20  Iteration 210/5640 Training loss: 2.8561 0.9823 sec/batch\n",
      "Epoch 1/20  Iteration 211/5640 Training loss: 2.8533 1.0333 sec/batch\n",
      "Epoch 1/20  Iteration 212/5640 Training loss: 2.8508 0.9828 sec/batch\n",
      "Epoch 1/20  Iteration 213/5640 Training loss: 2.8482 0.9822 sec/batch\n",
      "Epoch 1/20  Iteration 214/5640 Training loss: 2.8455 1.0151 sec/batch\n",
      "Epoch 1/20  Iteration 215/5640 Training loss: 2.8429 1.0468 sec/batch\n",
      "Epoch 1/20  Iteration 216/5640 Training loss: 2.8402 0.9882 sec/batch\n",
      "Epoch 1/20  Iteration 217/5640 Training loss: 2.8375 0.9622 sec/batch\n",
      "Epoch 1/20  Iteration 218/5640 Training loss: 2.8348 1.0303 sec/batch\n",
      "Epoch 1/20  Iteration 219/5640 Training loss: 2.8322 1.0388 sec/batch\n",
      "Epoch 1/20  Iteration 220/5640 Training loss: 2.8296 1.0028 sec/batch\n",
      "Epoch 1/20  Iteration 221/5640 Training loss: 2.8270 1.0365 sec/batch\n",
      "Epoch 1/20  Iteration 222/5640 Training loss: 2.8244 1.0187 sec/batch\n",
      "Epoch 1/20  Iteration 223/5640 Training loss: 2.8218 0.9802 sec/batch\n",
      "Epoch 1/20  Iteration 224/5640 Training loss: 2.8192 1.0195 sec/batch\n",
      "Epoch 1/20  Iteration 225/5640 Training loss: 2.8167 1.0399 sec/batch\n",
      "Epoch 1/20  Iteration 226/5640 Training loss: 2.8140 0.9689 sec/batch\n",
      "Epoch 1/20  Iteration 227/5640 Training loss: 2.8116 1.0132 sec/batch\n",
      "Epoch 1/20  Iteration 228/5640 Training loss: 2.8092 0.9842 sec/batch\n",
      "Epoch 1/20  Iteration 229/5640 Training loss: 2.8068 0.9714 sec/batch\n",
      "Epoch 1/20  Iteration 230/5640 Training loss: 2.8045 1.0154 sec/batch\n",
      "Epoch 1/20  Iteration 231/5640 Training loss: 2.8020 0.9925 sec/batch\n",
      "Epoch 1/20  Iteration 232/5640 Training loss: 2.7996 0.9625 sec/batch\n",
      "Epoch 1/20  Iteration 233/5640 Training loss: 2.7971 0.9879 sec/batch\n",
      "Epoch 1/20  Iteration 234/5640 Training loss: 2.7946 0.9693 sec/batch\n",
      "Epoch 1/20  Iteration 235/5640 Training loss: 2.7921 0.9884 sec/batch\n",
      "Epoch 1/20  Iteration 236/5640 Training loss: 2.7898 0.9822 sec/batch\n",
      "Epoch 1/20  Iteration 237/5640 Training loss: 2.7874 0.9641 sec/batch\n",
      "Epoch 1/20  Iteration 238/5640 Training loss: 2.7852 1.0097 sec/batch\n",
      "Epoch 1/20  Iteration 239/5640 Training loss: 2.7829 0.9706 sec/batch\n",
      "Epoch 1/20  Iteration 240/5640 Training loss: 2.7805 0.9695 sec/batch\n",
      "Epoch 1/20  Iteration 241/5640 Training loss: 2.7782 1.0038 sec/batch\n",
      "Epoch 1/20  Iteration 242/5640 Training loss: 2.7759 0.9716 sec/batch\n",
      "Epoch 1/20  Iteration 243/5640 Training loss: 2.7735 0.9799 sec/batch\n",
      "Epoch 1/20  Iteration 244/5640 Training loss: 2.7712 1.0079 sec/batch\n",
      "Epoch 1/20  Iteration 245/5640 Training loss: 2.7689 0.9745 sec/batch\n",
      "Epoch 1/20  Iteration 246/5640 Training loss: 2.7666 0.9754 sec/batch\n",
      "Epoch 1/20  Iteration 247/5640 Training loss: 2.7643 1.0104 sec/batch\n",
      "Epoch 1/20  Iteration 248/5640 Training loss: 2.7621 0.9839 sec/batch\n",
      "Epoch 1/20  Iteration 249/5640 Training loss: 2.7599 1.0153 sec/batch\n",
      "Epoch 1/20  Iteration 250/5640 Training loss: 2.7576 1.0858 sec/batch\n",
      "Validation loss: 2.13609 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 251/5640 Training loss: 2.7553 0.9346 sec/batch\n",
      "Epoch 1/20  Iteration 252/5640 Training loss: 2.7531 0.9377 sec/batch\n",
      "Epoch 1/20  Iteration 253/5640 Training loss: 2.7508 0.9142 sec/batch\n",
      "Epoch 1/20  Iteration 254/5640 Training loss: 2.7486 0.9746 sec/batch\n",
      "Epoch 1/20  Iteration 255/5640 Training loss: 2.7464 0.9457 sec/batch\n",
      "Epoch 1/20  Iteration 256/5640 Training loss: 2.7442 0.8797 sec/batch\n",
      "Epoch 1/20  Iteration 257/5640 Training loss: 2.7421 0.9995 sec/batch\n",
      "Epoch 1/20  Iteration 258/5640 Training loss: 2.7399 0.9018 sec/batch\n",
      "Epoch 1/20  Iteration 259/5640 Training loss: 2.7378 0.8989 sec/batch\n",
      "Epoch 1/20  Iteration 260/5640 Training loss: 2.7358 0.9964 sec/batch\n",
      "Epoch 1/20  Iteration 261/5640 Training loss: 2.7336 0.8901 sec/batch\n",
      "Epoch 1/20  Iteration 262/5640 Training loss: 2.7314 0.9724 sec/batch\n",
      "Epoch 1/20  Iteration 263/5640 Training loss: 2.7292 0.9712 sec/batch\n",
      "Epoch 1/20  Iteration 264/5640 Training loss: 2.7271 0.8653 sec/batch\n",
      "Epoch 1/20  Iteration 265/5640 Training loss: 2.7251 1.0026 sec/batch\n",
      "Epoch 1/20  Iteration 266/5640 Training loss: 2.7229 0.8918 sec/batch\n",
      "Epoch 1/20  Iteration 267/5640 Training loss: 2.7208 0.9197 sec/batch\n",
      "Epoch 1/20  Iteration 268/5640 Training loss: 2.7186 1.0048 sec/batch\n",
      "Epoch 1/20  Iteration 269/5640 Training loss: 2.7166 0.8210 sec/batch\n",
      "Epoch 1/20  Iteration 270/5640 Training loss: 2.7144 0.9837 sec/batch\n",
      "Epoch 1/20  Iteration 271/5640 Training loss: 2.7124 0.9457 sec/batch\n",
      "Epoch 1/20  Iteration 272/5640 Training loss: 2.7103 0.9055 sec/batch\n",
      "Epoch 1/20  Iteration 273/5640 Training loss: 2.7083 1.0073 sec/batch\n",
      "Epoch 1/20  Iteration 274/5640 Training loss: 2.7063 0.9064 sec/batch\n",
      "Epoch 1/20  Iteration 275/5640 Training loss: 2.7043 0.9320 sec/batch\n",
      "Epoch 1/20  Iteration 276/5640 Training loss: 2.7022 0.9495 sec/batch\n",
      "Epoch 1/20  Iteration 277/5640 Training loss: 2.7002 0.9353 sec/batch\n",
      "Epoch 1/20  Iteration 278/5640 Training loss: 2.6981 0.9438 sec/batch\n",
      "Epoch 1/20  Iteration 279/5640 Training loss: 2.6961 1.0186 sec/batch\n",
      "Epoch 1/20  Iteration 280/5640 Training loss: 2.6941 0.9429 sec/batch\n",
      "Epoch 1/20  Iteration 281/5640 Training loss: 2.6920 0.9203 sec/batch\n",
      "Epoch 1/20  Iteration 282/5640 Training loss: 2.6900 1.0139 sec/batch\n",
      "Epoch 2/20  Iteration 283/5640 Training loss: 2.1756 0.9522 sec/batch\n",
      "Epoch 2/20  Iteration 284/5640 Training loss: 2.1508 0.8524 sec/batch\n",
      "Epoch 2/20  Iteration 285/5640 Training loss: 2.1386 1.0271 sec/batch\n",
      "Epoch 2/20  Iteration 286/5640 Training loss: 2.1347 0.9484 sec/batch\n",
      "Epoch 2/20  Iteration 287/5640 Training loss: 2.1303 0.9582 sec/batch\n",
      "Epoch 2/20  Iteration 288/5640 Training loss: 2.1271 1.0166 sec/batch\n",
      "Epoch 2/20  Iteration 289/5640 Training loss: 2.1225 0.8883 sec/batch\n",
      "Epoch 2/20  Iteration 290/5640 Training loss: 2.1215 0.9654 sec/batch\n",
      "Epoch 2/20  Iteration 291/5640 Training loss: 2.1194 0.9864 sec/batch\n",
      "Epoch 2/20  Iteration 292/5640 Training loss: 2.1157 0.9334 sec/batch\n",
      "Epoch 2/20  Iteration 293/5640 Training loss: 2.1149 0.9701 sec/batch\n",
      "Epoch 2/20  Iteration 294/5640 Training loss: 2.1099 0.9380 sec/batch\n",
      "Epoch 2/20  Iteration 295/5640 Training loss: 2.1079 0.9158 sec/batch\n",
      "Epoch 2/20  Iteration 296/5640 Training loss: 2.1093 1.0098 sec/batch\n",
      "Epoch 2/20  Iteration 297/5640 Training loss: 2.1082 0.8789 sec/batch\n",
      "Epoch 2/20  Iteration 298/5640 Training loss: 2.1083 0.9344 sec/batch\n",
      "Epoch 2/20  Iteration 299/5640 Training loss: 2.1065 0.9994 sec/batch\n",
      "Epoch 2/20  Iteration 300/5640 Training loss: 2.1047 0.8546 sec/batch\n",
      "Validation loss: 2.01704 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 301/5640 Training loss: 2.1037 0.9537 sec/batch\n",
      "Epoch 2/20  Iteration 302/5640 Training loss: 2.1021 1.0189 sec/batch\n",
      "Epoch 2/20  Iteration 303/5640 Training loss: 2.1007 0.9653 sec/batch\n",
      "Epoch 2/20  Iteration 304/5640 Training loss: 2.0987 0.9646 sec/batch\n",
      "Epoch 2/20  Iteration 305/5640 Training loss: 2.0970 0.9981 sec/batch\n",
      "Epoch 2/20  Iteration 306/5640 Training loss: 2.0955 0.9580 sec/batch\n",
      "Epoch 2/20  Iteration 307/5640 Training loss: 2.0947 0.9967 sec/batch\n",
      "Epoch 2/20  Iteration 308/5640 Training loss: 2.0937 0.9902 sec/batch\n",
      "Epoch 2/20  Iteration 309/5640 Training loss: 2.0923 0.9661 sec/batch\n",
      "Epoch 2/20  Iteration 310/5640 Training loss: 2.0919 1.0078 sec/batch\n",
      "Epoch 2/20  Iteration 311/5640 Training loss: 2.0919 0.9864 sec/batch\n",
      "Epoch 2/20  Iteration 312/5640 Training loss: 2.0908 0.9353 sec/batch\n",
      "Epoch 2/20  Iteration 313/5640 Training loss: 2.0892 1.0062 sec/batch\n",
      "Epoch 2/20  Iteration 314/5640 Training loss: 2.0886 0.9932 sec/batch\n",
      "Epoch 2/20  Iteration 315/5640 Training loss: 2.0875 0.9503 sec/batch\n",
      "Epoch 2/20  Iteration 316/5640 Training loss: 2.0873 0.9830 sec/batch\n",
      "Epoch 2/20  Iteration 317/5640 Training loss: 2.0862 0.9870 sec/batch\n",
      "Epoch 2/20  Iteration 318/5640 Training loss: 2.0858 0.9721 sec/batch\n",
      "Epoch 2/20  Iteration 319/5640 Training loss: 2.0842 0.9905 sec/batch\n",
      "Epoch 2/20  Iteration 320/5640 Training loss: 2.0830 0.9664 sec/batch\n",
      "Epoch 2/20  Iteration 321/5640 Training loss: 2.0820 0.9899 sec/batch\n",
      "Epoch 2/20  Iteration 322/5640 Training loss: 2.0815 1.0014 sec/batch\n",
      "Epoch 2/20  Iteration 323/5640 Training loss: 2.0807 0.9703 sec/batch\n",
      "Epoch 2/20  Iteration 324/5640 Training loss: 2.0804 0.9954 sec/batch\n",
      "Epoch 2/20  Iteration 325/5640 Training loss: 2.0797 1.0092 sec/batch\n",
      "Epoch 2/20  Iteration 326/5640 Training loss: 2.0782 0.9640 sec/batch\n",
      "Epoch 2/20  Iteration 327/5640 Training loss: 2.0771 1.0037 sec/batch\n",
      "Epoch 2/20  Iteration 328/5640 Training loss: 2.0766 0.9590 sec/batch\n",
      "Epoch 2/20  Iteration 329/5640 Training loss: 2.0759 0.9709 sec/batch\n",
      "Epoch 2/20  Iteration 330/5640 Training loss: 2.0753 1.0055 sec/batch\n",
      "Epoch 2/20  Iteration 331/5640 Training loss: 2.0746 0.9807 sec/batch\n",
      "Epoch 2/20  Iteration 332/5640 Training loss: 2.0738 0.9742 sec/batch\n",
      "Epoch 2/20  Iteration 333/5640 Training loss: 2.0732 1.0086 sec/batch\n",
      "Epoch 2/20  Iteration 334/5640 Training loss: 2.0726 0.9945 sec/batch\n",
      "Epoch 2/20  Iteration 335/5640 Training loss: 2.0720 0.9742 sec/batch\n",
      "Epoch 2/20  Iteration 336/5640 Training loss: 2.0716 0.9822 sec/batch\n",
      "Epoch 2/20  Iteration 337/5640 Training loss: 2.0706 1.0180 sec/batch\n",
      "Epoch 2/20  Iteration 338/5640 Training loss: 2.0697 0.9616 sec/batch\n",
      "Epoch 2/20  Iteration 339/5640 Training loss: 2.0688 0.9868 sec/batch\n",
      "Epoch 2/20  Iteration 340/5640 Training loss: 2.0681 0.9988 sec/batch\n",
      "Epoch 2/20  Iteration 341/5640 Training loss: 2.0676 0.9864 sec/batch\n",
      "Epoch 2/20  Iteration 342/5640 Training loss: 2.0666 0.9781 sec/batch\n",
      "Epoch 2/20  Iteration 343/5640 Training loss: 2.0657 0.9943 sec/batch\n",
      "Epoch 2/20  Iteration 344/5640 Training loss: 2.0649 0.9833 sec/batch\n",
      "Epoch 2/20  Iteration 345/5640 Training loss: 2.0640 0.9793 sec/batch\n",
      "Epoch 2/20  Iteration 346/5640 Training loss: 2.0627 0.9861 sec/batch\n",
      "Epoch 2/20  Iteration 347/5640 Training loss: 2.0623 0.9733 sec/batch\n",
      "Epoch 2/20  Iteration 348/5640 Training loss: 2.0615 0.9901 sec/batch\n",
      "Epoch 2/20  Iteration 349/5640 Training loss: 2.0609 0.9671 sec/batch\n",
      "Epoch 2/20  Iteration 350/5640 Training loss: 2.0598 0.9978 sec/batch\n",
      "Validation loss: 1.93215 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 351/5640 Training loss: 2.0592 0.9898 sec/batch\n",
      "Epoch 2/20  Iteration 352/5640 Training loss: 2.0579 0.9666 sec/batch\n",
      "Epoch 2/20  Iteration 353/5640 Training loss: 2.0574 0.9819 sec/batch\n",
      "Epoch 2/20  Iteration 354/5640 Training loss: 2.0565 0.9936 sec/batch\n",
      "Epoch 2/20  Iteration 355/5640 Training loss: 2.0559 0.9624 sec/batch\n",
      "Epoch 2/20  Iteration 356/5640 Training loss: 2.0549 0.9770 sec/batch\n",
      "Epoch 2/20  Iteration 357/5640 Training loss: 2.0543 0.9665 sec/batch\n",
      "Epoch 2/20  Iteration 358/5640 Training loss: 2.0536 1.0088 sec/batch\n",
      "Epoch 2/20  Iteration 359/5640 Training loss: 2.0528 0.9739 sec/batch\n",
      "Epoch 2/20  Iteration 360/5640 Training loss: 2.0524 0.9826 sec/batch\n",
      "Epoch 2/20  Iteration 361/5640 Training loss: 2.0517 1.0000 sec/batch\n",
      "Epoch 2/20  Iteration 362/5640 Training loss: 2.0514 0.9620 sec/batch\n",
      "Epoch 2/20  Iteration 363/5640 Training loss: 2.0509 0.9804 sec/batch\n",
      "Epoch 2/20  Iteration 364/5640 Training loss: 2.0501 1.0109 sec/batch\n",
      "Epoch 2/20  Iteration 365/5640 Training loss: 2.0493 0.9809 sec/batch\n",
      "Epoch 2/20  Iteration 366/5640 Training loss: 2.0482 0.9691 sec/batch\n",
      "Epoch 2/20  Iteration 367/5640 Training loss: 2.0473 0.9990 sec/batch\n",
      "Epoch 2/20  Iteration 368/5640 Training loss: 2.0465 0.9900 sec/batch\n",
      "Epoch 2/20  Iteration 369/5640 Training loss: 2.0456 0.9479 sec/batch\n",
      "Epoch 2/20  Iteration 370/5640 Training loss: 2.0448 0.9696 sec/batch\n",
      "Epoch 2/20  Iteration 371/5640 Training loss: 2.0443 1.0020 sec/batch\n",
      "Epoch 2/20  Iteration 372/5640 Training loss: 2.0435 0.9397 sec/batch\n",
      "Epoch 2/20  Iteration 373/5640 Training loss: 2.0429 0.9312 sec/batch\n",
      "Epoch 2/20  Iteration 374/5640 Training loss: 2.0422 0.9082 sec/batch\n",
      "Epoch 2/20  Iteration 375/5640 Training loss: 2.0419 0.9310 sec/batch\n",
      "Epoch 2/20  Iteration 376/5640 Training loss: 2.0409 0.8984 sec/batch\n",
      "Epoch 2/20  Iteration 377/5640 Training loss: 2.0401 0.9487 sec/batch\n",
      "Epoch 2/20  Iteration 378/5640 Training loss: 2.0397 0.9224 sec/batch\n",
      "Epoch 2/20  Iteration 379/5640 Training loss: 2.0390 0.9514 sec/batch\n",
      "Epoch 2/20  Iteration 380/5640 Training loss: 2.0381 0.9465 sec/batch\n",
      "Epoch 2/20  Iteration 381/5640 Training loss: 2.0376 0.9008 sec/batch\n",
      "Epoch 2/20  Iteration 382/5640 Training loss: 2.0368 0.9794 sec/batch\n",
      "Epoch 2/20  Iteration 383/5640 Training loss: 2.0359 0.9473 sec/batch\n",
      "Epoch 2/20  Iteration 384/5640 Training loss: 2.0350 0.9131 sec/batch\n",
      "Epoch 2/20  Iteration 385/5640 Training loss: 2.0340 0.9877 sec/batch\n",
      "Epoch 2/20  Iteration 386/5640 Training loss: 2.0330 0.8941 sec/batch\n",
      "Epoch 2/20  Iteration 387/5640 Training loss: 2.0321 0.9190 sec/batch\n",
      "Epoch 2/20  Iteration 388/5640 Training loss: 2.0312 1.0176 sec/batch\n",
      "Epoch 2/20  Iteration 389/5640 Training loss: 2.0303 0.8554 sec/batch\n",
      "Epoch 2/20  Iteration 390/5640 Training loss: 2.0292 0.9479 sec/batch\n",
      "Epoch 2/20  Iteration 391/5640 Training loss: 2.0284 0.9632 sec/batch\n",
      "Epoch 2/20  Iteration 392/5640 Training loss: 2.0276 0.8327 sec/batch\n",
      "Epoch 2/20  Iteration 393/5640 Training loss: 2.0267 0.9620 sec/batch\n",
      "Epoch 2/20  Iteration 394/5640 Training loss: 2.0260 0.9120 sec/batch\n",
      "Epoch 2/20  Iteration 395/5640 Training loss: 2.0251 0.8941 sec/batch\n",
      "Epoch 2/20  Iteration 396/5640 Training loss: 2.0243 0.9638 sec/batch\n",
      "Epoch 2/20  Iteration 397/5640 Training loss: 2.0235 0.9199 sec/batch\n",
      "Epoch 2/20  Iteration 398/5640 Training loss: 2.0228 0.8638 sec/batch\n",
      "Epoch 2/20  Iteration 399/5640 Training loss: 2.0221 0.9341 sec/batch\n",
      "Epoch 2/20  Iteration 400/5640 Training loss: 2.0213 0.9174 sec/batch\n",
      "Validation loss: 1.86887 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 401/5640 Training loss: 2.0208 0.8860 sec/batch\n",
      "Epoch 2/20  Iteration 402/5640 Training loss: 2.0200 0.9467 sec/batch\n",
      "Epoch 2/20  Iteration 403/5640 Training loss: 2.0194 0.9316 sec/batch\n",
      "Epoch 2/20  Iteration 404/5640 Training loss: 2.0190 0.8979 sec/batch\n",
      "Epoch 2/20  Iteration 405/5640 Training loss: 2.0181 0.9951 sec/batch\n",
      "Epoch 2/20  Iteration 406/5640 Training loss: 2.0175 0.8752 sec/batch\n",
      "Epoch 2/20  Iteration 407/5640 Training loss: 2.0171 0.9214 sec/batch\n",
      "Epoch 2/20  Iteration 408/5640 Training loss: 2.0163 1.0190 sec/batch\n",
      "Epoch 2/20  Iteration 409/5640 Training loss: 2.0156 0.8424 sec/batch\n",
      "Epoch 2/20  Iteration 410/5640 Training loss: 2.0150 0.9704 sec/batch\n",
      "Epoch 2/20  Iteration 411/5640 Training loss: 2.0143 0.9389 sec/batch\n",
      "Epoch 2/20  Iteration 412/5640 Training loss: 2.0136 0.8624 sec/batch\n",
      "Epoch 2/20  Iteration 413/5640 Training loss: 2.0130 1.0054 sec/batch\n",
      "Epoch 2/20  Iteration 414/5640 Training loss: 2.0122 0.9110 sec/batch\n",
      "Epoch 2/20  Iteration 415/5640 Training loss: 2.0112 0.9064 sec/batch\n",
      "Epoch 2/20  Iteration 416/5640 Training loss: 2.0104 0.9610 sec/batch\n",
      "Epoch 2/20  Iteration 417/5640 Training loss: 2.0098 0.8934 sec/batch\n",
      "Epoch 2/20  Iteration 418/5640 Training loss: 2.0091 0.8619 sec/batch\n",
      "Epoch 2/20  Iteration 419/5640 Training loss: 2.0084 0.9786 sec/batch\n",
      "Epoch 2/20  Iteration 420/5640 Training loss: 2.0076 0.9039 sec/batch\n",
      "Epoch 2/20  Iteration 421/5640 Training loss: 2.0069 0.8825 sec/batch\n",
      "Epoch 2/20  Iteration 422/5640 Training loss: 2.0062 0.9611 sec/batch\n",
      "Epoch 2/20  Iteration 423/5640 Training loss: 2.0055 0.7037 sec/batch\n",
      "Epoch 2/20  Iteration 424/5640 Training loss: 2.0048 0.8388 sec/batch\n",
      "Epoch 2/20  Iteration 425/5640 Training loss: 2.0041 1.0201 sec/batch\n",
      "Epoch 2/20  Iteration 426/5640 Training loss: 2.0031 0.9373 sec/batch\n",
      "Epoch 2/20  Iteration 427/5640 Training loss: 2.0022 1.0203 sec/batch\n",
      "Epoch 2/20  Iteration 428/5640 Training loss: 2.0013 0.9796 sec/batch\n",
      "Epoch 2/20  Iteration 429/5640 Training loss: 2.0006 0.9465 sec/batch\n",
      "Epoch 2/20  Iteration 430/5640 Training loss: 1.9999 1.0104 sec/batch\n",
      "Epoch 2/20  Iteration 431/5640 Training loss: 1.9993 0.9792 sec/batch\n",
      "Epoch 2/20  Iteration 432/5640 Training loss: 1.9988 0.9395 sec/batch\n",
      "Epoch 2/20  Iteration 433/5640 Training loss: 1.9982 0.9874 sec/batch\n",
      "Epoch 2/20  Iteration 434/5640 Training loss: 1.9976 0.9522 sec/batch\n",
      "Epoch 2/20  Iteration 435/5640 Training loss: 1.9971 0.9848 sec/batch\n",
      "Epoch 2/20  Iteration 436/5640 Training loss: 1.9965 0.9815 sec/batch\n",
      "Epoch 2/20  Iteration 437/5640 Training loss: 1.9958 0.9649 sec/batch\n",
      "Epoch 2/20  Iteration 438/5640 Training loss: 1.9953 1.0069 sec/batch\n",
      "Epoch 2/20  Iteration 439/5640 Training loss: 1.9945 1.0198 sec/batch\n",
      "Epoch 2/20  Iteration 440/5640 Training loss: 1.9939 0.9626 sec/batch\n",
      "Epoch 2/20  Iteration 441/5640 Training loss: 1.9931 0.9958 sec/batch\n",
      "Epoch 2/20  Iteration 442/5640 Training loss: 1.9923 0.9821 sec/batch\n",
      "Epoch 2/20  Iteration 443/5640 Training loss: 1.9917 0.9696 sec/batch\n",
      "Epoch 2/20  Iteration 444/5640 Training loss: 1.9909 1.0241 sec/batch\n",
      "Epoch 2/20  Iteration 445/5640 Training loss: 1.9902 0.9640 sec/batch\n",
      "Epoch 2/20  Iteration 446/5640 Training loss: 1.9896 0.9604 sec/batch\n",
      "Epoch 2/20  Iteration 447/5640 Training loss: 1.9891 1.0062 sec/batch\n",
      "Epoch 2/20  Iteration 448/5640 Training loss: 1.9886 1.0060 sec/batch\n",
      "Epoch 2/20  Iteration 449/5640 Training loss: 1.9879 0.9327 sec/batch\n",
      "Epoch 2/20  Iteration 450/5640 Training loss: 1.9873 1.0083 sec/batch\n",
      "Validation loss: 1.80264 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 451/5640 Training loss: 1.9868 0.9816 sec/batch\n",
      "Epoch 2/20  Iteration 452/5640 Training loss: 1.9862 1.0001 sec/batch\n",
      "Epoch 2/20  Iteration 453/5640 Training loss: 1.9855 0.9709 sec/batch\n",
      "Epoch 2/20  Iteration 454/5640 Training loss: 1.9848 0.9877 sec/batch\n",
      "Epoch 2/20  Iteration 455/5640 Training loss: 1.9839 0.9399 sec/batch\n",
      "Epoch 2/20  Iteration 456/5640 Training loss: 1.9833 1.0234 sec/batch\n",
      "Epoch 2/20  Iteration 457/5640 Training loss: 1.9825 0.9658 sec/batch\n",
      "Epoch 2/20  Iteration 458/5640 Training loss: 1.9819 0.9732 sec/batch\n",
      "Epoch 2/20  Iteration 459/5640 Training loss: 1.9815 1.0072 sec/batch\n",
      "Epoch 2/20  Iteration 460/5640 Training loss: 1.9807 0.9775 sec/batch\n",
      "Epoch 2/20  Iteration 461/5640 Training loss: 1.9803 0.9743 sec/batch\n",
      "Epoch 2/20  Iteration 462/5640 Training loss: 1.9798 1.0049 sec/batch\n",
      "Epoch 2/20  Iteration 463/5640 Training loss: 1.9792 0.9892 sec/batch\n",
      "Epoch 2/20  Iteration 464/5640 Training loss: 1.9787 0.9724 sec/batch\n",
      "Epoch 2/20  Iteration 465/5640 Training loss: 1.9782 0.9944 sec/batch\n",
      "Epoch 2/20  Iteration 466/5640 Training loss: 1.9777 1.0010 sec/batch\n",
      "Epoch 2/20  Iteration 467/5640 Training loss: 1.9772 0.9800 sec/batch\n",
      "Epoch 2/20  Iteration 468/5640 Training loss: 1.9765 0.9787 sec/batch\n",
      "Epoch 2/20  Iteration 469/5640 Training loss: 1.9757 1.0285 sec/batch\n",
      "Epoch 2/20  Iteration 470/5640 Training loss: 1.9750 0.9768 sec/batch\n",
      "Epoch 2/20  Iteration 471/5640 Training loss: 1.9743 0.9686 sec/batch\n",
      "Epoch 2/20  Iteration 472/5640 Training loss: 1.9736 1.0133 sec/batch\n",
      "Epoch 2/20  Iteration 473/5640 Training loss: 1.9731 0.9860 sec/batch\n",
      "Epoch 2/20  Iteration 474/5640 Training loss: 1.9724 0.9792 sec/batch\n",
      "Epoch 2/20  Iteration 475/5640 Training loss: 1.9718 0.9979 sec/batch\n",
      "Epoch 2/20  Iteration 476/5640 Training loss: 1.9712 0.9851 sec/batch\n",
      "Epoch 2/20  Iteration 477/5640 Training loss: 1.9707 0.9896 sec/batch\n",
      "Epoch 2/20  Iteration 478/5640 Training loss: 1.9702 1.0036 sec/batch\n",
      "Epoch 2/20  Iteration 479/5640 Training loss: 1.9695 0.9563 sec/batch\n",
      "Epoch 2/20  Iteration 480/5640 Training loss: 1.9690 0.9837 sec/batch\n",
      "Epoch 2/20  Iteration 481/5640 Training loss: 1.9684 0.9986 sec/batch\n",
      "Epoch 2/20  Iteration 482/5640 Training loss: 1.9678 0.9804 sec/batch\n",
      "Epoch 2/20  Iteration 483/5640 Training loss: 1.9673 0.9491 sec/batch\n",
      "Epoch 2/20  Iteration 484/5640 Training loss: 1.9666 0.9998 sec/batch\n",
      "Epoch 2/20  Iteration 485/5640 Training loss: 1.9660 0.9802 sec/batch\n",
      "Epoch 2/20  Iteration 486/5640 Training loss: 1.9654 0.9868 sec/batch\n",
      "Epoch 2/20  Iteration 487/5640 Training loss: 1.9648 0.9733 sec/batch\n",
      "Epoch 2/20  Iteration 488/5640 Training loss: 1.9643 1.0059 sec/batch\n",
      "Epoch 2/20  Iteration 489/5640 Training loss: 1.9636 0.9756 sec/batch\n",
      "Epoch 2/20  Iteration 490/5640 Training loss: 1.9629 0.9745 sec/batch\n",
      "Epoch 2/20  Iteration 491/5640 Training loss: 1.9625 1.0095 sec/batch\n",
      "Epoch 2/20  Iteration 492/5640 Training loss: 1.9617 0.9534 sec/batch\n",
      "Epoch 2/20  Iteration 493/5640 Training loss: 1.9611 0.9925 sec/batch\n",
      "Epoch 2/20  Iteration 494/5640 Training loss: 1.9606 0.9918 sec/batch\n",
      "Epoch 2/20  Iteration 495/5640 Training loss: 1.9600 0.9846 sec/batch\n",
      "Epoch 2/20  Iteration 496/5640 Training loss: 1.9594 0.9931 sec/batch\n",
      "Epoch 2/20  Iteration 497/5640 Training loss: 1.9588 0.9661 sec/batch\n",
      "Epoch 2/20  Iteration 498/5640 Training loss: 1.9582 0.9958 sec/batch\n",
      "Epoch 2/20  Iteration 499/5640 Training loss: 1.9575 0.9835 sec/batch\n",
      "Epoch 2/20  Iteration 500/5640 Training loss: 1.9568 0.9772 sec/batch\n",
      "Validation loss: 1.74888 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 501/5640 Training loss: 1.9563 0.9533 sec/batch\n",
      "Epoch 2/20  Iteration 502/5640 Training loss: 1.9556 0.9958 sec/batch\n",
      "Epoch 2/20  Iteration 503/5640 Training loss: 1.9550 0.9917 sec/batch\n",
      "Epoch 2/20  Iteration 504/5640 Training loss: 1.9543 0.9462 sec/batch\n",
      "Epoch 2/20  Iteration 505/5640 Training loss: 1.9537 0.9904 sec/batch\n",
      "Epoch 2/20  Iteration 506/5640 Training loss: 1.9530 0.9626 sec/batch\n",
      "Epoch 2/20  Iteration 507/5640 Training loss: 1.9524 0.9677 sec/batch\n",
      "Epoch 2/20  Iteration 508/5640 Training loss: 1.9517 1.0113 sec/batch\n",
      "Epoch 2/20  Iteration 509/5640 Training loss: 1.9511 0.9786 sec/batch\n",
      "Epoch 2/20  Iteration 510/5640 Training loss: 1.9506 0.9907 sec/batch\n",
      "Epoch 2/20  Iteration 511/5640 Training loss: 1.9501 0.9966 sec/batch\n",
      "Epoch 2/20  Iteration 512/5640 Training loss: 1.9496 0.9599 sec/batch\n",
      "Epoch 2/20  Iteration 513/5640 Training loss: 1.9490 0.9967 sec/batch\n",
      "Epoch 2/20  Iteration 514/5640 Training loss: 1.9485 0.9922 sec/batch\n",
      "Epoch 2/20  Iteration 515/5640 Training loss: 1.9478 0.9778 sec/batch\n",
      "Epoch 2/20  Iteration 516/5640 Training loss: 1.9471 0.9958 sec/batch\n",
      "Epoch 2/20  Iteration 517/5640 Training loss: 1.9464 0.9926 sec/batch\n",
      "Epoch 2/20  Iteration 518/5640 Training loss: 1.9459 0.9586 sec/batch\n",
      "Epoch 2/20  Iteration 519/5640 Training loss: 1.9453 1.0155 sec/batch\n",
      "Epoch 2/20  Iteration 520/5640 Training loss: 1.9449 1.0116 sec/batch\n",
      "Epoch 2/20  Iteration 521/5640 Training loss: 1.9443 0.9289 sec/batch\n",
      "Epoch 2/20  Iteration 522/5640 Training loss: 1.9437 1.0143 sec/batch\n",
      "Epoch 2/20  Iteration 523/5640 Training loss: 1.9431 0.9939 sec/batch\n",
      "Epoch 2/20  Iteration 524/5640 Training loss: 1.9425 0.9326 sec/batch\n",
      "Epoch 2/20  Iteration 525/5640 Training loss: 1.9418 1.0048 sec/batch\n",
      "Epoch 2/20  Iteration 526/5640 Training loss: 1.9412 0.9827 sec/batch\n",
      "Epoch 2/20  Iteration 527/5640 Training loss: 1.9405 0.9549 sec/batch\n",
      "Epoch 2/20  Iteration 528/5640 Training loss: 1.9400 0.9737 sec/batch\n",
      "Epoch 2/20  Iteration 529/5640 Training loss: 1.9394 0.9835 sec/batch\n",
      "Epoch 2/20  Iteration 530/5640 Training loss: 1.9389 0.9687 sec/batch\n",
      "Epoch 2/20  Iteration 531/5640 Training loss: 1.9384 0.9966 sec/batch\n",
      "Epoch 2/20  Iteration 532/5640 Training loss: 1.9379 0.9689 sec/batch\n",
      "Epoch 2/20  Iteration 533/5640 Training loss: 1.9372 0.9826 sec/batch\n",
      "Epoch 2/20  Iteration 534/5640 Training loss: 1.9367 1.0141 sec/batch\n",
      "Epoch 2/20  Iteration 535/5640 Training loss: 1.9361 0.9291 sec/batch\n",
      "Epoch 2/20  Iteration 536/5640 Training loss: 1.9355 1.0012 sec/batch\n",
      "Epoch 2/20  Iteration 537/5640 Training loss: 1.9350 1.0219 sec/batch\n",
      "Epoch 2/20  Iteration 538/5640 Training loss: 1.9344 0.9347 sec/batch\n",
      "Epoch 2/20  Iteration 539/5640 Training loss: 1.9338 0.9866 sec/batch\n",
      "Epoch 2/20  Iteration 540/5640 Training loss: 1.9333 1.0110 sec/batch\n",
      "Epoch 2/20  Iteration 541/5640 Training loss: 1.9328 0.9351 sec/batch\n",
      "Epoch 2/20  Iteration 542/5640 Training loss: 1.9324 1.0236 sec/batch\n",
      "Epoch 2/20  Iteration 543/5640 Training loss: 1.9318 0.9509 sec/batch\n",
      "Epoch 2/20  Iteration 544/5640 Training loss: 1.9313 0.9253 sec/batch\n",
      "Epoch 2/20  Iteration 545/5640 Training loss: 1.9307 1.0466 sec/batch\n",
      "Epoch 2/20  Iteration 546/5640 Training loss: 1.9303 0.9563 sec/batch\n",
      "Epoch 2/20  Iteration 547/5640 Training loss: 1.9298 0.9644 sec/batch\n",
      "Epoch 2/20  Iteration 548/5640 Training loss: 1.9292 0.9956 sec/batch\n",
      "Epoch 2/20  Iteration 549/5640 Training loss: 1.9287 0.9558 sec/batch\n",
      "Epoch 2/20  Iteration 550/5640 Training loss: 1.9281 0.9758 sec/batch\n",
      "Validation loss: 1.70768 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 551/5640 Training loss: 1.9277 1.0057 sec/batch\n",
      "Epoch 2/20  Iteration 552/5640 Training loss: 1.9271 1.0190 sec/batch\n",
      "Epoch 2/20  Iteration 553/5640 Training loss: 1.9266 0.9515 sec/batch\n",
      "Epoch 2/20  Iteration 554/5640 Training loss: 1.9260 1.0053 sec/batch\n",
      "Epoch 2/20  Iteration 555/5640 Training loss: 1.9255 0.9963 sec/batch\n",
      "Epoch 2/20  Iteration 556/5640 Training loss: 1.9250 0.9363 sec/batch\n",
      "Epoch 2/20  Iteration 557/5640 Training loss: 1.9245 1.0286 sec/batch\n",
      "Epoch 2/20  Iteration 558/5640 Training loss: 1.9239 0.9585 sec/batch\n",
      "Epoch 2/20  Iteration 559/5640 Training loss: 1.9234 0.9249 sec/batch\n",
      "Epoch 2/20  Iteration 560/5640 Training loss: 1.9229 1.0350 sec/batch\n",
      "Epoch 2/20  Iteration 561/5640 Training loss: 1.9224 0.9661 sec/batch\n",
      "Epoch 2/20  Iteration 562/5640 Training loss: 1.9219 0.9595 sec/batch\n",
      "Epoch 2/20  Iteration 563/5640 Training loss: 1.9213 0.9855 sec/batch\n",
      "Epoch 2/20  Iteration 564/5640 Training loss: 1.9207 0.9493 sec/batch\n",
      "Epoch 3/20  Iteration 565/5640 Training loss: 1.8199 0.9838 sec/batch\n",
      "Epoch 3/20  Iteration 566/5640 Training loss: 1.8039 1.0293 sec/batch\n",
      "Epoch 3/20  Iteration 567/5640 Training loss: 1.7884 0.9336 sec/batch\n",
      "Epoch 3/20  Iteration 568/5640 Training loss: 1.7888 0.9861 sec/batch\n",
      "Epoch 3/20  Iteration 569/5640 Training loss: 1.7860 1.0051 sec/batch\n",
      "Epoch 3/20  Iteration 570/5640 Training loss: 1.7844 0.9476 sec/batch\n",
      "Epoch 3/20  Iteration 571/5640 Training loss: 1.7798 1.0057 sec/batch\n",
      "Epoch 3/20  Iteration 572/5640 Training loss: 1.7785 1.0094 sec/batch\n",
      "Epoch 3/20  Iteration 573/5640 Training loss: 1.7758 0.9291 sec/batch\n",
      "Epoch 3/20  Iteration 574/5640 Training loss: 1.7725 1.0238 sec/batch\n",
      "Epoch 3/20  Iteration 575/5640 Training loss: 1.7704 0.9833 sec/batch\n",
      "Epoch 3/20  Iteration 576/5640 Training loss: 1.7661 0.9377 sec/batch\n",
      "Epoch 3/20  Iteration 577/5640 Training loss: 1.7657 1.0297 sec/batch\n",
      "Epoch 3/20  Iteration 578/5640 Training loss: 1.7669 0.9475 sec/batch\n",
      "Epoch 3/20  Iteration 579/5640 Training loss: 1.7660 0.9724 sec/batch\n",
      "Epoch 3/20  Iteration 580/5640 Training loss: 1.7669 1.0394 sec/batch\n",
      "Epoch 3/20  Iteration 581/5640 Training loss: 1.7649 0.9666 sec/batch\n",
      "Epoch 3/20  Iteration 582/5640 Training loss: 1.7627 0.9644 sec/batch\n",
      "Epoch 3/20  Iteration 583/5640 Training loss: 1.7609 0.9954 sec/batch\n",
      "Epoch 3/20  Iteration 584/5640 Training loss: 1.7597 0.9622 sec/batch\n",
      "Epoch 3/20  Iteration 585/5640 Training loss: 1.7585 0.9886 sec/batch\n",
      "Epoch 3/20  Iteration 586/5640 Training loss: 1.7577 1.0367 sec/batch\n",
      "Epoch 3/20  Iteration 587/5640 Training loss: 1.7569 0.9445 sec/batch\n",
      "Epoch 3/20  Iteration 588/5640 Training loss: 1.7555 1.0149 sec/batch\n",
      "Epoch 3/20  Iteration 589/5640 Training loss: 1.7556 1.0484 sec/batch\n",
      "Epoch 3/20  Iteration 590/5640 Training loss: 1.7555 0.9722 sec/batch\n",
      "Epoch 3/20  Iteration 591/5640 Training loss: 1.7548 1.0483 sec/batch\n",
      "Epoch 3/20  Iteration 592/5640 Training loss: 1.7555 1.0241 sec/batch\n",
      "Epoch 3/20  Iteration 593/5640 Training loss: 1.7557 0.9722 sec/batch\n",
      "Epoch 3/20  Iteration 594/5640 Training loss: 1.7551 1.0669 sec/batch\n",
      "Epoch 3/20  Iteration 595/5640 Training loss: 1.7535 1.0651 sec/batch\n",
      "Epoch 3/20  Iteration 596/5640 Training loss: 1.7533 0.9298 sec/batch\n",
      "Epoch 3/20  Iteration 597/5640 Training loss: 1.7523 1.0211 sec/batch\n",
      "Epoch 3/20  Iteration 598/5640 Training loss: 1.7521 0.9672 sec/batch\n",
      "Epoch 3/20  Iteration 599/5640 Training loss: 1.7520 0.9528 sec/batch\n",
      "Epoch 3/20  Iteration 600/5640 Training loss: 1.7523 1.0735 sec/batch\n",
      "Validation loss: 1.65645 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 601/5640 Training loss: 1.7524 0.9688 sec/batch\n",
      "Epoch 3/20  Iteration 602/5640 Training loss: 1.7511 0.9666 sec/batch\n",
      "Epoch 3/20  Iteration 603/5640 Training loss: 1.7505 1.0021 sec/batch\n",
      "Epoch 3/20  Iteration 604/5640 Training loss: 1.7504 1.0050 sec/batch\n",
      "Epoch 3/20  Iteration 605/5640 Training loss: 1.7497 0.9366 sec/batch\n",
      "Epoch 3/20  Iteration 606/5640 Training loss: 1.7498 1.0121 sec/batch\n",
      "Epoch 3/20  Iteration 607/5640 Training loss: 1.7497 0.9903 sec/batch\n",
      "Epoch 3/20  Iteration 608/5640 Training loss: 1.7488 0.9387 sec/batch\n",
      "Epoch 3/20  Iteration 609/5640 Training loss: 1.7485 0.9894 sec/batch\n",
      "Epoch 3/20  Iteration 610/5640 Training loss: 1.7480 0.9869 sec/batch\n",
      "Epoch 3/20  Iteration 611/5640 Training loss: 1.7476 0.9613 sec/batch\n",
      "Epoch 3/20  Iteration 612/5640 Training loss: 1.7474 0.9820 sec/batch\n",
      "Epoch 3/20  Iteration 613/5640 Training loss: 1.7473 0.9607 sec/batch\n",
      "Epoch 3/20  Iteration 614/5640 Training loss: 1.7465 1.0013 sec/batch\n",
      "Epoch 3/20  Iteration 615/5640 Training loss: 1.7463 0.9949 sec/batch\n",
      "Epoch 3/20  Iteration 616/5640 Training loss: 1.7462 0.9566 sec/batch\n",
      "Epoch 3/20  Iteration 617/5640 Training loss: 1.7460 0.9757 sec/batch\n",
      "Epoch 3/20  Iteration 618/5640 Training loss: 1.7458 0.9914 sec/batch\n",
      "Epoch 3/20  Iteration 619/5640 Training loss: 1.7452 0.9595 sec/batch\n",
      "Epoch 3/20  Iteration 620/5640 Training loss: 1.7446 1.0039 sec/batch\n",
      "Epoch 3/20  Iteration 621/5640 Training loss: 1.7441 0.9827 sec/batch\n",
      "Epoch 3/20  Iteration 622/5640 Training loss: 1.7441 0.9390 sec/batch\n",
      "Epoch 3/20  Iteration 623/5640 Training loss: 1.7438 1.0132 sec/batch\n",
      "Epoch 3/20  Iteration 624/5640 Training loss: 1.7432 0.9753 sec/batch\n",
      "Epoch 3/20  Iteration 625/5640 Training loss: 1.7427 0.9513 sec/batch\n",
      "Epoch 3/20  Iteration 626/5640 Training loss: 1.7421 0.9908 sec/batch\n",
      "Epoch 3/20  Iteration 627/5640 Training loss: 1.7415 0.9787 sec/batch\n",
      "Epoch 3/20  Iteration 628/5640 Training loss: 1.7407 0.9808 sec/batch\n",
      "Epoch 3/20  Iteration 629/5640 Training loss: 1.7408 0.9699 sec/batch\n",
      "Epoch 3/20  Iteration 630/5640 Training loss: 1.7405 0.9911 sec/batch\n",
      "Epoch 3/20  Iteration 631/5640 Training loss: 1.7404 0.9604 sec/batch\n",
      "Epoch 3/20  Iteration 632/5640 Training loss: 1.7396 1.0150 sec/batch\n",
      "Epoch 3/20  Iteration 633/5640 Training loss: 1.7393 0.9747 sec/batch\n",
      "Epoch 3/20  Iteration 634/5640 Training loss: 1.7385 0.9727 sec/batch\n",
      "Epoch 3/20  Iteration 635/5640 Training loss: 1.7384 1.0032 sec/batch\n",
      "Epoch 3/20  Iteration 636/5640 Training loss: 1.7378 0.9608 sec/batch\n",
      "Epoch 3/20  Iteration 637/5640 Training loss: 1.7375 0.9971 sec/batch\n",
      "Epoch 3/20  Iteration 638/5640 Training loss: 1.7372 0.9689 sec/batch\n",
      "Epoch 3/20  Iteration 639/5640 Training loss: 1.7369 0.9716 sec/batch\n",
      "Epoch 3/20  Iteration 640/5640 Training loss: 1.7371 1.0024 sec/batch\n",
      "Epoch 3/20  Iteration 641/5640 Training loss: 1.7367 1.0024 sec/batch\n",
      "Epoch 3/20  Iteration 642/5640 Training loss: 1.7369 0.9479 sec/batch\n",
      "Epoch 3/20  Iteration 643/5640 Training loss: 1.7366 1.0008 sec/batch\n",
      "Epoch 3/20  Iteration 644/5640 Training loss: 1.7368 0.9868 sec/batch\n",
      "Epoch 3/20  Iteration 645/5640 Training loss: 1.7368 0.9494 sec/batch\n",
      "Epoch 3/20  Iteration 646/5640 Training loss: 1.7363 0.9688 sec/batch\n",
      "Epoch 3/20  Iteration 647/5640 Training loss: 1.7359 0.9755 sec/batch\n",
      "Epoch 3/20  Iteration 648/5640 Training loss: 1.7352 0.9707 sec/batch\n",
      "Epoch 3/20  Iteration 649/5640 Training loss: 1.7348 1.0044 sec/batch\n",
      "Epoch 3/20  Iteration 650/5640 Training loss: 1.7346 1.0414 sec/batch\n",
      "Validation loss: 1.61946 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 651/5640 Training loss: 1.7345 0.9696 sec/batch\n",
      "Epoch 3/20  Iteration 652/5640 Training loss: 1.7342 0.9852 sec/batch\n",
      "Epoch 3/20  Iteration 653/5640 Training loss: 1.7339 1.0257 sec/batch\n",
      "Epoch 3/20  Iteration 654/5640 Training loss: 1.7335 0.9425 sec/batch\n",
      "Epoch 3/20  Iteration 655/5640 Training loss: 1.7334 0.9957 sec/batch\n",
      "Epoch 3/20  Iteration 656/5640 Training loss: 1.7332 1.0270 sec/batch\n",
      "Epoch 3/20  Iteration 657/5640 Training loss: 1.7331 0.9480 sec/batch\n",
      "Epoch 3/20  Iteration 658/5640 Training loss: 1.7327 1.0059 sec/batch\n",
      "Epoch 3/20  Iteration 659/5640 Training loss: 1.7322 1.0156 sec/batch\n",
      "Epoch 3/20  Iteration 660/5640 Training loss: 1.7321 0.9552 sec/batch\n",
      "Epoch 3/20  Iteration 661/5640 Training loss: 1.7318 1.0276 sec/batch\n",
      "Epoch 3/20  Iteration 662/5640 Training loss: 1.7313 1.0001 sec/batch\n",
      "Epoch 3/20  Iteration 663/5640 Training loss: 1.7309 0.9441 sec/batch\n",
      "Epoch 3/20  Iteration 664/5640 Training loss: 1.7306 1.0189 sec/batch\n",
      "Epoch 3/20  Iteration 665/5640 Training loss: 1.7299 1.0543 sec/batch\n",
      "Epoch 3/20  Iteration 666/5640 Training loss: 1.7294 0.9757 sec/batch\n",
      "Epoch 3/20  Iteration 667/5640 Training loss: 1.7288 1.0328 sec/batch\n",
      "Epoch 3/20  Iteration 668/5640 Training loss: 1.7280 0.9586 sec/batch\n",
      "Epoch 3/20  Iteration 669/5640 Training loss: 1.7276 0.9684 sec/batch\n",
      "Epoch 3/20  Iteration 670/5640 Training loss: 1.7270 1.0049 sec/batch\n",
      "Epoch 3/20  Iteration 671/5640 Training loss: 1.7263 0.9689 sec/batch\n",
      "Epoch 3/20  Iteration 672/5640 Training loss: 1.7255 0.9936 sec/batch\n",
      "Epoch 3/20  Iteration 673/5640 Training loss: 1.7249 0.8957 sec/batch\n",
      "Epoch 3/20  Iteration 674/5640 Training loss: 1.7244 0.8663 sec/batch\n",
      "Epoch 3/20  Iteration 675/5640 Training loss: 1.7239 0.9593 sec/batch\n",
      "Epoch 3/20  Iteration 676/5640 Training loss: 1.7235 0.9207 sec/batch\n",
      "Epoch 3/20  Iteration 677/5640 Training loss: 1.7229 0.8929 sec/batch\n",
      "Epoch 3/20  Iteration 678/5640 Training loss: 1.7224 0.9607 sec/batch\n",
      "Epoch 3/20  Iteration 679/5640 Training loss: 1.7221 0.9428 sec/batch\n",
      "Epoch 3/20  Iteration 680/5640 Training loss: 1.7219 0.8278 sec/batch\n",
      "Epoch 3/20  Iteration 681/5640 Training loss: 1.7215 0.9999 sec/batch\n",
      "Epoch 3/20  Iteration 682/5640 Training loss: 1.7210 0.8412 sec/batch\n",
      "Epoch 3/20  Iteration 683/5640 Training loss: 1.7206 0.9848 sec/batch\n",
      "Epoch 3/20  Iteration 684/5640 Training loss: 1.7201 0.9367 sec/batch\n",
      "Epoch 3/20  Iteration 685/5640 Training loss: 1.7199 0.8776 sec/batch\n",
      "Epoch 3/20  Iteration 686/5640 Training loss: 1.7198 1.0036 sec/batch\n",
      "Epoch 3/20  Iteration 687/5640 Training loss: 1.7192 0.8866 sec/batch\n",
      "Epoch 3/20  Iteration 688/5640 Training loss: 1.7190 0.8919 sec/batch\n",
      "Epoch 3/20  Iteration 689/5640 Training loss: 1.7189 0.9328 sec/batch\n",
      "Epoch 3/20  Iteration 690/5640 Training loss: 1.7183 0.9130 sec/batch\n",
      "Epoch 3/20  Iteration 691/5640 Training loss: 1.7179 0.8414 sec/batch\n",
      "Epoch 3/20  Iteration 692/5640 Training loss: 1.7175 0.9837 sec/batch\n",
      "Epoch 3/20  Iteration 693/5640 Training loss: 1.7170 0.8504 sec/batch\n",
      "Epoch 3/20  Iteration 694/5640 Training loss: 1.7167 0.9547 sec/batch\n",
      "Epoch 3/20  Iteration 695/5640 Training loss: 1.7164 0.9718 sec/batch\n",
      "Epoch 3/20  Iteration 696/5640 Training loss: 1.7158 0.8933 sec/batch\n",
      "Epoch 3/20  Iteration 697/5640 Training loss: 1.7151 0.9398 sec/batch\n",
      "Epoch 3/20  Iteration 698/5640 Training loss: 1.7147 0.9093 sec/batch\n",
      "Epoch 3/20  Iteration 699/5640 Training loss: 1.7143 0.8977 sec/batch\n",
      "Epoch 3/20  Iteration 700/5640 Training loss: 1.7140 0.9812 sec/batch\n",
      "Validation loss: 1.58357 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 701/5640 Training loss: 1.7139 0.9283 sec/batch\n",
      "Epoch 3/20  Iteration 702/5640 Training loss: 1.7135 0.9375 sec/batch\n",
      "Epoch 3/20  Iteration 703/5640 Training loss: 1.7131 0.9037 sec/batch\n",
      "Epoch 3/20  Iteration 704/5640 Training loss: 1.7128 0.9810 sec/batch\n",
      "Epoch 3/20  Iteration 705/5640 Training loss: 1.7123 0.8889 sec/batch\n",
      "Epoch 3/20  Iteration 706/5640 Training loss: 1.7120 0.8912 sec/batch\n",
      "Epoch 3/20  Iteration 707/5640 Training loss: 1.7116 0.9917 sec/batch\n",
      "Epoch 3/20  Iteration 708/5640 Training loss: 1.7110 0.9151 sec/batch\n",
      "Epoch 3/20  Iteration 709/5640 Training loss: 1.7104 0.9455 sec/batch\n",
      "Epoch 3/20  Iteration 710/5640 Training loss: 1.7099 0.9804 sec/batch\n",
      "Epoch 3/20  Iteration 711/5640 Training loss: 1.7095 0.8618 sec/batch\n",
      "Epoch 3/20  Iteration 712/5640 Training loss: 1.7092 0.9736 sec/batch\n",
      "Epoch 3/20  Iteration 713/5640 Training loss: 1.7089 0.9267 sec/batch\n",
      "Epoch 3/20  Iteration 714/5640 Training loss: 1.7086 0.8700 sec/batch\n",
      "Epoch 3/20  Iteration 715/5640 Training loss: 1.7083 1.0030 sec/batch\n",
      "Epoch 3/20  Iteration 716/5640 Training loss: 1.7080 0.8859 sec/batch\n",
      "Epoch 3/20  Iteration 717/5640 Training loss: 1.7078 0.9407 sec/batch\n",
      "Epoch 3/20  Iteration 718/5640 Training loss: 1.7075 1.0079 sec/batch\n",
      "Epoch 3/20  Iteration 719/5640 Training loss: 1.7071 0.9041 sec/batch\n",
      "Epoch 3/20  Iteration 720/5640 Training loss: 1.7068 0.9338 sec/batch\n",
      "Epoch 3/20  Iteration 721/5640 Training loss: 1.7063 0.9575 sec/batch\n",
      "Epoch 3/20  Iteration 722/5640 Training loss: 1.7060 0.8641 sec/batch\n",
      "Epoch 3/20  Iteration 723/5640 Training loss: 1.7055 0.6986 sec/batch\n",
      "Epoch 3/20  Iteration 724/5640 Training loss: 1.7051 0.9499 sec/batch\n",
      "Epoch 3/20  Iteration 725/5640 Training loss: 1.7047 0.9453 sec/batch\n",
      "Epoch 3/20  Iteration 726/5640 Training loss: 1.7043 1.0615 sec/batch\n",
      "Epoch 3/20  Iteration 727/5640 Training loss: 1.7039 0.9515 sec/batch\n",
      "Epoch 3/20  Iteration 728/5640 Training loss: 1.7035 0.9584 sec/batch\n",
      "Epoch 3/20  Iteration 729/5640 Training loss: 1.7033 1.0318 sec/batch\n",
      "Epoch 3/20  Iteration 730/5640 Training loss: 1.7031 0.9284 sec/batch\n",
      "Epoch 3/20  Iteration 731/5640 Training loss: 1.7027 0.9703 sec/batch\n",
      "Epoch 3/20  Iteration 732/5640 Training loss: 1.7024 0.9945 sec/batch\n",
      "Epoch 3/20  Iteration 733/5640 Training loss: 1.7021 0.9618 sec/batch\n",
      "Epoch 3/20  Iteration 734/5640 Training loss: 1.7019 0.9882 sec/batch\n",
      "Epoch 3/20  Iteration 735/5640 Training loss: 1.7016 0.9816 sec/batch\n",
      "Epoch 3/20  Iteration 736/5640 Training loss: 1.7012 0.9567 sec/batch\n",
      "Epoch 3/20  Iteration 737/5640 Training loss: 1.7007 1.0170 sec/batch\n",
      "Epoch 3/20  Iteration 738/5640 Training loss: 1.7003 0.9817 sec/batch\n",
      "Epoch 3/20  Iteration 739/5640 Training loss: 1.6999 0.9572 sec/batch\n",
      "Epoch 3/20  Iteration 740/5640 Training loss: 1.6996 1.0239 sec/batch\n",
      "Epoch 3/20  Iteration 741/5640 Training loss: 1.6994 0.9975 sec/batch\n",
      "Epoch 3/20  Iteration 742/5640 Training loss: 1.6989 0.9266 sec/batch\n",
      "Epoch 3/20  Iteration 743/5640 Training loss: 1.6988 1.0119 sec/batch\n",
      "Epoch 3/20  Iteration 744/5640 Training loss: 1.6987 0.9871 sec/batch\n",
      "Epoch 3/20  Iteration 745/5640 Training loss: 1.6983 0.9682 sec/batch\n",
      "Epoch 3/20  Iteration 746/5640 Training loss: 1.6980 0.9836 sec/batch\n",
      "Epoch 3/20  Iteration 747/5640 Training loss: 1.6978 0.9865 sec/batch\n",
      "Epoch 3/20  Iteration 748/5640 Training loss: 1.6974 0.9716 sec/batch\n",
      "Epoch 3/20  Iteration 749/5640 Training loss: 1.6971 0.9734 sec/batch\n",
      "Epoch 3/20  Iteration 750/5640 Training loss: 1.6965 0.9951 sec/batch\n",
      "Validation loss: 1.54249 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 751/5640 Training loss: 1.6962 0.9620 sec/batch\n",
      "Epoch 3/20  Iteration 752/5640 Training loss: 1.6957 1.0317 sec/batch\n",
      "Epoch 3/20  Iteration 753/5640 Training loss: 1.6952 0.9993 sec/batch\n",
      "Epoch 3/20  Iteration 754/5640 Training loss: 1.6947 1.0526 sec/batch\n",
      "Epoch 3/20  Iteration 755/5640 Training loss: 1.6944 1.0116 sec/batch\n",
      "Epoch 3/20  Iteration 756/5640 Training loss: 1.6940 0.9749 sec/batch\n",
      "Epoch 3/20  Iteration 757/5640 Training loss: 1.6936 0.9715 sec/batch\n",
      "Epoch 3/20  Iteration 758/5640 Training loss: 1.6932 1.0093 sec/batch\n",
      "Epoch 3/20  Iteration 759/5640 Training loss: 1.6930 1.0002 sec/batch\n",
      "Epoch 3/20  Iteration 760/5640 Training loss: 1.6928 0.9667 sec/batch\n",
      "Epoch 3/20  Iteration 761/5640 Training loss: 1.6923 0.9941 sec/batch\n",
      "Epoch 3/20  Iteration 762/5640 Training loss: 1.6921 1.0096 sec/batch\n",
      "Epoch 3/20  Iteration 763/5640 Training loss: 1.6918 0.9735 sec/batch\n",
      "Epoch 3/20  Iteration 764/5640 Training loss: 1.6914 0.9803 sec/batch\n",
      "Epoch 3/20  Iteration 765/5640 Training loss: 1.6910 1.0056 sec/batch\n",
      "Epoch 3/20  Iteration 766/5640 Training loss: 1.6906 0.9668 sec/batch\n",
      "Epoch 3/20  Iteration 767/5640 Training loss: 1.6904 0.9801 sec/batch\n",
      "Epoch 3/20  Iteration 768/5640 Training loss: 1.6901 1.0049 sec/batch\n",
      "Epoch 3/20  Iteration 769/5640 Training loss: 1.6897 0.9756 sec/batch\n",
      "Epoch 3/20  Iteration 770/5640 Training loss: 1.6895 0.9931 sec/batch\n",
      "Epoch 3/20  Iteration 771/5640 Training loss: 1.6890 1.0121 sec/batch\n",
      "Epoch 3/20  Iteration 772/5640 Training loss: 1.6885 0.9735 sec/batch\n",
      "Epoch 3/20  Iteration 773/5640 Training loss: 1.6883 0.9826 sec/batch\n",
      "Epoch 3/20  Iteration 774/5640 Training loss: 1.6878 1.0062 sec/batch\n",
      "Epoch 3/20  Iteration 775/5640 Training loss: 1.6873 0.9745 sec/batch\n",
      "Epoch 3/20  Iteration 776/5640 Training loss: 1.6870 0.9894 sec/batch\n",
      "Epoch 3/20  Iteration 777/5640 Training loss: 1.6867 0.9802 sec/batch\n",
      "Epoch 3/20  Iteration 778/5640 Training loss: 1.6863 1.0165 sec/batch\n",
      "Epoch 3/20  Iteration 779/5640 Training loss: 1.6859 0.9788 sec/batch\n",
      "Epoch 3/20  Iteration 780/5640 Training loss: 1.6856 0.9970 sec/batch\n",
      "Epoch 3/20  Iteration 781/5640 Training loss: 1.6851 1.0177 sec/batch\n",
      "Epoch 3/20  Iteration 782/5640 Training loss: 1.6846 0.9459 sec/batch\n",
      "Epoch 3/20  Iteration 783/5640 Training loss: 1.6842 0.9777 sec/batch\n",
      "Epoch 3/20  Iteration 784/5640 Training loss: 1.6838 0.9965 sec/batch\n",
      "Epoch 3/20  Iteration 785/5640 Training loss: 1.6834 0.9845 sec/batch\n",
      "Epoch 3/20  Iteration 786/5640 Training loss: 1.6830 0.9695 sec/batch\n",
      "Epoch 3/20  Iteration 787/5640 Training loss: 1.6826 1.0016 sec/batch\n",
      "Epoch 3/20  Iteration 788/5640 Training loss: 1.6822 0.9754 sec/batch\n",
      "Epoch 3/20  Iteration 789/5640 Training loss: 1.6819 0.9712 sec/batch\n",
      "Epoch 3/20  Iteration 790/5640 Training loss: 1.6815 0.9625 sec/batch\n",
      "Epoch 3/20  Iteration 791/5640 Training loss: 1.6811 0.9782 sec/batch\n",
      "Epoch 3/20  Iteration 792/5640 Training loss: 1.6809 1.0115 sec/batch\n",
      "Epoch 3/20  Iteration 793/5640 Training loss: 1.6807 0.9606 sec/batch\n",
      "Epoch 3/20  Iteration 794/5640 Training loss: 1.6803 1.0049 sec/batch\n",
      "Epoch 3/20  Iteration 795/5640 Training loss: 1.6800 0.9815 sec/batch\n",
      "Epoch 3/20  Iteration 796/5640 Training loss: 1.6797 0.9819 sec/batch\n",
      "Epoch 3/20  Iteration 797/5640 Training loss: 1.6793 0.9872 sec/batch\n",
      "Epoch 3/20  Iteration 798/5640 Training loss: 1.6788 0.9797 sec/batch\n",
      "Epoch 3/20  Iteration 799/5640 Training loss: 1.6785 0.9686 sec/batch\n",
      "Epoch 3/20  Iteration 800/5640 Training loss: 1.6782 1.0169 sec/batch\n",
      "Validation loss: 1.51177 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 801/5640 Training loss: 1.6781 0.8907 sec/batch\n",
      "Epoch 3/20  Iteration 802/5640 Training loss: 1.6779 1.0181 sec/batch\n",
      "Epoch 3/20  Iteration 803/5640 Training loss: 1.6775 0.9116 sec/batch\n",
      "Epoch 3/20  Iteration 804/5640 Training loss: 1.6771 0.8601 sec/batch\n",
      "Epoch 3/20  Iteration 805/5640 Training loss: 1.6768 0.9882 sec/batch\n",
      "Epoch 3/20  Iteration 806/5640 Training loss: 1.6764 0.8910 sec/batch\n",
      "Epoch 3/20  Iteration 807/5640 Training loss: 1.6760 0.9365 sec/batch\n",
      "Epoch 3/20  Iteration 808/5640 Training loss: 1.6756 0.9852 sec/batch\n",
      "Epoch 3/20  Iteration 809/5640 Training loss: 1.6751 0.8626 sec/batch\n",
      "Epoch 3/20  Iteration 810/5640 Training loss: 1.6747 0.9857 sec/batch\n",
      "Epoch 3/20  Iteration 811/5640 Training loss: 1.6743 0.8515 sec/batch\n",
      "Epoch 3/20  Iteration 812/5640 Training loss: 1.6741 0.9405 sec/batch\n",
      "Epoch 3/20  Iteration 813/5640 Training loss: 1.6739 0.9572 sec/batch\n",
      "Epoch 3/20  Iteration 814/5640 Training loss: 1.6736 0.8936 sec/batch\n",
      "Epoch 3/20  Iteration 815/5640 Training loss: 1.6732 0.8565 sec/batch\n",
      "Epoch 3/20  Iteration 816/5640 Training loss: 1.6728 0.9426 sec/batch\n",
      "Epoch 3/20  Iteration 817/5640 Training loss: 1.6724 0.9136 sec/batch\n",
      "Epoch 3/20  Iteration 818/5640 Training loss: 1.6721 0.9027 sec/batch\n",
      "Epoch 3/20  Iteration 819/5640 Training loss: 1.6718 0.9718 sec/batch\n",
      "Epoch 3/20  Iteration 820/5640 Training loss: 1.6714 0.8903 sec/batch\n",
      "Epoch 3/20  Iteration 821/5640 Training loss: 1.6710 1.0040 sec/batch\n",
      "Epoch 3/20  Iteration 822/5640 Training loss: 1.6708 0.8211 sec/batch\n",
      "Epoch 3/20  Iteration 823/5640 Training loss: 1.6705 0.9447 sec/batch\n",
      "Epoch 3/20  Iteration 824/5640 Training loss: 1.6703 0.9654 sec/batch\n",
      "Epoch 3/20  Iteration 825/5640 Training loss: 1.6700 0.8473 sec/batch\n",
      "Epoch 3/20  Iteration 826/5640 Training loss: 1.6697 0.9856 sec/batch\n",
      "Epoch 3/20  Iteration 827/5640 Training loss: 1.6694 0.9168 sec/batch\n",
      "Epoch 3/20  Iteration 828/5640 Training loss: 1.6692 0.9154 sec/batch\n",
      "Epoch 3/20  Iteration 829/5640 Training loss: 1.6689 0.9381 sec/batch\n",
      "Epoch 3/20  Iteration 830/5640 Training loss: 1.6685 0.9394 sec/batch\n",
      "Epoch 3/20  Iteration 831/5640 Training loss: 1.6682 0.9171 sec/batch\n",
      "Epoch 3/20  Iteration 832/5640 Training loss: 1.6677 0.9328 sec/batch\n",
      "Epoch 3/20  Iteration 833/5640 Training loss: 1.6675 0.9374 sec/batch\n",
      "Epoch 3/20  Iteration 834/5640 Training loss: 1.6671 0.9042 sec/batch\n",
      "Epoch 3/20  Iteration 835/5640 Training loss: 1.6668 0.9860 sec/batch\n",
      "Epoch 3/20  Iteration 836/5640 Training loss: 1.6664 0.9293 sec/batch\n",
      "Epoch 3/20  Iteration 837/5640 Training loss: 1.6662 0.9048 sec/batch\n",
      "Epoch 3/20  Iteration 838/5640 Training loss: 1.6659 1.0061 sec/batch\n",
      "Epoch 3/20  Iteration 839/5640 Training loss: 1.6656 0.9070 sec/batch\n",
      "Epoch 3/20  Iteration 840/5640 Training loss: 1.6652 0.8806 sec/batch\n",
      "Epoch 3/20  Iteration 841/5640 Training loss: 1.6649 0.9652 sec/batch\n",
      "Epoch 3/20  Iteration 842/5640 Training loss: 1.6647 0.9057 sec/batch\n",
      "Epoch 3/20  Iteration 843/5640 Training loss: 1.6644 0.9566 sec/batch\n",
      "Epoch 3/20  Iteration 844/5640 Training loss: 1.6641 0.9209 sec/batch\n",
      "Epoch 3/20  Iteration 845/5640 Training loss: 1.6637 0.9123 sec/batch\n",
      "Epoch 3/20  Iteration 846/5640 Training loss: 1.6633 1.0048 sec/batch\n",
      "Epoch 4/20  Iteration 847/5640 Training loss: 1.6425 0.9045 sec/batch\n",
      "Epoch 4/20  Iteration 848/5640 Training loss: 1.6240 0.9202 sec/batch\n",
      "Epoch 4/20  Iteration 849/5640 Training loss: 1.6086 1.0309 sec/batch\n",
      "Epoch 4/20  Iteration 850/5640 Training loss: 1.6049 0.8598 sec/batch\n",
      "Validation loss: 1.4839 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 851/5640 Training loss: 1.6102 0.9631 sec/batch\n",
      "Epoch 4/20  Iteration 852/5640 Training loss: 1.6060 0.9975 sec/batch\n",
      "Epoch 4/20  Iteration 853/5640 Training loss: 1.5989 0.9655 sec/batch\n",
      "Epoch 4/20  Iteration 854/5640 Training loss: 1.5958 0.9689 sec/batch\n",
      "Epoch 4/20  Iteration 855/5640 Training loss: 1.5929 0.9978 sec/batch\n",
      "Epoch 4/20  Iteration 856/5640 Training loss: 1.5881 0.9537 sec/batch\n",
      "Epoch 4/20  Iteration 857/5640 Training loss: 1.5851 0.9854 sec/batch\n",
      "Epoch 4/20  Iteration 858/5640 Training loss: 1.5809 0.9808 sec/batch\n",
      "Epoch 4/20  Iteration 859/5640 Training loss: 1.5791 0.9779 sec/batch\n",
      "Epoch 4/20  Iteration 860/5640 Training loss: 1.5802 1.0072 sec/batch\n",
      "Epoch 4/20  Iteration 861/5640 Training loss: 1.5783 0.9799 sec/batch\n",
      "Epoch 4/20  Iteration 862/5640 Training loss: 1.5791 0.9563 sec/batch\n",
      "Epoch 4/20  Iteration 863/5640 Training loss: 1.5782 1.0140 sec/batch\n",
      "Epoch 4/20  Iteration 864/5640 Training loss: 1.5764 1.0074 sec/batch\n",
      "Epoch 4/20  Iteration 865/5640 Training loss: 1.5754 0.9500 sec/batch\n",
      "Epoch 4/20  Iteration 866/5640 Training loss: 1.5741 1.0067 sec/batch\n",
      "Epoch 4/20  Iteration 867/5640 Training loss: 1.5734 0.9707 sec/batch\n",
      "Epoch 4/20  Iteration 868/5640 Training loss: 1.5725 0.9660 sec/batch\n",
      "Epoch 4/20  Iteration 869/5640 Training loss: 1.5716 0.9693 sec/batch\n",
      "Epoch 4/20  Iteration 870/5640 Training loss: 1.5702 1.0018 sec/batch\n",
      "Epoch 4/20  Iteration 871/5640 Training loss: 1.5705 0.9676 sec/batch\n",
      "Epoch 4/20  Iteration 872/5640 Training loss: 1.5702 0.9783 sec/batch\n",
      "Epoch 4/20  Iteration 873/5640 Training loss: 1.5698 1.0238 sec/batch\n",
      "Epoch 4/20  Iteration 874/5640 Training loss: 1.5703 0.9617 sec/batch\n",
      "Epoch 4/20  Iteration 875/5640 Training loss: 1.5703 0.9823 sec/batch\n",
      "Epoch 4/20  Iteration 876/5640 Training loss: 1.5698 1.0140 sec/batch\n",
      "Epoch 4/20  Iteration 877/5640 Training loss: 1.5681 0.9569 sec/batch\n",
      "Epoch 4/20  Iteration 878/5640 Training loss: 1.5678 0.9852 sec/batch\n",
      "Epoch 4/20  Iteration 879/5640 Training loss: 1.5670 0.9692 sec/batch\n",
      "Epoch 4/20  Iteration 880/5640 Training loss: 1.5669 0.9934 sec/batch\n",
      "Epoch 4/20  Iteration 881/5640 Training loss: 1.5668 1.0027 sec/batch\n",
      "Epoch 4/20  Iteration 882/5640 Training loss: 1.5669 0.9648 sec/batch\n",
      "Epoch 4/20  Iteration 883/5640 Training loss: 1.5662 1.0142 sec/batch\n",
      "Epoch 4/20  Iteration 884/5640 Training loss: 1.5653 0.9485 sec/batch\n",
      "Epoch 4/20  Iteration 885/5640 Training loss: 1.5647 0.9738 sec/batch\n",
      "Epoch 4/20  Iteration 886/5640 Training loss: 1.5644 1.0082 sec/batch\n",
      "Epoch 4/20  Iteration 887/5640 Training loss: 1.5639 0.9965 sec/batch\n",
      "Epoch 4/20  Iteration 888/5640 Training loss: 1.5644 0.9949 sec/batch\n",
      "Epoch 4/20  Iteration 889/5640 Training loss: 1.5639 1.0056 sec/batch\n",
      "Epoch 4/20  Iteration 890/5640 Training loss: 1.5637 0.9866 sec/batch\n",
      "Epoch 4/20  Iteration 891/5640 Training loss: 1.5634 0.9672 sec/batch\n",
      "Epoch 4/20  Iteration 892/5640 Training loss: 1.5631 1.0064 sec/batch\n",
      "Epoch 4/20  Iteration 893/5640 Training loss: 1.5632 0.9886 sec/batch\n",
      "Epoch 4/20  Iteration 894/5640 Training loss: 1.5632 0.9641 sec/batch\n",
      "Epoch 4/20  Iteration 895/5640 Training loss: 1.5632 1.0067 sec/batch\n",
      "Epoch 4/20  Iteration 896/5640 Training loss: 1.5628 1.0058 sec/batch\n",
      "Epoch 4/20  Iteration 897/5640 Training loss: 1.5629 0.9541 sec/batch\n",
      "Epoch 4/20  Iteration 898/5640 Training loss: 1.5628 0.9944 sec/batch\n",
      "Epoch 4/20  Iteration 899/5640 Training loss: 1.5626 0.9916 sec/batch\n",
      "Epoch 4/20  Iteration 900/5640 Training loss: 1.5626 0.9503 sec/batch\n",
      "Validation loss: 1.46109 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 901/5640 Training loss: 1.5632 1.0044 sec/batch\n",
      "Epoch 4/20  Iteration 902/5640 Training loss: 1.5629 1.0020 sec/batch\n",
      "Epoch 4/20  Iteration 903/5640 Training loss: 1.5625 0.9730 sec/batch\n",
      "Epoch 4/20  Iteration 904/5640 Training loss: 1.5626 0.9766 sec/batch\n",
      "Epoch 4/20  Iteration 905/5640 Training loss: 1.5624 1.0290 sec/batch\n",
      "Epoch 4/20  Iteration 906/5640 Training loss: 1.5621 0.9460 sec/batch\n",
      "Epoch 4/20  Iteration 907/5640 Training loss: 1.5616 0.9785 sec/batch\n",
      "Epoch 4/20  Iteration 908/5640 Training loss: 1.5610 1.0220 sec/batch\n",
      "Epoch 4/20  Iteration 909/5640 Training loss: 1.5605 0.9147 sec/batch\n",
      "Epoch 4/20  Iteration 910/5640 Training loss: 1.5599 1.0106 sec/batch\n",
      "Epoch 4/20  Iteration 911/5640 Training loss: 1.5599 0.9947 sec/batch\n",
      "Epoch 4/20  Iteration 912/5640 Training loss: 1.5597 0.9455 sec/batch\n",
      "Epoch 4/20  Iteration 913/5640 Training loss: 1.5598 1.0265 sec/batch\n",
      "Epoch 4/20  Iteration 914/5640 Training loss: 1.5591 0.9453 sec/batch\n",
      "Epoch 4/20  Iteration 915/5640 Training loss: 1.5585 0.9229 sec/batch\n",
      "Epoch 4/20  Iteration 916/5640 Training loss: 1.5577 1.0442 sec/batch\n",
      "Epoch 4/20  Iteration 917/5640 Training loss: 1.5576 0.9517 sec/batch\n",
      "Epoch 4/20  Iteration 918/5640 Training loss: 1.5572 0.9588 sec/batch\n",
      "Epoch 4/20  Iteration 919/5640 Training loss: 1.5571 0.9902 sec/batch\n",
      "Epoch 4/20  Iteration 920/5640 Training loss: 1.5570 0.9811 sec/batch\n",
      "Epoch 4/20  Iteration 921/5640 Training loss: 1.5570 0.9858 sec/batch\n",
      "Epoch 4/20  Iteration 922/5640 Training loss: 1.5574 1.0082 sec/batch\n",
      "Epoch 4/20  Iteration 923/5640 Training loss: 1.5572 0.9509 sec/batch\n",
      "Epoch 4/20  Iteration 924/5640 Training loss: 1.5577 0.9744 sec/batch\n",
      "Epoch 4/20  Iteration 925/5640 Training loss: 1.5576 1.0278 sec/batch\n",
      "Epoch 4/20  Iteration 926/5640 Training loss: 1.5578 0.9224 sec/batch\n",
      "Epoch 4/20  Iteration 927/5640 Training loss: 1.5578 1.0084 sec/batch\n",
      "Epoch 4/20  Iteration 928/5640 Training loss: 1.5572 1.0016 sec/batch\n",
      "Epoch 4/20  Iteration 929/5640 Training loss: 1.5570 0.9389 sec/batch\n",
      "Epoch 4/20  Iteration 930/5640 Training loss: 1.5564 1.0185 sec/batch\n",
      "Epoch 4/20  Iteration 931/5640 Training loss: 1.5561 0.9997 sec/batch\n",
      "Epoch 4/20  Iteration 932/5640 Training loss: 1.5559 0.9186 sec/batch\n",
      "Epoch 4/20  Iteration 933/5640 Training loss: 1.5553 1.0097 sec/batch\n",
      "Epoch 4/20  Iteration 934/5640 Training loss: 1.5549 0.9566 sec/batch\n",
      "Epoch 4/20  Iteration 935/5640 Training loss: 1.5548 0.9502 sec/batch\n",
      "Epoch 4/20  Iteration 936/5640 Training loss: 1.5544 0.9993 sec/batch\n",
      "Epoch 4/20  Iteration 937/5640 Training loss: 1.5543 0.9700 sec/batch\n",
      "Epoch 4/20  Iteration 938/5640 Training loss: 1.5540 0.9850 sec/batch\n",
      "Epoch 4/20  Iteration 939/5640 Training loss: 1.5540 0.9962 sec/batch\n",
      "Epoch 4/20  Iteration 940/5640 Training loss: 1.5536 0.9855 sec/batch\n",
      "Epoch 4/20  Iteration 941/5640 Training loss: 1.5533 0.9789 sec/batch\n",
      "Epoch 4/20  Iteration 942/5640 Training loss: 1.5533 1.0113 sec/batch\n",
      "Epoch 4/20  Iteration 943/5640 Training loss: 1.5531 0.9538 sec/batch\n",
      "Epoch 4/20  Iteration 944/5640 Training loss: 1.5528 0.9939 sec/batch\n",
      "Epoch 4/20  Iteration 945/5640 Training loss: 1.5526 1.0191 sec/batch\n",
      "Epoch 4/20  Iteration 946/5640 Training loss: 1.5522 0.9412 sec/batch\n",
      "Epoch 4/20  Iteration 947/5640 Training loss: 1.5517 1.0223 sec/batch\n",
      "Epoch 4/20  Iteration 948/5640 Training loss: 1.5514 0.9908 sec/batch\n",
      "Epoch 4/20  Iteration 949/5640 Training loss: 1.5510 0.9640 sec/batch\n",
      "Epoch 4/20  Iteration 950/5640 Training loss: 1.5506 1.0191 sec/batch\n",
      "Validation loss: 1.44274 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 951/5640 Training loss: 1.5508 1.0090 sec/batch\n",
      "Epoch 4/20  Iteration 952/5640 Training loss: 1.5503 0.9798 sec/batch\n",
      "Epoch 4/20  Iteration 953/5640 Training loss: 1.5498 0.9772 sec/batch\n",
      "Epoch 4/20  Iteration 954/5640 Training loss: 1.5492 1.0028 sec/batch\n",
      "Epoch 4/20  Iteration 955/5640 Training loss: 1.5488 1.0040 sec/batch\n",
      "Epoch 4/20  Iteration 956/5640 Training loss: 1.5485 0.9444 sec/batch\n",
      "Epoch 4/20  Iteration 957/5640 Training loss: 1.5481 1.0048 sec/batch\n",
      "Epoch 4/20  Iteration 958/5640 Training loss: 1.5478 0.9832 sec/batch\n",
      "Epoch 4/20  Iteration 959/5640 Training loss: 1.5474 0.9575 sec/batch\n",
      "Epoch 4/20  Iteration 960/5640 Training loss: 1.5472 0.9737 sec/batch\n",
      "Epoch 4/20  Iteration 961/5640 Training loss: 1.5471 0.9968 sec/batch\n",
      "Epoch 4/20  Iteration 962/5640 Training loss: 1.5469 0.9848 sec/batch\n",
      "Epoch 4/20  Iteration 963/5640 Training loss: 1.5466 0.9749 sec/batch\n",
      "Epoch 4/20  Iteration 964/5640 Training loss: 1.5462 1.0086 sec/batch\n",
      "Epoch 4/20  Iteration 965/5640 Training loss: 1.5459 0.9692 sec/batch\n",
      "Epoch 4/20  Iteration 966/5640 Training loss: 1.5455 0.9817 sec/batch\n",
      "Epoch 4/20  Iteration 967/5640 Training loss: 1.5453 0.9797 sec/batch\n",
      "Epoch 4/20  Iteration 968/5640 Training loss: 1.5453 0.9599 sec/batch\n",
      "Epoch 4/20  Iteration 969/5640 Training loss: 1.5448 0.9902 sec/batch\n",
      "Epoch 4/20  Iteration 970/5640 Training loss: 1.5446 0.9693 sec/batch\n",
      "Epoch 4/20  Iteration 971/5640 Training loss: 1.5446 1.0037 sec/batch\n",
      "Epoch 4/20  Iteration 972/5640 Training loss: 1.5442 0.9592 sec/batch\n",
      "Epoch 4/20  Iteration 973/5640 Training loss: 1.5439 0.9824 sec/batch\n",
      "Epoch 4/20  Iteration 974/5640 Training loss: 1.5435 1.0064 sec/batch\n",
      "Epoch 4/20  Iteration 975/5640 Training loss: 1.5431 0.9841 sec/batch\n",
      "Epoch 4/20  Iteration 976/5640 Training loss: 1.5428 0.9525 sec/batch\n",
      "Epoch 4/20  Iteration 977/5640 Training loss: 1.5425 1.0065 sec/batch\n",
      "Epoch 4/20  Iteration 978/5640 Training loss: 1.5420 0.9924 sec/batch\n",
      "Epoch 4/20  Iteration 979/5640 Training loss: 1.5414 0.9414 sec/batch\n",
      "Epoch 4/20  Iteration 980/5640 Training loss: 1.5410 0.9691 sec/batch\n",
      "Epoch 4/20  Iteration 981/5640 Training loss: 1.5408 1.0376 sec/batch\n",
      "Epoch 4/20  Iteration 982/5640 Training loss: 1.5406 0.9561 sec/batch\n",
      "Epoch 4/20  Iteration 983/5640 Training loss: 1.5403 0.9677 sec/batch\n",
      "Epoch 4/20  Iteration 984/5640 Training loss: 1.5398 1.0076 sec/batch\n",
      "Epoch 4/20  Iteration 985/5640 Training loss: 1.5396 0.9683 sec/batch\n",
      "Epoch 4/20  Iteration 986/5640 Training loss: 1.5394 0.9814 sec/batch\n",
      "Epoch 4/20  Iteration 987/5640 Training loss: 1.5390 0.9808 sec/batch\n",
      "Epoch 4/20  Iteration 988/5640 Training loss: 1.5389 0.9655 sec/batch\n",
      "Epoch 4/20  Iteration 989/5640 Training loss: 1.5386 1.0167 sec/batch\n",
      "Epoch 4/20  Iteration 990/5640 Training loss: 1.5381 0.9618 sec/batch\n",
      "Epoch 4/20  Iteration 991/5640 Training loss: 1.5377 0.9809 sec/batch\n",
      "Epoch 4/20  Iteration 992/5640 Training loss: 1.5372 0.9866 sec/batch\n",
      "Epoch 4/20  Iteration 993/5640 Training loss: 1.5368 0.9571 sec/batch\n",
      "Epoch 4/20  Iteration 994/5640 Training loss: 1.5366 1.0058 sec/batch\n",
      "Epoch 4/20  Iteration 995/5640 Training loss: 1.5365 0.9758 sec/batch\n",
      "Epoch 4/20  Iteration 996/5640 Training loss: 1.5363 0.9538 sec/batch\n",
      "Epoch 4/20  Iteration 997/5640 Training loss: 1.5360 1.0073 sec/batch\n",
      "Epoch 4/20  Iteration 998/5640 Training loss: 1.5360 0.9910 sec/batch\n",
      "Epoch 4/20  Iteration 999/5640 Training loss: 1.5359 0.9462 sec/batch\n",
      "Epoch 4/20  Iteration 1000/5640 Training loss: 1.5358 1.0030 sec/batch\n",
      "Validation loss: 1.41613 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1001/5640 Training loss: 1.5360 0.9212 sec/batch\n",
      "Epoch 4/20  Iteration 1002/5640 Training loss: 1.5359 0.9012 sec/batch\n",
      "Epoch 4/20  Iteration 1003/5640 Training loss: 1.5355 1.0034 sec/batch\n",
      "Epoch 4/20  Iteration 1004/5640 Training loss: 1.5352 0.9006 sec/batch\n",
      "Epoch 4/20  Iteration 1005/5640 Training loss: 1.5349 0.9080 sec/batch\n",
      "Epoch 4/20  Iteration 1006/5640 Training loss: 1.5346 0.9569 sec/batch\n",
      "Epoch 4/20  Iteration 1007/5640 Training loss: 1.5343 0.9313 sec/batch\n",
      "Epoch 4/20  Iteration 1008/5640 Training loss: 1.5340 0.8567 sec/batch\n",
      "Epoch 4/20  Iteration 1009/5640 Training loss: 1.5337 0.9626 sec/batch\n",
      "Epoch 4/20  Iteration 1010/5640 Training loss: 1.5335 0.9326 sec/batch\n",
      "Epoch 4/20  Iteration 1011/5640 Training loss: 1.5334 0.9038 sec/batch\n",
      "Epoch 4/20  Iteration 1012/5640 Training loss: 1.5335 0.9909 sec/batch\n",
      "Epoch 4/20  Iteration 1013/5640 Training loss: 1.5332 0.8961 sec/batch\n",
      "Epoch 4/20  Iteration 1014/5640 Training loss: 1.5331 0.9341 sec/batch\n",
      "Epoch 4/20  Iteration 1015/5640 Training loss: 1.5330 0.9445 sec/batch\n",
      "Epoch 4/20  Iteration 1016/5640 Training loss: 1.5328 0.8658 sec/batch\n",
      "Epoch 4/20  Iteration 1017/5640 Training loss: 1.5326 1.0022 sec/batch\n",
      "Epoch 4/20  Iteration 1018/5640 Training loss: 1.5323 0.9135 sec/batch\n",
      "Epoch 4/20  Iteration 1019/5640 Training loss: 1.5320 0.9148 sec/batch\n",
      "Epoch 4/20  Iteration 1020/5640 Training loss: 1.5316 1.0163 sec/batch\n",
      "Epoch 4/20  Iteration 1021/5640 Training loss: 1.5315 0.9057 sec/batch\n",
      "Epoch 4/20  Iteration 1022/5640 Training loss: 1.5312 0.8869 sec/batch\n",
      "Epoch 4/20  Iteration 1023/5640 Training loss: 1.5312 0.9904 sec/batch\n",
      "Epoch 4/20  Iteration 1024/5640 Training loss: 1.5309 0.8440 sec/batch\n",
      "Epoch 4/20  Iteration 1025/5640 Training loss: 1.5309 0.9879 sec/batch\n",
      "Epoch 4/20  Iteration 1026/5640 Training loss: 1.5308 0.9470 sec/batch\n",
      "Epoch 4/20  Iteration 1027/5640 Training loss: 1.5306 0.8490 sec/batch\n",
      "Epoch 4/20  Iteration 1028/5640 Training loss: 1.5305 0.9686 sec/batch\n",
      "Epoch 4/20  Iteration 1029/5640 Training loss: 1.5304 0.9100 sec/batch\n",
      "Epoch 4/20  Iteration 1030/5640 Training loss: 1.5302 0.9058 sec/batch\n",
      "Epoch 4/20  Iteration 1031/5640 Training loss: 1.5299 0.9442 sec/batch\n",
      "Epoch 4/20  Iteration 1032/5640 Training loss: 1.5295 0.9466 sec/batch\n",
      "Epoch 4/20  Iteration 1033/5640 Training loss: 1.5290 0.9232 sec/batch\n",
      "Epoch 4/20  Iteration 1034/5640 Training loss: 1.5286 0.9454 sec/batch\n",
      "Epoch 4/20  Iteration 1035/5640 Training loss: 1.5282 0.9213 sec/batch\n",
      "Epoch 4/20  Iteration 1036/5640 Training loss: 1.5278 0.8902 sec/batch\n",
      "Epoch 4/20  Iteration 1037/5640 Training loss: 1.5276 0.9951 sec/batch\n",
      "Epoch 4/20  Iteration 1038/5640 Training loss: 1.5274 0.8663 sec/batch\n",
      "Epoch 4/20  Iteration 1039/5640 Training loss: 1.5270 0.9070 sec/batch\n",
      "Epoch 4/20  Iteration 1040/5640 Training loss: 1.5269 1.0077 sec/batch\n",
      "Epoch 4/20  Iteration 1041/5640 Training loss: 1.5268 0.8188 sec/batch\n",
      "Epoch 4/20  Iteration 1042/5640 Training loss: 1.5266 0.9894 sec/batch\n",
      "Epoch 4/20  Iteration 1043/5640 Training loss: 1.5263 0.9356 sec/batch\n",
      "Epoch 4/20  Iteration 1044/5640 Training loss: 1.5263 0.8594 sec/batch\n",
      "Epoch 4/20  Iteration 1045/5640 Training loss: 1.5261 1.0114 sec/batch\n",
      "Epoch 4/20  Iteration 1046/5640 Training loss: 1.5259 0.8820 sec/batch\n",
      "Epoch 4/20  Iteration 1047/5640 Training loss: 1.5257 0.9371 sec/batch\n",
      "Epoch 4/20  Iteration 1048/5640 Training loss: 1.5254 1.0105 sec/batch\n",
      "Epoch 4/20  Iteration 1049/5640 Training loss: 1.5253 0.8946 sec/batch\n",
      "Epoch 4/20  Iteration 1050/5640 Training loss: 1.5252 0.9087 sec/batch\n",
      "Validation loss: 1.39488 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1051/5640 Training loss: 1.5251 0.6888 sec/batch\n",
      "Epoch 4/20  Iteration 1052/5640 Training loss: 1.5250 0.7306 sec/batch\n",
      "Epoch 4/20  Iteration 1053/5640 Training loss: 1.5247 0.6615 sec/batch\n",
      "Epoch 4/20  Iteration 1054/5640 Training loss: 1.5243 0.6916 sec/batch\n",
      "Epoch 4/20  Iteration 1055/5640 Training loss: 1.5242 0.7254 sec/batch\n",
      "Epoch 4/20  Iteration 1056/5640 Training loss: 1.5238 0.6396 sec/batch\n",
      "Epoch 4/20  Iteration 1057/5640 Training loss: 1.5235 0.6567 sec/batch\n",
      "Epoch 4/20  Iteration 1058/5640 Training loss: 1.5233 0.6910 sec/batch\n",
      "Epoch 4/20  Iteration 1059/5640 Training loss: 1.5231 0.6452 sec/batch\n",
      "Epoch 4/20  Iteration 1060/5640 Training loss: 1.5229 0.7348 sec/batch\n",
      "Epoch 4/20  Iteration 1061/5640 Training loss: 1.5227 0.6632 sec/batch\n",
      "Epoch 4/20  Iteration 1062/5640 Training loss: 1.5224 0.6844 sec/batch\n",
      "Epoch 4/20  Iteration 1063/5640 Training loss: 1.5221 0.7255 sec/batch\n",
      "Epoch 4/20  Iteration 1064/5640 Training loss: 1.5216 0.6594 sec/batch\n",
      "Epoch 4/20  Iteration 1065/5640 Training loss: 1.5213 0.6868 sec/batch\n",
      "Epoch 4/20  Iteration 1066/5640 Training loss: 1.5210 0.7261 sec/batch\n",
      "Epoch 4/20  Iteration 1067/5640 Training loss: 1.5207 0.6705 sec/batch\n",
      "Epoch 4/20  Iteration 1068/5640 Training loss: 1.5204 0.6777 sec/batch\n",
      "Epoch 4/20  Iteration 1069/5640 Training loss: 1.5201 0.7351 sec/batch\n",
      "Epoch 4/20  Iteration 1070/5640 Training loss: 1.5199 0.6668 sec/batch\n",
      "Epoch 4/20  Iteration 1071/5640 Training loss: 1.5196 0.6796 sec/batch\n",
      "Epoch 4/20  Iteration 1072/5640 Training loss: 1.5193 0.7362 sec/batch\n",
      "Epoch 4/20  Iteration 1073/5640 Training loss: 1.5191 0.6653 sec/batch\n",
      "Epoch 4/20  Iteration 1074/5640 Training loss: 1.5190 0.6701 sec/batch\n",
      "Epoch 4/20  Iteration 1075/5640 Training loss: 1.5189 0.7214 sec/batch\n",
      "Epoch 4/20  Iteration 1076/5640 Training loss: 1.5187 0.6067 sec/batch\n",
      "Epoch 4/20  Iteration 1077/5640 Training loss: 1.5184 0.6975 sec/batch\n",
      "Epoch 4/20  Iteration 1078/5640 Training loss: 1.5183 0.6910 sec/batch\n",
      "Epoch 4/20  Iteration 1079/5640 Training loss: 1.5180 0.6662 sec/batch\n",
      "Epoch 4/20  Iteration 1080/5640 Training loss: 1.5176 0.7058 sec/batch\n",
      "Epoch 4/20  Iteration 1081/5640 Training loss: 1.5174 0.6948 sec/batch\n",
      "Epoch 4/20  Iteration 1082/5640 Training loss: 1.5172 0.6366 sec/batch\n",
      "Epoch 4/20  Iteration 1083/5640 Training loss: 1.5171 0.7331 sec/batch\n",
      "Epoch 4/20  Iteration 1084/5640 Training loss: 1.5170 0.6298 sec/batch\n",
      "Epoch 4/20  Iteration 1085/5640 Training loss: 1.5168 0.6649 sec/batch\n",
      "Epoch 4/20  Iteration 1086/5640 Training loss: 1.5165 0.7249 sec/batch\n",
      "Epoch 4/20  Iteration 1087/5640 Training loss: 1.5163 0.6691 sec/batch\n",
      "Epoch 4/20  Iteration 1088/5640 Training loss: 1.5161 0.6878 sec/batch\n",
      "Epoch 4/20  Iteration 1089/5640 Training loss: 1.5158 0.7230 sec/batch\n",
      "Epoch 4/20  Iteration 1090/5640 Training loss: 1.5154 0.6584 sec/batch\n",
      "Epoch 4/20  Iteration 1091/5640 Training loss: 1.5151 0.6998 sec/batch\n",
      "Epoch 4/20  Iteration 1092/5640 Training loss: 1.5149 0.7293 sec/batch\n",
      "Epoch 4/20  Iteration 1093/5640 Training loss: 1.5147 0.6642 sec/batch\n",
      "Epoch 4/20  Iteration 1094/5640 Training loss: 1.5146 0.6619 sec/batch\n",
      "Epoch 4/20  Iteration 1095/5640 Training loss: 1.5145 0.6985 sec/batch\n",
      "Epoch 4/20  Iteration 1096/5640 Training loss: 1.5143 0.6460 sec/batch\n",
      "Epoch 4/20  Iteration 1097/5640 Training loss: 1.5140 0.7504 sec/batch\n",
      "Epoch 4/20  Iteration 1098/5640 Training loss: 1.5138 0.6716 sec/batch\n",
      "Epoch 4/20  Iteration 1099/5640 Training loss: 1.5135 0.7089 sec/batch\n",
      "Epoch 4/20  Iteration 1100/5640 Training loss: 1.5133 0.7410 sec/batch\n",
      "Validation loss: 1.37854 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 1101/5640 Training loss: 1.5133 0.6700 sec/batch\n",
      "Epoch 4/20  Iteration 1102/5640 Training loss: 1.5130 0.7095 sec/batch\n",
      "Epoch 4/20  Iteration 1103/5640 Training loss: 1.5127 0.6917 sec/batch\n",
      "Epoch 4/20  Iteration 1104/5640 Training loss: 1.5126 0.6781 sec/batch\n",
      "Epoch 4/20  Iteration 1105/5640 Training loss: 1.5125 0.7148 sec/batch\n",
      "Epoch 4/20  Iteration 1106/5640 Training loss: 1.5124 0.6881 sec/batch\n",
      "Epoch 4/20  Iteration 1107/5640 Training loss: 1.5122 0.6430 sec/batch\n",
      "Epoch 4/20  Iteration 1108/5640 Training loss: 1.5120 0.7261 sec/batch\n",
      "Epoch 4/20  Iteration 1109/5640 Training loss: 1.5119 0.6097 sec/batch\n",
      "Epoch 4/20  Iteration 1110/5640 Training loss: 1.5117 0.6593 sec/batch\n",
      "Epoch 4/20  Iteration 1111/5640 Training loss: 1.5115 0.6931 sec/batch\n",
      "Epoch 4/20  Iteration 1112/5640 Training loss: 1.5112 0.6709 sec/batch\n",
      "Epoch 4/20  Iteration 1113/5640 Training loss: 1.5110 0.7017 sec/batch\n",
      "Epoch 4/20  Iteration 1114/5640 Training loss: 1.5107 0.6950 sec/batch\n",
      "Epoch 4/20  Iteration 1115/5640 Training loss: 1.5106 0.6784 sec/batch\n",
      "Epoch 4/20  Iteration 1116/5640 Training loss: 1.5103 0.7037 sec/batch\n",
      "Epoch 4/20  Iteration 1117/5640 Training loss: 1.5101 0.6888 sec/batch\n",
      "Epoch 4/20  Iteration 1118/5640 Training loss: 1.5098 0.6641 sec/batch\n",
      "Epoch 4/20  Iteration 1119/5640 Training loss: 1.5097 0.7147 sec/batch\n",
      "Epoch 4/20  Iteration 1120/5640 Training loss: 1.5095 0.5705 sec/batch\n",
      "Epoch 4/20  Iteration 1121/5640 Training loss: 1.5093 0.6757 sec/batch\n",
      "Epoch 4/20  Iteration 1122/5640 Training loss: 1.5091 0.5969 sec/batch\n",
      "Epoch 4/20  Iteration 1123/5640 Training loss: 1.5089 0.6369 sec/batch\n",
      "Epoch 4/20  Iteration 1124/5640 Training loss: 1.5088 0.5774 sec/batch\n",
      "Epoch 4/20  Iteration 1125/5640 Training loss: 1.5086 0.6203 sec/batch\n",
      "Epoch 4/20  Iteration 1126/5640 Training loss: 1.5085 0.6424 sec/batch\n",
      "Epoch 4/20  Iteration 1127/5640 Training loss: 1.5082 0.5611 sec/batch\n",
      "Epoch 4/20  Iteration 1128/5640 Training loss: 1.5080 0.6535 sec/batch\n",
      "Epoch 5/20  Iteration 1129/5640 Training loss: 1.5267 0.6208 sec/batch\n",
      "Epoch 5/20  Iteration 1130/5640 Training loss: 1.5007 0.5592 sec/batch\n",
      "Epoch 5/20  Iteration 1131/5640 Training loss: 1.4858 0.6598 sec/batch\n",
      "Epoch 5/20  Iteration 1132/5640 Training loss: 1.4830 0.6137 sec/batch\n",
      "Epoch 5/20  Iteration 1133/5640 Training loss: 1.4777 0.6019 sec/batch\n",
      "Epoch 5/20  Iteration 1134/5640 Training loss: 1.4777 0.6438 sec/batch\n",
      "Epoch 5/20  Iteration 1135/5640 Training loss: 1.4706 0.5763 sec/batch\n",
      "Epoch 5/20  Iteration 1136/5640 Training loss: 1.4678 0.7111 sec/batch\n",
      "Epoch 5/20  Iteration 1137/5640 Training loss: 1.4650 0.6205 sec/batch\n",
      "Epoch 5/20  Iteration 1138/5640 Training loss: 1.4603 0.6427 sec/batch\n",
      "Epoch 5/20  Iteration 1139/5640 Training loss: 1.4589 0.5555 sec/batch\n",
      "Epoch 5/20  Iteration 1140/5640 Training loss: 1.4560 0.6480 sec/batch\n",
      "Epoch 5/20  Iteration 1141/5640 Training loss: 1.4536 0.6161 sec/batch\n",
      "Epoch 5/20  Iteration 1142/5640 Training loss: 1.4549 0.6358 sec/batch\n",
      "Epoch 5/20  Iteration 1143/5640 Training loss: 1.4535 0.6290 sec/batch\n",
      "Epoch 5/20  Iteration 1144/5640 Training loss: 1.4541 0.6077 sec/batch\n",
      "Epoch 5/20  Iteration 1145/5640 Training loss: 1.4531 0.6488 sec/batch\n",
      "Epoch 5/20  Iteration 1146/5640 Training loss: 1.4511 0.5530 sec/batch\n",
      "Epoch 5/20  Iteration 1147/5640 Training loss: 1.4496 0.6509 sec/batch\n",
      "Epoch 5/20  Iteration 1148/5640 Training loss: 1.4486 0.6102 sec/batch\n",
      "Epoch 5/20  Iteration 1149/5640 Training loss: 1.4479 0.5747 sec/batch\n",
      "Epoch 5/20  Iteration 1150/5640 Training loss: 1.4470 0.6506 sec/batch\n",
      "Validation loss: 1.35081 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1151/5640 Training loss: 1.4500 0.6448 sec/batch\n",
      "Epoch 5/20  Iteration 1152/5640 Training loss: 1.4483 0.5535 sec/batch\n",
      "Epoch 5/20  Iteration 1153/5640 Training loss: 1.4482 0.5686 sec/batch\n",
      "Epoch 5/20  Iteration 1154/5640 Training loss: 1.4482 0.6693 sec/batch\n",
      "Epoch 5/20  Iteration 1155/5640 Training loss: 1.4477 0.5616 sec/batch\n",
      "Epoch 5/20  Iteration 1156/5640 Training loss: 1.4481 0.6486 sec/batch\n",
      "Epoch 5/20  Iteration 1157/5640 Training loss: 1.4480 0.5796 sec/batch\n",
      "Epoch 5/20  Iteration 1158/5640 Training loss: 1.4474 0.6404 sec/batch\n",
      "Epoch 5/20  Iteration 1159/5640 Training loss: 1.4458 0.5961 sec/batch\n",
      "Epoch 5/20  Iteration 1160/5640 Training loss: 1.4453 0.4592 sec/batch\n",
      "Epoch 5/20  Iteration 1161/5640 Training loss: 1.4444 0.4971 sec/batch\n",
      "Epoch 5/20  Iteration 1162/5640 Training loss: 1.4441 0.5019 sec/batch\n",
      "Epoch 5/20  Iteration 1163/5640 Training loss: 1.4442 0.6749 sec/batch\n",
      "Epoch 5/20  Iteration 1164/5640 Training loss: 1.4442 0.5928 sec/batch\n",
      "Epoch 5/20  Iteration 1165/5640 Training loss: 1.4434 0.6421 sec/batch\n",
      "Epoch 5/20  Iteration 1166/5640 Training loss: 1.4424 0.5617 sec/batch\n",
      "Epoch 5/20  Iteration 1167/5640 Training loss: 1.4417 0.6641 sec/batch\n",
      "Epoch 5/20  Iteration 1168/5640 Training loss: 1.4413 0.5756 sec/batch\n",
      "Epoch 5/20  Iteration 1169/5640 Training loss: 1.4406 0.5828 sec/batch\n",
      "Epoch 5/20  Iteration 1170/5640 Training loss: 1.4411 0.6585 sec/batch\n",
      "Epoch 5/20  Iteration 1171/5640 Training loss: 1.4403 0.5971 sec/batch\n",
      "Epoch 5/20  Iteration 1172/5640 Training loss: 1.4401 0.6530 sec/batch\n",
      "Epoch 5/20  Iteration 1173/5640 Training loss: 1.4394 0.6207 sec/batch\n",
      "Epoch 5/20  Iteration 1174/5640 Training loss: 1.4391 0.6249 sec/batch\n",
      "Epoch 5/20  Iteration 1175/5640 Training loss: 1.4390 0.5519 sec/batch\n",
      "Epoch 5/20  Iteration 1176/5640 Training loss: 1.4388 0.6407 sec/batch\n",
      "Epoch 5/20  Iteration 1177/5640 Training loss: 1.4387 0.4736 sec/batch\n",
      "Epoch 5/20  Iteration 1178/5640 Training loss: 1.4383 0.3867 sec/batch\n",
      "Epoch 5/20  Iteration 1179/5640 Training loss: 1.4382 0.3820 sec/batch\n",
      "Epoch 5/20  Iteration 1180/5640 Training loss: 1.4381 0.3816 sec/batch\n",
      "Epoch 5/20  Iteration 1181/5640 Training loss: 1.4380 0.3835 sec/batch\n",
      "Epoch 5/20  Iteration 1182/5640 Training loss: 1.4380 0.3816 sec/batch\n",
      "Epoch 5/20  Iteration 1183/5640 Training loss: 1.4374 0.3844 sec/batch\n",
      "Epoch 5/20  Iteration 1184/5640 Training loss: 1.4371 0.3900 sec/batch\n",
      "Epoch 5/20  Iteration 1185/5640 Training loss: 1.4367 0.3964 sec/batch\n",
      "Epoch 5/20  Iteration 1186/5640 Training loss: 1.4367 0.4032 sec/batch\n",
      "Epoch 5/20  Iteration 1187/5640 Training loss: 1.4366 0.4112 sec/batch\n",
      "Epoch 5/20  Iteration 1188/5640 Training loss: 1.4362 0.4153 sec/batch\n",
      "Epoch 5/20  Iteration 1189/5640 Training loss: 1.4356 0.4143 sec/batch\n",
      "Epoch 5/20  Iteration 1190/5640 Training loss: 1.4348 0.4123 sec/batch\n",
      "Epoch 5/20  Iteration 1191/5640 Training loss: 1.4342 0.4137 sec/batch\n",
      "Epoch 5/20  Iteration 1192/5640 Training loss: 1.4336 0.4133 sec/batch\n",
      "Epoch 5/20  Iteration 1193/5640 Training loss: 1.4338 0.4127 sec/batch\n",
      "Epoch 5/20  Iteration 1194/5640 Training loss: 1.4336 0.4141 sec/batch\n",
      "Epoch 5/20  Iteration 1195/5640 Training loss: 1.4335 0.4141 sec/batch\n",
      "Epoch 5/20  Iteration 1196/5640 Training loss: 1.4329 0.4138 sec/batch\n",
      "Epoch 5/20  Iteration 1197/5640 Training loss: 1.4324 0.4148 sec/batch\n",
      "Epoch 5/20  Iteration 1198/5640 Training loss: 1.4317 0.4152 sec/batch\n",
      "Epoch 5/20  Iteration 1199/5640 Training loss: 1.4314 0.4151 sec/batch\n",
      "Epoch 5/20  Iteration 1200/5640 Training loss: 1.4311 0.4172 sec/batch\n",
      "Validation loss: 1.33057 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1201/5640 Training loss: 1.4324 0.3918 sec/batch\n",
      "Epoch 5/20  Iteration 1202/5640 Training loss: 1.4324 0.3903 sec/batch\n",
      "Epoch 5/20  Iteration 1203/5640 Training loss: 1.4324 0.3914 sec/batch\n",
      "Epoch 5/20  Iteration 1204/5640 Training loss: 1.4330 0.3901 sec/batch\n",
      "Epoch 5/20  Iteration 1205/5640 Training loss: 1.4329 0.3913 sec/batch\n",
      "Epoch 5/20  Iteration 1206/5640 Training loss: 1.4334 0.3942 sec/batch\n",
      "Epoch 5/20  Iteration 1207/5640 Training loss: 1.4333 0.3996 sec/batch\n",
      "Epoch 5/20  Iteration 1208/5640 Training loss: 1.4335 0.3990 sec/batch\n",
      "Epoch 5/20  Iteration 1209/5640 Training loss: 1.4337 0.3969 sec/batch\n",
      "Epoch 5/20  Iteration 1210/5640 Training loss: 1.4333 0.3986 sec/batch\n",
      "Epoch 5/20  Iteration 1211/5640 Training loss: 1.4331 0.4059 sec/batch\n",
      "Epoch 5/20  Iteration 1212/5640 Training loss: 1.4327 0.4079 sec/batch\n",
      "Epoch 5/20  Iteration 1213/5640 Training loss: 1.4324 0.4066 sec/batch\n",
      "Epoch 5/20  Iteration 1214/5640 Training loss: 1.4323 0.4063 sec/batch\n",
      "Epoch 5/20  Iteration 1215/5640 Training loss: 1.4319 0.4066 sec/batch\n",
      "Epoch 5/20  Iteration 1216/5640 Training loss: 1.4315 0.4060 sec/batch\n",
      "Epoch 5/20  Iteration 1217/5640 Training loss: 1.4314 0.4068 sec/batch\n",
      "Epoch 5/20  Iteration 1218/5640 Training loss: 1.4310 0.4061 sec/batch\n",
      "Epoch 5/20  Iteration 1219/5640 Training loss: 1.4308 0.4076 sec/batch\n",
      "Epoch 5/20  Iteration 1220/5640 Training loss: 1.4307 0.4088 sec/batch\n",
      "Epoch 5/20  Iteration 1221/5640 Training loss: 1.4306 0.4080 sec/batch\n",
      "Epoch 5/20  Iteration 1222/5640 Training loss: 1.4303 0.4066 sec/batch\n",
      "Epoch 5/20  Iteration 1223/5640 Training loss: 1.4299 0.4067 sec/batch\n",
      "Epoch 5/20  Iteration 1224/5640 Training loss: 1.4298 0.4072 sec/batch\n",
      "Epoch 5/20  Iteration 1225/5640 Training loss: 1.4298 0.4053 sec/batch\n",
      "Epoch 5/20  Iteration 1226/5640 Training loss: 1.4296 0.4054 sec/batch\n",
      "Epoch 5/20  Iteration 1227/5640 Training loss: 1.4294 0.4086 sec/batch\n",
      "Epoch 5/20  Iteration 1228/5640 Training loss: 1.4292 0.4072 sec/batch\n",
      "Epoch 5/20  Iteration 1229/5640 Training loss: 1.4287 0.4098 sec/batch\n",
      "Epoch 5/20  Iteration 1230/5640 Training loss: 1.4284 0.4087 sec/batch\n",
      "Epoch 5/20  Iteration 1231/5640 Training loss: 1.4281 0.4064 sec/batch\n",
      "Epoch 5/20  Iteration 1232/5640 Training loss: 1.4275 0.4078 sec/batch\n",
      "Epoch 5/20  Iteration 1233/5640 Training loss: 1.4272 0.4080 sec/batch\n",
      "Epoch 5/20  Iteration 1234/5640 Training loss: 1.4267 0.4072 sec/batch\n",
      "Epoch 5/20  Iteration 1235/5640 Training loss: 1.4262 0.4098 sec/batch\n",
      "Epoch 5/20  Iteration 1236/5640 Training loss: 1.4255 0.4092 sec/batch\n",
      "Epoch 5/20  Iteration 1237/5640 Training loss: 1.4252 0.4101 sec/batch\n",
      "Epoch 5/20  Iteration 1238/5640 Training loss: 1.4249 0.4085 sec/batch\n",
      "Epoch 5/20  Iteration 1239/5640 Training loss: 1.4244 0.4087 sec/batch\n",
      "Epoch 5/20  Iteration 1240/5640 Training loss: 1.4243 0.4078 sec/batch\n",
      "Epoch 5/20  Iteration 1241/5640 Training loss: 1.4239 0.4104 sec/batch\n",
      "Epoch 5/20  Iteration 1242/5640 Training loss: 1.4237 0.4090 sec/batch\n",
      "Epoch 5/20  Iteration 1243/5640 Training loss: 1.4235 0.4081 sec/batch\n",
      "Epoch 5/20  Iteration 1244/5640 Training loss: 1.4234 0.4080 sec/batch\n",
      "Epoch 5/20  Iteration 1245/5640 Training loss: 1.4231 0.4091 sec/batch\n",
      "Epoch 5/20  Iteration 1246/5640 Training loss: 1.4227 0.4080 sec/batch\n",
      "Epoch 5/20  Iteration 1247/5640 Training loss: 1.4224 0.4069 sec/batch\n",
      "Epoch 5/20  Iteration 1248/5640 Training loss: 1.4221 0.4069 sec/batch\n",
      "Epoch 5/20  Iteration 1249/5640 Training loss: 1.4219 0.4092 sec/batch\n",
      "Epoch 5/20  Iteration 1250/5640 Training loss: 1.4219 0.4063 sec/batch\n",
      "Validation loss: 1.31309 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1251/5640 Training loss: 1.4223 0.3881 sec/batch\n",
      "Epoch 5/20  Iteration 1252/5640 Training loss: 1.4221 0.3840 sec/batch\n",
      "Epoch 5/20  Iteration 1253/5640 Training loss: 1.4222 0.3852 sec/batch\n",
      "Epoch 5/20  Iteration 1254/5640 Training loss: 1.4217 0.3837 sec/batch\n",
      "Epoch 5/20  Iteration 1255/5640 Training loss: 1.4216 0.3866 sec/batch\n",
      "Epoch 5/20  Iteration 1256/5640 Training loss: 1.4212 0.3878 sec/batch\n",
      "Epoch 5/20  Iteration 1257/5640 Training loss: 1.4209 0.3885 sec/batch\n",
      "Epoch 5/20  Iteration 1258/5640 Training loss: 1.4206 0.3870 sec/batch\n",
      "Epoch 5/20  Iteration 1259/5640 Training loss: 1.4204 0.3891 sec/batch\n",
      "Epoch 5/20  Iteration 1260/5640 Training loss: 1.4198 0.3899 sec/batch\n",
      "Epoch 5/20  Iteration 1261/5640 Training loss: 1.4194 0.3952 sec/batch\n",
      "Epoch 5/20  Iteration 1262/5640 Training loss: 1.4190 0.3954 sec/batch\n",
      "Epoch 5/20  Iteration 1263/5640 Training loss: 1.4188 0.3987 sec/batch\n",
      "Epoch 5/20  Iteration 1264/5640 Training loss: 1.4186 0.4001 sec/batch\n",
      "Epoch 5/20  Iteration 1265/5640 Training loss: 1.4183 0.3991 sec/batch\n",
      "Epoch 5/20  Iteration 1266/5640 Training loss: 1.4178 0.3998 sec/batch\n",
      "Epoch 5/20  Iteration 1267/5640 Training loss: 1.4177 0.4015 sec/batch\n",
      "Epoch 5/20  Iteration 1268/5640 Training loss: 1.4175 0.3997 sec/batch\n",
      "Epoch 5/20  Iteration 1269/5640 Training loss: 1.4171 0.4011 sec/batch\n",
      "Epoch 5/20  Iteration 1270/5640 Training loss: 1.4171 0.3999 sec/batch\n",
      "Epoch 5/20  Iteration 1271/5640 Training loss: 1.4169 0.3995 sec/batch\n",
      "Epoch 5/20  Iteration 1272/5640 Training loss: 1.4165 0.3993 sec/batch\n",
      "Epoch 5/20  Iteration 1273/5640 Training loss: 1.4160 0.3999 sec/batch\n",
      "Epoch 5/20  Iteration 1274/5640 Training loss: 1.4155 0.4030 sec/batch\n",
      "Epoch 5/20  Iteration 1275/5640 Training loss: 1.4152 0.4003 sec/batch\n",
      "Epoch 5/20  Iteration 1276/5640 Training loss: 1.4150 0.3996 sec/batch\n",
      "Epoch 5/20  Iteration 1277/5640 Training loss: 1.4148 0.3996 sec/batch\n",
      "Epoch 5/20  Iteration 1278/5640 Training loss: 1.4146 0.4000 sec/batch\n",
      "Epoch 5/20  Iteration 1279/5640 Training loss: 1.4143 0.4004 sec/batch\n",
      "Epoch 5/20  Iteration 1280/5640 Training loss: 1.4142 0.3997 sec/batch\n",
      "Epoch 5/20  Iteration 1281/5640 Training loss: 1.4142 0.4005 sec/batch\n",
      "Epoch 5/20  Iteration 1282/5640 Training loss: 1.4141 0.4008 sec/batch\n",
      "Epoch 5/20  Iteration 1283/5640 Training loss: 1.4139 0.4006 sec/batch\n",
      "Epoch 5/20  Iteration 1284/5640 Training loss: 1.4139 0.4039 sec/batch\n",
      "Epoch 5/20  Iteration 1285/5640 Training loss: 1.4135 0.4050 sec/batch\n",
      "Epoch 5/20  Iteration 1286/5640 Training loss: 1.4133 0.4070 sec/batch\n",
      "Epoch 5/20  Iteration 1287/5640 Training loss: 1.4129 0.4056 sec/batch\n",
      "Epoch 5/20  Iteration 1288/5640 Training loss: 1.4126 0.4057 sec/batch\n",
      "Epoch 5/20  Iteration 1289/5640 Training loss: 1.4124 0.4088 sec/batch\n",
      "Epoch 5/20  Iteration 1290/5640 Training loss: 1.4122 0.4068 sec/batch\n",
      "Epoch 5/20  Iteration 1291/5640 Training loss: 1.4119 0.4084 sec/batch\n",
      "Epoch 5/20  Iteration 1292/5640 Training loss: 1.4116 0.4074 sec/batch\n",
      "Epoch 5/20  Iteration 1293/5640 Training loss: 1.4117 0.4074 sec/batch\n",
      "Epoch 5/20  Iteration 1294/5640 Training loss: 1.4118 0.4077 sec/batch\n",
      "Epoch 5/20  Iteration 1295/5640 Training loss: 1.4116 0.4083 sec/batch\n",
      "Epoch 5/20  Iteration 1296/5640 Training loss: 1.4115 0.4080 sec/batch\n",
      "Epoch 5/20  Iteration 1297/5640 Training loss: 1.4114 0.4094 sec/batch\n",
      "Epoch 5/20  Iteration 1298/5640 Training loss: 1.4113 0.4092 sec/batch\n",
      "Epoch 5/20  Iteration 1299/5640 Training loss: 1.4111 0.4078 sec/batch\n",
      "Epoch 5/20  Iteration 1300/5640 Training loss: 1.4108 0.4074 sec/batch\n",
      "Validation loss: 1.29772 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1301/5640 Training loss: 1.4112 0.3863 sec/batch\n",
      "Epoch 5/20  Iteration 1302/5640 Training loss: 1.4109 0.3830 sec/batch\n",
      "Epoch 5/20  Iteration 1303/5640 Training loss: 1.4108 0.3832 sec/batch\n",
      "Epoch 5/20  Iteration 1304/5640 Training loss: 1.4106 0.3850 sec/batch\n",
      "Epoch 5/20  Iteration 1305/5640 Training loss: 1.4107 0.3872 sec/batch\n",
      "Epoch 5/20  Iteration 1306/5640 Training loss: 1.4103 0.3908 sec/batch\n",
      "Epoch 5/20  Iteration 1307/5640 Training loss: 1.4103 0.4173 sec/batch\n",
      "Epoch 5/20  Iteration 1308/5640 Training loss: 1.4103 0.4230 sec/batch\n",
      "Epoch 5/20  Iteration 1309/5640 Training loss: 1.4101 0.3893 sec/batch\n",
      "Epoch 5/20  Iteration 1310/5640 Training loss: 1.4101 0.3939 sec/batch\n",
      "Epoch 5/20  Iteration 1311/5640 Training loss: 1.4101 0.3935 sec/batch\n",
      "Epoch 5/20  Iteration 1312/5640 Training loss: 1.4098 0.3932 sec/batch\n",
      "Epoch 5/20  Iteration 1313/5640 Training loss: 1.4096 0.3927 sec/batch\n",
      "Epoch 5/20  Iteration 1314/5640 Training loss: 1.4093 0.3958 sec/batch\n",
      "Epoch 5/20  Iteration 1315/5640 Training loss: 1.4088 0.3927 sec/batch\n",
      "Epoch 5/20  Iteration 1316/5640 Training loss: 1.4084 0.3939 sec/batch\n",
      "Epoch 5/20  Iteration 1317/5640 Training loss: 1.4081 0.3933 sec/batch\n",
      "Epoch 5/20  Iteration 1318/5640 Training loss: 1.4076 0.3940 sec/batch\n",
      "Epoch 5/20  Iteration 1319/5640 Training loss: 1.4075 0.5457 sec/batch\n",
      "Epoch 5/20  Iteration 1320/5640 Training loss: 1.4072 0.3930 sec/batch\n",
      "Epoch 5/20  Iteration 1321/5640 Training loss: 1.4069 0.3968 sec/batch\n",
      "Epoch 5/20  Iteration 1322/5640 Training loss: 1.4068 0.3965 sec/batch\n",
      "Epoch 5/20  Iteration 1323/5640 Training loss: 1.4067 0.3981 sec/batch\n",
      "Epoch 5/20  Iteration 1324/5640 Training loss: 1.4067 0.3963 sec/batch\n",
      "Epoch 5/20  Iteration 1325/5640 Training loss: 1.4064 0.3966 sec/batch\n",
      "Epoch 5/20  Iteration 1326/5640 Training loss: 1.4064 0.3964 sec/batch\n",
      "Epoch 5/20  Iteration 1327/5640 Training loss: 1.4062 0.3965 sec/batch\n",
      "Epoch 5/20  Iteration 1328/5640 Training loss: 1.4060 0.3991 sec/batch\n",
      "Epoch 5/20  Iteration 1329/5640 Training loss: 1.4059 0.4165 sec/batch\n",
      "Epoch 5/20  Iteration 1330/5640 Training loss: 1.4058 0.4052 sec/batch\n",
      "Epoch 5/20  Iteration 1331/5640 Training loss: 1.4056 0.4381 sec/batch\n",
      "Epoch 5/20  Iteration 1332/5640 Training loss: 1.4055 0.4051 sec/batch\n",
      "Epoch 5/20  Iteration 1333/5640 Training loss: 1.4052 0.4124 sec/batch\n",
      "Epoch 5/20  Iteration 1334/5640 Training loss: 1.4052 0.3991 sec/batch\n",
      "Epoch 5/20  Iteration 1335/5640 Training loss: 1.4048 0.3985 sec/batch\n",
      "Epoch 5/20  Iteration 1336/5640 Training loss: 1.4045 0.4005 sec/batch\n",
      "Epoch 5/20  Iteration 1337/5640 Training loss: 1.4044 0.4371 sec/batch\n",
      "Epoch 5/20  Iteration 1338/5640 Training loss: 1.4041 0.4134 sec/batch\n",
      "Epoch 5/20  Iteration 1339/5640 Training loss: 1.4037 0.4256 sec/batch\n",
      "Epoch 5/20  Iteration 1340/5640 Training loss: 1.4035 0.4387 sec/batch\n",
      "Epoch 5/20  Iteration 1341/5640 Training loss: 1.4035 0.4063 sec/batch\n",
      "Epoch 5/20  Iteration 1342/5640 Training loss: 1.4033 0.4051 sec/batch\n",
      "Epoch 5/20  Iteration 1343/5640 Training loss: 1.4031 0.4353 sec/batch\n",
      "Epoch 5/20  Iteration 1344/5640 Training loss: 1.4029 0.4117 sec/batch\n",
      "Epoch 5/20  Iteration 1345/5640 Training loss: 1.4027 0.4016 sec/batch\n",
      "Epoch 5/20  Iteration 1346/5640 Training loss: 1.4023 0.4251 sec/batch\n",
      "Epoch 5/20  Iteration 1347/5640 Training loss: 1.4020 0.4326 sec/batch\n",
      "Epoch 5/20  Iteration 1348/5640 Training loss: 1.4017 0.4083 sec/batch\n",
      "Epoch 5/20  Iteration 1349/5640 Training loss: 1.4015 0.4094 sec/batch\n",
      "Epoch 5/20  Iteration 1350/5640 Training loss: 1.4013 0.4087 sec/batch\n",
      "Validation loss: 1.28915 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1351/5640 Training loss: 1.4017 0.3898 sec/batch\n",
      "Epoch 5/20  Iteration 1352/5640 Training loss: 1.4016 0.3898 sec/batch\n",
      "Epoch 5/20  Iteration 1353/5640 Training loss: 1.4015 0.3888 sec/batch\n",
      "Epoch 5/20  Iteration 1354/5640 Training loss: 1.4012 0.3889 sec/batch\n",
      "Epoch 5/20  Iteration 1355/5640 Training loss: 1.4011 0.3885 sec/batch\n",
      "Epoch 5/20  Iteration 1356/5640 Training loss: 1.4011 0.3901 sec/batch\n",
      "Epoch 5/20  Iteration 1357/5640 Training loss: 1.4011 0.3883 sec/batch\n",
      "Epoch 5/20  Iteration 1358/5640 Training loss: 1.4009 0.3881 sec/batch\n",
      "Epoch 5/20  Iteration 1359/5640 Training loss: 1.4008 0.3916 sec/batch\n",
      "Epoch 5/20  Iteration 1360/5640 Training loss: 1.4007 0.3958 sec/batch\n",
      "Epoch 5/20  Iteration 1361/5640 Training loss: 1.4005 0.3982 sec/batch\n",
      "Epoch 5/20  Iteration 1362/5640 Training loss: 1.4002 0.3997 sec/batch\n",
      "Epoch 5/20  Iteration 1363/5640 Training loss: 1.4001 0.4019 sec/batch\n",
      "Epoch 5/20  Iteration 1364/5640 Training loss: 1.4000 0.4010 sec/batch\n",
      "Epoch 5/20  Iteration 1365/5640 Training loss: 1.3998 0.4009 sec/batch\n",
      "Epoch 5/20  Iteration 1366/5640 Training loss: 1.3998 0.3992 sec/batch\n",
      "Epoch 5/20  Iteration 1367/5640 Training loss: 1.3997 0.4006 sec/batch\n",
      "Epoch 5/20  Iteration 1368/5640 Training loss: 1.3994 0.4004 sec/batch\n",
      "Epoch 5/20  Iteration 1369/5640 Training loss: 1.3993 0.4000 sec/batch\n",
      "Epoch 5/20  Iteration 1370/5640 Training loss: 1.3992 0.4007 sec/batch\n",
      "Epoch 5/20  Iteration 1371/5640 Training loss: 1.3990 0.4019 sec/batch\n",
      "Epoch 5/20  Iteration 1372/5640 Training loss: 1.3988 0.4004 sec/batch\n",
      "Epoch 5/20  Iteration 1373/5640 Training loss: 1.3985 0.4002 sec/batch\n",
      "Epoch 5/20  Iteration 1374/5640 Training loss: 1.3983 0.3980 sec/batch\n",
      "Epoch 5/20  Iteration 1375/5640 Training loss: 1.3981 0.3996 sec/batch\n",
      "Epoch 5/20  Iteration 1376/5640 Training loss: 1.3981 0.3990 sec/batch\n",
      "Epoch 5/20  Iteration 1377/5640 Training loss: 1.3980 0.3979 sec/batch\n",
      "Epoch 5/20  Iteration 1378/5640 Training loss: 1.3979 0.3972 sec/batch\n",
      "Epoch 5/20  Iteration 1379/5640 Training loss: 1.3976 0.3975 sec/batch\n",
      "Epoch 5/20  Iteration 1380/5640 Training loss: 1.3975 0.3973 sec/batch\n",
      "Epoch 5/20  Iteration 1381/5640 Training loss: 1.3972 0.3979 sec/batch\n",
      "Epoch 5/20  Iteration 1382/5640 Training loss: 1.3971 0.3988 sec/batch\n",
      "Epoch 5/20  Iteration 1383/5640 Training loss: 1.3969 0.3975 sec/batch\n",
      "Epoch 5/20  Iteration 1384/5640 Training loss: 1.3967 0.3994 sec/batch\n",
      "Epoch 5/20  Iteration 1385/5640 Training loss: 1.3965 0.3974 sec/batch\n",
      "Epoch 5/20  Iteration 1386/5640 Training loss: 1.3964 0.3949 sec/batch\n",
      "Epoch 5/20  Iteration 1387/5640 Training loss: 1.3963 0.3976 sec/batch\n",
      "Epoch 5/20  Iteration 1388/5640 Training loss: 1.3963 0.3974 sec/batch\n",
      "Epoch 5/20  Iteration 1389/5640 Training loss: 1.3961 0.3980 sec/batch\n",
      "Epoch 5/20  Iteration 1390/5640 Training loss: 1.3960 0.3990 sec/batch\n",
      "Epoch 5/20  Iteration 1391/5640 Training loss: 1.3959 0.3979 sec/batch\n",
      "Epoch 5/20  Iteration 1392/5640 Training loss: 1.3958 0.3977 sec/batch\n",
      "Epoch 5/20  Iteration 1393/5640 Training loss: 1.3958 0.3982 sec/batch\n",
      "Epoch 5/20  Iteration 1394/5640 Training loss: 1.3955 0.3970 sec/batch\n",
      "Epoch 5/20  Iteration 1395/5640 Training loss: 1.3953 0.3981 sec/batch\n",
      "Epoch 5/20  Iteration 1396/5640 Training loss: 1.3951 0.4027 sec/batch\n",
      "Epoch 5/20  Iteration 1397/5640 Training loss: 1.3950 0.4024 sec/batch\n",
      "Epoch 5/20  Iteration 1398/5640 Training loss: 1.3948 0.3991 sec/batch\n",
      "Epoch 5/20  Iteration 1399/5640 Training loss: 1.3946 0.4019 sec/batch\n",
      "Epoch 5/20  Iteration 1400/5640 Training loss: 1.3944 0.4012 sec/batch\n",
      "Validation loss: 1.27398 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 1401/5640 Training loss: 1.3947 0.3877 sec/batch\n",
      "Epoch 5/20  Iteration 1402/5640 Training loss: 1.3945 0.3846 sec/batch\n",
      "Epoch 5/20  Iteration 1403/5640 Training loss: 1.3944 0.3856 sec/batch\n",
      "Epoch 5/20  Iteration 1404/5640 Training loss: 1.3942 0.3853 sec/batch\n",
      "Epoch 5/20  Iteration 1405/5640 Training loss: 1.3941 0.3845 sec/batch\n",
      "Epoch 5/20  Iteration 1406/5640 Training loss: 1.3941 0.3862 sec/batch\n",
      "Epoch 5/20  Iteration 1407/5640 Training loss: 1.3940 0.3908 sec/batch\n",
      "Epoch 5/20  Iteration 1408/5640 Training loss: 1.3939 0.3907 sec/batch\n",
      "Epoch 5/20  Iteration 1409/5640 Training loss: 1.3936 0.3917 sec/batch\n",
      "Epoch 5/20  Iteration 1410/5640 Training loss: 1.3935 0.3906 sec/batch\n",
      "Epoch 6/20  Iteration 1411/5640 Training loss: 1.4498 0.3908 sec/batch\n",
      "Epoch 6/20  Iteration 1412/5640 Training loss: 1.4124 0.3900 sec/batch\n",
      "Epoch 6/20  Iteration 1413/5640 Training loss: 1.3971 0.3892 sec/batch\n",
      "Epoch 6/20  Iteration 1414/5640 Training loss: 1.3938 0.3944 sec/batch\n",
      "Epoch 6/20  Iteration 1415/5640 Training loss: 1.3868 0.3959 sec/batch\n",
      "Epoch 6/20  Iteration 1416/5640 Training loss: 1.3846 0.3989 sec/batch\n",
      "Epoch 6/20  Iteration 1417/5640 Training loss: 1.3784 0.3961 sec/batch\n",
      "Epoch 6/20  Iteration 1418/5640 Training loss: 1.3768 0.4000 sec/batch\n",
      "Epoch 6/20  Iteration 1419/5640 Training loss: 1.3732 0.3998 sec/batch\n",
      "Epoch 6/20  Iteration 1420/5640 Training loss: 1.3688 0.4016 sec/batch\n",
      "Epoch 6/20  Iteration 1421/5640 Training loss: 1.3680 0.4011 sec/batch\n",
      "Epoch 6/20  Iteration 1422/5640 Training loss: 1.3643 0.4038 sec/batch\n",
      "Epoch 6/20  Iteration 1423/5640 Training loss: 1.3633 0.4011 sec/batch\n",
      "Epoch 6/20  Iteration 1424/5640 Training loss: 1.3645 0.4014 sec/batch\n",
      "Epoch 6/20  Iteration 1425/5640 Training loss: 1.3625 0.3995 sec/batch\n",
      "Epoch 6/20  Iteration 1426/5640 Training loss: 1.3630 0.4001 sec/batch\n",
      "Epoch 6/20  Iteration 1427/5640 Training loss: 1.3622 0.4016 sec/batch\n",
      "Epoch 6/20  Iteration 1428/5640 Training loss: 1.3604 0.4008 sec/batch\n",
      "Epoch 6/20  Iteration 1429/5640 Training loss: 1.3587 0.4003 sec/batch\n",
      "Epoch 6/20  Iteration 1430/5640 Training loss: 1.3574 0.4027 sec/batch\n",
      "Epoch 6/20  Iteration 1431/5640 Training loss: 1.3561 0.4006 sec/batch\n",
      "Epoch 6/20  Iteration 1432/5640 Training loss: 1.3556 0.4008 sec/batch\n",
      "Epoch 6/20  Iteration 1433/5640 Training loss: 1.3553 0.4005 sec/batch\n",
      "Epoch 6/20  Iteration 1434/5640 Training loss: 1.3537 0.4019 sec/batch\n",
      "Epoch 6/20  Iteration 1435/5640 Training loss: 1.3536 0.4023 sec/batch\n",
      "Epoch 6/20  Iteration 1436/5640 Training loss: 1.3534 0.4007 sec/batch\n",
      "Epoch 6/20  Iteration 1437/5640 Training loss: 1.3532 0.4013 sec/batch\n",
      "Epoch 6/20  Iteration 1438/5640 Training loss: 1.3538 0.3993 sec/batch\n",
      "Epoch 6/20  Iteration 1439/5640 Training loss: 1.3537 0.4014 sec/batch\n",
      "Epoch 6/20  Iteration 1440/5640 Training loss: 1.3532 0.4011 sec/batch\n",
      "Epoch 6/20  Iteration 1441/5640 Training loss: 1.3518 0.4009 sec/batch\n",
      "Epoch 6/20  Iteration 1442/5640 Training loss: 1.3513 0.3994 sec/batch\n",
      "Epoch 6/20  Iteration 1443/5640 Training loss: 1.3504 0.4009 sec/batch\n",
      "Epoch 6/20  Iteration 1444/5640 Training loss: 1.3500 0.4003 sec/batch\n",
      "Epoch 6/20  Iteration 1445/5640 Training loss: 1.3500 0.4025 sec/batch\n",
      "Epoch 6/20  Iteration 1446/5640 Training loss: 1.3502 0.4007 sec/batch\n",
      "Epoch 6/20  Iteration 1447/5640 Training loss: 1.3492 0.4008 sec/batch\n",
      "Epoch 6/20  Iteration 1448/5640 Training loss: 1.3485 0.4012 sec/batch\n",
      "Epoch 6/20  Iteration 1449/5640 Training loss: 1.3483 0.4011 sec/batch\n",
      "Epoch 6/20  Iteration 1450/5640 Training loss: 1.3479 0.3990 sec/batch\n",
      "Validation loss: 1.26257 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1451/5640 Training loss: 1.3501 0.3844 sec/batch\n",
      "Epoch 6/20  Iteration 1452/5640 Training loss: 1.3507 0.3855 sec/batch\n",
      "Epoch 6/20  Iteration 1453/5640 Training loss: 1.3498 0.3837 sec/batch\n",
      "Epoch 6/20  Iteration 1454/5640 Training loss: 1.3495 0.3867 sec/batch\n",
      "Epoch 6/20  Iteration 1455/5640 Training loss: 1.3491 0.3854 sec/batch\n",
      "Epoch 6/20  Iteration 1456/5640 Training loss: 1.3486 0.3867 sec/batch\n",
      "Epoch 6/20  Iteration 1457/5640 Training loss: 1.3486 0.3837 sec/batch\n",
      "Epoch 6/20  Iteration 1458/5640 Training loss: 1.3487 0.3890 sec/batch\n",
      "Epoch 6/20  Iteration 1459/5640 Training loss: 1.3488 0.3895 sec/batch\n",
      "Epoch 6/20  Iteration 1460/5640 Training loss: 1.3486 0.3927 sec/batch\n",
      "Epoch 6/20  Iteration 1461/5640 Training loss: 1.3484 0.3970 sec/batch\n",
      "Epoch 6/20  Iteration 1462/5640 Training loss: 1.3482 0.3983 sec/batch\n",
      "Epoch 6/20  Iteration 1463/5640 Training loss: 1.3481 0.3974 sec/batch\n",
      "Epoch 6/20  Iteration 1464/5640 Training loss: 1.3482 0.3968 sec/batch\n",
      "Epoch 6/20  Iteration 1465/5640 Training loss: 1.3479 0.3976 sec/batch\n",
      "Epoch 6/20  Iteration 1466/5640 Training loss: 1.3477 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1467/5640 Training loss: 1.3474 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 1468/5640 Training loss: 1.3476 0.3968 sec/batch\n",
      "Epoch 6/20  Iteration 1469/5640 Training loss: 1.3476 0.3975 sec/batch\n",
      "Epoch 6/20  Iteration 1470/5640 Training loss: 1.3472 0.3985 sec/batch\n",
      "Epoch 6/20  Iteration 1471/5640 Training loss: 1.3467 0.3991 sec/batch\n",
      "Epoch 6/20  Iteration 1472/5640 Training loss: 1.3462 0.3974 sec/batch\n",
      "Epoch 6/20  Iteration 1473/5640 Training loss: 1.3458 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1474/5640 Training loss: 1.3453 0.3979 sec/batch\n",
      "Epoch 6/20  Iteration 1475/5640 Training loss: 1.3456 0.3967 sec/batch\n",
      "Epoch 6/20  Iteration 1476/5640 Training loss: 1.3454 0.3979 sec/batch\n",
      "Epoch 6/20  Iteration 1477/5640 Training loss: 1.3453 0.3993 sec/batch\n",
      "Epoch 6/20  Iteration 1478/5640 Training loss: 1.3449 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 1479/5640 Training loss: 1.3445 0.3973 sec/batch\n",
      "Epoch 6/20  Iteration 1480/5640 Training loss: 1.3439 0.3965 sec/batch\n",
      "Epoch 6/20  Iteration 1481/5640 Training loss: 1.3437 0.3970 sec/batch\n",
      "Epoch 6/20  Iteration 1482/5640 Training loss: 1.3434 0.3975 sec/batch\n",
      "Epoch 6/20  Iteration 1483/5640 Training loss: 1.3436 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1484/5640 Training loss: 1.3438 0.3979 sec/batch\n",
      "Epoch 6/20  Iteration 1485/5640 Training loss: 1.3439 0.4002 sec/batch\n",
      "Epoch 6/20  Iteration 1486/5640 Training loss: 1.3444 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 1487/5640 Training loss: 1.3442 0.3982 sec/batch\n",
      "Epoch 6/20  Iteration 1488/5640 Training loss: 1.3449 0.3967 sec/batch\n",
      "Epoch 6/20  Iteration 1489/5640 Training loss: 1.3448 0.3970 sec/batch\n",
      "Epoch 6/20  Iteration 1490/5640 Training loss: 1.3452 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 1491/5640 Training loss: 1.3453 0.3982 sec/batch\n",
      "Epoch 6/20  Iteration 1492/5640 Training loss: 1.3449 0.3982 sec/batch\n",
      "Epoch 6/20  Iteration 1493/5640 Training loss: 1.3447 0.3983 sec/batch\n",
      "Epoch 6/20  Iteration 1494/5640 Training loss: 1.3444 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1495/5640 Training loss: 1.3441 0.3972 sec/batch\n",
      "Epoch 6/20  Iteration 1496/5640 Training loss: 1.3440 0.3986 sec/batch\n",
      "Epoch 6/20  Iteration 1497/5640 Training loss: 1.3436 0.3975 sec/batch\n",
      "Epoch 6/20  Iteration 1498/5640 Training loss: 1.3433 0.3996 sec/batch\n",
      "Epoch 6/20  Iteration 1499/5640 Training loss: 1.3432 0.4015 sec/batch\n",
      "Epoch 6/20  Iteration 1500/5640 Training loss: 1.3430 0.4037 sec/batch\n",
      "Validation loss: 1.25411 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1501/5640 Training loss: 1.3443 0.3850 sec/batch\n",
      "Epoch 6/20  Iteration 1502/5640 Training loss: 1.3442 0.3840 sec/batch\n",
      "Epoch 6/20  Iteration 1503/5640 Training loss: 1.3442 0.3828 sec/batch\n",
      "Epoch 6/20  Iteration 1504/5640 Training loss: 1.3439 0.3809 sec/batch\n",
      "Epoch 6/20  Iteration 1505/5640 Training loss: 1.3436 0.3849 sec/batch\n",
      "Epoch 6/20  Iteration 1506/5640 Training loss: 1.3436 0.3851 sec/batch\n",
      "Epoch 6/20  Iteration 1507/5640 Training loss: 1.3436 0.3853 sec/batch\n",
      "Epoch 6/20  Iteration 1508/5640 Training loss: 1.3435 0.3856 sec/batch\n",
      "Epoch 6/20  Iteration 1509/5640 Training loss: 1.3434 0.3904 sec/batch\n",
      "Epoch 6/20  Iteration 1510/5640 Training loss: 1.3432 0.3918 sec/batch\n",
      "Epoch 6/20  Iteration 1511/5640 Training loss: 1.3427 0.3921 sec/batch\n",
      "Epoch 6/20  Iteration 1512/5640 Training loss: 1.3425 0.3951 sec/batch\n",
      "Epoch 6/20  Iteration 1513/5640 Training loss: 1.3422 0.3938 sec/batch\n",
      "Epoch 6/20  Iteration 1514/5640 Training loss: 1.3417 0.3938 sec/batch\n",
      "Epoch 6/20  Iteration 1515/5640 Training loss: 1.3414 0.3935 sec/batch\n",
      "Epoch 6/20  Iteration 1516/5640 Training loss: 1.3409 0.3943 sec/batch\n",
      "Epoch 6/20  Iteration 1517/5640 Training loss: 1.3405 0.3952 sec/batch\n",
      "Epoch 6/20  Iteration 1518/5640 Training loss: 1.3400 0.3938 sec/batch\n",
      "Epoch 6/20  Iteration 1519/5640 Training loss: 1.3398 0.3937 sec/batch\n",
      "Epoch 6/20  Iteration 1520/5640 Training loss: 1.3396 0.3944 sec/batch\n",
      "Epoch 6/20  Iteration 1521/5640 Training loss: 1.3393 0.3972 sec/batch\n",
      "Epoch 6/20  Iteration 1522/5640 Training loss: 1.3392 0.3929 sec/batch\n",
      "Epoch 6/20  Iteration 1523/5640 Training loss: 1.3390 0.6606 sec/batch\n",
      "Epoch 6/20  Iteration 1524/5640 Training loss: 1.3388 0.3901 sec/batch\n",
      "Epoch 6/20  Iteration 1525/5640 Training loss: 1.3387 0.3883 sec/batch\n",
      "Epoch 6/20  Iteration 1526/5640 Training loss: 1.3386 0.3901 sec/batch\n",
      "Epoch 6/20  Iteration 1527/5640 Training loss: 1.3385 0.3906 sec/batch\n",
      "Epoch 6/20  Iteration 1528/5640 Training loss: 1.3382 0.3921 sec/batch\n",
      "Epoch 6/20  Iteration 1529/5640 Training loss: 1.3379 0.3942 sec/batch\n",
      "Epoch 6/20  Iteration 1530/5640 Training loss: 1.3375 0.3941 sec/batch\n",
      "Epoch 6/20  Iteration 1531/5640 Training loss: 1.3374 0.3948 sec/batch\n",
      "Epoch 6/20  Iteration 1532/5640 Training loss: 1.3374 0.3945 sec/batch\n",
      "Epoch 6/20  Iteration 1533/5640 Training loss: 1.3370 0.3966 sec/batch\n",
      "Epoch 6/20  Iteration 1534/5640 Training loss: 1.3369 0.3957 sec/batch\n",
      "Epoch 6/20  Iteration 1535/5640 Training loss: 1.3372 0.3930 sec/batch\n",
      "Epoch 6/20  Iteration 1536/5640 Training loss: 1.3368 0.3930 sec/batch\n",
      "Epoch 6/20  Iteration 1537/5640 Training loss: 1.3366 0.3934 sec/batch\n",
      "Epoch 6/20  Iteration 1538/5640 Training loss: 1.3363 0.3934 sec/batch\n",
      "Epoch 6/20  Iteration 1539/5640 Training loss: 1.3360 0.3969 sec/batch\n",
      "Epoch 6/20  Iteration 1540/5640 Training loss: 1.3358 0.3927 sec/batch\n",
      "Epoch 6/20  Iteration 1541/5640 Training loss: 1.3356 0.3957 sec/batch\n",
      "Epoch 6/20  Iteration 1542/5640 Training loss: 1.3352 0.3973 sec/batch\n",
      "Epoch 6/20  Iteration 1543/5640 Training loss: 1.3347 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 1544/5640 Training loss: 1.3344 0.3964 sec/batch\n",
      "Epoch 6/20  Iteration 1545/5640 Training loss: 1.3342 0.3980 sec/batch\n",
      "Epoch 6/20  Iteration 1546/5640 Training loss: 1.3341 0.3982 sec/batch\n",
      "Epoch 6/20  Iteration 1547/5640 Training loss: 1.3339 0.3991 sec/batch\n",
      "Epoch 6/20  Iteration 1548/5640 Training loss: 1.3336 0.3962 sec/batch\n",
      "Epoch 6/20  Iteration 1549/5640 Training loss: 1.3334 0.3976 sec/batch\n",
      "Epoch 6/20  Iteration 1550/5640 Training loss: 1.3334 0.3982 sec/batch\n",
      "Validation loss: 1.24507 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1551/5640 Training loss: 1.3340 0.3853 sec/batch\n",
      "Epoch 6/20  Iteration 1552/5640 Training loss: 1.3340 0.3861 sec/batch\n",
      "Epoch 6/20  Iteration 1553/5640 Training loss: 1.3340 0.3852 sec/batch\n",
      "Epoch 6/20  Iteration 1554/5640 Training loss: 1.3337 0.3852 sec/batch\n",
      "Epoch 6/20  Iteration 1555/5640 Training loss: 1.3334 0.3853 sec/batch\n",
      "Epoch 6/20  Iteration 1556/5640 Training loss: 1.3330 0.3858 sec/batch\n",
      "Epoch 6/20  Iteration 1557/5640 Training loss: 1.3327 0.3887 sec/batch\n",
      "Epoch 6/20  Iteration 1558/5640 Training loss: 1.3328 0.3917 sec/batch\n",
      "Epoch 6/20  Iteration 1559/5640 Training loss: 1.3327 0.3883 sec/batch\n",
      "Epoch 6/20  Iteration 1560/5640 Training loss: 1.3326 0.3912 sec/batch\n",
      "Epoch 6/20  Iteration 1561/5640 Training loss: 1.3325 0.3933 sec/batch\n",
      "Epoch 6/20  Iteration 1562/5640 Training loss: 1.3325 0.3942 sec/batch\n",
      "Epoch 6/20  Iteration 1563/5640 Training loss: 1.3326 0.3932 sec/batch\n",
      "Epoch 6/20  Iteration 1564/5640 Training loss: 1.3325 0.3955 sec/batch\n",
      "Epoch 6/20  Iteration 1565/5640 Training loss: 1.3323 0.3944 sec/batch\n",
      "Epoch 6/20  Iteration 1566/5640 Training loss: 1.3324 0.3938 sec/batch\n",
      "Epoch 6/20  Iteration 1567/5640 Training loss: 1.3321 0.3943 sec/batch\n",
      "Epoch 6/20  Iteration 1568/5640 Training loss: 1.3321 0.3960 sec/batch\n",
      "Epoch 6/20  Iteration 1569/5640 Training loss: 1.3318 0.3938 sec/batch\n",
      "Epoch 6/20  Iteration 1570/5640 Training loss: 1.3316 0.3950 sec/batch\n",
      "Epoch 6/20  Iteration 1571/5640 Training loss: 1.3314 0.3960 sec/batch\n",
      "Epoch 6/20  Iteration 1572/5640 Training loss: 1.3312 0.3953 sec/batch\n",
      "Epoch 6/20  Iteration 1573/5640 Training loss: 1.3310 0.3954 sec/batch\n",
      "Epoch 6/20  Iteration 1574/5640 Training loss: 1.3308 0.3949 sec/batch\n",
      "Epoch 6/20  Iteration 1575/5640 Training loss: 1.3308 0.5410 sec/batch\n",
      "Epoch 6/20  Iteration 1576/5640 Training loss: 1.3310 0.3933 sec/batch\n",
      "Epoch 6/20  Iteration 1577/5640 Training loss: 1.3308 0.4024 sec/batch\n",
      "Epoch 6/20  Iteration 1578/5640 Training loss: 1.3307 0.3956 sec/batch\n",
      "Epoch 6/20  Iteration 1579/5640 Training loss: 1.3307 0.3974 sec/batch\n",
      "Epoch 6/20  Iteration 1580/5640 Training loss: 1.3307 0.3933 sec/batch\n",
      "Epoch 6/20  Iteration 1581/5640 Training loss: 1.3305 0.3951 sec/batch\n",
      "Epoch 6/20  Iteration 1582/5640 Training loss: 1.3303 0.4028 sec/batch\n",
      "Epoch 6/20  Iteration 1583/5640 Training loss: 1.3301 0.3968 sec/batch\n",
      "Epoch 6/20  Iteration 1584/5640 Training loss: 1.3300 0.3934 sec/batch\n",
      "Epoch 6/20  Iteration 1585/5640 Training loss: 1.3300 0.3941 sec/batch\n",
      "Epoch 6/20  Iteration 1586/5640 Training loss: 1.3299 0.3948 sec/batch\n",
      "Epoch 6/20  Iteration 1587/5640 Training loss: 1.3299 0.3965 sec/batch\n",
      "Epoch 6/20  Iteration 1588/5640 Training loss: 1.3296 0.3928 sec/batch\n",
      "Epoch 6/20  Iteration 1589/5640 Training loss: 1.3297 0.3943 sec/batch\n",
      "Epoch 6/20  Iteration 1590/5640 Training loss: 1.3297 0.3941 sec/batch\n",
      "Epoch 6/20  Iteration 1591/5640 Training loss: 1.3297 0.3927 sec/batch\n",
      "Epoch 6/20  Iteration 1592/5640 Training loss: 1.3297 0.3944 sec/batch\n",
      "Epoch 6/20  Iteration 1593/5640 Training loss: 1.3297 0.3939 sec/batch\n",
      "Epoch 6/20  Iteration 1594/5640 Training loss: 1.3295 0.3965 sec/batch\n",
      "Epoch 6/20  Iteration 1595/5640 Training loss: 1.3294 0.3931 sec/batch\n",
      "Epoch 6/20  Iteration 1596/5640 Training loss: 1.3290 0.3968 sec/batch\n",
      "Epoch 6/20  Iteration 1597/5640 Training loss: 1.3286 0.3952 sec/batch\n",
      "Epoch 6/20  Iteration 1598/5640 Training loss: 1.3283 0.3947 sec/batch\n",
      "Epoch 6/20  Iteration 1599/5640 Training loss: 1.3280 0.3926 sec/batch\n",
      "Epoch 6/20  Iteration 1600/5640 Training loss: 1.3277 0.3952 sec/batch\n",
      "Validation loss: 1.23459 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1601/5640 Training loss: 1.3283 0.3834 sec/batch\n",
      "Epoch 6/20  Iteration 1602/5640 Training loss: 1.3282 0.3814 sec/batch\n",
      "Epoch 6/20  Iteration 1603/5640 Training loss: 1.3280 0.3839 sec/batch\n",
      "Epoch 6/20  Iteration 1604/5640 Training loss: 1.3279 0.3847 sec/batch\n",
      "Epoch 6/20  Iteration 1605/5640 Training loss: 1.3278 0.3827 sec/batch\n",
      "Epoch 6/20  Iteration 1606/5640 Training loss: 1.3277 0.3817 sec/batch\n",
      "Epoch 6/20  Iteration 1607/5640 Training loss: 1.3276 0.3842 sec/batch\n",
      "Epoch 6/20  Iteration 1608/5640 Training loss: 1.3276 0.3898 sec/batch\n",
      "Epoch 6/20  Iteration 1609/5640 Training loss: 1.3275 0.3889 sec/batch\n",
      "Epoch 6/20  Iteration 1610/5640 Training loss: 1.3273 0.3873 sec/batch\n",
      "Epoch 6/20  Iteration 1611/5640 Training loss: 1.3272 0.3896 sec/batch\n",
      "Epoch 6/20  Iteration 1612/5640 Training loss: 1.3271 0.3908 sec/batch\n",
      "Epoch 6/20  Iteration 1613/5640 Training loss: 1.3270 0.3907 sec/batch\n",
      "Epoch 6/20  Iteration 1614/5640 Training loss: 1.3270 0.3934 sec/batch\n",
      "Epoch 6/20  Iteration 1615/5640 Training loss: 1.3267 0.3956 sec/batch\n",
      "Epoch 6/20  Iteration 1616/5640 Training loss: 1.3267 0.3938 sec/batch\n",
      "Epoch 6/20  Iteration 1617/5640 Training loss: 1.3264 0.3937 sec/batch\n",
      "Epoch 6/20  Iteration 1618/5640 Training loss: 1.3262 0.3933 sec/batch\n",
      "Epoch 6/20  Iteration 1619/5640 Training loss: 1.3261 0.3944 sec/batch\n",
      "Epoch 6/20  Iteration 1620/5640 Training loss: 1.3259 0.3955 sec/batch\n",
      "Epoch 6/20  Iteration 1621/5640 Training loss: 1.3256 0.3955 sec/batch\n",
      "Epoch 6/20  Iteration 1622/5640 Training loss: 1.3255 0.3934 sec/batch\n",
      "Epoch 6/20  Iteration 1623/5640 Training loss: 1.3255 0.3958 sec/batch\n",
      "Epoch 6/20  Iteration 1624/5640 Training loss: 1.3254 0.3941 sec/batch\n",
      "Epoch 6/20  Iteration 1625/5640 Training loss: 1.3253 0.3964 sec/batch\n",
      "Epoch 6/20  Iteration 1626/5640 Training loss: 1.3252 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1627/5640 Training loss: 1.3250 0.3984 sec/batch\n",
      "Epoch 6/20  Iteration 1628/5640 Training loss: 1.3247 0.3980 sec/batch\n",
      "Epoch 6/20  Iteration 1629/5640 Training loss: 1.3245 0.3976 sec/batch\n",
      "Epoch 6/20  Iteration 1630/5640 Training loss: 1.3243 0.3991 sec/batch\n",
      "Epoch 6/20  Iteration 1631/5640 Training loss: 1.3242 0.3982 sec/batch\n",
      "Epoch 6/20  Iteration 1632/5640 Training loss: 1.3240 0.3975 sec/batch\n",
      "Epoch 6/20  Iteration 1633/5640 Training loss: 1.3239 0.3973 sec/batch\n",
      "Epoch 6/20  Iteration 1634/5640 Training loss: 1.3238 0.3985 sec/batch\n",
      "Epoch 6/20  Iteration 1635/5640 Training loss: 1.3237 0.3993 sec/batch\n",
      "Epoch 6/20  Iteration 1636/5640 Training loss: 1.3235 0.3976 sec/batch\n",
      "Epoch 6/20  Iteration 1637/5640 Training loss: 1.3234 0.3999 sec/batch\n",
      "Epoch 6/20  Iteration 1638/5640 Training loss: 1.3234 0.3970 sec/batch\n",
      "Epoch 6/20  Iteration 1639/5640 Training loss: 1.3233 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1640/5640 Training loss: 1.3232 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 1641/5640 Training loss: 1.3231 0.3965 sec/batch\n",
      "Epoch 6/20  Iteration 1642/5640 Training loss: 1.3231 0.3993 sec/batch\n",
      "Epoch 6/20  Iteration 1643/5640 Training loss: 1.3228 0.3965 sec/batch\n",
      "Epoch 6/20  Iteration 1644/5640 Training loss: 1.3226 0.3983 sec/batch\n",
      "Epoch 6/20  Iteration 1645/5640 Training loss: 1.3224 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1646/5640 Training loss: 1.3224 0.3986 sec/batch\n",
      "Epoch 6/20  Iteration 1647/5640 Training loss: 1.3223 0.3973 sec/batch\n",
      "Epoch 6/20  Iteration 1648/5640 Training loss: 1.3224 0.3979 sec/batch\n",
      "Epoch 6/20  Iteration 1649/5640 Training loss: 1.3222 0.3976 sec/batch\n",
      "Epoch 6/20  Iteration 1650/5640 Training loss: 1.3221 0.3981 sec/batch\n",
      "Validation loss: 1.23284 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1651/5640 Training loss: 1.3226 0.3842 sec/batch\n",
      "Epoch 6/20  Iteration 1652/5640 Training loss: 1.3225 0.3834 sec/batch\n",
      "Epoch 6/20  Iteration 1653/5640 Training loss: 1.3224 0.3827 sec/batch\n",
      "Epoch 6/20  Iteration 1654/5640 Training loss: 1.3222 0.3824 sec/batch\n",
      "Epoch 6/20  Iteration 1655/5640 Training loss: 1.3220 0.3854 sec/batch\n",
      "Epoch 6/20  Iteration 1656/5640 Training loss: 1.3219 0.3856 sec/batch\n",
      "Epoch 6/20  Iteration 1657/5640 Training loss: 1.3217 0.3883 sec/batch\n",
      "Epoch 6/20  Iteration 1658/5640 Training loss: 1.3218 0.3905 sec/batch\n",
      "Epoch 6/20  Iteration 1659/5640 Training loss: 1.3218 0.3960 sec/batch\n",
      "Epoch 6/20  Iteration 1660/5640 Training loss: 1.3217 0.3946 sec/batch\n",
      "Epoch 6/20  Iteration 1661/5640 Training loss: 1.3215 0.3952 sec/batch\n",
      "Epoch 6/20  Iteration 1662/5640 Training loss: 1.3214 0.3899 sec/batch\n",
      "Epoch 6/20  Iteration 1663/5640 Training loss: 1.3212 0.3968 sec/batch\n",
      "Epoch 6/20  Iteration 1664/5640 Training loss: 1.3212 0.3919 sec/batch\n",
      "Epoch 6/20  Iteration 1665/5640 Training loss: 1.3210 0.3905 sec/batch\n",
      "Epoch 6/20  Iteration 1666/5640 Training loss: 1.3209 0.3893 sec/batch\n",
      "Epoch 6/20  Iteration 1667/5640 Training loss: 1.3207 0.3914 sec/batch\n",
      "Epoch 6/20  Iteration 1668/5640 Training loss: 1.3207 0.3927 sec/batch\n",
      "Epoch 6/20  Iteration 1669/5640 Training loss: 1.3207 0.3956 sec/batch\n",
      "Epoch 6/20  Iteration 1670/5640 Training loss: 1.3207 0.3965 sec/batch\n",
      "Epoch 6/20  Iteration 1671/5640 Training loss: 1.3206 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1672/5640 Training loss: 1.3205 0.3963 sec/batch\n",
      "Epoch 6/20  Iteration 1673/5640 Training loss: 1.3205 0.3967 sec/batch\n",
      "Epoch 6/20  Iteration 1674/5640 Training loss: 1.3204 0.3967 sec/batch\n",
      "Epoch 6/20  Iteration 1675/5640 Training loss: 1.3204 0.3978 sec/batch\n",
      "Epoch 6/20  Iteration 1676/5640 Training loss: 1.3202 0.3970 sec/batch\n",
      "Epoch 6/20  Iteration 1677/5640 Training loss: 1.3201 0.3970 sec/batch\n",
      "Epoch 6/20  Iteration 1678/5640 Training loss: 1.3199 0.3971 sec/batch\n",
      "Epoch 6/20  Iteration 1679/5640 Training loss: 1.3198 0.3972 sec/batch\n",
      "Epoch 6/20  Iteration 1680/5640 Training loss: 1.3196 0.3967 sec/batch\n",
      "Epoch 6/20  Iteration 1681/5640 Training loss: 1.3195 0.3985 sec/batch\n",
      "Epoch 6/20  Iteration 1682/5640 Training loss: 1.3193 0.3977 sec/batch\n",
      "Epoch 6/20  Iteration 1683/5640 Training loss: 1.3193 0.3973 sec/batch\n",
      "Epoch 6/20  Iteration 1684/5640 Training loss: 1.3192 0.3985 sec/batch\n",
      "Epoch 6/20  Iteration 1685/5640 Training loss: 1.3191 0.3959 sec/batch\n",
      "Epoch 6/20  Iteration 1686/5640 Training loss: 1.3190 0.3968 sec/batch\n",
      "Epoch 6/20  Iteration 1687/5640 Training loss: 1.3189 0.3964 sec/batch\n",
      "Epoch 6/20  Iteration 1688/5640 Training loss: 1.3189 0.3968 sec/batch\n",
      "Epoch 6/20  Iteration 1689/5640 Training loss: 1.3188 0.3971 sec/batch\n",
      "Epoch 6/20  Iteration 1690/5640 Training loss: 1.3187 0.4018 sec/batch\n",
      "Epoch 6/20  Iteration 1691/5640 Training loss: 1.3185 0.4012 sec/batch\n",
      "Epoch 6/20  Iteration 1692/5640 Training loss: 1.3184 0.3980 sec/batch\n",
      "Epoch 7/20  Iteration 1693/5640 Training loss: 1.4043 0.3958 sec/batch\n",
      "Epoch 7/20  Iteration 1694/5640 Training loss: 1.3640 0.3971 sec/batch\n",
      "Epoch 7/20  Iteration 1695/5640 Training loss: 1.3460 0.4197 sec/batch\n",
      "Epoch 7/20  Iteration 1696/5640 Training loss: 1.3393 0.4122 sec/batch\n",
      "Epoch 7/20  Iteration 1697/5640 Training loss: 1.3347 0.3971 sec/batch\n",
      "Epoch 7/20  Iteration 1698/5640 Training loss: 1.3320 0.4017 sec/batch\n",
      "Epoch 7/20  Iteration 1699/5640 Training loss: 1.3256 0.4044 sec/batch\n",
      "Epoch 7/20  Iteration 1700/5640 Training loss: 1.3225 0.3985 sec/batch\n",
      "Validation loss: 1.22541 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1701/5640 Training loss: 1.3313 0.3813 sec/batch\n",
      "Epoch 7/20  Iteration 1702/5640 Training loss: 1.3259 0.3904 sec/batch\n",
      "Epoch 7/20  Iteration 1703/5640 Training loss: 1.3240 0.3821 sec/batch\n",
      "Epoch 7/20  Iteration 1704/5640 Training loss: 1.3199 0.3820 sec/batch\n",
      "Epoch 7/20  Iteration 1705/5640 Training loss: 1.3178 0.3829 sec/batch\n",
      "Epoch 7/20  Iteration 1706/5640 Training loss: 1.3178 0.3912 sec/batch\n",
      "Epoch 7/20  Iteration 1707/5640 Training loss: 1.3155 0.3858 sec/batch\n",
      "Epoch 7/20  Iteration 1708/5640 Training loss: 1.3156 0.3841 sec/batch\n",
      "Epoch 7/20  Iteration 1709/5640 Training loss: 1.3141 0.3859 sec/batch\n",
      "Epoch 7/20  Iteration 1710/5640 Training loss: 1.3119 0.3891 sec/batch\n",
      "Epoch 7/20  Iteration 1711/5640 Training loss: 1.3107 0.3888 sec/batch\n",
      "Epoch 7/20  Iteration 1712/5640 Training loss: 1.3088 0.3842 sec/batch\n",
      "Epoch 7/20  Iteration 1713/5640 Training loss: 1.3075 0.3885 sec/batch\n",
      "Epoch 7/20  Iteration 1714/5640 Training loss: 1.3065 0.3868 sec/batch\n",
      "Epoch 7/20  Iteration 1715/5640 Training loss: 1.3056 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1716/5640 Training loss: 1.3036 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1717/5640 Training loss: 1.3033 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1718/5640 Training loss: 1.3024 0.3951 sec/batch\n",
      "Epoch 7/20  Iteration 1719/5640 Training loss: 1.3021 0.3942 sec/batch\n",
      "Epoch 7/20  Iteration 1720/5640 Training loss: 1.3019 0.3940 sec/batch\n",
      "Epoch 7/20  Iteration 1721/5640 Training loss: 1.3014 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1722/5640 Training loss: 1.3009 0.3949 sec/batch\n",
      "Epoch 7/20  Iteration 1723/5640 Training loss: 1.2995 0.3946 sec/batch\n",
      "Epoch 7/20  Iteration 1724/5640 Training loss: 1.2985 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1725/5640 Training loss: 1.2974 0.3932 sec/batch\n",
      "Epoch 7/20  Iteration 1726/5640 Training loss: 1.2966 0.3946 sec/batch\n",
      "Epoch 7/20  Iteration 1727/5640 Training loss: 1.2963 0.3928 sec/batch\n",
      "Epoch 7/20  Iteration 1728/5640 Training loss: 1.2963 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1729/5640 Training loss: 1.2953 0.3936 sec/batch\n",
      "Epoch 7/20  Iteration 1730/5640 Training loss: 1.2942 0.3952 sec/batch\n",
      "Epoch 7/20  Iteration 1731/5640 Training loss: 1.2939 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1732/5640 Training loss: 1.2930 0.3931 sec/batch\n",
      "Epoch 7/20  Iteration 1733/5640 Training loss: 1.2923 0.3929 sec/batch\n",
      "Epoch 7/20  Iteration 1734/5640 Training loss: 1.2931 0.3965 sec/batch\n",
      "Epoch 7/20  Iteration 1735/5640 Training loss: 1.2921 0.3934 sec/batch\n",
      "Epoch 7/20  Iteration 1736/5640 Training loss: 1.2919 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1737/5640 Training loss: 1.2914 0.3941 sec/batch\n",
      "Epoch 7/20  Iteration 1738/5640 Training loss: 1.2909 0.3951 sec/batch\n",
      "Epoch 7/20  Iteration 1739/5640 Training loss: 1.2907 0.3943 sec/batch\n",
      "Epoch 7/20  Iteration 1740/5640 Training loss: 1.2906 0.3922 sec/batch\n",
      "Epoch 7/20  Iteration 1741/5640 Training loss: 1.2906 0.3920 sec/batch\n",
      "Epoch 7/20  Iteration 1742/5640 Training loss: 1.2903 0.3932 sec/batch\n",
      "Epoch 7/20  Iteration 1743/5640 Training loss: 1.2903 0.3931 sec/batch\n",
      "Epoch 7/20  Iteration 1744/5640 Training loss: 1.2901 0.3951 sec/batch\n",
      "Epoch 7/20  Iteration 1745/5640 Training loss: 1.2899 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1746/5640 Training loss: 1.2899 0.3947 sec/batch\n",
      "Epoch 7/20  Iteration 1747/5640 Training loss: 1.2895 0.4011 sec/batch\n",
      "Epoch 7/20  Iteration 1748/5640 Training loss: 1.2892 0.4061 sec/batch\n",
      "Epoch 7/20  Iteration 1749/5640 Training loss: 1.2888 0.4186 sec/batch\n",
      "Epoch 7/20  Iteration 1750/5640 Training loss: 1.2888 0.3977 sec/batch\n",
      "Validation loss: 1.20711 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1751/5640 Training loss: 1.2913 0.3885 sec/batch\n",
      "Epoch 7/20  Iteration 1752/5640 Training loss: 1.2910 0.3861 sec/batch\n",
      "Epoch 7/20  Iteration 1753/5640 Training loss: 1.2908 0.3868 sec/batch\n",
      "Epoch 7/20  Iteration 1754/5640 Training loss: 1.2902 0.3847 sec/batch\n",
      "Epoch 7/20  Iteration 1755/5640 Training loss: 1.2899 0.3813 sec/batch\n",
      "Epoch 7/20  Iteration 1756/5640 Training loss: 1.2895 0.3814 sec/batch\n",
      "Epoch 7/20  Iteration 1757/5640 Training loss: 1.2898 0.3826 sec/batch\n",
      "Epoch 7/20  Iteration 1758/5640 Training loss: 1.2896 0.3851 sec/batch\n",
      "Epoch 7/20  Iteration 1759/5640 Training loss: 1.2895 0.3999 sec/batch\n",
      "Epoch 7/20  Iteration 1760/5640 Training loss: 1.2891 0.4036 sec/batch\n",
      "Epoch 7/20  Iteration 1761/5640 Training loss: 1.2887 0.4220 sec/batch\n",
      "Epoch 7/20  Iteration 1762/5640 Training loss: 1.2880 0.4058 sec/batch\n",
      "Epoch 7/20  Iteration 1763/5640 Training loss: 1.2878 0.4155 sec/batch\n",
      "Epoch 7/20  Iteration 1764/5640 Training loss: 1.2875 0.4036 sec/batch\n",
      "Epoch 7/20  Iteration 1765/5640 Training loss: 1.2876 0.4241 sec/batch\n",
      "Epoch 7/20  Iteration 1766/5640 Training loss: 1.2878 0.4234 sec/batch\n",
      "Epoch 7/20  Iteration 1767/5640 Training loss: 1.2878 0.4328 sec/batch\n",
      "Epoch 7/20  Iteration 1768/5640 Training loss: 1.2882 0.4278 sec/batch\n",
      "Epoch 7/20  Iteration 1769/5640 Training loss: 1.2881 0.4170 sec/batch\n",
      "Epoch 7/20  Iteration 1770/5640 Training loss: 1.2887 0.4321 sec/batch\n",
      "Epoch 7/20  Iteration 1771/5640 Training loss: 1.2887 0.4286 sec/batch\n",
      "Epoch 7/20  Iteration 1772/5640 Training loss: 1.2890 0.3948 sec/batch\n",
      "Epoch 7/20  Iteration 1773/5640 Training loss: 1.2890 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1774/5640 Training loss: 1.2886 0.4100 sec/batch\n",
      "Epoch 7/20  Iteration 1775/5640 Training loss: 1.2883 0.4143 sec/batch\n",
      "Epoch 7/20  Iteration 1776/5640 Training loss: 1.2880 0.4237 sec/batch\n",
      "Epoch 7/20  Iteration 1777/5640 Training loss: 1.2876 0.3932 sec/batch\n",
      "Epoch 7/20  Iteration 1778/5640 Training loss: 1.2875 0.3931 sec/batch\n",
      "Epoch 7/20  Iteration 1779/5640 Training loss: 1.2871 0.4281 sec/batch\n",
      "Epoch 7/20  Iteration 1780/5640 Training loss: 1.2867 0.4183 sec/batch\n",
      "Epoch 7/20  Iteration 1781/5640 Training loss: 1.2866 0.4480 sec/batch\n",
      "Epoch 7/20  Iteration 1782/5640 Training loss: 1.2864 0.4508 sec/batch\n",
      "Epoch 7/20  Iteration 1783/5640 Training loss: 1.2864 0.4457 sec/batch\n",
      "Epoch 7/20  Iteration 1784/5640 Training loss: 1.2863 0.4418 sec/batch\n",
      "Epoch 7/20  Iteration 1785/5640 Training loss: 1.2863 0.4164 sec/batch\n",
      "Epoch 7/20  Iteration 1786/5640 Training loss: 1.2860 0.4201 sec/batch\n",
      "Epoch 7/20  Iteration 1787/5640 Training loss: 1.2857 0.3898 sec/batch\n",
      "Epoch 7/20  Iteration 1788/5640 Training loss: 1.2857 0.3964 sec/batch\n",
      "Epoch 7/20  Iteration 1789/5640 Training loss: 1.2856 0.3974 sec/batch\n",
      "Epoch 7/20  Iteration 1790/5640 Training loss: 1.2856 0.3895 sec/batch\n",
      "Epoch 7/20  Iteration 1791/5640 Training loss: 1.2853 0.3915 sec/batch\n",
      "Epoch 7/20  Iteration 1792/5640 Training loss: 1.2850 0.3901 sec/batch\n",
      "Epoch 7/20  Iteration 1793/5640 Training loss: 1.2846 0.3906 sec/batch\n",
      "Epoch 7/20  Iteration 1794/5640 Training loss: 1.2844 0.3893 sec/batch\n",
      "Epoch 7/20  Iteration 1795/5640 Training loss: 1.2842 0.3921 sec/batch\n",
      "Epoch 7/20  Iteration 1796/5640 Training loss: 1.2838 0.3897 sec/batch\n",
      "Epoch 7/20  Iteration 1797/5640 Training loss: 1.2835 0.3924 sec/batch\n",
      "Epoch 7/20  Iteration 1798/5640 Training loss: 1.2831 0.3951 sec/batch\n",
      "Epoch 7/20  Iteration 1799/5640 Training loss: 1.2828 0.4024 sec/batch\n",
      "Epoch 7/20  Iteration 1800/5640 Training loss: 1.2822 0.4020 sec/batch\n",
      "Validation loss: 1.20603 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1801/5640 Training loss: 1.2832 0.3817 sec/batch\n",
      "Epoch 7/20  Iteration 1802/5640 Training loss: 1.2830 0.3815 sec/batch\n",
      "Epoch 7/20  Iteration 1803/5640 Training loss: 1.2827 0.3813 sec/batch\n",
      "Epoch 7/20  Iteration 1804/5640 Training loss: 1.2826 0.3815 sec/batch\n",
      "Epoch 7/20  Iteration 1805/5640 Training loss: 1.2824 0.3817 sec/batch\n",
      "Epoch 7/20  Iteration 1806/5640 Training loss: 1.2822 0.3839 sec/batch\n",
      "Epoch 7/20  Iteration 1807/5640 Training loss: 1.2822 0.3883 sec/batch\n",
      "Epoch 7/20  Iteration 1808/5640 Training loss: 1.2822 0.3893 sec/batch\n",
      "Epoch 7/20  Iteration 1809/5640 Training loss: 1.2820 0.3864 sec/batch\n",
      "Epoch 7/20  Iteration 1810/5640 Training loss: 1.2818 0.3866 sec/batch\n",
      "Epoch 7/20  Iteration 1811/5640 Training loss: 1.2816 0.3871 sec/batch\n",
      "Epoch 7/20  Iteration 1812/5640 Training loss: 1.2813 0.3877 sec/batch\n",
      "Epoch 7/20  Iteration 1813/5640 Training loss: 1.2812 0.3874 sec/batch\n",
      "Epoch 7/20  Iteration 1814/5640 Training loss: 1.2813 0.3890 sec/batch\n",
      "Epoch 7/20  Iteration 1815/5640 Training loss: 1.2810 0.3905 sec/batch\n",
      "Epoch 7/20  Iteration 1816/5640 Training loss: 1.2809 0.3952 sec/batch\n",
      "Epoch 7/20  Iteration 1817/5640 Training loss: 1.2812 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1818/5640 Training loss: 1.2808 0.3936 sec/batch\n",
      "Epoch 7/20  Iteration 1819/5640 Training loss: 1.2807 0.3937 sec/batch\n",
      "Epoch 7/20  Iteration 1820/5640 Training loss: 1.2803 0.3944 sec/batch\n",
      "Epoch 7/20  Iteration 1821/5640 Training loss: 1.2800 0.3933 sec/batch\n",
      "Epoch 7/20  Iteration 1822/5640 Training loss: 1.2798 0.3942 sec/batch\n",
      "Epoch 7/20  Iteration 1823/5640 Training loss: 1.2797 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1824/5640 Training loss: 1.2792 0.3941 sec/batch\n",
      "Epoch 7/20  Iteration 1825/5640 Training loss: 1.2788 0.3940 sec/batch\n",
      "Epoch 7/20  Iteration 1826/5640 Training loss: 1.2785 0.3929 sec/batch\n",
      "Epoch 7/20  Iteration 1827/5640 Training loss: 1.2784 0.3940 sec/batch\n",
      "Epoch 7/20  Iteration 1828/5640 Training loss: 1.2783 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1829/5640 Training loss: 1.2782 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1830/5640 Training loss: 1.2779 0.3942 sec/batch\n",
      "Epoch 7/20  Iteration 1831/5640 Training loss: 1.2777 0.3958 sec/batch\n",
      "Epoch 7/20  Iteration 1832/5640 Training loss: 1.2777 0.3941 sec/batch\n",
      "Epoch 7/20  Iteration 1833/5640 Training loss: 1.2774 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1834/5640 Training loss: 1.2774 0.3936 sec/batch\n",
      "Epoch 7/20  Iteration 1835/5640 Training loss: 1.2773 0.3946 sec/batch\n",
      "Epoch 7/20  Iteration 1836/5640 Training loss: 1.2770 0.3928 sec/batch\n",
      "Epoch 7/20  Iteration 1837/5640 Training loss: 1.2766 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1838/5640 Training loss: 1.2761 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1839/5640 Training loss: 1.2758 0.3957 sec/batch\n",
      "Epoch 7/20  Iteration 1840/5640 Training loss: 1.2758 0.3936 sec/batch\n",
      "Epoch 7/20  Iteration 1841/5640 Training loss: 1.2758 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1842/5640 Training loss: 1.2755 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1843/5640 Training loss: 1.2753 0.3966 sec/batch\n",
      "Epoch 7/20  Iteration 1844/5640 Training loss: 1.2754 0.3969 sec/batch\n",
      "Epoch 7/20  Iteration 1845/5640 Training loss: 1.2755 0.3989 sec/batch\n",
      "Epoch 7/20  Iteration 1846/5640 Training loss: 1.2755 0.3978 sec/batch\n",
      "Epoch 7/20  Iteration 1847/5640 Training loss: 1.2753 0.3962 sec/batch\n",
      "Epoch 7/20  Iteration 1848/5640 Training loss: 1.2753 0.3964 sec/batch\n",
      "Epoch 7/20  Iteration 1849/5640 Training loss: 1.2751 0.3967 sec/batch\n",
      "Epoch 7/20  Iteration 1850/5640 Training loss: 1.2750 0.3970 sec/batch\n",
      "Validation loss: 1.20103 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1851/5640 Training loss: 1.2757 0.3842 sec/batch\n",
      "Epoch 7/20  Iteration 1852/5640 Training loss: 1.2755 0.3836 sec/batch\n",
      "Epoch 7/20  Iteration 1853/5640 Training loss: 1.2754 0.3820 sec/batch\n",
      "Epoch 7/20  Iteration 1854/5640 Training loss: 1.2753 0.3823 sec/batch\n",
      "Epoch 7/20  Iteration 1855/5640 Training loss: 1.2751 0.3853 sec/batch\n",
      "Epoch 7/20  Iteration 1856/5640 Training loss: 1.2749 0.3900 sec/batch\n",
      "Epoch 7/20  Iteration 1857/5640 Training loss: 1.2750 0.3883 sec/batch\n",
      "Epoch 7/20  Iteration 1858/5640 Training loss: 1.2752 0.3879 sec/batch\n",
      "Epoch 7/20  Iteration 1859/5640 Training loss: 1.2750 0.3892 sec/batch\n",
      "Epoch 7/20  Iteration 1860/5640 Training loss: 1.2750 0.3900 sec/batch\n",
      "Epoch 7/20  Iteration 1861/5640 Training loss: 1.2750 0.3896 sec/batch\n",
      "Epoch 7/20  Iteration 1862/5640 Training loss: 1.2750 0.3891 sec/batch\n",
      "Epoch 7/20  Iteration 1863/5640 Training loss: 1.2749 0.3899 sec/batch\n",
      "Epoch 7/20  Iteration 1864/5640 Training loss: 1.2747 0.3905 sec/batch\n",
      "Epoch 7/20  Iteration 1865/5640 Training loss: 1.2745 0.3913 sec/batch\n",
      "Epoch 7/20  Iteration 1866/5640 Training loss: 1.2744 0.3897 sec/batch\n",
      "Epoch 7/20  Iteration 1867/5640 Training loss: 1.2743 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1868/5640 Training loss: 1.2742 0.3947 sec/batch\n",
      "Epoch 7/20  Iteration 1869/5640 Training loss: 1.2742 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1870/5640 Training loss: 1.2740 0.3950 sec/batch\n",
      "Epoch 7/20  Iteration 1871/5640 Training loss: 1.2741 0.3929 sec/batch\n",
      "Epoch 7/20  Iteration 1872/5640 Training loss: 1.2742 0.3951 sec/batch\n",
      "Epoch 7/20  Iteration 1873/5640 Training loss: 1.2742 0.3925 sec/batch\n",
      "Epoch 7/20  Iteration 1874/5640 Training loss: 1.2742 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1875/5640 Training loss: 1.2742 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1876/5640 Training loss: 1.2740 0.3934 sec/batch\n",
      "Epoch 7/20  Iteration 1877/5640 Training loss: 1.2739 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1878/5640 Training loss: 1.2736 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1879/5640 Training loss: 1.2732 0.3945 sec/batch\n",
      "Epoch 7/20  Iteration 1880/5640 Training loss: 1.2729 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1881/5640 Training loss: 1.2727 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1882/5640 Training loss: 1.2724 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1883/5640 Training loss: 1.2723 0.3945 sec/batch\n",
      "Epoch 7/20  Iteration 1884/5640 Training loss: 1.2721 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1885/5640 Training loss: 1.2719 0.3933 sec/batch\n",
      "Epoch 7/20  Iteration 1886/5640 Training loss: 1.2719 0.3942 sec/batch\n",
      "Epoch 7/20  Iteration 1887/5640 Training loss: 1.2718 0.3946 sec/batch\n",
      "Epoch 7/20  Iteration 1888/5640 Training loss: 1.2718 0.3929 sec/batch\n",
      "Epoch 7/20  Iteration 1889/5640 Training loss: 1.2716 0.3960 sec/batch\n",
      "Epoch 7/20  Iteration 1890/5640 Training loss: 1.2717 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1891/5640 Training loss: 1.2716 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1892/5640 Training loss: 1.2714 0.3960 sec/batch\n",
      "Epoch 7/20  Iteration 1893/5640 Training loss: 1.2714 0.3975 sec/batch\n",
      "Epoch 7/20  Iteration 1894/5640 Training loss: 1.2713 0.4002 sec/batch\n",
      "Epoch 7/20  Iteration 1895/5640 Training loss: 1.2712 0.3976 sec/batch\n",
      "Epoch 7/20  Iteration 1896/5640 Training loss: 1.2712 0.3966 sec/batch\n",
      "Epoch 7/20  Iteration 1897/5640 Training loss: 1.2709 0.3969 sec/batch\n",
      "Epoch 7/20  Iteration 1898/5640 Training loss: 1.2709 0.3968 sec/batch\n",
      "Epoch 7/20  Iteration 1899/5640 Training loss: 1.2707 0.3977 sec/batch\n",
      "Epoch 7/20  Iteration 1900/5640 Training loss: 1.2705 0.3967 sec/batch\n",
      "Validation loss: 1.19268 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1901/5640 Training loss: 1.2711 0.3833 sec/batch\n",
      "Epoch 7/20  Iteration 1902/5640 Training loss: 1.2709 0.3816 sec/batch\n",
      "Epoch 7/20  Iteration 1903/5640 Training loss: 1.2707 0.3825 sec/batch\n",
      "Epoch 7/20  Iteration 1904/5640 Training loss: 1.2706 0.3835 sec/batch\n",
      "Epoch 7/20  Iteration 1905/5640 Training loss: 1.2706 0.3838 sec/batch\n",
      "Epoch 7/20  Iteration 1906/5640 Training loss: 1.2706 0.3850 sec/batch\n",
      "Epoch 7/20  Iteration 1907/5640 Training loss: 1.2705 0.3871 sec/batch\n",
      "Epoch 7/20  Iteration 1908/5640 Training loss: 1.2704 0.3888 sec/batch\n",
      "Epoch 7/20  Iteration 1909/5640 Training loss: 1.2703 0.3878 sec/batch\n",
      "Epoch 7/20  Iteration 1910/5640 Training loss: 1.2700 0.3877 sec/batch\n",
      "Epoch 7/20  Iteration 1911/5640 Training loss: 1.2698 0.3875 sec/batch\n",
      "Epoch 7/20  Iteration 1912/5640 Training loss: 1.2696 0.3889 sec/batch\n",
      "Epoch 7/20  Iteration 1913/5640 Training loss: 1.2695 0.3872 sec/batch\n",
      "Epoch 7/20  Iteration 1914/5640 Training loss: 1.2694 0.3882 sec/batch\n",
      "Epoch 7/20  Iteration 1915/5640 Training loss: 1.2693 0.3890 sec/batch\n",
      "Epoch 7/20  Iteration 1916/5640 Training loss: 1.2692 0.3897 sec/batch\n",
      "Epoch 7/20  Iteration 1917/5640 Training loss: 1.2692 0.3904 sec/batch\n",
      "Epoch 7/20  Iteration 1918/5640 Training loss: 1.2690 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1919/5640 Training loss: 1.2689 0.3940 sec/batch\n",
      "Epoch 7/20  Iteration 1920/5640 Training loss: 1.2690 0.3941 sec/batch\n",
      "Epoch 7/20  Iteration 1921/5640 Training loss: 1.2690 0.3939 sec/batch\n",
      "Epoch 7/20  Iteration 1922/5640 Training loss: 1.2689 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1923/5640 Training loss: 1.2688 0.3936 sec/batch\n",
      "Epoch 7/20  Iteration 1924/5640 Training loss: 1.2688 0.3943 sec/batch\n",
      "Epoch 7/20  Iteration 1925/5640 Training loss: 1.2686 0.3944 sec/batch\n",
      "Epoch 7/20  Iteration 1926/5640 Training loss: 1.2684 0.3936 sec/batch\n",
      "Epoch 7/20  Iteration 1927/5640 Training loss: 1.2684 0.3957 sec/batch\n",
      "Epoch 7/20  Iteration 1928/5640 Training loss: 1.2683 0.3925 sec/batch\n",
      "Epoch 7/20  Iteration 1929/5640 Training loss: 1.2683 0.3924 sec/batch\n",
      "Epoch 7/20  Iteration 1930/5640 Training loss: 1.2683 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1931/5640 Training loss: 1.2682 0.3946 sec/batch\n",
      "Epoch 7/20  Iteration 1932/5640 Training loss: 1.2680 0.3933 sec/batch\n",
      "Epoch 7/20  Iteration 1933/5640 Training loss: 1.2680 0.3945 sec/batch\n",
      "Epoch 7/20  Iteration 1934/5640 Training loss: 1.2679 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1935/5640 Training loss: 1.2677 0.3949 sec/batch\n",
      "Epoch 7/20  Iteration 1936/5640 Training loss: 1.2675 0.3924 sec/batch\n",
      "Epoch 7/20  Iteration 1937/5640 Training loss: 1.2673 0.3938 sec/batch\n",
      "Epoch 7/20  Iteration 1938/5640 Training loss: 1.2672 0.3932 sec/batch\n",
      "Epoch 7/20  Iteration 1939/5640 Training loss: 1.2671 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1940/5640 Training loss: 1.2670 0.3922 sec/batch\n",
      "Epoch 7/20  Iteration 1941/5640 Training loss: 1.2670 0.3937 sec/batch\n",
      "Epoch 7/20  Iteration 1942/5640 Training loss: 1.2669 0.3941 sec/batch\n",
      "Epoch 7/20  Iteration 1943/5640 Training loss: 1.2668 0.3954 sec/batch\n",
      "Epoch 7/20  Iteration 1944/5640 Training loss: 1.2667 0.3927 sec/batch\n",
      "Epoch 7/20  Iteration 1945/5640 Training loss: 1.2665 0.3937 sec/batch\n",
      "Epoch 7/20  Iteration 1946/5640 Training loss: 1.2664 0.3944 sec/batch\n",
      "Epoch 7/20  Iteration 1947/5640 Training loss: 1.2663 0.3931 sec/batch\n",
      "Epoch 7/20  Iteration 1948/5640 Training loss: 1.2662 0.3925 sec/batch\n",
      "Epoch 7/20  Iteration 1949/5640 Training loss: 1.2660 0.3934 sec/batch\n",
      "Epoch 7/20  Iteration 1950/5640 Training loss: 1.2660 0.3949 sec/batch\n",
      "Validation loss: 1.18882 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1951/5640 Training loss: 1.2665 0.3827 sec/batch\n",
      "Epoch 7/20  Iteration 1952/5640 Training loss: 1.2665 0.3819 sec/batch\n",
      "Epoch 7/20  Iteration 1953/5640 Training loss: 1.2665 0.3829 sec/batch\n",
      "Epoch 7/20  Iteration 1954/5640 Training loss: 1.2665 0.3818 sec/batch\n",
      "Epoch 7/20  Iteration 1955/5640 Training loss: 1.2665 0.3817 sec/batch\n",
      "Epoch 7/20  Iteration 1956/5640 Training loss: 1.2665 0.3811 sec/batch\n",
      "Epoch 7/20  Iteration 1957/5640 Training loss: 1.2665 0.3811 sec/batch\n",
      "Epoch 7/20  Iteration 1958/5640 Training loss: 1.2663 0.3818 sec/batch\n",
      "Epoch 7/20  Iteration 1959/5640 Training loss: 1.2662 0.3858 sec/batch\n",
      "Epoch 7/20  Iteration 1960/5640 Training loss: 1.2661 0.3856 sec/batch\n",
      "Epoch 7/20  Iteration 1961/5640 Training loss: 1.2660 0.3900 sec/batch\n",
      "Epoch 7/20  Iteration 1962/5640 Training loss: 1.2659 0.3926 sec/batch\n",
      "Epoch 7/20  Iteration 1963/5640 Training loss: 1.2659 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1964/5640 Training loss: 1.2657 0.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1965/5640 Training loss: 1.2657 0.3932 sec/batch\n",
      "Epoch 7/20  Iteration 1966/5640 Training loss: 1.2656 0.3937 sec/batch\n",
      "Epoch 7/20  Iteration 1967/5640 Training loss: 1.2655 0.3942 sec/batch\n",
      "Epoch 7/20  Iteration 1968/5640 Training loss: 1.2654 0.3952 sec/batch\n",
      "Epoch 7/20  Iteration 1969/5640 Training loss: 1.2654 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1970/5640 Training loss: 1.2654 0.3930 sec/batch\n",
      "Epoch 7/20  Iteration 1971/5640 Training loss: 1.2654 0.3940 sec/batch\n",
      "Epoch 7/20  Iteration 1972/5640 Training loss: 1.2654 0.3954 sec/batch\n",
      "Epoch 7/20  Iteration 1973/5640 Training loss: 1.2652 0.3933 sec/batch\n",
      "Epoch 7/20  Iteration 1974/5640 Training loss: 1.2651 0.3962 sec/batch\n",
      "Epoch 8/20  Iteration 1975/5640 Training loss: 1.3514 0.3964 sec/batch\n",
      "Epoch 8/20  Iteration 1976/5640 Training loss: 1.3101 0.3976 sec/batch\n",
      "Epoch 8/20  Iteration 1977/5640 Training loss: 1.2935 0.3962 sec/batch\n",
      "Epoch 8/20  Iteration 1978/5640 Training loss: 1.2856 0.3960 sec/batch\n",
      "Epoch 8/20  Iteration 1979/5640 Training loss: 1.2788 0.3969 sec/batch\n",
      "Epoch 8/20  Iteration 1980/5640 Training loss: 1.2795 0.3971 sec/batch\n",
      "Epoch 8/20  Iteration 1981/5640 Training loss: 1.2736 0.3962 sec/batch\n",
      "Epoch 8/20  Iteration 1982/5640 Training loss: 1.2718 0.3966 sec/batch\n",
      "Epoch 8/20  Iteration 1983/5640 Training loss: 1.2681 0.3983 sec/batch\n",
      "Epoch 8/20  Iteration 1984/5640 Training loss: 1.2644 0.3992 sec/batch\n",
      "Epoch 8/20  Iteration 1985/5640 Training loss: 1.2635 0.3952 sec/batch\n",
      "Epoch 8/20  Iteration 1986/5640 Training loss: 1.2603 0.3965 sec/batch\n",
      "Epoch 8/20  Iteration 1987/5640 Training loss: 1.2594 0.3966 sec/batch\n",
      "Epoch 8/20  Iteration 1988/5640 Training loss: 1.2596 0.3971 sec/batch\n",
      "Epoch 8/20  Iteration 1989/5640 Training loss: 1.2574 0.3973 sec/batch\n",
      "Epoch 8/20  Iteration 1990/5640 Training loss: 1.2579 0.3963 sec/batch\n",
      "Epoch 8/20  Iteration 1991/5640 Training loss: 1.2576 0.3980 sec/batch\n",
      "Epoch 8/20  Iteration 1992/5640 Training loss: 1.2560 0.3968 sec/batch\n",
      "Epoch 8/20  Iteration 1993/5640 Training loss: 1.2556 0.3967 sec/batch\n",
      "Epoch 8/20  Iteration 1994/5640 Training loss: 1.2538 0.3967 sec/batch\n",
      "Epoch 8/20  Iteration 1995/5640 Training loss: 1.2530 0.3961 sec/batch\n",
      "Epoch 8/20  Iteration 1996/5640 Training loss: 1.2523 0.3976 sec/batch\n",
      "Epoch 8/20  Iteration 1997/5640 Training loss: 1.2516 0.3988 sec/batch\n",
      "Epoch 8/20  Iteration 1998/5640 Training loss: 1.2504 0.3990 sec/batch\n",
      "Epoch 8/20  Iteration 1999/5640 Training loss: 1.2505 0.3970 sec/batch\n",
      "Epoch 8/20  Iteration 2000/5640 Training loss: 1.2500 0.3970 sec/batch\n",
      "Validation loss: 1.17823 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2001/5640 Training loss: 1.2547 0.3827 sec/batch\n",
      "Epoch 8/20  Iteration 2002/5640 Training loss: 1.2546 0.3811 sec/batch\n",
      "Epoch 8/20  Iteration 2003/5640 Training loss: 1.2542 0.3826 sec/batch\n",
      "Epoch 8/20  Iteration 2004/5640 Training loss: 1.2534 0.3823 sec/batch\n",
      "Epoch 8/20  Iteration 2005/5640 Training loss: 1.2521 0.3833 sec/batch\n",
      "Epoch 8/20  Iteration 2006/5640 Training loss: 1.2516 0.3821 sec/batch\n",
      "Epoch 8/20  Iteration 2007/5640 Training loss: 1.2505 0.3819 sec/batch\n",
      "Epoch 8/20  Iteration 2008/5640 Training loss: 1.2499 0.3825 sec/batch\n",
      "Epoch 8/20  Iteration 2009/5640 Training loss: 1.2501 0.3851 sec/batch\n",
      "Epoch 8/20  Iteration 2010/5640 Training loss: 1.2501 0.3819 sec/batch\n",
      "Epoch 8/20  Iteration 2011/5640 Training loss: 1.2490 0.3807 sec/batch\n",
      "Epoch 8/20  Iteration 2012/5640 Training loss: 1.2481 0.3825 sec/batch\n",
      "Epoch 8/20  Iteration 2013/5640 Training loss: 1.2475 0.3847 sec/batch\n",
      "Epoch 8/20  Iteration 2014/5640 Training loss: 1.2467 0.3866 sec/batch\n",
      "Epoch 8/20  Iteration 2015/5640 Training loss: 1.2458 0.3871 sec/batch\n",
      "Epoch 8/20  Iteration 2016/5640 Training loss: 1.2465 0.3883 sec/batch\n",
      "Epoch 8/20  Iteration 2017/5640 Training loss: 1.2455 0.3863 sec/batch\n",
      "Epoch 8/20  Iteration 2018/5640 Training loss: 1.2454 0.3866 sec/batch\n",
      "Epoch 8/20  Iteration 2019/5640 Training loss: 1.2449 0.3866 sec/batch\n",
      "Epoch 8/20  Iteration 2020/5640 Training loss: 1.2445 0.3890 sec/batch\n",
      "Epoch 8/20  Iteration 2021/5640 Training loss: 1.2445 0.3892 sec/batch\n",
      "Epoch 8/20  Iteration 2022/5640 Training loss: 1.2445 0.3911 sec/batch\n",
      "Epoch 8/20  Iteration 2023/5640 Training loss: 1.2446 0.3888 sec/batch\n",
      "Epoch 8/20  Iteration 2024/5640 Training loss: 1.2441 0.3910 sec/batch\n",
      "Epoch 8/20  Iteration 2025/5640 Training loss: 1.2440 0.3891 sec/batch\n",
      "Epoch 8/20  Iteration 2026/5640 Training loss: 1.2438 0.3894 sec/batch\n",
      "Epoch 8/20  Iteration 2027/5640 Training loss: 1.2437 0.3898 sec/batch\n",
      "Epoch 8/20  Iteration 2028/5640 Training loss: 1.2437 0.3894 sec/batch\n",
      "Epoch 8/20  Iteration 2029/5640 Training loss: 1.2432 0.3904 sec/batch\n",
      "Epoch 8/20  Iteration 2030/5640 Training loss: 1.2429 0.3902 sec/batch\n",
      "Epoch 8/20  Iteration 2031/5640 Training loss: 1.2424 0.3901 sec/batch\n",
      "Epoch 8/20  Iteration 2032/5640 Training loss: 1.2426 0.3904 sec/batch\n",
      "Epoch 8/20  Iteration 2033/5640 Training loss: 1.2425 0.3912 sec/batch\n",
      "Epoch 8/20  Iteration 2034/5640 Training loss: 1.2423 0.3938 sec/batch\n",
      "Epoch 8/20  Iteration 2035/5640 Training loss: 1.2420 0.3983 sec/batch\n",
      "Epoch 8/20  Iteration 2036/5640 Training loss: 1.2414 0.4025 sec/batch\n",
      "Epoch 8/20  Iteration 2037/5640 Training loss: 1.2412 0.3973 sec/batch\n",
      "Epoch 8/20  Iteration 2038/5640 Training loss: 1.2408 0.3975 sec/batch\n",
      "Epoch 8/20  Iteration 2039/5640 Training loss: 1.2412 0.3978 sec/batch\n",
      "Epoch 8/20  Iteration 2040/5640 Training loss: 1.2410 0.4006 sec/batch\n",
      "Epoch 8/20  Iteration 2041/5640 Training loss: 1.2410 0.4509 sec/batch\n",
      "Epoch 8/20  Iteration 2042/5640 Training loss: 1.2406 0.3994 sec/batch\n",
      "Epoch 8/20  Iteration 2043/5640 Training loss: 1.2402 0.3964 sec/batch\n",
      "Epoch 8/20  Iteration 2044/5640 Training loss: 1.2396 0.3976 sec/batch\n",
      "Epoch 8/20  Iteration 2045/5640 Training loss: 1.2395 0.3957 sec/batch\n",
      "Epoch 8/20  Iteration 2046/5640 Training loss: 1.2393 0.3978 sec/batch\n",
      "Epoch 8/20  Iteration 2047/5640 Training loss: 1.2394 0.3984 sec/batch\n",
      "Epoch 8/20  Iteration 2048/5640 Training loss: 1.2396 0.3988 sec/batch\n",
      "Epoch 8/20  Iteration 2049/5640 Training loss: 1.2397 0.3969 sec/batch\n",
      "Epoch 8/20  Iteration 2050/5640 Training loss: 1.2403 0.3977 sec/batch\n",
      "Validation loss: 1.17366 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2051/5640 Training loss: 1.2421 0.3831 sec/batch\n",
      "Epoch 8/20  Iteration 2052/5640 Training loss: 1.2426 0.3835 sec/batch\n",
      "Epoch 8/20  Iteration 2053/5640 Training loss: 1.2426 0.3828 sec/batch\n",
      "Epoch 8/20  Iteration 2054/5640 Training loss: 1.2429 0.3818 sec/batch\n",
      "Epoch 8/20  Iteration 2055/5640 Training loss: 1.2431 0.3872 sec/batch\n",
      "Epoch 8/20  Iteration 2056/5640 Training loss: 1.2427 0.4127 sec/batch\n",
      "Epoch 8/20  Iteration 2057/5640 Training loss: 1.2424 0.4336 sec/batch\n",
      "Epoch 8/20  Iteration 2058/5640 Training loss: 1.2421 0.4355 sec/batch\n",
      "Epoch 8/20  Iteration 2059/5640 Training loss: 1.2418 0.4291 sec/batch\n",
      "Epoch 8/20  Iteration 2060/5640 Training loss: 1.2418 0.4321 sec/batch\n",
      "Epoch 8/20  Iteration 2061/5640 Training loss: 1.2415 0.4338 sec/batch\n",
      "Epoch 8/20  Iteration 2062/5640 Training loss: 1.2412 0.4291 sec/batch\n",
      "Epoch 8/20  Iteration 2063/5640 Training loss: 1.2410 0.4418 sec/batch\n",
      "Epoch 8/20  Iteration 2064/5640 Training loss: 1.2408 0.4475 sec/batch\n",
      "Epoch 8/20  Iteration 2065/5640 Training loss: 1.2409 0.4215 sec/batch\n",
      "Epoch 8/20  Iteration 2066/5640 Training loss: 1.2407 0.3812 sec/batch\n",
      "Epoch 8/20  Iteration 2067/5640 Training loss: 1.2407 0.4083 sec/batch\n",
      "Epoch 8/20  Iteration 2068/5640 Training loss: 1.2405 0.3869 sec/batch\n",
      "Epoch 8/20  Iteration 2069/5640 Training loss: 1.2402 0.3880 sec/batch\n",
      "Epoch 8/20  Iteration 2070/5640 Training loss: 1.2402 0.3874 sec/batch\n",
      "Epoch 8/20  Iteration 2071/5640 Training loss: 1.2402 0.3926 sec/batch\n",
      "Epoch 8/20  Iteration 2072/5640 Training loss: 1.2402 0.3919 sec/batch\n",
      "Epoch 8/20  Iteration 2073/5640 Training loss: 1.2400 0.3901 sec/batch\n",
      "Epoch 8/20  Iteration 2074/5640 Training loss: 1.2398 0.3937 sec/batch\n",
      "Epoch 8/20  Iteration 2075/5640 Training loss: 1.2394 0.3935 sec/batch\n",
      "Epoch 8/20  Iteration 2076/5640 Training loss: 1.2392 0.3939 sec/batch\n",
      "Epoch 8/20  Iteration 2077/5640 Training loss: 1.2390 0.3931 sec/batch\n",
      "Epoch 8/20  Iteration 2078/5640 Training loss: 1.2386 0.3935 sec/batch\n",
      "Epoch 8/20  Iteration 2079/5640 Training loss: 1.2383 0.3951 sec/batch\n",
      "Epoch 8/20  Iteration 2080/5640 Training loss: 1.2378 0.3934 sec/batch\n",
      "Epoch 8/20  Iteration 2081/5640 Training loss: 1.2375 0.3938 sec/batch\n",
      "Epoch 8/20  Iteration 2082/5640 Training loss: 1.2370 0.3945 sec/batch\n",
      "Epoch 8/20  Iteration 2083/5640 Training loss: 1.2369 0.3937 sec/batch\n",
      "Epoch 8/20  Iteration 2084/5640 Training loss: 1.2367 0.3951 sec/batch\n",
      "Epoch 8/20  Iteration 2085/5640 Training loss: 1.2364 0.3935 sec/batch\n",
      "Epoch 8/20  Iteration 2086/5640 Training loss: 1.2363 0.3971 sec/batch\n",
      "Epoch 8/20  Iteration 2087/5640 Training loss: 1.2361 0.3946 sec/batch\n",
      "Epoch 8/20  Iteration 2088/5640 Training loss: 1.2360 0.3928 sec/batch\n",
      "Epoch 8/20  Iteration 2089/5640 Training loss: 1.2359 0.3929 sec/batch\n",
      "Epoch 8/20  Iteration 2090/5640 Training loss: 1.2359 0.3936 sec/batch\n",
      "Epoch 8/20  Iteration 2091/5640 Training loss: 1.2358 0.3937 sec/batch\n",
      "Epoch 8/20  Iteration 2092/5640 Training loss: 1.2356 0.3942 sec/batch\n",
      "Epoch 8/20  Iteration 2093/5640 Training loss: 1.2354 0.3951 sec/batch\n",
      "Epoch 8/20  Iteration 2094/5640 Training loss: 1.2351 0.3955 sec/batch\n",
      "Epoch 8/20  Iteration 2095/5640 Training loss: 1.2350 0.3929 sec/batch\n",
      "Epoch 8/20  Iteration 2096/5640 Training loss: 1.2350 0.3923 sec/batch\n",
      "Epoch 8/20  Iteration 2097/5640 Training loss: 1.2347 0.3957 sec/batch\n",
      "Epoch 8/20  Iteration 2098/5640 Training loss: 1.2346 0.3932 sec/batch\n",
      "Epoch 8/20  Iteration 2099/5640 Training loss: 1.2349 0.3931 sec/batch\n",
      "Epoch 8/20  Iteration 2100/5640 Training loss: 1.2345 0.3933 sec/batch\n",
      "Validation loss: 1.17218 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2101/5640 Training loss: 1.2356 0.3825 sec/batch\n",
      "Epoch 8/20  Iteration 2102/5640 Training loss: 1.2353 0.3818 sec/batch\n",
      "Epoch 8/20  Iteration 2103/5640 Training loss: 1.2351 0.3830 sec/batch\n",
      "Epoch 8/20  Iteration 2104/5640 Training loss: 1.2349 0.3830 sec/batch\n",
      "Epoch 8/20  Iteration 2105/5640 Training loss: 1.2348 0.3827 sec/batch\n",
      "Epoch 8/20  Iteration 2106/5640 Training loss: 1.2344 0.3821 sec/batch\n",
      "Epoch 8/20  Iteration 2107/5640 Training loss: 1.2341 0.3820 sec/batch\n",
      "Epoch 8/20  Iteration 2108/5640 Training loss: 1.2338 0.3824 sec/batch\n",
      "Epoch 8/20  Iteration 2109/5640 Training loss: 1.2337 0.3812 sec/batch\n",
      "Epoch 8/20  Iteration 2110/5640 Training loss: 1.2336 0.3851 sec/batch\n",
      "Epoch 8/20  Iteration 2111/5640 Training loss: 1.2335 0.3871 sec/batch\n",
      "Epoch 8/20  Iteration 2112/5640 Training loss: 1.2333 0.3903 sec/batch\n",
      "Epoch 8/20  Iteration 2113/5640 Training loss: 1.2332 0.3892 sec/batch\n",
      "Epoch 8/20  Iteration 2114/5640 Training loss: 1.2332 0.3887 sec/batch\n",
      "Epoch 8/20  Iteration 2115/5640 Training loss: 1.2331 0.3901 sec/batch\n",
      "Epoch 8/20  Iteration 2116/5640 Training loss: 1.2330 0.3893 sec/batch\n",
      "Epoch 8/20  Iteration 2117/5640 Training loss: 1.2330 0.3903 sec/batch\n",
      "Epoch 8/20  Iteration 2118/5640 Training loss: 1.2327 0.3912 sec/batch\n",
      "Epoch 8/20  Iteration 2119/5640 Training loss: 1.2323 0.3955 sec/batch\n",
      "Epoch 8/20  Iteration 2120/5640 Training loss: 1.2320 0.3932 sec/batch\n",
      "Epoch 8/20  Iteration 2121/5640 Training loss: 1.2317 0.4088 sec/batch\n",
      "Epoch 8/20  Iteration 2122/5640 Training loss: 1.2317 0.3972 sec/batch\n",
      "Epoch 8/20  Iteration 2123/5640 Training loss: 1.2317 0.3949 sec/batch\n",
      "Epoch 8/20  Iteration 2124/5640 Training loss: 1.2315 0.3995 sec/batch\n",
      "Epoch 8/20  Iteration 2125/5640 Training loss: 1.2313 0.3932 sec/batch\n",
      "Epoch 8/20  Iteration 2126/5640 Training loss: 1.2314 0.3942 sec/batch\n",
      "Epoch 8/20  Iteration 2127/5640 Training loss: 1.2316 0.3970 sec/batch\n",
      "Epoch 8/20  Iteration 2128/5640 Training loss: 1.2316 0.3953 sec/batch\n",
      "Epoch 8/20  Iteration 2129/5640 Training loss: 1.2315 0.3983 sec/batch\n",
      "Epoch 8/20  Iteration 2130/5640 Training loss: 1.2316 0.3949 sec/batch\n",
      "Epoch 8/20  Iteration 2131/5640 Training loss: 1.2314 0.3929 sec/batch\n",
      "Epoch 8/20  Iteration 2132/5640 Training loss: 1.2313 0.3946 sec/batch\n",
      "Epoch 8/20  Iteration 2133/5640 Training loss: 1.2312 0.3944 sec/batch\n",
      "Epoch 8/20  Iteration 2134/5640 Training loss: 1.2310 0.3951 sec/batch\n",
      "Epoch 8/20  Iteration 2135/5640 Training loss: 1.2309 0.3992 sec/batch\n",
      "Epoch 8/20  Iteration 2136/5640 Training loss: 1.2307 0.3941 sec/batch\n",
      "Epoch 8/20  Iteration 2137/5640 Training loss: 1.2306 0.3962 sec/batch\n",
      "Epoch 8/20  Iteration 2138/5640 Training loss: 1.2305 0.3935 sec/batch\n",
      "Epoch 8/20  Iteration 2139/5640 Training loss: 1.2305 0.4259 sec/batch\n",
      "Epoch 8/20  Iteration 2140/5640 Training loss: 1.2307 0.4095 sec/batch\n",
      "Epoch 8/20  Iteration 2141/5640 Training loss: 1.2306 0.4119 sec/batch\n",
      "Epoch 8/20  Iteration 2142/5640 Training loss: 1.2305 0.4053 sec/batch\n",
      "Epoch 8/20  Iteration 2143/5640 Training loss: 1.2306 0.3975 sec/batch\n",
      "Epoch 8/20  Iteration 2144/5640 Training loss: 1.2306 0.3950 sec/batch\n",
      "Epoch 8/20  Iteration 2145/5640 Training loss: 1.2305 0.3970 sec/batch\n",
      "Epoch 8/20  Iteration 2146/5640 Training loss: 1.2303 0.3950 sec/batch\n",
      "Epoch 8/20  Iteration 2147/5640 Training loss: 1.2302 0.3952 sec/batch\n",
      "Epoch 8/20  Iteration 2148/5640 Training loss: 1.2301 0.3952 sec/batch\n",
      "Epoch 8/20  Iteration 2149/5640 Training loss: 1.2301 0.3959 sec/batch\n",
      "Epoch 8/20  Iteration 2150/5640 Training loss: 1.2299 0.3947 sec/batch\n",
      "Validation loss: 1.16605 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2151/5640 Training loss: 1.2308 0.3851 sec/batch\n",
      "Epoch 8/20  Iteration 2152/5640 Training loss: 1.2306 0.3842 sec/batch\n",
      "Epoch 8/20  Iteration 2153/5640 Training loss: 1.2307 0.3858 sec/batch\n",
      "Epoch 8/20  Iteration 2154/5640 Training loss: 1.2308 0.3823 sec/batch\n",
      "Epoch 8/20  Iteration 2155/5640 Training loss: 1.2308 0.3844 sec/batch\n",
      "Epoch 8/20  Iteration 2156/5640 Training loss: 1.2308 0.3828 sec/batch\n",
      "Epoch 8/20  Iteration 2157/5640 Training loss: 1.2309 0.3833 sec/batch\n",
      "Epoch 8/20  Iteration 2158/5640 Training loss: 1.2307 0.3845 sec/batch\n",
      "Epoch 8/20  Iteration 2159/5640 Training loss: 1.2307 0.3863 sec/batch\n",
      "Epoch 8/20  Iteration 2160/5640 Training loss: 1.2304 0.3877 sec/batch\n",
      "Epoch 8/20  Iteration 2161/5640 Training loss: 1.2300 0.3826 sec/batch\n",
      "Epoch 8/20  Iteration 2162/5640 Training loss: 1.2296 0.3846 sec/batch\n",
      "Epoch 8/20  Iteration 2163/5640 Training loss: 1.2295 0.3864 sec/batch\n",
      "Epoch 8/20  Iteration 2164/5640 Training loss: 1.2293 0.3837 sec/batch\n",
      "Epoch 8/20  Iteration 2165/5640 Training loss: 1.2292 0.3888 sec/batch\n",
      "Epoch 8/20  Iteration 2166/5640 Training loss: 1.2290 0.3890 sec/batch\n",
      "Epoch 8/20  Iteration 2167/5640 Training loss: 1.2288 0.3927 sec/batch\n",
      "Epoch 8/20  Iteration 2168/5640 Training loss: 1.2288 0.3905 sec/batch\n",
      "Epoch 8/20  Iteration 2169/5640 Training loss: 1.2288 0.3895 sec/batch\n",
      "Epoch 8/20  Iteration 2170/5640 Training loss: 1.2288 0.3913 sec/batch\n",
      "Epoch 8/20  Iteration 2171/5640 Training loss: 1.2286 0.3912 sec/batch\n",
      "Epoch 8/20  Iteration 2172/5640 Training loss: 1.2288 0.3916 sec/batch\n",
      "Epoch 8/20  Iteration 2173/5640 Training loss: 1.2287 0.3928 sec/batch\n",
      "Epoch 8/20  Iteration 2174/5640 Training loss: 1.2286 0.3938 sec/batch\n",
      "Epoch 8/20  Iteration 2175/5640 Training loss: 1.2285 0.3967 sec/batch\n",
      "Epoch 8/20  Iteration 2176/5640 Training loss: 1.2284 0.3941 sec/batch\n",
      "Epoch 8/20  Iteration 2177/5640 Training loss: 1.2284 0.3944 sec/batch\n",
      "Epoch 8/20  Iteration 2178/5640 Training loss: 1.2284 0.3959 sec/batch\n",
      "Epoch 8/20  Iteration 2179/5640 Training loss: 1.2281 0.3962 sec/batch\n",
      "Epoch 8/20  Iteration 2180/5640 Training loss: 1.2281 0.3929 sec/batch\n",
      "Epoch 8/20  Iteration 2181/5640 Training loss: 1.2279 0.3957 sec/batch\n",
      "Epoch 8/20  Iteration 2182/5640 Training loss: 1.2278 0.3974 sec/batch\n",
      "Epoch 8/20  Iteration 2183/5640 Training loss: 1.2278 0.3939 sec/batch\n",
      "Epoch 8/20  Iteration 2184/5640 Training loss: 1.2276 0.3970 sec/batch\n",
      "Epoch 8/20  Iteration 2185/5640 Training loss: 1.2274 0.3943 sec/batch\n",
      "Epoch 8/20  Iteration 2186/5640 Training loss: 1.2273 0.3957 sec/batch\n",
      "Epoch 8/20  Iteration 2187/5640 Training loss: 1.2273 0.3939 sec/batch\n",
      "Epoch 8/20  Iteration 2188/5640 Training loss: 1.2273 0.3925 sec/batch\n",
      "Epoch 8/20  Iteration 2189/5640 Training loss: 1.2273 0.3940 sec/batch\n",
      "Epoch 8/20  Iteration 2190/5640 Training loss: 1.2272 0.3961 sec/batch\n",
      "Epoch 8/20  Iteration 2191/5640 Training loss: 1.2271 0.3945 sec/batch\n",
      "Epoch 8/20  Iteration 2192/5640 Training loss: 1.2269 0.3939 sec/batch\n",
      "Epoch 8/20  Iteration 2193/5640 Training loss: 1.2267 0.3946 sec/batch\n",
      "Epoch 8/20  Iteration 2194/5640 Training loss: 1.2265 0.3946 sec/batch\n",
      "Epoch 8/20  Iteration 2195/5640 Training loss: 1.2264 0.3941 sec/batch\n",
      "Epoch 8/20  Iteration 2196/5640 Training loss: 1.2263 0.3925 sec/batch\n",
      "Epoch 8/20  Iteration 2197/5640 Training loss: 1.2262 0.3936 sec/batch\n",
      "Epoch 8/20  Iteration 2198/5640 Training loss: 1.2261 0.3978 sec/batch\n",
      "Epoch 8/20  Iteration 2199/5640 Training loss: 1.2260 0.3947 sec/batch\n",
      "Epoch 8/20  Iteration 2200/5640 Training loss: 1.2258 0.3949 sec/batch\n",
      "Validation loss: 1.16292 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2201/5640 Training loss: 1.2265 0.3867 sec/batch\n",
      "Epoch 8/20  Iteration 2202/5640 Training loss: 1.2266 0.3839 sec/batch\n",
      "Epoch 8/20  Iteration 2203/5640 Training loss: 1.2266 0.3858 sec/batch\n",
      "Epoch 8/20  Iteration 2204/5640 Training loss: 1.2266 0.3818 sec/batch\n",
      "Epoch 8/20  Iteration 2205/5640 Training loss: 1.2265 0.3863 sec/batch\n",
      "Epoch 8/20  Iteration 2206/5640 Training loss: 1.2265 0.3816 sec/batch\n",
      "Epoch 8/20  Iteration 2207/5640 Training loss: 1.2263 0.3844 sec/batch\n",
      "Epoch 8/20  Iteration 2208/5640 Training loss: 1.2261 0.3842 sec/batch\n",
      "Epoch 8/20  Iteration 2209/5640 Training loss: 1.2261 0.3829 sec/batch\n",
      "Epoch 8/20  Iteration 2210/5640 Training loss: 1.2261 0.3815 sec/batch\n",
      "Epoch 8/20  Iteration 2211/5640 Training loss: 1.2260 0.3835 sec/batch\n",
      "Epoch 8/20  Iteration 2212/5640 Training loss: 1.2261 0.3856 sec/batch\n",
      "Epoch 8/20  Iteration 2213/5640 Training loss: 1.2260 0.3857 sec/batch\n",
      "Epoch 8/20  Iteration 2214/5640 Training loss: 1.2259 0.3838 sec/batch\n",
      "Epoch 8/20  Iteration 2215/5640 Training loss: 1.2259 0.3889 sec/batch\n",
      "Epoch 8/20  Iteration 2216/5640 Training loss: 1.2258 0.3897 sec/batch\n",
      "Epoch 8/20  Iteration 2217/5640 Training loss: 1.2257 0.3920 sec/batch\n",
      "Epoch 8/20  Iteration 2218/5640 Training loss: 1.2255 0.3944 sec/batch\n",
      "Epoch 8/20  Iteration 2219/5640 Training loss: 1.2253 0.3935 sec/batch\n",
      "Epoch 8/20  Iteration 2220/5640 Training loss: 1.2252 0.3942 sec/batch\n",
      "Epoch 8/20  Iteration 2221/5640 Training loss: 1.2251 0.3929 sec/batch\n",
      "Epoch 8/20  Iteration 2222/5640 Training loss: 1.2251 0.3945 sec/batch\n",
      "Epoch 8/20  Iteration 2223/5640 Training loss: 1.2251 0.3980 sec/batch\n",
      "Epoch 8/20  Iteration 2224/5640 Training loss: 1.2251 0.3954 sec/batch\n",
      "Epoch 8/20  Iteration 2225/5640 Training loss: 1.2250 0.3933 sec/batch\n",
      "Epoch 8/20  Iteration 2226/5640 Training loss: 1.2248 0.3940 sec/batch\n",
      "Epoch 8/20  Iteration 2227/5640 Training loss: 1.2247 0.3945 sec/batch\n",
      "Epoch 8/20  Iteration 2228/5640 Training loss: 1.2246 0.3941 sec/batch\n",
      "Epoch 8/20  Iteration 2229/5640 Training loss: 1.2245 0.3932 sec/batch\n",
      "Epoch 8/20  Iteration 2230/5640 Training loss: 1.2244 0.3945 sec/batch\n",
      "Epoch 8/20  Iteration 2231/5640 Training loss: 1.2243 0.3959 sec/batch\n",
      "Epoch 8/20  Iteration 2232/5640 Training loss: 1.2243 0.3940 sec/batch\n",
      "Epoch 8/20  Iteration 2233/5640 Training loss: 1.2242 0.3932 sec/batch\n",
      "Epoch 8/20  Iteration 2234/5640 Training loss: 1.2243 0.3947 sec/batch\n",
      "Epoch 8/20  Iteration 2235/5640 Training loss: 1.2242 0.3946 sec/batch\n",
      "Epoch 8/20  Iteration 2236/5640 Training loss: 1.2242 0.3967 sec/batch\n",
      "Epoch 8/20  Iteration 2237/5640 Training loss: 1.2242 0.3926 sec/batch\n",
      "Epoch 8/20  Iteration 2238/5640 Training loss: 1.2242 0.3968 sec/batch\n",
      "Epoch 8/20  Iteration 2239/5640 Training loss: 1.2242 0.3939 sec/batch\n",
      "Epoch 8/20  Iteration 2240/5640 Training loss: 1.2241 0.3953 sec/batch\n",
      "Epoch 8/20  Iteration 2241/5640 Training loss: 1.2240 0.3929 sec/batch\n",
      "Epoch 8/20  Iteration 2242/5640 Training loss: 1.2238 0.3939 sec/batch\n",
      "Epoch 8/20  Iteration 2243/5640 Training loss: 1.2238 0.3943 sec/batch\n",
      "Epoch 8/20  Iteration 2244/5640 Training loss: 1.2237 0.3942 sec/batch\n",
      "Epoch 8/20  Iteration 2245/5640 Training loss: 1.2236 0.3931 sec/batch\n",
      "Epoch 8/20  Iteration 2246/5640 Training loss: 1.2235 0.3964 sec/batch\n",
      "Epoch 8/20  Iteration 2247/5640 Training loss: 1.2234 0.4121 sec/batch\n",
      "Epoch 8/20  Iteration 2248/5640 Training loss: 1.2233 0.4409 sec/batch\n",
      "Epoch 8/20  Iteration 2249/5640 Training loss: 1.2233 0.3974 sec/batch\n",
      "Epoch 8/20  Iteration 2250/5640 Training loss: 1.2232 0.3947 sec/batch\n",
      "Validation loss: 1.15872 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 2251/5640 Training loss: 1.2237 0.3850 sec/batch\n",
      "Epoch 8/20  Iteration 2252/5640 Training loss: 1.2237 0.3805 sec/batch\n",
      "Epoch 8/20  Iteration 2253/5640 Training loss: 1.2237 0.3826 sec/batch\n",
      "Epoch 8/20  Iteration 2254/5640 Training loss: 1.2237 0.3831 sec/batch\n",
      "Epoch 8/20  Iteration 2255/5640 Training loss: 1.2236 0.3847 sec/batch\n",
      "Epoch 8/20  Iteration 2256/5640 Training loss: 1.2235 0.3821 sec/batch\n",
      "Epoch 9/20  Iteration 2257/5640 Training loss: 1.3024 0.3831 sec/batch\n",
      "Epoch 9/20  Iteration 2258/5640 Training loss: 1.2690 0.3836 sec/batch\n",
      "Epoch 9/20  Iteration 2259/5640 Training loss: 1.2538 0.3814 sec/batch\n",
      "Epoch 9/20  Iteration 2260/5640 Training loss: 1.2450 0.3821 sec/batch\n",
      "Epoch 9/20  Iteration 2261/5640 Training loss: 1.2398 0.3835 sec/batch\n",
      "Epoch 9/20  Iteration 2262/5640 Training loss: 1.2386 0.3851 sec/batch\n",
      "Epoch 9/20  Iteration 2263/5640 Training loss: 1.2335 0.3873 sec/batch\n",
      "Epoch 9/20  Iteration 2264/5640 Training loss: 1.2303 0.3867 sec/batch\n",
      "Epoch 9/20  Iteration 2265/5640 Training loss: 1.2271 0.3860 sec/batch\n",
      "Epoch 9/20  Iteration 2266/5640 Training loss: 1.2232 0.3896 sec/batch\n",
      "Epoch 9/20  Iteration 2267/5640 Training loss: 1.2232 0.3921 sec/batch\n",
      "Epoch 9/20  Iteration 2268/5640 Training loss: 1.2205 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 2269/5640 Training loss: 1.2193 0.3938 sec/batch\n",
      "Epoch 9/20  Iteration 2270/5640 Training loss: 1.2197 0.3927 sec/batch\n",
      "Epoch 9/20  Iteration 2271/5640 Training loss: 1.2176 0.3943 sec/batch\n",
      "Epoch 9/20  Iteration 2272/5640 Training loss: 1.2181 0.3964 sec/batch\n",
      "Epoch 9/20  Iteration 2273/5640 Training loss: 1.2181 0.3965 sec/batch\n",
      "Epoch 9/20  Iteration 2274/5640 Training loss: 1.2168 0.3968 sec/batch\n",
      "Epoch 9/20  Iteration 2275/5640 Training loss: 1.2161 0.3931 sec/batch\n",
      "Epoch 9/20  Iteration 2276/5640 Training loss: 1.2147 0.3946 sec/batch\n",
      "Epoch 9/20  Iteration 2277/5640 Training loss: 1.2143 0.3946 sec/batch\n",
      "Epoch 9/20  Iteration 2278/5640 Training loss: 1.2140 0.3945 sec/batch\n",
      "Epoch 9/20  Iteration 2279/5640 Training loss: 1.2136 0.3975 sec/batch\n",
      "Epoch 9/20  Iteration 2280/5640 Training loss: 1.2117 0.3990 sec/batch\n",
      "Epoch 9/20  Iteration 2281/5640 Training loss: 1.2116 0.3943 sec/batch\n",
      "Epoch 9/20  Iteration 2282/5640 Training loss: 1.2108 0.3939 sec/batch\n",
      "Epoch 9/20  Iteration 2283/5640 Training loss: 1.2110 0.3935 sec/batch\n",
      "Epoch 9/20  Iteration 2284/5640 Training loss: 1.2107 0.3937 sec/batch\n",
      "Epoch 9/20  Iteration 2285/5640 Training loss: 1.2104 0.3966 sec/batch\n",
      "Epoch 9/20  Iteration 2286/5640 Training loss: 1.2100 0.4018 sec/batch\n",
      "Epoch 9/20  Iteration 2287/5640 Training loss: 1.2089 0.3949 sec/batch\n",
      "Epoch 9/20  Iteration 2288/5640 Training loss: 1.2081 0.3951 sec/batch\n",
      "Epoch 9/20  Iteration 2289/5640 Training loss: 1.2075 0.3964 sec/batch\n",
      "Epoch 9/20  Iteration 2290/5640 Training loss: 1.2068 0.3990 sec/batch\n",
      "Epoch 9/20  Iteration 2291/5640 Training loss: 1.2071 0.3945 sec/batch\n",
      "Epoch 9/20  Iteration 2292/5640 Training loss: 1.2071 0.3929 sec/batch\n",
      "Epoch 9/20  Iteration 2293/5640 Training loss: 1.2064 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 2294/5640 Training loss: 1.2056 0.3968 sec/batch\n",
      "Epoch 9/20  Iteration 2295/5640 Training loss: 1.2055 0.3948 sec/batch\n",
      "Epoch 9/20  Iteration 2296/5640 Training loss: 1.2047 0.3945 sec/batch\n",
      "Epoch 9/20  Iteration 2297/5640 Training loss: 1.2040 0.3970 sec/batch\n",
      "Epoch 9/20  Iteration 2298/5640 Training loss: 1.2049 0.3947 sec/batch\n",
      "Epoch 9/20  Iteration 2299/5640 Training loss: 1.2042 0.4108 sec/batch\n",
      "Epoch 9/20  Iteration 2300/5640 Training loss: 1.2041 0.4169 sec/batch\n",
      "Validation loss: 1.15326 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 2301/5640 Training loss: 1.2071 0.3802 sec/batch\n",
      "Epoch 9/20  Iteration 2302/5640 Training loss: 1.2068 0.4087 sec/batch\n",
      "Epoch 9/20  Iteration 2303/5640 Training loss: 1.2068 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 2304/5640 Training loss: 1.2072 0.4023 sec/batch\n",
      "Epoch 9/20  Iteration 2305/5640 Training loss: 1.2073 0.3816 sec/batch\n",
      "Epoch 9/20  Iteration 2306/5640 Training loss: 1.2070 0.3807 sec/batch\n",
      "Epoch 9/20  Iteration 2307/5640 Training loss: 1.2069 0.3817 sec/batch\n",
      "Epoch 9/20  Iteration 2308/5640 Training loss: 1.2067 0.3815 sec/batch\n",
      "Epoch 9/20  Iteration 2309/5640 Training loss: 1.2067 0.3811 sec/batch\n",
      "Epoch 9/20  Iteration 2310/5640 Training loss: 1.2068 0.3814 sec/batch\n",
      "Epoch 9/20  Iteration 2311/5640 Training loss: 1.2063 0.3864 sec/batch\n",
      "Epoch 9/20  Iteration 2312/5640 Training loss: 1.2060 0.3855 sec/batch\n",
      "Epoch 9/20  Iteration 2313/5640 Training loss: 1.2058 0.3868 sec/batch\n",
      "Epoch 9/20  Iteration 2314/5640 Training loss: 1.2058 0.3887 sec/batch\n",
      "Epoch 9/20  Iteration 2315/5640 Training loss: 1.2058 0.3940 sec/batch\n",
      "Epoch 9/20  Iteration 2316/5640 Training loss: 1.2057 0.3921 sec/batch\n",
      "Epoch 9/20  Iteration 2317/5640 Training loss: 1.2056 0.3931 sec/batch\n",
      "Epoch 9/20  Iteration 2318/5640 Training loss: 1.2052 0.3930 sec/batch\n",
      "Epoch 9/20  Iteration 2319/5640 Training loss: 1.2049 0.3994 sec/batch\n",
      "Epoch 9/20  Iteration 2320/5640 Training loss: 1.2045 0.3924 sec/batch\n",
      "Epoch 9/20  Iteration 2321/5640 Training loss: 1.2048 0.3964 sec/batch\n",
      "Epoch 9/20  Iteration 2322/5640 Training loss: 1.2048 0.3982 sec/batch\n",
      "Epoch 9/20  Iteration 2323/5640 Training loss: 1.2048 0.3938 sec/batch\n",
      "Epoch 9/20  Iteration 2324/5640 Training loss: 1.2046 0.3950 sec/batch\n",
      "Epoch 9/20  Iteration 2325/5640 Training loss: 1.2042 0.3940 sec/batch\n",
      "Epoch 9/20  Iteration 2326/5640 Training loss: 1.2037 0.3953 sec/batch\n",
      "Epoch 9/20  Iteration 2327/5640 Training loss: 1.2035 0.3934 sec/batch\n",
      "Epoch 9/20  Iteration 2328/5640 Training loss: 1.2033 0.3923 sec/batch\n",
      "Epoch 9/20  Iteration 2329/5640 Training loss: 1.2035 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 2330/5640 Training loss: 1.2036 0.3941 sec/batch\n",
      "Epoch 9/20  Iteration 2331/5640 Training loss: 1.2036 0.3942 sec/batch\n",
      "Epoch 9/20  Iteration 2332/5640 Training loss: 1.2041 0.3937 sec/batch\n",
      "Epoch 9/20  Iteration 2333/5640 Training loss: 1.2041 0.3943 sec/batch\n",
      "Epoch 9/20  Iteration 2334/5640 Training loss: 1.2046 0.3959 sec/batch\n",
      "Epoch 9/20  Iteration 2335/5640 Training loss: 1.2048 0.3936 sec/batch\n",
      "Epoch 9/20  Iteration 2336/5640 Training loss: 1.2050 0.4284 sec/batch\n",
      "Epoch 9/20  Iteration 2337/5640 Training loss: 1.2052 0.3970 sec/batch\n",
      "Epoch 9/20  Iteration 2338/5640 Training loss: 1.2048 0.3968 sec/batch\n",
      "Epoch 9/20  Iteration 2339/5640 Training loss: 1.2047 0.3931 sec/batch\n",
      "Epoch 9/20  Iteration 2340/5640 Training loss: 1.2043 0.3939 sec/batch\n",
      "Epoch 9/20  Iteration 2341/5640 Training loss: 1.2041 0.3935 sec/batch\n",
      "Epoch 9/20  Iteration 2342/5640 Training loss: 1.2040 0.4075 sec/batch\n",
      "Epoch 9/20  Iteration 2343/5640 Training loss: 1.2038 0.3973 sec/batch\n",
      "Epoch 9/20  Iteration 2344/5640 Training loss: 1.2035 0.3961 sec/batch\n",
      "Epoch 9/20  Iteration 2345/5640 Training loss: 1.2033 0.3958 sec/batch\n",
      "Epoch 9/20  Iteration 2346/5640 Training loss: 1.2031 0.3954 sec/batch\n",
      "Epoch 9/20  Iteration 2347/5640 Training loss: 1.2032 0.3979 sec/batch\n",
      "Epoch 9/20  Iteration 2348/5640 Training loss: 1.2032 0.3946 sec/batch\n",
      "Epoch 9/20  Iteration 2349/5640 Training loss: 1.2032 0.3976 sec/batch\n",
      "Epoch 9/20  Iteration 2350/5640 Training loss: 1.2030 0.3941 sec/batch\n",
      "Validation loss: 1.14978 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 2351/5640 Training loss: 1.2043 0.3845 sec/batch\n",
      "Epoch 9/20  Iteration 2352/5640 Training loss: 1.2044 0.3819 sec/batch\n",
      "Epoch 9/20  Iteration 2353/5640 Training loss: 1.2045 0.3830 sec/batch\n",
      "Epoch 9/20  Iteration 2354/5640 Training loss: 1.2047 0.3821 sec/batch\n",
      "Epoch 9/20  Iteration 2355/5640 Training loss: 1.2046 0.3817 sec/batch\n",
      "Epoch 9/20  Iteration 2356/5640 Training loss: 1.2044 0.3817 sec/batch\n",
      "Epoch 9/20  Iteration 2357/5640 Training loss: 1.2040 0.3836 sec/batch\n",
      "Epoch 9/20  Iteration 2358/5640 Training loss: 1.2039 0.3822 sec/batch\n",
      "Epoch 9/20  Iteration 2359/5640 Training loss: 1.2036 0.3831 sec/batch\n",
      "Epoch 9/20  Iteration 2360/5640 Training loss: 1.2033 0.3829 sec/batch\n",
      "Epoch 9/20  Iteration 2361/5640 Training loss: 1.2030 0.3831 sec/batch\n",
      "Epoch 9/20  Iteration 2362/5640 Training loss: 1.2027 0.3899 sec/batch\n",
      "Epoch 9/20  Iteration 2363/5640 Training loss: 1.2024 0.3885 sec/batch\n",
      "Epoch 9/20  Iteration 2364/5640 Training loss: 1.2019 0.3848 sec/batch\n",
      "Epoch 9/20  Iteration 2365/5640 Training loss: 1.2018 0.3867 sec/batch\n",
      "Epoch 9/20  Iteration 2366/5640 Training loss: 1.2016 0.3887 sec/batch\n",
      "Epoch 9/20  Iteration 2367/5640 Training loss: 1.2014 0.3959 sec/batch\n",
      "Epoch 9/20  Iteration 2368/5640 Training loss: 1.2014 0.3912 sec/batch\n",
      "Epoch 9/20  Iteration 2369/5640 Training loss: 1.2012 0.3908 sec/batch\n",
      "Epoch 9/20  Iteration 2370/5640 Training loss: 1.2010 0.3920 sec/batch\n",
      "Epoch 9/20  Iteration 2371/5640 Training loss: 1.2010 0.4116 sec/batch\n",
      "Epoch 9/20  Iteration 2372/5640 Training loss: 1.2010 0.4078 sec/batch\n",
      "Epoch 9/20  Iteration 2373/5640 Training loss: 1.2009 0.4038 sec/batch\n",
      "Epoch 9/20  Iteration 2374/5640 Training loss: 1.2007 0.4070 sec/batch\n",
      "Epoch 9/20  Iteration 2375/5640 Training loss: 1.2005 0.3975 sec/batch\n",
      "Epoch 9/20  Iteration 2376/5640 Training loss: 1.2002 0.4061 sec/batch\n",
      "Epoch 9/20  Iteration 2377/5640 Training loss: 1.2000 0.3911 sec/batch\n",
      "Epoch 9/20  Iteration 2378/5640 Training loss: 1.2000 0.3912 sec/batch\n",
      "Epoch 9/20  Iteration 2379/5640 Training loss: 1.1997 0.3906 sec/batch\n",
      "Epoch 9/20  Iteration 2380/5640 Training loss: 1.1997 0.3962 sec/batch\n",
      "Epoch 9/20  Iteration 2381/5640 Training loss: 1.2000 0.3914 sec/batch\n",
      "Epoch 9/20  Iteration 2382/5640 Training loss: 1.1996 0.3909 sec/batch\n",
      "Epoch 9/20  Iteration 2383/5640 Training loss: 1.1996 0.3934 sec/batch\n",
      "Epoch 9/20  Iteration 2384/5640 Training loss: 1.1993 0.3903 sec/batch\n",
      "Epoch 9/20  Iteration 2385/5640 Training loss: 1.1990 0.3917 sec/batch\n",
      "Epoch 9/20  Iteration 2386/5640 Training loss: 1.1988 0.3907 sec/batch\n",
      "Epoch 9/20  Iteration 2387/5640 Training loss: 1.1987 0.3908 sec/batch\n",
      "Epoch 9/20  Iteration 2388/5640 Training loss: 1.1983 0.3904 sec/batch\n",
      "Epoch 9/20  Iteration 2389/5640 Training loss: 1.1980 0.3908 sec/batch\n",
      "Epoch 9/20  Iteration 2390/5640 Training loss: 1.1977 0.3924 sec/batch\n",
      "Epoch 9/20  Iteration 2391/5640 Training loss: 1.1976 0.3938 sec/batch\n",
      "Epoch 9/20  Iteration 2392/5640 Training loss: 1.1976 0.3906 sec/batch\n",
      "Epoch 9/20  Iteration 2393/5640 Training loss: 1.1975 0.3949 sec/batch\n",
      "Epoch 9/20  Iteration 2394/5640 Training loss: 1.1973 0.4011 sec/batch\n",
      "Epoch 9/20  Iteration 2395/5640 Training loss: 1.1972 0.4088 sec/batch\n",
      "Epoch 9/20  Iteration 2396/5640 Training loss: 1.1972 0.4088 sec/batch\n",
      "Epoch 9/20  Iteration 2397/5640 Training loss: 1.1970 0.4092 sec/batch\n",
      "Epoch 9/20  Iteration 2398/5640 Training loss: 1.1970 0.4003 sec/batch\n",
      "Epoch 9/20  Iteration 2399/5640 Training loss: 1.1969 0.3948 sec/batch\n",
      "Epoch 9/20  Iteration 2400/5640 Training loss: 1.1967 0.3960 sec/batch\n",
      "Validation loss: 1.14534 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 2401/5640 Training loss: 1.1975 0.3831 sec/batch\n",
      "Epoch 9/20  Iteration 2402/5640 Training loss: 1.1972 0.3821 sec/batch\n",
      "Epoch 9/20  Iteration 2403/5640 Training loss: 1.1969 0.3827 sec/batch\n",
      "Epoch 9/20  Iteration 2404/5640 Training loss: 1.1969 0.3813 sec/batch\n",
      "Epoch 9/20  Iteration 2405/5640 Training loss: 1.1969 0.3822 sec/batch\n",
      "Epoch 9/20  Iteration 2406/5640 Training loss: 1.1967 0.3848 sec/batch\n",
      "Epoch 9/20  Iteration 2407/5640 Training loss: 1.1966 0.3887 sec/batch\n",
      "Epoch 9/20  Iteration 2408/5640 Training loss: 1.1967 0.3874 sec/batch\n",
      "Epoch 9/20  Iteration 2409/5640 Training loss: 1.1968 0.3899 sec/batch\n",
      "Epoch 9/20  Iteration 2410/5640 Training loss: 1.1968 0.3883 sec/batch\n",
      "Epoch 9/20  Iteration 2411/5640 Training loss: 1.1967 0.3874 sec/batch\n",
      "Epoch 9/20  Iteration 2412/5640 Training loss: 1.1968 0.3868 sec/batch\n",
      "Epoch 9/20  Iteration 2413/5640 Training loss: 1.1966 0.3933 sec/batch\n",
      "Epoch 9/20  Iteration 2414/5640 Training loss: 1.1965 0.3948 sec/batch\n",
      "Epoch 9/20  Iteration 2415/5640 Training loss: 1.1964 0.3948 sec/batch\n",
      "Epoch 9/20  Iteration 2416/5640 Training loss: 1.1963 0.3967 sec/batch\n",
      "Epoch 9/20  Iteration 2417/5640 Training loss: 1.1962 0.3970 sec/batch\n",
      "Epoch 9/20  Iteration 2418/5640 Training loss: 1.1961 0.3927 sec/batch\n",
      "Epoch 9/20  Iteration 2419/5640 Training loss: 1.1960 0.3907 sec/batch\n",
      "Epoch 9/20  Iteration 2420/5640 Training loss: 1.1958 0.3929 sec/batch\n",
      "Epoch 9/20  Iteration 2421/5640 Training loss: 1.1959 0.3968 sec/batch\n",
      "Epoch 9/20  Iteration 2422/5640 Training loss: 1.1960 0.3969 sec/batch\n",
      "Epoch 9/20  Iteration 2423/5640 Training loss: 1.1959 0.3922 sec/batch\n",
      "Epoch 9/20  Iteration 2424/5640 Training loss: 1.1959 0.3907 sec/batch\n",
      "Epoch 9/20  Iteration 2425/5640 Training loss: 1.1959 0.3952 sec/batch\n",
      "Epoch 9/20  Iteration 2426/5640 Training loss: 1.1959 0.3932 sec/batch\n",
      "Epoch 9/20  Iteration 2427/5640 Training loss: 1.1958 0.3927 sec/batch\n",
      "Epoch 9/20  Iteration 2428/5640 Training loss: 1.1956 0.3943 sec/batch\n",
      "Epoch 9/20  Iteration 2429/5640 Training loss: 1.1955 0.3932 sec/batch\n",
      "Epoch 9/20  Iteration 2430/5640 Training loss: 1.1954 0.3938 sec/batch\n",
      "Epoch 9/20  Iteration 2431/5640 Training loss: 1.1953 0.3898 sec/batch\n",
      "Epoch 9/20  Iteration 2432/5640 Training loss: 1.1953 0.3912 sec/batch\n",
      "Epoch 9/20  Iteration 2433/5640 Training loss: 1.1954 0.3919 sec/batch\n",
      "Epoch 9/20  Iteration 2434/5640 Training loss: 1.1952 0.3889 sec/batch\n",
      "Epoch 9/20  Iteration 2435/5640 Training loss: 1.1953 0.3907 sec/batch\n",
      "Epoch 9/20  Iteration 2436/5640 Training loss: 1.1953 0.3906 sec/batch\n",
      "Epoch 9/20  Iteration 2437/5640 Training loss: 1.1954 0.3903 sec/batch\n",
      "Epoch 9/20  Iteration 2438/5640 Training loss: 1.1954 0.3923 sec/batch\n",
      "Epoch 9/20  Iteration 2439/5640 Training loss: 1.1954 0.3908 sec/batch\n",
      "Epoch 9/20  Iteration 2440/5640 Training loss: 1.1953 0.3910 sec/batch\n",
      "Epoch 9/20  Iteration 2441/5640 Training loss: 1.1952 0.3900 sec/batch\n",
      "Epoch 9/20  Iteration 2442/5640 Training loss: 1.1949 0.3941 sec/batch\n",
      "Epoch 9/20  Iteration 2443/5640 Training loss: 1.1946 0.3952 sec/batch\n",
      "Epoch 9/20  Iteration 2444/5640 Training loss: 1.1942 0.3942 sec/batch\n",
      "Epoch 9/20  Iteration 2445/5640 Training loss: 1.1941 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 2446/5640 Training loss: 1.1939 0.3943 sec/batch\n",
      "Epoch 9/20  Iteration 2447/5640 Training loss: 1.1938 0.3939 sec/batch\n",
      "Epoch 9/20  Iteration 2448/5640 Training loss: 1.1936 0.3943 sec/batch\n",
      "Epoch 9/20  Iteration 2449/5640 Training loss: 1.1934 0.3929 sec/batch\n",
      "Epoch 9/20  Iteration 2450/5640 Training loss: 1.1934 0.3940 sec/batch\n",
      "Validation loss: 1.14211 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 2451/5640 Training loss: 1.1942 0.3824 sec/batch\n",
      "Epoch 9/20  Iteration 2452/5640 Training loss: 1.1942 0.3815 sec/batch\n",
      "Epoch 9/20  Iteration 2453/5640 Training loss: 1.1941 0.3840 sec/batch\n",
      "Epoch 9/20  Iteration 2454/5640 Training loss: 1.1942 0.3824 sec/batch\n",
      "Epoch 9/20  Iteration 2455/5640 Training loss: 1.1942 0.3827 sec/batch\n",
      "Epoch 9/20  Iteration 2456/5640 Training loss: 1.1941 0.3830 sec/batch\n",
      "Epoch 9/20  Iteration 2457/5640 Training loss: 1.1941 0.3826 sec/batch\n",
      "Epoch 9/20  Iteration 2458/5640 Training loss: 1.1940 0.3848 sec/batch\n",
      "Epoch 9/20  Iteration 2459/5640 Training loss: 1.1940 0.3851 sec/batch\n",
      "Epoch 9/20  Iteration 2460/5640 Training loss: 1.1940 0.3850 sec/batch\n",
      "Epoch 9/20  Iteration 2461/5640 Training loss: 1.1938 0.3849 sec/batch\n",
      "Epoch 9/20  Iteration 2462/5640 Training loss: 1.1938 0.3875 sec/batch\n",
      "Epoch 9/20  Iteration 2463/5640 Training loss: 1.1936 0.3895 sec/batch\n",
      "Epoch 9/20  Iteration 2464/5640 Training loss: 1.1935 0.3887 sec/batch\n",
      "Epoch 9/20  Iteration 2465/5640 Training loss: 1.1935 0.3884 sec/batch\n",
      "Epoch 9/20  Iteration 2466/5640 Training loss: 1.1934 0.3875 sec/batch\n",
      "Epoch 9/20  Iteration 2467/5640 Training loss: 1.1932 0.3866 sec/batch\n",
      "Epoch 9/20  Iteration 2468/5640 Training loss: 1.1931 0.3883 sec/batch\n",
      "Epoch 9/20  Iteration 2469/5640 Training loss: 1.1932 0.3911 sec/batch\n",
      "Epoch 9/20  Iteration 2470/5640 Training loss: 1.1932 0.3913 sec/batch\n",
      "Epoch 9/20  Iteration 2471/5640 Training loss: 1.1931 0.3950 sec/batch\n",
      "Epoch 9/20  Iteration 2472/5640 Training loss: 1.1931 0.3939 sec/batch\n",
      "Epoch 9/20  Iteration 2473/5640 Training loss: 1.1929 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 2474/5640 Training loss: 1.1927 0.3947 sec/batch\n",
      "Epoch 9/20  Iteration 2475/5640 Training loss: 1.1925 0.3931 sec/batch\n",
      "Epoch 9/20  Iteration 2476/5640 Training loss: 1.1923 0.3954 sec/batch\n",
      "Epoch 9/20  Iteration 2477/5640 Training loss: 1.1922 0.3942 sec/batch\n",
      "Epoch 9/20  Iteration 2478/5640 Training loss: 1.1921 0.3934 sec/batch\n",
      "Epoch 9/20  Iteration 2479/5640 Training loss: 1.1920 0.3965 sec/batch\n",
      "Epoch 9/20  Iteration 2480/5640 Training loss: 1.1919 0.3950 sec/batch\n",
      "Epoch 9/20  Iteration 2481/5640 Training loss: 1.1919 0.3938 sec/batch\n",
      "Epoch 9/20  Iteration 2482/5640 Training loss: 1.1918 0.3929 sec/batch\n",
      "Epoch 9/20  Iteration 2483/5640 Training loss: 1.1918 0.3955 sec/batch\n",
      "Epoch 9/20  Iteration 2484/5640 Training loss: 1.1919 0.3936 sec/batch\n",
      "Epoch 9/20  Iteration 2485/5640 Training loss: 1.1918 0.3945 sec/batch\n",
      "Epoch 9/20  Iteration 2486/5640 Training loss: 1.1918 0.3942 sec/batch\n",
      "Epoch 9/20  Iteration 2487/5640 Training loss: 1.1917 0.3944 sec/batch\n",
      "Epoch 9/20  Iteration 2488/5640 Training loss: 1.1917 0.3946 sec/batch\n",
      "Epoch 9/20  Iteration 2489/5640 Training loss: 1.1916 0.3966 sec/batch\n",
      "Epoch 9/20  Iteration 2490/5640 Training loss: 1.1914 0.3932 sec/batch\n",
      "Epoch 9/20  Iteration 2491/5640 Training loss: 1.1914 0.3945 sec/batch\n",
      "Epoch 9/20  Iteration 2492/5640 Training loss: 1.1914 0.3937 sec/batch\n",
      "Epoch 9/20  Iteration 2493/5640 Training loss: 1.1914 0.3937 sec/batch\n",
      "Epoch 9/20  Iteration 2494/5640 Training loss: 1.1915 0.3947 sec/batch\n",
      "Epoch 9/20  Iteration 2495/5640 Training loss: 1.1914 0.3948 sec/batch\n",
      "Epoch 9/20  Iteration 2496/5640 Training loss: 1.1913 0.3941 sec/batch\n",
      "Epoch 9/20  Iteration 2497/5640 Training loss: 1.1912 0.3943 sec/batch\n",
      "Epoch 9/20  Iteration 2498/5640 Training loss: 1.1912 0.3927 sec/batch\n",
      "Epoch 9/20  Iteration 2499/5640 Training loss: 1.1911 0.3946 sec/batch\n",
      "Epoch 9/20  Iteration 2500/5640 Training loss: 1.1909 0.3947 sec/batch\n",
      "Validation loss: 1.14159 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 2501/5640 Training loss: 1.1914 0.3821 sec/batch\n",
      "Epoch 9/20  Iteration 2502/5640 Training loss: 1.1913 0.3819 sec/batch\n",
      "Epoch 9/20  Iteration 2503/5640 Training loss: 1.1912 0.3834 sec/batch\n",
      "Epoch 9/20  Iteration 2504/5640 Training loss: 1.1912 0.3848 sec/batch\n",
      "Epoch 9/20  Iteration 2505/5640 Training loss: 1.1912 0.3823 sec/batch\n",
      "Epoch 9/20  Iteration 2506/5640 Training loss: 1.1912 0.3822 sec/batch\n",
      "Epoch 9/20  Iteration 2507/5640 Training loss: 1.1910 0.3825 sec/batch\n",
      "Epoch 9/20  Iteration 2508/5640 Training loss: 1.1910 0.3815 sec/batch\n",
      "Epoch 9/20  Iteration 2509/5640 Training loss: 1.1908 0.3822 sec/batch\n",
      "Epoch 9/20  Iteration 2510/5640 Training loss: 1.1908 0.3855 sec/batch\n",
      "Epoch 9/20  Iteration 2511/5640 Training loss: 1.1907 0.3867 sec/batch\n",
      "Epoch 9/20  Iteration 2512/5640 Training loss: 1.1906 0.3883 sec/batch\n",
      "Epoch 9/20  Iteration 2513/5640 Training loss: 1.1904 0.3880 sec/batch\n",
      "Epoch 9/20  Iteration 2514/5640 Training loss: 1.1904 0.3902 sec/batch\n",
      "Epoch 9/20  Iteration 2515/5640 Training loss: 1.1904 0.3870 sec/batch\n",
      "Epoch 9/20  Iteration 2516/5640 Training loss: 1.1905 0.3881 sec/batch\n",
      "Epoch 9/20  Iteration 2517/5640 Training loss: 1.1904 0.3875 sec/batch\n",
      "Epoch 9/20  Iteration 2518/5640 Training loss: 1.1905 0.3879 sec/batch\n",
      "Epoch 9/20  Iteration 2519/5640 Training loss: 1.1905 0.3880 sec/batch\n",
      "Epoch 9/20  Iteration 2520/5640 Training loss: 1.1905 0.3883 sec/batch\n",
      "Epoch 9/20  Iteration 2521/5640 Training loss: 1.1905 0.3881 sec/batch\n",
      "Epoch 9/20  Iteration 2522/5640 Training loss: 1.1903 0.3882 sec/batch\n",
      "Epoch 9/20  Iteration 2523/5640 Training loss: 1.1903 0.3870 sec/batch\n",
      "Epoch 9/20  Iteration 2524/5640 Training loss: 1.1901 0.3897 sec/batch\n",
      "Epoch 9/20  Iteration 2525/5640 Training loss: 1.1901 0.3876 sec/batch\n",
      "Epoch 9/20  Iteration 2526/5640 Training loss: 1.1901 0.3877 sec/batch\n",
      "Epoch 9/20  Iteration 2527/5640 Training loss: 1.1900 0.3899 sec/batch\n",
      "Epoch 9/20  Iteration 2528/5640 Training loss: 1.1899 0.3885 sec/batch\n",
      "Epoch 9/20  Iteration 2529/5640 Training loss: 1.1899 0.3891 sec/batch\n",
      "Epoch 9/20  Iteration 2530/5640 Training loss: 1.1898 0.3886 sec/batch\n",
      "Epoch 9/20  Iteration 2531/5640 Training loss: 1.1898 0.3898 sec/batch\n",
      "Epoch 9/20  Iteration 2532/5640 Training loss: 1.1897 0.3904 sec/batch\n",
      "Epoch 9/20  Iteration 2533/5640 Training loss: 1.1897 0.3909 sec/batch\n",
      "Epoch 9/20  Iteration 2534/5640 Training loss: 1.1898 0.3898 sec/batch\n",
      "Epoch 9/20  Iteration 2535/5640 Training loss: 1.1898 0.3928 sec/batch\n",
      "Epoch 9/20  Iteration 2536/5640 Training loss: 1.1899 0.3899 sec/batch\n",
      "Epoch 9/20  Iteration 2537/5640 Training loss: 1.1898 0.3904 sec/batch\n",
      "Epoch 9/20  Iteration 2538/5640 Training loss: 1.1897 0.3929 sec/batch\n",
      "Epoch 10/20  Iteration 2539/5640 Training loss: 1.2867 0.3937 sec/batch\n",
      "Epoch 10/20  Iteration 2540/5640 Training loss: 1.2489 0.3966 sec/batch\n",
      "Epoch 10/20  Iteration 2541/5640 Training loss: 1.2305 0.3960 sec/batch\n",
      "Epoch 10/20  Iteration 2542/5640 Training loss: 1.2227 0.4035 sec/batch\n",
      "Epoch 10/20  Iteration 2543/5640 Training loss: 1.2156 0.3948 sec/batch\n",
      "Epoch 10/20  Iteration 2544/5640 Training loss: 1.2140 0.3958 sec/batch\n",
      "Epoch 10/20  Iteration 2545/5640 Training loss: 1.2088 0.3941 sec/batch\n",
      "Epoch 10/20  Iteration 2546/5640 Training loss: 1.2063 0.3940 sec/batch\n",
      "Epoch 10/20  Iteration 2547/5640 Training loss: 1.2034 0.4020 sec/batch\n",
      "Epoch 10/20  Iteration 2548/5640 Training loss: 1.1996 0.3940 sec/batch\n",
      "Epoch 10/20  Iteration 2549/5640 Training loss: 1.1991 0.3951 sec/batch\n",
      "Epoch 10/20  Iteration 2550/5640 Training loss: 1.1959 0.3950 sec/batch\n",
      "Validation loss: 1.14108 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 2551/5640 Training loss: 1.2049 0.3829 sec/batch\n",
      "Epoch 10/20  Iteration 2552/5640 Training loss: 1.2048 0.3855 sec/batch\n",
      "Epoch 10/20  Iteration 2553/5640 Training loss: 1.2015 0.3822 sec/batch\n",
      "Epoch 10/20  Iteration 2554/5640 Training loss: 1.2019 0.3830 sec/batch\n",
      "Epoch 10/20  Iteration 2555/5640 Training loss: 1.2014 0.3825 sec/batch\n",
      "Epoch 10/20  Iteration 2556/5640 Training loss: 1.1999 0.3850 sec/batch\n",
      "Epoch 10/20  Iteration 2557/5640 Training loss: 1.1991 0.3846 sec/batch\n",
      "Epoch 10/20  Iteration 2558/5640 Training loss: 1.1974 0.3857 sec/batch\n",
      "Epoch 10/20  Iteration 2559/5640 Training loss: 1.1964 0.3852 sec/batch\n",
      "Epoch 10/20  Iteration 2560/5640 Training loss: 1.1957 0.3879 sec/batch\n",
      "Epoch 10/20  Iteration 2561/5640 Training loss: 1.1947 0.3967 sec/batch\n",
      "Epoch 10/20  Iteration 2562/5640 Training loss: 1.1928 0.3857 sec/batch\n",
      "Epoch 10/20  Iteration 2563/5640 Training loss: 1.1923 0.3879 sec/batch\n",
      "Epoch 10/20  Iteration 2564/5640 Training loss: 1.1916 0.3893 sec/batch\n",
      "Epoch 10/20  Iteration 2565/5640 Training loss: 1.1917 0.3923 sec/batch\n",
      "Epoch 10/20  Iteration 2566/5640 Training loss: 1.1912 0.3922 sec/batch\n",
      "Epoch 10/20  Iteration 2567/5640 Training loss: 1.1904 0.3920 sec/batch\n",
      "Epoch 10/20  Iteration 2568/5640 Training loss: 1.1898 0.3982 sec/batch\n",
      "Epoch 10/20  Iteration 2569/5640 Training loss: 1.1884 0.3903 sec/batch\n",
      "Epoch 10/20  Iteration 2570/5640 Training loss: 1.1876 0.3904 sec/batch\n",
      "Epoch 10/20  Iteration 2571/5640 Training loss: 1.1866 0.4047 sec/batch\n",
      "Epoch 10/20  Iteration 2572/5640 Training loss: 1.1857 0.4093 sec/batch\n",
      "Epoch 10/20  Iteration 2573/5640 Training loss: 1.1856 0.3936 sec/batch\n",
      "Epoch 10/20  Iteration 2574/5640 Training loss: 1.1853 0.3910 sec/batch\n",
      "Epoch 10/20  Iteration 2575/5640 Training loss: 1.1843 0.3912 sec/batch\n",
      "Epoch 10/20  Iteration 2576/5640 Training loss: 1.1831 0.3897 sec/batch\n",
      "Epoch 10/20  Iteration 2577/5640 Training loss: 1.1827 0.3958 sec/batch\n",
      "Epoch 10/20  Iteration 2578/5640 Training loss: 1.1818 0.3918 sec/batch\n",
      "Epoch 10/20  Iteration 2579/5640 Training loss: 1.1811 0.3957 sec/batch\n",
      "Epoch 10/20  Iteration 2580/5640 Training loss: 1.1817 0.3947 sec/batch\n",
      "Epoch 10/20  Iteration 2581/5640 Training loss: 1.1807 0.3968 sec/batch\n",
      "Epoch 10/20  Iteration 2582/5640 Training loss: 1.1804 0.3963 sec/batch\n",
      "Epoch 10/20  Iteration 2583/5640 Training loss: 1.1801 0.3923 sec/batch\n",
      "Epoch 10/20  Iteration 2584/5640 Training loss: 1.1797 0.3899 sec/batch\n",
      "Epoch 10/20  Iteration 2585/5640 Training loss: 1.1796 0.3912 sec/batch\n",
      "Epoch 10/20  Iteration 2586/5640 Training loss: 1.1796 0.3898 sec/batch\n",
      "Epoch 10/20  Iteration 2587/5640 Training loss: 1.1797 0.3917 sec/batch\n",
      "Epoch 10/20  Iteration 2588/5640 Training loss: 1.1793 0.3911 sec/batch\n",
      "Epoch 10/20  Iteration 2589/5640 Training loss: 1.1793 0.3940 sec/batch\n",
      "Epoch 10/20  Iteration 2590/5640 Training loss: 1.1791 0.3933 sec/batch\n",
      "Epoch 10/20  Iteration 2591/5640 Training loss: 1.1789 0.3976 sec/batch\n",
      "Epoch 10/20  Iteration 2592/5640 Training loss: 1.1790 0.3940 sec/batch\n",
      "Epoch 10/20  Iteration 2593/5640 Training loss: 1.1784 0.3926 sec/batch\n",
      "Epoch 10/20  Iteration 2594/5640 Training loss: 1.1780 0.3946 sec/batch\n",
      "Epoch 10/20  Iteration 2595/5640 Training loss: 1.1778 0.3950 sec/batch\n",
      "Epoch 10/20  Iteration 2596/5640 Training loss: 1.1779 0.3945 sec/batch\n",
      "Epoch 10/20  Iteration 2597/5640 Training loss: 1.1779 0.3971 sec/batch\n",
      "Epoch 10/20  Iteration 2598/5640 Training loss: 1.1777 0.3982 sec/batch\n",
      "Epoch 10/20  Iteration 2599/5640 Training loss: 1.1775 0.3938 sec/batch\n",
      "Epoch 10/20  Iteration 2600/5640 Training loss: 1.1769 0.3955 sec/batch\n",
      "Validation loss: 1.13081 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 2601/5640 Training loss: 1.1791 0.3824 sec/batch\n",
      "Epoch 10/20  Iteration 2602/5640 Training loss: 1.1786 0.3830 sec/batch\n",
      "Epoch 10/20  Iteration 2603/5640 Training loss: 1.1789 0.3839 sec/batch\n",
      "Epoch 10/20  Iteration 2604/5640 Training loss: 1.1787 0.3819 sec/batch\n",
      "Epoch 10/20  Iteration 2605/5640 Training loss: 1.1789 0.3837 sec/batch\n",
      "Epoch 10/20  Iteration 2606/5640 Training loss: 1.1784 0.3835 sec/batch\n",
      "Epoch 10/20  Iteration 2607/5640 Training loss: 1.1780 0.3821 sec/batch\n",
      "Epoch 10/20  Iteration 2608/5640 Training loss: 1.1773 0.3827 sec/batch\n",
      "Epoch 10/20  Iteration 2609/5640 Training loss: 1.1771 0.3828 sec/batch\n",
      "Epoch 10/20  Iteration 2610/5640 Training loss: 1.1770 0.3829 sec/batch\n",
      "Epoch 10/20  Iteration 2611/5640 Training loss: 1.1771 0.3848 sec/batch\n",
      "Epoch 10/20  Iteration 2612/5640 Training loss: 1.1771 0.3883 sec/batch\n",
      "Epoch 10/20  Iteration 2613/5640 Training loss: 1.1772 0.3888 sec/batch\n",
      "Epoch 10/20  Iteration 2614/5640 Training loss: 1.1778 0.3889 sec/batch\n",
      "Epoch 10/20  Iteration 2615/5640 Training loss: 1.1778 0.3891 sec/batch\n",
      "Epoch 10/20  Iteration 2616/5640 Training loss: 1.1783 0.3948 sec/batch\n",
      "Epoch 10/20  Iteration 2617/5640 Training loss: 1.1783 0.3906 sec/batch\n",
      "Epoch 10/20  Iteration 2618/5640 Training loss: 1.1785 0.3911 sec/batch\n",
      "Epoch 10/20  Iteration 2619/5640 Training loss: 1.1787 0.3916 sec/batch\n",
      "Epoch 10/20  Iteration 2620/5640 Training loss: 1.1783 0.3892 sec/batch\n",
      "Epoch 10/20  Iteration 2621/5640 Training loss: 1.1781 0.3919 sec/batch\n",
      "Epoch 10/20  Iteration 2622/5640 Training loss: 1.1777 0.3910 sec/batch\n",
      "Epoch 10/20  Iteration 2623/5640 Training loss: 1.1775 0.3964 sec/batch\n",
      "Epoch 10/20  Iteration 2624/5640 Training loss: 1.1773 0.4026 sec/batch\n",
      "Epoch 10/20  Iteration 2625/5640 Training loss: 1.1771 0.4029 sec/batch\n",
      "Epoch 10/20  Iteration 2626/5640 Training loss: 1.1768 0.3915 sec/batch\n",
      "Epoch 10/20  Iteration 2627/5640 Training loss: 1.1766 0.3953 sec/batch\n",
      "Epoch 10/20  Iteration 2628/5640 Training loss: 1.1765 0.3933 sec/batch\n",
      "Epoch 10/20  Iteration 2629/5640 Training loss: 1.1766 0.3942 sec/batch\n",
      "Epoch 10/20  Iteration 2630/5640 Training loss: 1.1765 0.3957 sec/batch\n",
      "Epoch 10/20  Iteration 2631/5640 Training loss: 1.1765 0.3994 sec/batch\n",
      "Epoch 10/20  Iteration 2632/5640 Training loss: 1.1763 0.4058 sec/batch\n",
      "Epoch 10/20  Iteration 2633/5640 Training loss: 1.1761 0.3977 sec/batch\n",
      "Epoch 10/20  Iteration 2634/5640 Training loss: 1.1761 0.3947 sec/batch\n",
      "Epoch 10/20  Iteration 2635/5640 Training loss: 1.1763 0.3978 sec/batch\n",
      "Epoch 10/20  Iteration 2636/5640 Training loss: 1.1763 0.3945 sec/batch\n",
      "Epoch 10/20  Iteration 2637/5640 Training loss: 1.1762 0.3952 sec/batch\n",
      "Epoch 10/20  Iteration 2638/5640 Training loss: 1.1759 0.3933 sec/batch\n",
      "Epoch 10/20  Iteration 2639/5640 Training loss: 1.1756 0.3954 sec/batch\n",
      "Epoch 10/20  Iteration 2640/5640 Training loss: 1.1755 0.3959 sec/batch\n",
      "Epoch 10/20  Iteration 2641/5640 Training loss: 1.1754 0.3964 sec/batch\n",
      "Epoch 10/20  Iteration 2642/5640 Training loss: 1.1751 0.3937 sec/batch\n",
      "Epoch 10/20  Iteration 2643/5640 Training loss: 1.1748 0.3946 sec/batch\n",
      "Epoch 10/20  Iteration 2644/5640 Training loss: 1.1746 0.3951 sec/batch\n",
      "Epoch 10/20  Iteration 2645/5640 Training loss: 1.1743 0.3975 sec/batch\n",
      "Epoch 10/20  Iteration 2646/5640 Training loss: 1.1739 0.3950 sec/batch\n",
      "Epoch 10/20  Iteration 2647/5640 Training loss: 1.1737 0.3957 sec/batch\n",
      "Epoch 10/20  Iteration 2648/5640 Training loss: 1.1737 0.3949 sec/batch\n",
      "Epoch 10/20  Iteration 2649/5640 Training loss: 1.1734 0.3945 sec/batch\n",
      "Epoch 10/20  Iteration 2650/5640 Training loss: 1.1735 0.3978 sec/batch\n",
      "Validation loss: 1.13452 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 2651/5640 Training loss: 1.1746 0.3830 sec/batch\n",
      "Epoch 10/20  Iteration 2652/5640 Training loss: 1.1745 0.3828 sec/batch\n",
      "Epoch 10/20  Iteration 2653/5640 Training loss: 1.1746 0.3839 sec/batch\n",
      "Epoch 10/20  Iteration 2654/5640 Training loss: 1.1745 0.3821 sec/batch\n",
      "Epoch 10/20  Iteration 2655/5640 Training loss: 1.1744 0.3825 sec/batch\n",
      "Epoch 10/20  Iteration 2656/5640 Training loss: 1.1742 0.3834 sec/batch\n",
      "Epoch 10/20  Iteration 2657/5640 Training loss: 1.1741 0.3817 sec/batch\n",
      "Epoch 10/20  Iteration 2658/5640 Training loss: 1.1738 0.3863 sec/batch\n",
      "Epoch 10/20  Iteration 2659/5640 Training loss: 1.1737 0.3863 sec/batch\n",
      "Epoch 10/20  Iteration 2660/5640 Training loss: 1.1737 0.3880 sec/batch\n",
      "Epoch 10/20  Iteration 2661/5640 Training loss: 1.1734 0.3850 sec/batch\n",
      "Epoch 10/20  Iteration 2662/5640 Training loss: 1.1733 0.3895 sec/batch\n",
      "Epoch 10/20  Iteration 2663/5640 Training loss: 1.1737 0.3948 sec/batch\n",
      "Epoch 10/20  Iteration 2664/5640 Training loss: 1.1733 0.4077 sec/batch\n",
      "Epoch 10/20  Iteration 2665/5640 Training loss: 1.1732 0.4030 sec/batch\n",
      "Epoch 10/20  Iteration 2666/5640 Training loss: 1.1729 0.4106 sec/batch\n",
      "Epoch 10/20  Iteration 2667/5640 Training loss: 1.1727 0.4091 sec/batch\n",
      "Epoch 10/20  Iteration 2668/5640 Training loss: 1.1726 0.3975 sec/batch\n",
      "Epoch 10/20  Iteration 2669/5640 Training loss: 1.1725 0.3912 sec/batch\n",
      "Epoch 10/20  Iteration 2670/5640 Training loss: 1.1722 0.3901 sec/batch\n",
      "Epoch 10/20  Iteration 2671/5640 Training loss: 1.1719 0.3945 sec/batch\n",
      "Epoch 10/20  Iteration 2672/5640 Training loss: 1.1715 0.3903 sec/batch\n",
      "Epoch 10/20  Iteration 2673/5640 Training loss: 1.1714 0.3909 sec/batch\n",
      "Epoch 10/20  Iteration 2674/5640 Training loss: 1.1714 0.3903 sec/batch\n",
      "Epoch 10/20  Iteration 2675/5640 Training loss: 1.1713 0.3935 sec/batch\n",
      "Epoch 10/20  Iteration 2676/5640 Training loss: 1.1712 0.3910 sec/batch\n",
      "Epoch 10/20  Iteration 2677/5640 Training loss: 1.1711 0.3917 sec/batch\n",
      "Epoch 10/20  Iteration 2678/5640 Training loss: 1.1711 0.3937 sec/batch\n",
      "Epoch 10/20  Iteration 2679/5640 Training loss: 1.1709 0.3979 sec/batch\n",
      "Epoch 10/20  Iteration 2680/5640 Training loss: 1.1709 0.3975 sec/batch\n",
      "Epoch 10/20  Iteration 2681/5640 Training loss: 1.1709 0.3975 sec/batch\n",
      "Epoch 10/20  Iteration 2682/5640 Training loss: 1.1706 0.3974 sec/batch\n",
      "Epoch 10/20  Iteration 2683/5640 Training loss: 1.1703 0.3973 sec/batch\n",
      "Epoch 10/20  Iteration 2684/5640 Training loss: 1.1700 0.3906 sec/batch\n",
      "Epoch 10/20  Iteration 2685/5640 Training loss: 1.1697 0.4012 sec/batch\n",
      "Epoch 10/20  Iteration 2686/5640 Training loss: 1.1698 0.3972 sec/batch\n",
      "Epoch 10/20  Iteration 2687/5640 Training loss: 1.1698 0.3981 sec/batch\n",
      "Epoch 10/20  Iteration 2688/5640 Training loss: 1.1695 0.3980 sec/batch\n",
      "Epoch 10/20  Iteration 2689/5640 Training loss: 1.1694 0.3958 sec/batch\n",
      "Epoch 10/20  Iteration 2690/5640 Training loss: 1.1695 0.3950 sec/batch\n",
      "Epoch 10/20  Iteration 2691/5640 Training loss: 1.1697 0.3980 sec/batch\n",
      "Epoch 10/20  Iteration 2692/5640 Training loss: 1.1696 0.3952 sec/batch\n",
      "Epoch 10/20  Iteration 2693/5640 Training loss: 1.1695 0.3942 sec/batch\n",
      "Epoch 10/20  Iteration 2694/5640 Training loss: 1.1696 0.3978 sec/batch\n",
      "Epoch 10/20  Iteration 2695/5640 Training loss: 1.1693 0.3981 sec/batch\n",
      "Epoch 10/20  Iteration 2696/5640 Training loss: 1.1692 0.3980 sec/batch\n",
      "Epoch 10/20  Iteration 2697/5640 Training loss: 1.1691 0.3980 sec/batch\n",
      "Epoch 10/20  Iteration 2698/5640 Training loss: 1.1689 0.3955 sec/batch\n",
      "Epoch 10/20  Iteration 2699/5640 Training loss: 1.1688 0.3991 sec/batch\n",
      "Epoch 10/20  Iteration 2700/5640 Training loss: 1.1686 0.3968 sec/batch\n",
      "Validation loss: 1.1281 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 2701/5640 Training loss: 1.1695 0.3865 sec/batch\n",
      "Epoch 10/20  Iteration 2702/5640 Training loss: 1.1693 0.3880 sec/batch\n",
      "Epoch 10/20  Iteration 2703/5640 Training loss: 1.1694 0.3886 sec/batch\n",
      "Epoch 10/20  Iteration 2704/5640 Training loss: 1.1695 0.3894 sec/batch\n",
      "Epoch 10/20  Iteration 2705/5640 Training loss: 1.1694 0.3864 sec/batch\n",
      "Epoch 10/20  Iteration 2706/5640 Training loss: 1.1694 0.3872 sec/batch\n",
      "Epoch 10/20  Iteration 2707/5640 Training loss: 1.1694 0.3904 sec/batch\n",
      "Epoch 10/20  Iteration 2708/5640 Training loss: 1.1694 0.3849 sec/batch\n",
      "Epoch 10/20  Iteration 2709/5640 Training loss: 1.1693 0.3870 sec/batch\n",
      "Epoch 10/20  Iteration 2710/5640 Training loss: 1.1692 0.3848 sec/batch\n",
      "Epoch 10/20  Iteration 2711/5640 Training loss: 1.1691 0.3893 sec/batch\n",
      "Epoch 10/20  Iteration 2712/5640 Training loss: 1.1691 0.3912 sec/batch\n",
      "Epoch 10/20  Iteration 2713/5640 Training loss: 1.1690 0.3906 sec/batch\n",
      "Epoch 10/20  Iteration 2714/5640 Training loss: 1.1689 0.3894 sec/batch\n",
      "Epoch 10/20  Iteration 2715/5640 Training loss: 1.1689 0.3903 sec/batch\n",
      "Epoch 10/20  Iteration 2716/5640 Training loss: 1.1687 0.3907 sec/batch\n",
      "Epoch 10/20  Iteration 2717/5640 Training loss: 1.1689 0.3948 sec/batch\n",
      "Epoch 10/20  Iteration 2718/5640 Training loss: 1.1689 0.3889 sec/batch\n",
      "Epoch 10/20  Iteration 2719/5640 Training loss: 1.1690 0.3932 sec/batch\n",
      "Epoch 10/20  Iteration 2720/5640 Training loss: 1.1690 0.3935 sec/batch\n",
      "Epoch 10/20  Iteration 2721/5640 Training loss: 1.1690 0.3910 sec/batch\n",
      "Epoch 10/20  Iteration 2722/5640 Training loss: 1.1689 0.3897 sec/batch\n",
      "Epoch 10/20  Iteration 2723/5640 Training loss: 1.1688 0.3923 sec/batch\n",
      "Epoch 10/20  Iteration 2724/5640 Training loss: 1.1685 0.3947 sec/batch\n",
      "Epoch 10/20  Iteration 2725/5640 Training loss: 1.1682 0.3908 sec/batch\n",
      "Epoch 10/20  Iteration 2726/5640 Training loss: 1.1678 0.3901 sec/batch\n",
      "Epoch 10/20  Iteration 2727/5640 Training loss: 1.1677 0.3918 sec/batch\n",
      "Epoch 10/20  Iteration 2728/5640 Training loss: 1.1674 0.3915 sec/batch\n",
      "Epoch 10/20  Iteration 2729/5640 Training loss: 1.1674 0.3915 sec/batch\n",
      "Epoch 10/20  Iteration 2730/5640 Training loss: 1.1672 0.3900 sec/batch\n",
      "Epoch 10/20  Iteration 2731/5640 Training loss: 1.1670 0.3904 sec/batch\n",
      "Epoch 10/20  Iteration 2732/5640 Training loss: 1.1670 0.3908 sec/batch\n",
      "Epoch 10/20  Iteration 2733/5640 Training loss: 1.1671 0.3900 sec/batch\n",
      "Epoch 10/20  Iteration 2734/5640 Training loss: 1.1670 0.3930 sec/batch\n",
      "Epoch 10/20  Iteration 2735/5640 Training loss: 1.1669 0.3957 sec/batch\n",
      "Epoch 10/20  Iteration 2736/5640 Training loss: 1.1670 0.3907 sec/batch\n",
      "Epoch 10/20  Iteration 2737/5640 Training loss: 1.1670 0.3908 sec/batch\n",
      "Epoch 10/20  Iteration 2738/5640 Training loss: 1.1669 0.3911 sec/batch\n",
      "Epoch 10/20  Iteration 2739/5640 Training loss: 1.1668 0.3894 sec/batch\n",
      "Epoch 10/20  Iteration 2740/5640 Training loss: 1.1667 0.3915 sec/batch\n",
      "Epoch 10/20  Iteration 2741/5640 Training loss: 1.1667 0.3942 sec/batch\n",
      "Epoch 10/20  Iteration 2742/5640 Training loss: 1.1667 0.3955 sec/batch\n",
      "Epoch 10/20  Iteration 2743/5640 Training loss: 1.1664 0.3914 sec/batch\n",
      "Epoch 10/20  Iteration 2744/5640 Training loss: 1.1664 0.3960 sec/batch\n",
      "Epoch 10/20  Iteration 2745/5640 Training loss: 1.1663 0.3985 sec/batch\n",
      "Epoch 10/20  Iteration 2746/5640 Training loss: 1.1661 0.3957 sec/batch\n",
      "Epoch 10/20  Iteration 2747/5640 Training loss: 1.1661 0.3943 sec/batch\n",
      "Epoch 10/20  Iteration 2748/5640 Training loss: 1.1660 0.3944 sec/batch\n",
      "Epoch 10/20  Iteration 2749/5640 Training loss: 1.1658 0.3944 sec/batch\n",
      "Epoch 10/20  Iteration 2750/5640 Training loss: 1.1658 0.3989 sec/batch\n",
      "Validation loss: 1.12457 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 2751/5640 Training loss: 1.1666 0.3865 sec/batch\n",
      "Epoch 10/20  Iteration 2752/5640 Training loss: 1.1666 0.3899 sec/batch\n",
      "Epoch 10/20  Iteration 2753/5640 Training loss: 1.1665 0.3829 sec/batch\n",
      "Epoch 10/20  Iteration 2754/5640 Training loss: 1.1664 0.3878 sec/batch\n",
      "Epoch 10/20  Iteration 2755/5640 Training loss: 1.1663 0.3868 sec/batch\n",
      "Epoch 10/20  Iteration 2756/5640 Training loss: 1.1661 0.3853 sec/batch\n",
      "Epoch 10/20  Iteration 2757/5640 Training loss: 1.1660 0.3834 sec/batch\n",
      "Epoch 10/20  Iteration 2758/5640 Training loss: 1.1658 0.3836 sec/batch\n",
      "Epoch 10/20  Iteration 2759/5640 Training loss: 1.1657 0.3834 sec/batch\n",
      "Epoch 10/20  Iteration 2760/5640 Training loss: 1.1656 0.3847 sec/batch\n",
      "Epoch 10/20  Iteration 2761/5640 Training loss: 1.1656 0.3835 sec/batch\n",
      "Epoch 10/20  Iteration 2762/5640 Training loss: 1.1655 0.3897 sec/batch\n",
      "Epoch 10/20  Iteration 2763/5640 Training loss: 1.1654 0.3916 sec/batch\n",
      "Epoch 10/20  Iteration 2764/5640 Training loss: 1.1653 0.3893 sec/batch\n",
      "Epoch 10/20  Iteration 2765/5640 Training loss: 1.1653 0.3900 sec/batch\n",
      "Epoch 10/20  Iteration 2766/5640 Training loss: 1.1654 0.3917 sec/batch\n",
      "Epoch 10/20  Iteration 2767/5640 Training loss: 1.1654 0.3931 sec/batch\n",
      "Epoch 10/20  Iteration 2768/5640 Training loss: 1.1654 0.3906 sec/batch\n",
      "Epoch 10/20  Iteration 2769/5640 Training loss: 1.1654 0.3885 sec/batch\n",
      "Epoch 10/20  Iteration 2770/5640 Training loss: 1.1653 0.3875 sec/batch\n",
      "Epoch 10/20  Iteration 2771/5640 Training loss: 1.1652 0.3937 sec/batch\n",
      "Epoch 10/20  Iteration 2772/5640 Training loss: 1.1650 0.4001 sec/batch\n",
      "Epoch 10/20  Iteration 2773/5640 Training loss: 1.1650 0.3975 sec/batch\n",
      "Epoch 10/20  Iteration 2774/5640 Training loss: 1.1650 0.3973 sec/batch\n",
      "Epoch 10/20  Iteration 2775/5640 Training loss: 1.1649 0.3978 sec/batch\n",
      "Epoch 10/20  Iteration 2776/5640 Training loss: 1.1650 0.3973 sec/batch\n",
      "Epoch 10/20  Iteration 2777/5640 Training loss: 1.1649 0.4005 sec/batch\n",
      "Epoch 10/20  Iteration 2778/5640 Training loss: 1.1648 0.3990 sec/batch\n",
      "Epoch 10/20  Iteration 2779/5640 Training loss: 1.1648 0.3988 sec/batch\n",
      "Epoch 10/20  Iteration 2780/5640 Training loss: 1.1647 0.3985 sec/batch\n",
      "Epoch 10/20  Iteration 2781/5640 Training loss: 1.1646 0.3989 sec/batch\n",
      "Epoch 10/20  Iteration 2782/5640 Training loss: 1.1644 0.3895 sec/batch\n",
      "Epoch 10/20  Iteration 2783/5640 Training loss: 1.1643 0.3927 sec/batch\n",
      "Epoch 10/20  Iteration 2784/5640 Training loss: 1.1641 0.3920 sec/batch\n",
      "Epoch 10/20  Iteration 2785/5640 Training loss: 1.1641 0.3905 sec/batch\n",
      "Epoch 10/20  Iteration 2786/5640 Training loss: 1.1641 0.3956 sec/batch\n",
      "Epoch 10/20  Iteration 2787/5640 Training loss: 1.1641 0.3936 sec/batch\n",
      "Epoch 10/20  Iteration 2788/5640 Training loss: 1.1641 0.3908 sec/batch\n",
      "Epoch 10/20  Iteration 2789/5640 Training loss: 1.1640 0.3906 sec/batch\n",
      "Epoch 10/20  Iteration 2790/5640 Training loss: 1.1639 0.3940 sec/batch\n",
      "Epoch 10/20  Iteration 2791/5640 Training loss: 1.1638 0.3897 sec/batch\n",
      "Epoch 10/20  Iteration 2792/5640 Training loss: 1.1637 0.3930 sec/batch\n",
      "Epoch 10/20  Iteration 2793/5640 Training loss: 1.1637 0.3932 sec/batch\n",
      "Epoch 10/20  Iteration 2794/5640 Training loss: 1.1635 0.3920 sec/batch\n",
      "Epoch 10/20  Iteration 2795/5640 Training loss: 1.1634 0.3915 sec/batch\n",
      "Epoch 10/20  Iteration 2796/5640 Training loss: 1.1634 0.3904 sec/batch\n",
      "Epoch 10/20  Iteration 2797/5640 Training loss: 1.1634 0.3925 sec/batch\n",
      "Epoch 10/20  Iteration 2798/5640 Training loss: 1.1634 0.3929 sec/batch\n",
      "Epoch 10/20  Iteration 2799/5640 Training loss: 1.1634 0.3930 sec/batch\n",
      "Epoch 10/20  Iteration 2800/5640 Training loss: 1.1634 0.3947 sec/batch\n",
      "Validation loss: 1.12324 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 2801/5640 Training loss: 1.1640 0.3856 sec/batch\n",
      "Epoch 10/20  Iteration 2802/5640 Training loss: 1.1640 0.3812 sec/batch\n",
      "Epoch 10/20  Iteration 2803/5640 Training loss: 1.1640 0.3820 sec/batch\n",
      "Epoch 10/20  Iteration 2804/5640 Training loss: 1.1639 0.3826 sec/batch\n",
      "Epoch 10/20  Iteration 2805/5640 Training loss: 1.1638 0.3827 sec/batch\n",
      "Epoch 10/20  Iteration 2806/5640 Training loss: 1.1636 0.3825 sec/batch\n",
      "Epoch 10/20  Iteration 2807/5640 Training loss: 1.1636 0.3826 sec/batch\n",
      "Epoch 10/20  Iteration 2808/5640 Training loss: 1.1636 0.3865 sec/batch\n",
      "Epoch 10/20  Iteration 2809/5640 Training loss: 1.1635 0.3866 sec/batch\n",
      "Epoch 10/20  Iteration 2810/5640 Training loss: 1.1634 0.3896 sec/batch\n",
      "Epoch 10/20  Iteration 2811/5640 Training loss: 1.1634 0.3912 sec/batch\n",
      "Epoch 10/20  Iteration 2812/5640 Training loss: 1.1633 0.3903 sec/batch\n",
      "Epoch 10/20  Iteration 2813/5640 Training loss: 1.1633 0.3895 sec/batch\n",
      "Epoch 10/20  Iteration 2814/5640 Training loss: 1.1632 0.3905 sec/batch\n",
      "Epoch 10/20  Iteration 2815/5640 Training loss: 1.1632 0.3917 sec/batch\n",
      "Epoch 10/20  Iteration 2816/5640 Training loss: 1.1632 0.3915 sec/batch\n",
      "Epoch 10/20  Iteration 2817/5640 Training loss: 1.1632 0.3914 sec/batch\n",
      "Epoch 10/20  Iteration 2818/5640 Training loss: 1.1632 0.3919 sec/batch\n",
      "Epoch 10/20  Iteration 2819/5640 Training loss: 1.1631 0.3906 sec/batch\n",
      "Epoch 10/20  Iteration 2820/5640 Training loss: 1.1630 0.3922 sec/batch\n",
      "Epoch 11/20  Iteration 2821/5640 Training loss: 1.2519 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 2822/5640 Training loss: 1.2125 0.3952 sec/batch\n",
      "Epoch 11/20  Iteration 2823/5640 Training loss: 1.1949 0.3940 sec/batch\n",
      "Epoch 11/20  Iteration 2824/5640 Training loss: 1.1867 0.3943 sec/batch\n",
      "Epoch 11/20  Iteration 2825/5640 Training loss: 1.1817 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 2826/5640 Training loss: 1.1781 0.3957 sec/batch\n",
      "Epoch 11/20  Iteration 2827/5640 Training loss: 1.1716 0.3941 sec/batch\n",
      "Epoch 11/20  Iteration 2828/5640 Training loss: 1.1698 0.3928 sec/batch\n",
      "Epoch 11/20  Iteration 2829/5640 Training loss: 1.1658 0.3959 sec/batch\n",
      "Epoch 11/20  Iteration 2830/5640 Training loss: 1.1620 0.3952 sec/batch\n",
      "Epoch 11/20  Iteration 2831/5640 Training loss: 1.1624 0.3984 sec/batch\n",
      "Epoch 11/20  Iteration 2832/5640 Training loss: 1.1599 0.3945 sec/batch\n",
      "Epoch 11/20  Iteration 2833/5640 Training loss: 1.1588 0.3940 sec/batch\n",
      "Epoch 11/20  Iteration 2834/5640 Training loss: 1.1589 0.3980 sec/batch\n",
      "Epoch 11/20  Iteration 2835/5640 Training loss: 1.1577 0.3930 sec/batch\n",
      "Epoch 11/20  Iteration 2836/5640 Training loss: 1.1585 0.3946 sec/batch\n",
      "Epoch 11/20  Iteration 2837/5640 Training loss: 1.1587 0.3957 sec/batch\n",
      "Epoch 11/20  Iteration 2838/5640 Training loss: 1.1574 0.3974 sec/batch\n",
      "Epoch 11/20  Iteration 2839/5640 Training loss: 1.1574 0.3973 sec/batch\n",
      "Epoch 11/20  Iteration 2840/5640 Training loss: 1.1562 0.3999 sec/batch\n",
      "Epoch 11/20  Iteration 2841/5640 Training loss: 1.1557 0.3990 sec/batch\n",
      "Epoch 11/20  Iteration 2842/5640 Training loss: 1.1556 0.3958 sec/batch\n",
      "Epoch 11/20  Iteration 2843/5640 Training loss: 1.1552 0.3956 sec/batch\n",
      "Epoch 11/20  Iteration 2844/5640 Training loss: 1.1535 0.3919 sec/batch\n",
      "Epoch 11/20  Iteration 2845/5640 Training loss: 1.1535 0.3890 sec/batch\n",
      "Epoch 11/20  Iteration 2846/5640 Training loss: 1.1529 0.3923 sec/batch\n",
      "Epoch 11/20  Iteration 2847/5640 Training loss: 1.1532 0.3915 sec/batch\n",
      "Epoch 11/20  Iteration 2848/5640 Training loss: 1.1533 0.3910 sec/batch\n",
      "Epoch 11/20  Iteration 2849/5640 Training loss: 1.1528 0.3935 sec/batch\n",
      "Epoch 11/20  Iteration 2850/5640 Training loss: 1.1525 0.3928 sec/batch\n",
      "Validation loss: 1.11842 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 2851/5640 Training loss: 1.1568 0.3822 sec/batch\n",
      "Epoch 11/20  Iteration 2852/5640 Training loss: 1.1562 0.3837 sec/batch\n",
      "Epoch 11/20  Iteration 2853/5640 Training loss: 1.1552 0.3816 sec/batch\n",
      "Epoch 11/20  Iteration 2854/5640 Training loss: 1.1548 0.3811 sec/batch\n",
      "Epoch 11/20  Iteration 2855/5640 Training loss: 1.1549 0.3830 sec/batch\n",
      "Epoch 11/20  Iteration 2856/5640 Training loss: 1.1550 0.3853 sec/batch\n",
      "Epoch 11/20  Iteration 2857/5640 Training loss: 1.1543 0.3813 sec/batch\n",
      "Epoch 11/20  Iteration 2858/5640 Training loss: 1.1532 0.3844 sec/batch\n",
      "Epoch 11/20  Iteration 2859/5640 Training loss: 1.1531 0.3871 sec/batch\n",
      "Epoch 11/20  Iteration 2860/5640 Training loss: 1.1524 0.3858 sec/batch\n",
      "Epoch 11/20  Iteration 2861/5640 Training loss: 1.1519 0.3838 sec/batch\n",
      "Epoch 11/20  Iteration 2862/5640 Training loss: 1.1524 0.3868 sec/batch\n",
      "Epoch 11/20  Iteration 2863/5640 Training loss: 1.1513 0.3864 sec/batch\n",
      "Epoch 11/20  Iteration 2864/5640 Training loss: 1.1511 0.3901 sec/batch\n",
      "Epoch 11/20  Iteration 2865/5640 Training loss: 1.1508 0.3883 sec/batch\n",
      "Epoch 11/20  Iteration 2866/5640 Training loss: 1.1506 0.3894 sec/batch\n",
      "Epoch 11/20  Iteration 2867/5640 Training loss: 1.1506 0.3951 sec/batch\n",
      "Epoch 11/20  Iteration 2868/5640 Training loss: 1.1508 0.3981 sec/batch\n",
      "Epoch 11/20  Iteration 2869/5640 Training loss: 1.1509 0.3995 sec/batch\n",
      "Epoch 11/20  Iteration 2870/5640 Training loss: 1.1506 0.4001 sec/batch\n",
      "Epoch 11/20  Iteration 2871/5640 Training loss: 1.1505 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 2872/5640 Training loss: 1.1506 0.3960 sec/batch\n",
      "Epoch 11/20  Iteration 2873/5640 Training loss: 1.1506 0.3937 sec/batch\n",
      "Epoch 11/20  Iteration 2874/5640 Training loss: 1.1507 0.3929 sec/batch\n",
      "Epoch 11/20  Iteration 2875/5640 Training loss: 1.1503 0.3935 sec/batch\n",
      "Epoch 11/20  Iteration 2876/5640 Training loss: 1.1502 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 2877/5640 Training loss: 1.1500 0.3943 sec/batch\n",
      "Epoch 11/20  Iteration 2878/5640 Training loss: 1.1501 0.3935 sec/batch\n",
      "Epoch 11/20  Iteration 2879/5640 Training loss: 1.1502 0.3959 sec/batch\n",
      "Epoch 11/20  Iteration 2880/5640 Training loss: 1.1501 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 2881/5640 Training loss: 1.1497 0.3967 sec/batch\n",
      "Epoch 11/20  Iteration 2882/5640 Training loss: 1.1494 0.3926 sec/batch\n",
      "Epoch 11/20  Iteration 2883/5640 Training loss: 1.1491 0.3937 sec/batch\n",
      "Epoch 11/20  Iteration 2884/5640 Training loss: 1.1487 0.3939 sec/batch\n",
      "Epoch 11/20  Iteration 2885/5640 Training loss: 1.1489 0.3943 sec/batch\n",
      "Epoch 11/20  Iteration 2886/5640 Training loss: 1.1487 0.3930 sec/batch\n",
      "Epoch 11/20  Iteration 2887/5640 Training loss: 1.1489 0.3960 sec/batch\n",
      "Epoch 11/20  Iteration 2888/5640 Training loss: 1.1485 0.3944 sec/batch\n",
      "Epoch 11/20  Iteration 2889/5640 Training loss: 1.1483 0.3943 sec/batch\n",
      "Epoch 11/20  Iteration 2890/5640 Training loss: 1.1480 0.3934 sec/batch\n",
      "Epoch 11/20  Iteration 2891/5640 Training loss: 1.1477 0.3948 sec/batch\n",
      "Epoch 11/20  Iteration 2892/5640 Training loss: 1.1477 0.4018 sec/batch\n",
      "Epoch 11/20  Iteration 2893/5640 Training loss: 1.1478 0.3948 sec/batch\n",
      "Epoch 11/20  Iteration 2894/5640 Training loss: 1.1479 0.3943 sec/batch\n",
      "Epoch 11/20  Iteration 2895/5640 Training loss: 1.1481 0.3952 sec/batch\n",
      "Epoch 11/20  Iteration 2896/5640 Training loss: 1.1487 0.3944 sec/batch\n",
      "Epoch 11/20  Iteration 2897/5640 Training loss: 1.1487 0.3942 sec/batch\n",
      "Epoch 11/20  Iteration 2898/5640 Training loss: 1.1493 0.3923 sec/batch\n",
      "Epoch 11/20  Iteration 2899/5640 Training loss: 1.1494 0.3939 sec/batch\n",
      "Epoch 11/20  Iteration 2900/5640 Training loss: 1.1496 0.3942 sec/batch\n",
      "Validation loss: 1.11615 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 2901/5640 Training loss: 1.1519 0.3826 sec/batch\n",
      "Epoch 11/20  Iteration 2902/5640 Training loss: 1.1517 0.3820 sec/batch\n",
      "Epoch 11/20  Iteration 2903/5640 Training loss: 1.1515 0.3820 sec/batch\n",
      "Epoch 11/20  Iteration 2904/5640 Training loss: 1.1511 0.3823 sec/batch\n",
      "Epoch 11/20  Iteration 2905/5640 Training loss: 1.1508 0.3842 sec/batch\n",
      "Epoch 11/20  Iteration 2906/5640 Training loss: 1.1506 0.3827 sec/batch\n",
      "Epoch 11/20  Iteration 2907/5640 Training loss: 1.1505 0.3829 sec/batch\n",
      "Epoch 11/20  Iteration 2908/5640 Training loss: 1.1503 0.3825 sec/batch\n",
      "Epoch 11/20  Iteration 2909/5640 Training loss: 1.1501 0.3817 sec/batch\n",
      "Epoch 11/20  Iteration 2910/5640 Training loss: 1.1499 0.3828 sec/batch\n",
      "Epoch 11/20  Iteration 2911/5640 Training loss: 1.1501 0.3815 sec/batch\n",
      "Epoch 11/20  Iteration 2912/5640 Training loss: 1.1500 0.3844 sec/batch\n",
      "Epoch 11/20  Iteration 2913/5640 Training loss: 1.1500 0.3832 sec/batch\n",
      "Epoch 11/20  Iteration 2914/5640 Training loss: 1.1498 0.3820 sec/batch\n",
      "Epoch 11/20  Iteration 2915/5640 Training loss: 1.1496 0.3820 sec/batch\n",
      "Epoch 11/20  Iteration 2916/5640 Training loss: 1.1496 0.3825 sec/batch\n",
      "Epoch 11/20  Iteration 2917/5640 Training loss: 1.1497 0.3869 sec/batch\n",
      "Epoch 11/20  Iteration 2918/5640 Training loss: 1.1499 0.3863 sec/batch\n",
      "Epoch 11/20  Iteration 2919/5640 Training loss: 1.1498 0.3878 sec/batch\n",
      "Epoch 11/20  Iteration 2920/5640 Training loss: 1.1496 0.3930 sec/batch\n",
      "Epoch 11/20  Iteration 2921/5640 Training loss: 1.1492 0.3907 sec/batch\n",
      "Epoch 11/20  Iteration 2922/5640 Training loss: 1.1491 0.3933 sec/batch\n",
      "Epoch 11/20  Iteration 2923/5640 Training loss: 1.1489 0.3939 sec/batch\n",
      "Epoch 11/20  Iteration 2924/5640 Training loss: 1.1487 0.3938 sec/batch\n",
      "Epoch 11/20  Iteration 2925/5640 Training loss: 1.1484 0.3941 sec/batch\n",
      "Epoch 11/20  Iteration 2926/5640 Training loss: 1.1480 0.3933 sec/batch\n",
      "Epoch 11/20  Iteration 2927/5640 Training loss: 1.1478 0.3951 sec/batch\n",
      "Epoch 11/20  Iteration 2928/5640 Training loss: 1.1474 0.3958 sec/batch\n",
      "Epoch 11/20  Iteration 2929/5640 Training loss: 1.1473 0.3911 sec/batch\n",
      "Epoch 11/20  Iteration 2930/5640 Training loss: 1.1472 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 2931/5640 Training loss: 1.1470 0.3940 sec/batch\n",
      "Epoch 11/20  Iteration 2932/5640 Training loss: 1.1470 0.3953 sec/batch\n",
      "Epoch 11/20  Iteration 2933/5640 Training loss: 1.1469 0.3929 sec/batch\n",
      "Epoch 11/20  Iteration 2934/5640 Training loss: 1.1468 0.3941 sec/batch\n",
      "Epoch 11/20  Iteration 2935/5640 Training loss: 1.1468 0.3948 sec/batch\n",
      "Epoch 11/20  Iteration 2936/5640 Training loss: 1.1468 0.3938 sec/batch\n",
      "Epoch 11/20  Iteration 2937/5640 Training loss: 1.1467 0.3926 sec/batch\n",
      "Epoch 11/20  Iteration 2938/5640 Training loss: 1.1466 0.3956 sec/batch\n",
      "Epoch 11/20  Iteration 2939/5640 Training loss: 1.1465 0.3937 sec/batch\n",
      "Epoch 11/20  Iteration 2940/5640 Training loss: 1.1462 0.3945 sec/batch\n",
      "Epoch 11/20  Iteration 2941/5640 Training loss: 1.1461 0.3929 sec/batch\n",
      "Epoch 11/20  Iteration 2942/5640 Training loss: 1.1461 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 2943/5640 Training loss: 1.1459 0.3955 sec/batch\n",
      "Epoch 11/20  Iteration 2944/5640 Training loss: 1.1458 0.3936 sec/batch\n",
      "Epoch 11/20  Iteration 2945/5640 Training loss: 1.1461 0.3946 sec/batch\n",
      "Epoch 11/20  Iteration 2946/5640 Training loss: 1.1458 0.3936 sec/batch\n",
      "Epoch 11/20  Iteration 2947/5640 Training loss: 1.1458 0.3940 sec/batch\n",
      "Epoch 11/20  Iteration 2948/5640 Training loss: 1.1454 0.3952 sec/batch\n",
      "Epoch 11/20  Iteration 2949/5640 Training loss: 1.1452 0.3943 sec/batch\n",
      "Epoch 11/20  Iteration 2950/5640 Training loss: 1.1450 0.3961 sec/batch\n",
      "Validation loss: 1.11807 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 2951/5640 Training loss: 1.1462 0.3879 sec/batch\n",
      "Epoch 11/20  Iteration 2952/5640 Training loss: 1.1458 0.3865 sec/batch\n",
      "Epoch 11/20  Iteration 2953/5640 Training loss: 1.1455 0.3899 sec/batch\n",
      "Epoch 11/20  Iteration 2954/5640 Training loss: 1.1452 0.3904 sec/batch\n",
      "Epoch 11/20  Iteration 2955/5640 Training loss: 1.1451 0.3900 sec/batch\n",
      "Epoch 11/20  Iteration 2956/5640 Training loss: 1.1451 0.3866 sec/batch\n",
      "Epoch 11/20  Iteration 2957/5640 Training loss: 1.1450 0.3853 sec/batch\n",
      "Epoch 11/20  Iteration 2958/5640 Training loss: 1.1448 0.3845 sec/batch\n",
      "Epoch 11/20  Iteration 2959/5640 Training loss: 1.1447 0.3877 sec/batch\n",
      "Epoch 11/20  Iteration 2960/5640 Training loss: 1.1447 0.3844 sec/batch\n",
      "Epoch 11/20  Iteration 2961/5640 Training loss: 1.1446 0.3873 sec/batch\n",
      "Epoch 11/20  Iteration 2962/5640 Training loss: 1.1446 0.3884 sec/batch\n",
      "Epoch 11/20  Iteration 2963/5640 Training loss: 1.1445 0.3902 sec/batch\n",
      "Epoch 11/20  Iteration 2964/5640 Training loss: 1.1443 0.3894 sec/batch\n",
      "Epoch 11/20  Iteration 2965/5640 Training loss: 1.1440 0.3958 sec/batch\n",
      "Epoch 11/20  Iteration 2966/5640 Training loss: 1.1438 0.3981 sec/batch\n",
      "Epoch 11/20  Iteration 2967/5640 Training loss: 1.1435 0.3896 sec/batch\n",
      "Epoch 11/20  Iteration 2968/5640 Training loss: 1.1436 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 2969/5640 Training loss: 1.1435 0.3928 sec/batch\n",
      "Epoch 11/20  Iteration 2970/5640 Training loss: 1.1433 0.3952 sec/batch\n",
      "Epoch 11/20  Iteration 2971/5640 Training loss: 1.1432 0.3905 sec/batch\n",
      "Epoch 11/20  Iteration 2972/5640 Training loss: 1.1432 0.3950 sec/batch\n",
      "Epoch 11/20  Iteration 2973/5640 Training loss: 1.1433 0.4007 sec/batch\n",
      "Epoch 11/20  Iteration 2974/5640 Training loss: 1.1433 0.3960 sec/batch\n",
      "Epoch 11/20  Iteration 2975/5640 Training loss: 1.1432 0.3955 sec/batch\n",
      "Epoch 11/20  Iteration 2976/5640 Training loss: 1.1433 0.3996 sec/batch\n",
      "Epoch 11/20  Iteration 2977/5640 Training loss: 1.1431 0.3970 sec/batch\n",
      "Epoch 11/20  Iteration 2978/5640 Training loss: 1.1431 0.3981 sec/batch\n",
      "Epoch 11/20  Iteration 2979/5640 Training loss: 1.1430 0.3979 sec/batch\n",
      "Epoch 11/20  Iteration 2980/5640 Training loss: 1.1428 0.3968 sec/batch\n",
      "Epoch 11/20  Iteration 2981/5640 Training loss: 1.1427 0.3995 sec/batch\n",
      "Epoch 11/20  Iteration 2982/5640 Training loss: 1.1426 0.3948 sec/batch\n",
      "Epoch 11/20  Iteration 2983/5640 Training loss: 1.1425 0.3981 sec/batch\n",
      "Epoch 11/20  Iteration 2984/5640 Training loss: 1.1424 0.3988 sec/batch\n",
      "Epoch 11/20  Iteration 2985/5640 Training loss: 1.1424 0.3955 sec/batch\n",
      "Epoch 11/20  Iteration 2986/5640 Training loss: 1.1426 0.3971 sec/batch\n",
      "Epoch 11/20  Iteration 2987/5640 Training loss: 1.1424 0.3970 sec/batch\n",
      "Epoch 11/20  Iteration 2988/5640 Training loss: 1.1425 0.3927 sec/batch\n",
      "Epoch 11/20  Iteration 2989/5640 Training loss: 1.1425 0.3928 sec/batch\n",
      "Epoch 11/20  Iteration 2990/5640 Training loss: 1.1425 0.3912 sec/batch\n",
      "Epoch 11/20  Iteration 2991/5640 Training loss: 1.1425 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 2992/5640 Training loss: 1.1423 0.3975 sec/batch\n",
      "Epoch 11/20  Iteration 2993/5640 Training loss: 1.1423 0.3932 sec/batch\n",
      "Epoch 11/20  Iteration 2994/5640 Training loss: 1.1422 0.3901 sec/batch\n",
      "Epoch 11/20  Iteration 2995/5640 Training loss: 1.1422 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 2996/5640 Training loss: 1.1421 0.3910 sec/batch\n",
      "Epoch 11/20  Iteration 2997/5640 Training loss: 1.1422 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 2998/5640 Training loss: 1.1420 0.3934 sec/batch\n",
      "Epoch 11/20  Iteration 2999/5640 Training loss: 1.1421 0.3952 sec/batch\n",
      "Epoch 11/20  Iteration 3000/5640 Training loss: 1.1422 0.3903 sec/batch\n",
      "Validation loss: 1.11267 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3001/5640 Training loss: 1.1432 0.3892 sec/batch\n",
      "Epoch 11/20  Iteration 3002/5640 Training loss: 1.1432 0.3831 sec/batch\n",
      "Epoch 11/20  Iteration 3003/5640 Training loss: 1.1432 0.3826 sec/batch\n",
      "Epoch 11/20  Iteration 3004/5640 Training loss: 1.1431 0.3849 sec/batch\n",
      "Epoch 11/20  Iteration 3005/5640 Training loss: 1.1430 0.3843 sec/batch\n",
      "Epoch 11/20  Iteration 3006/5640 Training loss: 1.1427 0.3820 sec/batch\n",
      "Epoch 11/20  Iteration 3007/5640 Training loss: 1.1424 0.3818 sec/batch\n",
      "Epoch 11/20  Iteration 3008/5640 Training loss: 1.1421 0.3839 sec/batch\n",
      "Epoch 11/20  Iteration 3009/5640 Training loss: 1.1419 0.3841 sec/batch\n",
      "Epoch 11/20  Iteration 3010/5640 Training loss: 1.1417 0.3818 sec/batch\n",
      "Epoch 11/20  Iteration 3011/5640 Training loss: 1.1417 0.3859 sec/batch\n",
      "Epoch 11/20  Iteration 3012/5640 Training loss: 1.1415 0.3880 sec/batch\n",
      "Epoch 11/20  Iteration 3013/5640 Training loss: 1.1413 0.3914 sec/batch\n",
      "Epoch 11/20  Iteration 3014/5640 Training loss: 1.1413 0.3935 sec/batch\n",
      "Epoch 11/20  Iteration 3015/5640 Training loss: 1.1414 0.3963 sec/batch\n",
      "Epoch 11/20  Iteration 3016/5640 Training loss: 1.1414 0.3998 sec/batch\n",
      "Epoch 11/20  Iteration 3017/5640 Training loss: 1.1413 0.3978 sec/batch\n",
      "Epoch 11/20  Iteration 3018/5640 Training loss: 1.1413 0.3965 sec/batch\n",
      "Epoch 11/20  Iteration 3019/5640 Training loss: 1.1413 0.4015 sec/batch\n",
      "Epoch 11/20  Iteration 3020/5640 Training loss: 1.1412 0.4029 sec/batch\n",
      "Epoch 11/20  Iteration 3021/5640 Training loss: 1.1412 0.4042 sec/batch\n",
      "Epoch 11/20  Iteration 3022/5640 Training loss: 1.1411 0.4102 sec/batch\n",
      "Epoch 11/20  Iteration 3023/5640 Training loss: 1.1411 0.4049 sec/batch\n",
      "Epoch 11/20  Iteration 3024/5640 Training loss: 1.1411 0.4037 sec/batch\n",
      "Epoch 11/20  Iteration 3025/5640 Training loss: 1.1409 0.4024 sec/batch\n",
      "Epoch 11/20  Iteration 3026/5640 Training loss: 1.1410 0.4002 sec/batch\n",
      "Epoch 11/20  Iteration 3027/5640 Training loss: 1.1408 0.4056 sec/batch\n",
      "Epoch 11/20  Iteration 3028/5640 Training loss: 1.1406 0.3953 sec/batch\n",
      "Epoch 11/20  Iteration 3029/5640 Training loss: 1.1407 0.3933 sec/batch\n",
      "Epoch 11/20  Iteration 3030/5640 Training loss: 1.1406 0.3905 sec/batch\n",
      "Epoch 11/20  Iteration 3031/5640 Training loss: 1.1404 0.3931 sec/batch\n",
      "Epoch 11/20  Iteration 3032/5640 Training loss: 1.1404 0.3908 sec/batch\n",
      "Epoch 11/20  Iteration 3033/5640 Training loss: 1.1404 0.4063 sec/batch\n",
      "Epoch 11/20  Iteration 3034/5640 Training loss: 1.1404 0.4042 sec/batch\n",
      "Epoch 11/20  Iteration 3035/5640 Training loss: 1.1403 0.4042 sec/batch\n",
      "Epoch 11/20  Iteration 3036/5640 Training loss: 1.1403 0.3956 sec/batch\n",
      "Epoch 11/20  Iteration 3037/5640 Training loss: 1.1402 0.3883 sec/batch\n",
      "Epoch 11/20  Iteration 3038/5640 Training loss: 1.1399 0.3890 sec/batch\n",
      "Epoch 11/20  Iteration 3039/5640 Training loss: 1.1398 0.3904 sec/batch\n",
      "Epoch 11/20  Iteration 3040/5640 Training loss: 1.1396 0.3894 sec/batch\n",
      "Epoch 11/20  Iteration 3041/5640 Training loss: 1.1396 0.3903 sec/batch\n",
      "Epoch 11/20  Iteration 3042/5640 Training loss: 1.1395 0.3894 sec/batch\n",
      "Epoch 11/20  Iteration 3043/5640 Training loss: 1.1394 0.3901 sec/batch\n",
      "Epoch 11/20  Iteration 3044/5640 Training loss: 1.1394 0.3908 sec/batch\n",
      "Epoch 11/20  Iteration 3045/5640 Training loss: 1.1393 0.3896 sec/batch\n",
      "Epoch 11/20  Iteration 3046/5640 Training loss: 1.1392 0.3915 sec/batch\n",
      "Epoch 11/20  Iteration 3047/5640 Training loss: 1.1393 0.3901 sec/batch\n",
      "Epoch 11/20  Iteration 3048/5640 Training loss: 1.1394 0.3901 sec/batch\n",
      "Epoch 11/20  Iteration 3049/5640 Training loss: 1.1394 0.3909 sec/batch\n",
      "Epoch 11/20  Iteration 3050/5640 Training loss: 1.1394 0.3887 sec/batch\n",
      "Validation loss: 1.11387 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3051/5640 Training loss: 1.1401 0.3841 sec/batch\n",
      "Epoch 11/20  Iteration 3052/5640 Training loss: 1.1401 0.3860 sec/batch\n",
      "Epoch 11/20  Iteration 3053/5640 Training loss: 1.1400 0.3836 sec/batch\n",
      "Epoch 11/20  Iteration 3054/5640 Training loss: 1.1398 0.3835 sec/batch\n",
      "Epoch 11/20  Iteration 3055/5640 Training loss: 1.1397 0.3827 sec/batch\n",
      "Epoch 11/20  Iteration 3056/5640 Training loss: 1.1398 0.3842 sec/batch\n",
      "Epoch 11/20  Iteration 3057/5640 Training loss: 1.1397 0.3884 sec/batch\n",
      "Epoch 11/20  Iteration 3058/5640 Training loss: 1.1398 0.3880 sec/batch\n",
      "Epoch 11/20  Iteration 3059/5640 Training loss: 1.1397 0.3881 sec/batch\n",
      "Epoch 11/20  Iteration 3060/5640 Training loss: 1.1397 0.3871 sec/batch\n",
      "Epoch 11/20  Iteration 3061/5640 Training loss: 1.1397 0.3890 sec/batch\n",
      "Epoch 11/20  Iteration 3062/5640 Training loss: 1.1396 0.3879 sec/batch\n",
      "Epoch 11/20  Iteration 3063/5640 Training loss: 1.1395 0.3884 sec/batch\n",
      "Epoch 11/20  Iteration 3064/5640 Training loss: 1.1393 0.3887 sec/batch\n",
      "Epoch 11/20  Iteration 3065/5640 Training loss: 1.1392 0.3896 sec/batch\n",
      "Epoch 11/20  Iteration 3066/5640 Training loss: 1.1391 0.3948 sec/batch\n",
      "Epoch 11/20  Iteration 3067/5640 Training loss: 1.1390 0.3949 sec/batch\n",
      "Epoch 11/20  Iteration 3068/5640 Training loss: 1.1390 0.3924 sec/batch\n",
      "Epoch 11/20  Iteration 3069/5640 Training loss: 1.1390 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 3070/5640 Training loss: 1.1390 0.3943 sec/batch\n",
      "Epoch 11/20  Iteration 3071/5640 Training loss: 1.1389 0.3955 sec/batch\n",
      "Epoch 11/20  Iteration 3072/5640 Training loss: 1.1388 0.3942 sec/batch\n",
      "Epoch 11/20  Iteration 3073/5640 Training loss: 1.1387 0.3953 sec/batch\n",
      "Epoch 11/20  Iteration 3074/5640 Training loss: 1.1387 0.3944 sec/batch\n",
      "Epoch 11/20  Iteration 3075/5640 Training loss: 1.1386 0.3952 sec/batch\n",
      "Epoch 11/20  Iteration 3076/5640 Training loss: 1.1385 0.3947 sec/batch\n",
      "Epoch 11/20  Iteration 3077/5640 Training loss: 1.1384 0.4043 sec/batch\n",
      "Epoch 11/20  Iteration 3078/5640 Training loss: 1.1384 0.4118 sec/batch\n",
      "Epoch 11/20  Iteration 3079/5640 Training loss: 1.1384 0.3976 sec/batch\n",
      "Epoch 11/20  Iteration 3080/5640 Training loss: 1.1384 0.3957 sec/batch\n",
      "Epoch 11/20  Iteration 3081/5640 Training loss: 1.1384 0.3956 sec/batch\n",
      "Epoch 11/20  Iteration 3082/5640 Training loss: 1.1384 0.4006 sec/batch\n",
      "Epoch 11/20  Iteration 3083/5640 Training loss: 1.1384 0.3936 sec/batch\n",
      "Epoch 11/20  Iteration 3084/5640 Training loss: 1.1385 0.4022 sec/batch\n",
      "Epoch 11/20  Iteration 3085/5640 Training loss: 1.1385 0.4175 sec/batch\n",
      "Epoch 11/20  Iteration 3086/5640 Training loss: 1.1383 0.4121 sec/batch\n",
      "Epoch 11/20  Iteration 3087/5640 Training loss: 1.1383 0.4253 sec/batch\n",
      "Epoch 11/20  Iteration 3088/5640 Training loss: 1.1382 0.4202 sec/batch\n",
      "Epoch 11/20  Iteration 3089/5640 Training loss: 1.1382 0.4163 sec/batch\n",
      "Epoch 11/20  Iteration 3090/5640 Training loss: 1.1381 0.4190 sec/batch\n",
      "Epoch 11/20  Iteration 3091/5640 Training loss: 1.1381 0.4311 sec/batch\n",
      "Epoch 11/20  Iteration 3092/5640 Training loss: 1.1379 0.4072 sec/batch\n",
      "Epoch 11/20  Iteration 3093/5640 Training loss: 1.1379 0.3929 sec/batch\n",
      "Epoch 11/20  Iteration 3094/5640 Training loss: 1.1379 0.3950 sec/batch\n",
      "Epoch 11/20  Iteration 3095/5640 Training loss: 1.1379 0.3979 sec/batch\n",
      "Epoch 11/20  Iteration 3096/5640 Training loss: 1.1378 0.4018 sec/batch\n",
      "Epoch 11/20  Iteration 3097/5640 Training loss: 1.1378 0.3977 sec/batch\n",
      "Epoch 11/20  Iteration 3098/5640 Training loss: 1.1379 0.3974 sec/batch\n",
      "Epoch 11/20  Iteration 3099/5640 Training loss: 1.1379 0.3928 sec/batch\n",
      "Epoch 11/20  Iteration 3100/5640 Training loss: 1.1379 0.3927 sec/batch\n",
      "Validation loss: 1.11101 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 3101/5640 Training loss: 1.1383 0.3994 sec/batch\n",
      "Epoch 11/20  Iteration 3102/5640 Training loss: 1.1383 0.3934 sec/batch\n",
      "Epoch 12/20  Iteration 3103/5640 Training loss: 1.2315 0.3829 sec/batch\n",
      "Epoch 12/20  Iteration 3104/5640 Training loss: 1.1928 0.3829 sec/batch\n",
      "Epoch 12/20  Iteration 3105/5640 Training loss: 1.1748 0.3837 sec/batch\n",
      "Epoch 12/20  Iteration 3106/5640 Training loss: 1.1646 0.3821 sec/batch\n",
      "Epoch 12/20  Iteration 3107/5640 Training loss: 1.1600 0.3892 sec/batch\n",
      "Epoch 12/20  Iteration 3108/5640 Training loss: 1.1579 0.3839 sec/batch\n",
      "Epoch 12/20  Iteration 3109/5640 Training loss: 1.1508 0.3832 sec/batch\n",
      "Epoch 12/20  Iteration 3110/5640 Training loss: 1.1484 0.3832 sec/batch\n",
      "Epoch 12/20  Iteration 3111/5640 Training loss: 1.1451 0.3811 sec/batch\n",
      "Epoch 12/20  Iteration 3112/5640 Training loss: 1.1414 0.3831 sec/batch\n",
      "Epoch 12/20  Iteration 3113/5640 Training loss: 1.1421 0.3811 sec/batch\n",
      "Epoch 12/20  Iteration 3114/5640 Training loss: 1.1399 0.3920 sec/batch\n",
      "Epoch 12/20  Iteration 3115/5640 Training loss: 1.1397 0.4181 sec/batch\n",
      "Epoch 12/20  Iteration 3116/5640 Training loss: 1.1398 0.4192 sec/batch\n",
      "Epoch 12/20  Iteration 3117/5640 Training loss: 1.1385 0.4215 sec/batch\n",
      "Epoch 12/20  Iteration 3118/5640 Training loss: 1.1396 0.4215 sec/batch\n",
      "Epoch 12/20  Iteration 3119/5640 Training loss: 1.1397 0.4126 sec/batch\n",
      "Epoch 12/20  Iteration 3120/5640 Training loss: 1.1381 0.3919 sec/batch\n",
      "Epoch 12/20  Iteration 3121/5640 Training loss: 1.1382 0.3972 sec/batch\n",
      "Epoch 12/20  Iteration 3122/5640 Training loss: 1.1366 0.3886 sec/batch\n",
      "Epoch 12/20  Iteration 3123/5640 Training loss: 1.1366 0.3958 sec/batch\n",
      "Epoch 12/20  Iteration 3124/5640 Training loss: 1.1364 0.3894 sec/batch\n",
      "Epoch 12/20  Iteration 3125/5640 Training loss: 1.1362 0.3888 sec/batch\n",
      "Epoch 12/20  Iteration 3126/5640 Training loss: 1.1346 0.3897 sec/batch\n",
      "Epoch 12/20  Iteration 3127/5640 Training loss: 1.1350 0.4018 sec/batch\n",
      "Epoch 12/20  Iteration 3128/5640 Training loss: 1.1345 0.4158 sec/batch\n",
      "Epoch 12/20  Iteration 3129/5640 Training loss: 1.1346 0.3950 sec/batch\n",
      "Epoch 12/20  Iteration 3130/5640 Training loss: 1.1343 0.3956 sec/batch\n",
      "Epoch 12/20  Iteration 3131/5640 Training loss: 1.1337 0.3974 sec/batch\n",
      "Epoch 12/20  Iteration 3132/5640 Training loss: 1.1335 0.4008 sec/batch\n",
      "Epoch 12/20  Iteration 3133/5640 Training loss: 1.1324 0.3922 sec/batch\n",
      "Epoch 12/20  Iteration 3134/5640 Training loss: 1.1319 0.3959 sec/batch\n",
      "Epoch 12/20  Iteration 3135/5640 Training loss: 1.1310 0.3904 sec/batch\n",
      "Epoch 12/20  Iteration 3136/5640 Training loss: 1.1305 0.3909 sec/batch\n",
      "Epoch 12/20  Iteration 3137/5640 Training loss: 1.1309 0.4006 sec/batch\n",
      "Epoch 12/20  Iteration 3138/5640 Training loss: 1.1310 0.4020 sec/batch\n",
      "Epoch 12/20  Iteration 3139/5640 Training loss: 1.1303 0.4199 sec/batch\n",
      "Epoch 12/20  Iteration 3140/5640 Training loss: 1.1294 0.4262 sec/batch\n",
      "Epoch 12/20  Iteration 3141/5640 Training loss: 1.1292 0.4238 sec/batch\n",
      "Epoch 12/20  Iteration 3142/5640 Training loss: 1.1286 0.3995 sec/batch\n",
      "Epoch 12/20  Iteration 3143/5640 Training loss: 1.1280 0.3943 sec/batch\n",
      "Epoch 12/20  Iteration 3144/5640 Training loss: 1.1287 0.4117 sec/batch\n",
      "Epoch 12/20  Iteration 3145/5640 Training loss: 1.1277 0.4429 sec/batch\n",
      "Epoch 12/20  Iteration 3146/5640 Training loss: 1.1278 0.4315 sec/batch\n",
      "Epoch 12/20  Iteration 3147/5640 Training loss: 1.1274 0.3946 sec/batch\n",
      "Epoch 12/20  Iteration 3148/5640 Training loss: 1.1273 0.3929 sec/batch\n",
      "Epoch 12/20  Iteration 3149/5640 Training loss: 1.1274 0.3954 sec/batch\n",
      "Epoch 12/20  Iteration 3150/5640 Training loss: 1.1277 0.3936 sec/batch\n",
      "Validation loss: 1.10584 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 3151/5640 Training loss: 1.1314 0.3957 sec/batch\n",
      "Epoch 12/20  Iteration 3152/5640 Training loss: 1.1312 0.3953 sec/batch\n",
      "Epoch 12/20  Iteration 3153/5640 Training loss: 1.1314 0.3953 sec/batch\n",
      "Epoch 12/20  Iteration 3154/5640 Training loss: 1.1314 0.3834 sec/batch\n",
      "Epoch 12/20  Iteration 3155/5640 Training loss: 1.1313 0.3916 sec/batch\n",
      "Epoch 12/20  Iteration 3156/5640 Training loss: 1.1313 0.3819 sec/batch\n",
      "Epoch 12/20  Iteration 3157/5640 Training loss: 1.1308 0.3953 sec/batch\n",
      "Epoch 12/20  Iteration 3158/5640 Training loss: 1.1304 0.3841 sec/batch\n",
      "Epoch 12/20  Iteration 3159/5640 Training loss: 1.1302 0.3952 sec/batch\n",
      "Epoch 12/20  Iteration 3160/5640 Training loss: 1.1303 0.4063 sec/batch\n",
      "Epoch 12/20  Iteration 3161/5640 Training loss: 1.1302 0.4152 sec/batch\n",
      "Epoch 12/20  Iteration 3162/5640 Training loss: 1.1298 0.4116 sec/batch\n",
      "Epoch 12/20  Iteration 3163/5640 Training loss: 1.1295 0.3879 sec/batch\n",
      "Epoch 12/20  Iteration 3164/5640 Training loss: 1.1291 0.4135 sec/batch\n",
      "Epoch 12/20  Iteration 3165/5640 Training loss: 1.1286 0.4181 sec/batch\n",
      "Epoch 12/20  Iteration 3166/5640 Training loss: 1.1281 0.4189 sec/batch\n",
      "Epoch 12/20  Iteration 3167/5640 Training loss: 1.1283 1.0074 sec/batch\n",
      "Epoch 12/20  Iteration 3168/5640 Training loss: 1.1282 0.4090 sec/batch\n",
      "Epoch 12/20  Iteration 3169/5640 Training loss: 1.1282 0.3949 sec/batch\n",
      "Epoch 12/20  Iteration 3170/5640 Training loss: 1.1279 0.4073 sec/batch\n",
      "Epoch 12/20  Iteration 3171/5640 Training loss: 1.1276 0.4133 sec/batch\n",
      "Epoch 12/20  Iteration 3172/5640 Training loss: 1.1271 0.3847 sec/batch\n",
      "Epoch 12/20  Iteration 3173/5640 Training loss: 1.1269 0.3821 sec/batch\n",
      "Epoch 12/20  Iteration 3174/5640 Training loss: 1.1269 0.3873 sec/batch\n",
      "Epoch 12/20  Iteration 3175/5640 Training loss: 1.1269 0.3843 sec/batch\n",
      "Epoch 12/20  Iteration 3176/5640 Training loss: 1.1269 0.3835 sec/batch\n",
      "Epoch 12/20  Iteration 3177/5640 Training loss: 1.1271 0.3858 sec/batch\n",
      "Epoch 12/20  Iteration 3178/5640 Training loss: 1.1277 0.3836 sec/batch\n",
      "Epoch 12/20  Iteration 3179/5640 Training loss: 1.1278 0.3810 sec/batch\n",
      "Epoch 12/20  Iteration 3180/5640 Training loss: 1.1284 0.3839 sec/batch\n",
      "Epoch 12/20  Iteration 3181/5640 Training loss: 1.1284 0.3872 sec/batch\n",
      "Epoch 12/20  Iteration 3182/5640 Training loss: 1.1286 0.3869 sec/batch\n",
      "Epoch 12/20  Iteration 3183/5640 Training loss: 1.1288 0.3899 sec/batch\n",
      "Epoch 12/20  Iteration 3184/5640 Training loss: 1.1285 0.3940 sec/batch\n",
      "Epoch 12/20  Iteration 3185/5640 Training loss: 1.1283 0.3972 sec/batch\n",
      "Epoch 12/20  Iteration 3186/5640 Training loss: 1.1280 0.3971 sec/batch\n",
      "Epoch 12/20  Iteration 3187/5640 Training loss: 1.1278 0.4041 sec/batch\n",
      "Epoch 12/20  Iteration 3188/5640 Training loss: 1.1276 0.3969 sec/batch\n",
      "Epoch 12/20  Iteration 3189/5640 Training loss: 1.1274 0.4013 sec/batch\n",
      "Epoch 12/20  Iteration 3190/5640 Training loss: 1.1271 0.3988 sec/batch\n",
      "Epoch 12/20  Iteration 3191/5640 Training loss: 1.1270 0.3987 sec/batch\n",
      "Epoch 12/20  Iteration 3192/5640 Training loss: 1.1268 0.3986 sec/batch\n",
      "Epoch 12/20  Iteration 3193/5640 Training loss: 1.1268 0.3971 sec/batch\n",
      "Epoch 12/20  Iteration 3194/5640 Training loss: 1.1268 0.3983 sec/batch\n",
      "Epoch 12/20  Iteration 3195/5640 Training loss: 1.1268 0.4041 sec/batch\n",
      "Epoch 12/20  Iteration 3196/5640 Training loss: 1.1267 0.4006 sec/batch\n",
      "Epoch 12/20  Iteration 3197/5640 Training loss: 1.1265 0.4009 sec/batch\n",
      "Epoch 12/20  Iteration 3198/5640 Training loss: 1.1264 0.4005 sec/batch\n",
      "Epoch 12/20  Iteration 3199/5640 Training loss: 1.1265 0.4024 sec/batch\n",
      "Epoch 12/20  Iteration 3200/5640 Training loss: 1.1265 0.4031 sec/batch\n",
      "Validation loss: 1.10474 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 3201/5640 Training loss: 1.1283 0.3855 sec/batch\n",
      "Epoch 12/20  Iteration 3202/5640 Training loss: 1.1280 0.3870 sec/batch\n",
      "Epoch 12/20  Iteration 3203/5640 Training loss: 1.1276 0.3872 sec/batch\n",
      "Epoch 12/20  Iteration 3204/5640 Training loss: 1.1275 0.3827 sec/batch\n",
      "Epoch 12/20  Iteration 3205/5640 Training loss: 1.1272 0.3878 sec/batch\n",
      "Epoch 12/20  Iteration 3206/5640 Training loss: 1.1270 0.3833 sec/batch\n",
      "Epoch 12/20  Iteration 3207/5640 Training loss: 1.1267 0.3823 sec/batch\n",
      "Epoch 12/20  Iteration 3208/5640 Training loss: 1.1264 0.3836 sec/batch\n",
      "Epoch 12/20  Iteration 3209/5640 Training loss: 1.1261 0.3871 sec/batch\n",
      "Epoch 12/20  Iteration 3210/5640 Training loss: 1.1257 0.3871 sec/batch\n",
      "Epoch 12/20  Iteration 3211/5640 Training loss: 1.1255 0.3843 sec/batch\n",
      "Epoch 12/20  Iteration 3212/5640 Training loss: 1.1253 0.3843 sec/batch\n",
      "Epoch 12/20  Iteration 3213/5640 Training loss: 1.1251 0.3837 sec/batch\n",
      "Epoch 12/20  Iteration 3214/5640 Training loss: 1.1252 0.3851 sec/batch\n",
      "Epoch 12/20  Iteration 3215/5640 Training loss: 1.1251 0.3888 sec/batch\n",
      "Epoch 12/20  Iteration 3216/5640 Training loss: 1.1250 0.3913 sec/batch\n",
      "Epoch 12/20  Iteration 3217/5640 Training loss: 1.1249 0.3931 sec/batch\n",
      "Epoch 12/20  Iteration 3218/5640 Training loss: 1.1249 0.3929 sec/batch\n",
      "Epoch 12/20  Iteration 3219/5640 Training loss: 1.1249 0.3905 sec/batch\n",
      "Epoch 12/20  Iteration 3220/5640 Training loss: 1.1248 0.3954 sec/batch\n",
      "Epoch 12/20  Iteration 3221/5640 Training loss: 1.1246 0.3919 sec/batch\n",
      "Epoch 12/20  Iteration 3222/5640 Training loss: 1.1244 0.3905 sec/batch\n",
      "Epoch 12/20  Iteration 3223/5640 Training loss: 1.1243 0.3906 sec/batch\n",
      "Epoch 12/20  Iteration 3224/5640 Training loss: 1.1243 0.3981 sec/batch\n",
      "Epoch 12/20  Iteration 3225/5640 Training loss: 1.1240 0.3988 sec/batch\n",
      "Epoch 12/20  Iteration 3226/5640 Training loss: 1.1240 0.3975 sec/batch\n",
      "Epoch 12/20  Iteration 3227/5640 Training loss: 1.1244 0.3947 sec/batch\n",
      "Epoch 12/20  Iteration 3228/5640 Training loss: 1.1240 0.3957 sec/batch\n",
      "Epoch 12/20  Iteration 3229/5640 Training loss: 1.1239 0.3931 sec/batch\n",
      "Epoch 12/20  Iteration 3230/5640 Training loss: 1.1236 0.3947 sec/batch\n",
      "Epoch 12/20  Iteration 3231/5640 Training loss: 1.1234 0.3949 sec/batch\n",
      "Epoch 12/20  Iteration 3232/5640 Training loss: 1.1232 0.3960 sec/batch\n",
      "Epoch 12/20  Iteration 3233/5640 Training loss: 1.1232 0.3932 sec/batch\n",
      "Epoch 12/20  Iteration 3234/5640 Training loss: 1.1229 0.3934 sec/batch\n",
      "Epoch 12/20  Iteration 3235/5640 Training loss: 1.1226 0.3956 sec/batch\n",
      "Epoch 12/20  Iteration 3236/5640 Training loss: 1.1223 0.3942 sec/batch\n",
      "Epoch 12/20  Iteration 3237/5640 Training loss: 1.1222 0.3924 sec/batch\n",
      "Epoch 12/20  Iteration 3238/5640 Training loss: 1.1222 0.3941 sec/batch\n",
      "Epoch 12/20  Iteration 3239/5640 Training loss: 1.1222 0.3947 sec/batch\n",
      "Epoch 12/20  Iteration 3240/5640 Training loss: 1.1219 0.3928 sec/batch\n",
      "Epoch 12/20  Iteration 3241/5640 Training loss: 1.1220 0.3957 sec/batch\n",
      "Epoch 12/20  Iteration 3242/5640 Training loss: 1.1220 0.3925 sec/batch\n",
      "Epoch 12/20  Iteration 3243/5640 Training loss: 1.1219 0.3983 sec/batch\n",
      "Epoch 12/20  Iteration 3244/5640 Training loss: 1.1219 0.3937 sec/batch\n",
      "Epoch 12/20  Iteration 3245/5640 Training loss: 1.1220 0.3975 sec/batch\n",
      "Epoch 12/20  Iteration 3246/5640 Training loss: 1.1218 0.3954 sec/batch\n",
      "Epoch 12/20  Iteration 3247/5640 Training loss: 1.1215 0.3960 sec/batch\n",
      "Epoch 12/20  Iteration 3248/5640 Training loss: 1.1213 0.3970 sec/batch\n",
      "Epoch 12/20  Iteration 3249/5640 Training loss: 1.1210 0.3987 sec/batch\n",
      "Epoch 12/20  Iteration 3250/5640 Training loss: 1.1211 0.3953 sec/batch\n",
      "Validation loss: 1.10609 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 3251/5640 Training loss: 1.1222 0.3825 sec/batch\n",
      "Epoch 12/20  Iteration 3252/5640 Training loss: 1.1221 0.3825 sec/batch\n",
      "Epoch 12/20  Iteration 3253/5640 Training loss: 1.1220 0.3829 sec/batch\n",
      "Epoch 12/20  Iteration 3254/5640 Training loss: 1.1220 0.3842 sec/batch\n",
      "Epoch 12/20  Iteration 3255/5640 Training loss: 1.1222 0.3838 sec/batch\n",
      "Epoch 12/20  Iteration 3256/5640 Training loss: 1.1222 0.3869 sec/batch\n",
      "Epoch 12/20  Iteration 3257/5640 Training loss: 1.1222 0.3848 sec/batch\n",
      "Epoch 12/20  Iteration 3258/5640 Training loss: 1.1222 0.3963 sec/batch\n",
      "Epoch 12/20  Iteration 3259/5640 Training loss: 1.1220 0.3864 sec/batch\n",
      "Epoch 12/20  Iteration 3260/5640 Training loss: 1.1220 0.3847 sec/batch\n",
      "Epoch 12/20  Iteration 3261/5640 Training loss: 1.1219 0.3826 sec/batch\n",
      "Epoch 12/20  Iteration 3262/5640 Training loss: 1.1217 0.3826 sec/batch\n",
      "Epoch 12/20  Iteration 3263/5640 Training loss: 1.1216 0.3912 sec/batch\n",
      "Epoch 12/20  Iteration 3264/5640 Training loss: 1.1215 0.4008 sec/batch\n",
      "Epoch 12/20  Iteration 3265/5640 Training loss: 1.1214 0.3999 sec/batch\n",
      "Epoch 12/20  Iteration 3266/5640 Training loss: 1.1213 0.3951 sec/batch\n",
      "Epoch 12/20  Iteration 3267/5640 Training loss: 1.1213 0.3929 sec/batch\n",
      "Epoch 12/20  Iteration 3268/5640 Training loss: 1.1214 0.3995 sec/batch\n",
      "Epoch 12/20  Iteration 3269/5640 Training loss: 1.1213 0.3950 sec/batch\n",
      "Epoch 12/20  Iteration 3270/5640 Training loss: 1.1214 0.3946 sec/batch\n",
      "Epoch 12/20  Iteration 3271/5640 Training loss: 1.1214 0.4039 sec/batch\n",
      "Epoch 12/20  Iteration 3272/5640 Training loss: 1.1214 0.3941 sec/batch\n",
      "Epoch 12/20  Iteration 3273/5640 Training loss: 1.1213 0.3949 sec/batch\n",
      "Epoch 12/20  Iteration 3274/5640 Training loss: 1.1212 0.3952 sec/batch\n",
      "Epoch 12/20  Iteration 3275/5640 Training loss: 1.1211 0.3947 sec/batch\n",
      "Epoch 12/20  Iteration 3276/5640 Training loss: 1.1210 0.3976 sec/batch\n",
      "Epoch 12/20  Iteration 3277/5640 Training loss: 1.1210 0.3925 sec/batch\n",
      "Epoch 12/20  Iteration 3278/5640 Training loss: 1.1210 0.3951 sec/batch\n",
      "Epoch 12/20  Iteration 3279/5640 Training loss: 1.1210 0.3972 sec/batch\n",
      "Epoch 12/20  Iteration 3280/5640 Training loss: 1.1208 0.3959 sec/batch\n",
      "Epoch 12/20  Iteration 3281/5640 Training loss: 1.1210 0.3965 sec/batch\n",
      "Epoch 12/20  Iteration 3282/5640 Training loss: 1.1210 0.3979 sec/batch\n",
      "Epoch 12/20  Iteration 3283/5640 Training loss: 1.1210 0.3978 sec/batch\n",
      "Epoch 12/20  Iteration 3284/5640 Training loss: 1.1211 0.4005 sec/batch\n",
      "Epoch 12/20  Iteration 3285/5640 Training loss: 1.1211 0.3964 sec/batch\n",
      "Epoch 12/20  Iteration 3286/5640 Training loss: 1.1210 0.3943 sec/batch\n",
      "Epoch 12/20  Iteration 3287/5640 Training loss: 1.1209 0.3930 sec/batch\n",
      "Epoch 12/20  Iteration 3288/5640 Training loss: 1.1207 0.3922 sec/batch\n",
      "Epoch 12/20  Iteration 3289/5640 Training loss: 1.1204 0.3964 sec/batch\n",
      "Epoch 12/20  Iteration 3290/5640 Training loss: 1.1201 0.3932 sec/batch\n",
      "Epoch 12/20  Iteration 3291/5640 Training loss: 1.1200 0.3954 sec/batch\n",
      "Epoch 12/20  Iteration 3292/5640 Training loss: 1.1198 0.3940 sec/batch\n",
      "Epoch 12/20  Iteration 3293/5640 Training loss: 1.1197 0.3929 sec/batch\n",
      "Epoch 12/20  Iteration 3294/5640 Training loss: 1.1195 0.3935 sec/batch\n",
      "Epoch 12/20  Iteration 3295/5640 Training loss: 1.1193 0.3956 sec/batch\n",
      "Epoch 12/20  Iteration 3296/5640 Training loss: 1.1193 0.3949 sec/batch\n",
      "Epoch 12/20  Iteration 3297/5640 Training loss: 1.1193 0.3998 sec/batch\n",
      "Epoch 12/20  Iteration 3298/5640 Training loss: 1.1194 0.3975 sec/batch\n",
      "Epoch 12/20  Iteration 3299/5640 Training loss: 1.1193 0.3943 sec/batch\n",
      "Epoch 12/20  Iteration 3300/5640 Training loss: 1.1194 0.3934 sec/batch\n",
      "Validation loss: 1.10239 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 3301/5640 Training loss: 1.1201 0.3838 sec/batch\n",
      "Epoch 12/20  Iteration 3302/5640 Training loss: 1.1201 0.3851 sec/batch\n",
      "Epoch 12/20  Iteration 3303/5640 Training loss: 1.1201 0.3850 sec/batch\n",
      "Epoch 12/20  Iteration 3304/5640 Training loss: 1.1200 0.3814 sec/batch\n",
      "Epoch 12/20  Iteration 3305/5640 Training loss: 1.1200 0.3862 sec/batch\n",
      "Epoch 12/20  Iteration 3306/5640 Training loss: 1.1200 0.3835 sec/batch\n",
      "Epoch 12/20  Iteration 3307/5640 Training loss: 1.1198 0.3864 sec/batch\n",
      "Epoch 12/20  Iteration 3308/5640 Training loss: 1.1198 0.3833 sec/batch\n",
      "Epoch 12/20  Iteration 3309/5640 Training loss: 1.1196 0.3909 sec/batch\n",
      "Epoch 12/20  Iteration 3310/5640 Training loss: 1.1195 0.3913 sec/batch\n",
      "Epoch 12/20  Iteration 3311/5640 Training loss: 1.1195 0.3823 sec/batch\n",
      "Epoch 12/20  Iteration 3312/5640 Training loss: 1.1194 0.3826 sec/batch\n",
      "Epoch 12/20  Iteration 3313/5640 Training loss: 1.1193 0.3830 sec/batch\n",
      "Epoch 12/20  Iteration 3314/5640 Training loss: 1.1193 0.3820 sec/batch\n",
      "Epoch 12/20  Iteration 3315/5640 Training loss: 1.1193 0.3847 sec/batch\n",
      "Epoch 12/20  Iteration 3316/5640 Training loss: 1.1193 0.3890 sec/batch\n",
      "Epoch 12/20  Iteration 3317/5640 Training loss: 1.1192 0.3890 sec/batch\n",
      "Epoch 12/20  Iteration 3318/5640 Training loss: 1.1192 0.3879 sec/batch\n",
      "Epoch 12/20  Iteration 3319/5640 Training loss: 1.1191 0.3901 sec/batch\n",
      "Epoch 12/20  Iteration 3320/5640 Training loss: 1.1188 0.3913 sec/batch\n",
      "Epoch 12/20  Iteration 3321/5640 Training loss: 1.1187 0.3887 sec/batch\n",
      "Epoch 12/20  Iteration 3322/5640 Training loss: 1.1186 0.3889 sec/batch\n",
      "Epoch 12/20  Iteration 3323/5640 Training loss: 1.1185 0.3887 sec/batch\n",
      "Epoch 12/20  Iteration 3324/5640 Training loss: 1.1184 0.3956 sec/batch\n",
      "Epoch 12/20  Iteration 3325/5640 Training loss: 1.1183 0.4013 sec/batch\n",
      "Epoch 12/20  Iteration 3326/5640 Training loss: 1.1182 0.3977 sec/batch\n",
      "Epoch 12/20  Iteration 3327/5640 Training loss: 1.1182 0.3901 sec/batch\n",
      "Epoch 12/20  Iteration 3328/5640 Training loss: 1.1181 0.3905 sec/batch\n",
      "Epoch 12/20  Iteration 3329/5640 Training loss: 1.1181 0.3893 sec/batch\n",
      "Epoch 12/20  Iteration 3330/5640 Training loss: 1.1181 0.3944 sec/batch\n",
      "Epoch 12/20  Iteration 3331/5640 Training loss: 1.1182 0.3937 sec/batch\n",
      "Epoch 12/20  Iteration 3332/5640 Training loss: 1.1181 0.3966 sec/batch\n",
      "Epoch 12/20  Iteration 3333/5640 Training loss: 1.1181 0.3968 sec/batch\n",
      "Epoch 12/20  Iteration 3334/5640 Training loss: 1.1180 0.3934 sec/batch\n",
      "Epoch 12/20  Iteration 3335/5640 Training loss: 1.1180 0.3945 sec/batch\n",
      "Epoch 12/20  Iteration 3336/5640 Training loss: 1.1178 0.3949 sec/batch\n",
      "Epoch 12/20  Iteration 3337/5640 Training loss: 1.1177 0.3942 sec/batch\n",
      "Epoch 12/20  Iteration 3338/5640 Training loss: 1.1178 0.3932 sec/batch\n",
      "Epoch 12/20  Iteration 3339/5640 Training loss: 1.1177 0.3963 sec/batch\n",
      "Epoch 12/20  Iteration 3340/5640 Training loss: 1.1178 0.3941 sec/batch\n",
      "Epoch 12/20  Iteration 3341/5640 Training loss: 1.1177 0.3939 sec/batch\n",
      "Epoch 12/20  Iteration 3342/5640 Training loss: 1.1176 0.3941 sec/batch\n",
      "Epoch 12/20  Iteration 3343/5640 Training loss: 1.1176 0.3964 sec/batch\n",
      "Epoch 12/20  Iteration 3344/5640 Training loss: 1.1175 0.3952 sec/batch\n",
      "Epoch 12/20  Iteration 3345/5640 Training loss: 1.1174 0.3948 sec/batch\n",
      "Epoch 12/20  Iteration 3346/5640 Training loss: 1.1172 0.3944 sec/batch\n",
      "Epoch 12/20  Iteration 3347/5640 Training loss: 1.1171 0.3956 sec/batch\n",
      "Epoch 12/20  Iteration 3348/5640 Training loss: 1.1170 0.3939 sec/batch\n",
      "Epoch 12/20  Iteration 3349/5640 Training loss: 1.1169 0.3944 sec/batch\n",
      "Epoch 12/20  Iteration 3350/5640 Training loss: 1.1169 0.3946 sec/batch\n",
      "Validation loss: 1.10372 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 3351/5640 Training loss: 1.1176 0.3825 sec/batch\n",
      "Epoch 12/20  Iteration 3352/5640 Training loss: 1.1176 0.3821 sec/batch\n",
      "Epoch 12/20  Iteration 3353/5640 Training loss: 1.1175 0.3829 sec/batch\n",
      "Epoch 12/20  Iteration 3354/5640 Training loss: 1.1174 0.3843 sec/batch\n",
      "Epoch 12/20  Iteration 3355/5640 Training loss: 1.1173 0.3823 sec/batch\n",
      "Epoch 12/20  Iteration 3356/5640 Training loss: 1.1173 0.3850 sec/batch\n",
      "Epoch 12/20  Iteration 3357/5640 Training loss: 1.1172 0.3847 sec/batch\n",
      "Epoch 12/20  Iteration 3358/5640 Training loss: 1.1171 0.3845 sec/batch\n",
      "Epoch 12/20  Iteration 3359/5640 Training loss: 1.1170 0.3833 sec/batch\n",
      "Epoch 12/20  Iteration 3360/5640 Training loss: 1.1169 0.3813 sec/batch\n",
      "Epoch 12/20  Iteration 3361/5640 Training loss: 1.1169 0.3844 sec/batch\n",
      "Epoch 12/20  Iteration 3362/5640 Training loss: 1.1170 0.3870 sec/batch\n",
      "Epoch 12/20  Iteration 3363/5640 Training loss: 1.1169 0.3894 sec/batch\n",
      "Epoch 12/20  Iteration 3364/5640 Training loss: 1.1169 0.3894 sec/batch\n",
      "Epoch 12/20  Iteration 3365/5640 Training loss: 1.1170 0.3930 sec/batch\n",
      "Epoch 12/20  Iteration 3366/5640 Training loss: 1.1169 0.3912 sec/batch\n",
      "Epoch 12/20  Iteration 3367/5640 Training loss: 1.1169 0.3897 sec/batch\n",
      "Epoch 12/20  Iteration 3368/5640 Training loss: 1.1168 0.3909 sec/batch\n",
      "Epoch 12/20  Iteration 3369/5640 Training loss: 1.1168 0.3907 sec/batch\n",
      "Epoch 12/20  Iteration 3370/5640 Training loss: 1.1166 0.3902 sec/batch\n",
      "Epoch 12/20  Iteration 3371/5640 Training loss: 1.1166 0.3918 sec/batch\n",
      "Epoch 12/20  Iteration 3372/5640 Training loss: 1.1165 0.3919 sec/batch\n",
      "Epoch 12/20  Iteration 3373/5640 Training loss: 1.1165 0.3917 sec/batch\n",
      "Epoch 12/20  Iteration 3374/5640 Training loss: 1.1164 0.3917 sec/batch\n",
      "Epoch 12/20  Iteration 3375/5640 Training loss: 1.1164 0.3936 sec/batch\n",
      "Epoch 12/20  Iteration 3376/5640 Training loss: 1.1163 0.3940 sec/batch\n",
      "Epoch 12/20  Iteration 3377/5640 Training loss: 1.1162 0.3950 sec/batch\n",
      "Epoch 12/20  Iteration 3378/5640 Training loss: 1.1162 0.3930 sec/batch\n",
      "Epoch 12/20  Iteration 3379/5640 Training loss: 1.1162 0.3933 sec/batch\n",
      "Epoch 12/20  Iteration 3380/5640 Training loss: 1.1163 0.3960 sec/batch\n",
      "Epoch 12/20  Iteration 3381/5640 Training loss: 1.1163 0.3949 sec/batch\n",
      "Epoch 12/20  Iteration 3382/5640 Training loss: 1.1163 0.3935 sec/batch\n",
      "Epoch 12/20  Iteration 3383/5640 Training loss: 1.1162 0.3954 sec/batch\n",
      "Epoch 12/20  Iteration 3384/5640 Training loss: 1.1161 0.3950 sec/batch\n",
      "Epoch 13/20  Iteration 3385/5640 Training loss: 1.2094 0.3943 sec/batch\n",
      "Epoch 13/20  Iteration 3386/5640 Training loss: 1.1724 0.3921 sec/batch\n",
      "Epoch 13/20  Iteration 3387/5640 Training loss: 1.1560 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3388/5640 Training loss: 1.1461 0.3958 sec/batch\n",
      "Epoch 13/20  Iteration 3389/5640 Training loss: 1.1418 0.3931 sec/batch\n",
      "Epoch 13/20  Iteration 3390/5640 Training loss: 1.1391 0.3942 sec/batch\n",
      "Epoch 13/20  Iteration 3391/5640 Training loss: 1.1328 0.3939 sec/batch\n",
      "Epoch 13/20  Iteration 3392/5640 Training loss: 1.1305 0.3949 sec/batch\n",
      "Epoch 13/20  Iteration 3393/5640 Training loss: 1.1257 0.3942 sec/batch\n",
      "Epoch 13/20  Iteration 3394/5640 Training loss: 1.1212 0.3933 sec/batch\n",
      "Epoch 13/20  Iteration 3395/5640 Training loss: 1.1209 0.3955 sec/batch\n",
      "Epoch 13/20  Iteration 3396/5640 Training loss: 1.1188 0.3954 sec/batch\n",
      "Epoch 13/20  Iteration 3397/5640 Training loss: 1.1181 0.3951 sec/batch\n",
      "Epoch 13/20  Iteration 3398/5640 Training loss: 1.1181 0.3931 sec/batch\n",
      "Epoch 13/20  Iteration 3399/5640 Training loss: 1.1159 0.3939 sec/batch\n",
      "Epoch 13/20  Iteration 3400/5640 Training loss: 1.1170 0.3942 sec/batch\n",
      "Validation loss: 1.09862 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 3401/5640 Training loss: 1.1272 0.3856 sec/batch\n",
      "Epoch 13/20  Iteration 3402/5640 Training loss: 1.1257 0.3830 sec/batch\n",
      "Epoch 13/20  Iteration 3403/5640 Training loss: 1.1252 0.3816 sec/batch\n",
      "Epoch 13/20  Iteration 3404/5640 Training loss: 1.1233 0.3833 sec/batch\n",
      "Epoch 13/20  Iteration 3405/5640 Training loss: 1.1226 0.3836 sec/batch\n",
      "Epoch 13/20  Iteration 3406/5640 Training loss: 1.1224 0.3820 sec/batch\n",
      "Epoch 13/20  Iteration 3407/5640 Training loss: 1.1219 0.3824 sec/batch\n",
      "Epoch 13/20  Iteration 3408/5640 Training loss: 1.1200 0.3826 sec/batch\n",
      "Epoch 13/20  Iteration 3409/5640 Training loss: 1.1195 0.3840 sec/batch\n",
      "Epoch 13/20  Iteration 3410/5640 Training loss: 1.1188 0.3838 sec/batch\n",
      "Epoch 13/20  Iteration 3411/5640 Training loss: 1.1185 0.3867 sec/batch\n",
      "Epoch 13/20  Iteration 3412/5640 Training loss: 1.1181 0.3856 sec/batch\n",
      "Epoch 13/20  Iteration 3413/5640 Training loss: 1.1174 0.3856 sec/batch\n",
      "Epoch 13/20  Iteration 3414/5640 Training loss: 1.1170 0.3852 sec/batch\n",
      "Epoch 13/20  Iteration 3415/5640 Training loss: 1.1160 0.3880 sec/batch\n",
      "Epoch 13/20  Iteration 3416/5640 Training loss: 1.1154 0.3907 sec/batch\n",
      "Epoch 13/20  Iteration 3417/5640 Training loss: 1.1143 0.3917 sec/batch\n",
      "Epoch 13/20  Iteration 3418/5640 Training loss: 1.1138 0.3929 sec/batch\n",
      "Epoch 13/20  Iteration 3419/5640 Training loss: 1.1143 0.3936 sec/batch\n",
      "Epoch 13/20  Iteration 3420/5640 Training loss: 1.1141 0.3949 sec/batch\n",
      "Epoch 13/20  Iteration 3421/5640 Training loss: 1.1132 0.3950 sec/batch\n",
      "Epoch 13/20  Iteration 3422/5640 Training loss: 1.1124 0.3964 sec/batch\n",
      "Epoch 13/20  Iteration 3423/5640 Training loss: 1.1120 0.3947 sec/batch\n",
      "Epoch 13/20  Iteration 3424/5640 Training loss: 1.1110 0.3942 sec/batch\n",
      "Epoch 13/20  Iteration 3425/5640 Training loss: 1.1103 0.3921 sec/batch\n",
      "Epoch 13/20  Iteration 3426/5640 Training loss: 1.1109 0.3955 sec/batch\n",
      "Epoch 13/20  Iteration 3427/5640 Training loss: 1.1101 0.3939 sec/batch\n",
      "Epoch 13/20  Iteration 3428/5640 Training loss: 1.1102 0.3959 sec/batch\n",
      "Epoch 13/20  Iteration 3429/5640 Training loss: 1.1099 0.3945 sec/batch\n",
      "Epoch 13/20  Iteration 3430/5640 Training loss: 1.1096 0.3984 sec/batch\n",
      "Epoch 13/20  Iteration 3431/5640 Training loss: 1.1097 0.3970 sec/batch\n",
      "Epoch 13/20  Iteration 3432/5640 Training loss: 1.1100 0.3969 sec/batch\n",
      "Epoch 13/20  Iteration 3433/5640 Training loss: 1.1100 0.3946 sec/batch\n",
      "Epoch 13/20  Iteration 3434/5640 Training loss: 1.1097 0.3992 sec/batch\n",
      "Epoch 13/20  Iteration 3435/5640 Training loss: 1.1097 0.3961 sec/batch\n",
      "Epoch 13/20  Iteration 3436/5640 Training loss: 1.1097 0.4037 sec/batch\n",
      "Epoch 13/20  Iteration 3437/5640 Training loss: 1.1096 0.3977 sec/batch\n",
      "Epoch 13/20  Iteration 3438/5640 Training loss: 1.1097 0.3938 sec/batch\n",
      "Epoch 13/20  Iteration 3439/5640 Training loss: 1.1093 0.3963 sec/batch\n",
      "Epoch 13/20  Iteration 3440/5640 Training loss: 1.1092 0.3945 sec/batch\n",
      "Epoch 13/20  Iteration 3441/5640 Training loss: 1.1089 0.3975 sec/batch\n",
      "Epoch 13/20  Iteration 3442/5640 Training loss: 1.1090 0.3985 sec/batch\n",
      "Epoch 13/20  Iteration 3443/5640 Training loss: 1.1088 0.3966 sec/batch\n",
      "Epoch 13/20  Iteration 3444/5640 Training loss: 1.1086 0.3980 sec/batch\n",
      "Epoch 13/20  Iteration 3445/5640 Training loss: 1.1085 0.3983 sec/batch\n",
      "Epoch 13/20  Iteration 3446/5640 Training loss: 1.1081 0.3944 sec/batch\n",
      "Epoch 13/20  Iteration 3447/5640 Training loss: 1.1077 0.4011 sec/batch\n",
      "Epoch 13/20  Iteration 3448/5640 Training loss: 1.1073 0.3950 sec/batch\n",
      "Epoch 13/20  Iteration 3449/5640 Training loss: 1.1076 0.3950 sec/batch\n",
      "Epoch 13/20  Iteration 3450/5640 Training loss: 1.1074 0.3943 sec/batch\n",
      "Validation loss: 1.09763 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 3451/5640 Training loss: 1.1103 0.3895 sec/batch\n",
      "Epoch 13/20  Iteration 3452/5640 Training loss: 1.1101 0.3848 sec/batch\n",
      "Epoch 13/20  Iteration 3453/5640 Training loss: 1.1099 0.3890 sec/batch\n",
      "Epoch 13/20  Iteration 3454/5640 Training loss: 1.1095 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3455/5640 Training loss: 1.1093 0.3836 sec/batch\n",
      "Epoch 13/20  Iteration 3456/5640 Training loss: 1.1091 0.3831 sec/batch\n",
      "Epoch 13/20  Iteration 3457/5640 Training loss: 1.1091 0.3826 sec/batch\n",
      "Epoch 13/20  Iteration 3458/5640 Training loss: 1.1091 0.3812 sec/batch\n",
      "Epoch 13/20  Iteration 3459/5640 Training loss: 1.1092 0.3827 sec/batch\n",
      "Epoch 13/20  Iteration 3460/5640 Training loss: 1.1098 0.3847 sec/batch\n",
      "Epoch 13/20  Iteration 3461/5640 Training loss: 1.1098 0.3868 sec/batch\n",
      "Epoch 13/20  Iteration 3462/5640 Training loss: 1.1104 0.3856 sec/batch\n",
      "Epoch 13/20  Iteration 3463/5640 Training loss: 1.1103 0.3883 sec/batch\n",
      "Epoch 13/20  Iteration 3464/5640 Training loss: 1.1105 0.3883 sec/batch\n",
      "Epoch 13/20  Iteration 3465/5640 Training loss: 1.1108 0.3870 sec/batch\n",
      "Epoch 13/20  Iteration 3466/5640 Training loss: 1.1105 0.3875 sec/batch\n",
      "Epoch 13/20  Iteration 3467/5640 Training loss: 1.1104 0.3877 sec/batch\n",
      "Epoch 13/20  Iteration 3468/5640 Training loss: 1.1101 0.3880 sec/batch\n",
      "Epoch 13/20  Iteration 3469/5640 Training loss: 1.1099 0.3879 sec/batch\n",
      "Epoch 13/20  Iteration 3470/5640 Training loss: 1.1097 0.3896 sec/batch\n",
      "Epoch 13/20  Iteration 3471/5640 Training loss: 1.1095 0.3942 sec/batch\n",
      "Epoch 13/20  Iteration 3472/5640 Training loss: 1.1092 0.3996 sec/batch\n",
      "Epoch 13/20  Iteration 3473/5640 Training loss: 1.1091 0.4002 sec/batch\n",
      "Epoch 13/20  Iteration 3474/5640 Training loss: 1.1088 0.4001 sec/batch\n",
      "Epoch 13/20  Iteration 3475/5640 Training loss: 1.1091 0.3980 sec/batch\n",
      "Epoch 13/20  Iteration 3476/5640 Training loss: 1.1091 0.3978 sec/batch\n",
      "Epoch 13/20  Iteration 3477/5640 Training loss: 1.1090 0.3993 sec/batch\n",
      "Epoch 13/20  Iteration 3478/5640 Training loss: 1.1088 0.3980 sec/batch\n",
      "Epoch 13/20  Iteration 3479/5640 Training loss: 1.1087 0.4008 sec/batch\n",
      "Epoch 13/20  Iteration 3480/5640 Training loss: 1.1087 0.4000 sec/batch\n",
      "Epoch 13/20  Iteration 3481/5640 Training loss: 1.1089 0.3959 sec/batch\n",
      "Epoch 13/20  Iteration 3482/5640 Training loss: 1.1089 0.3942 sec/batch\n",
      "Epoch 13/20  Iteration 3483/5640 Training loss: 1.1089 0.3947 sec/batch\n",
      "Epoch 13/20  Iteration 3484/5640 Training loss: 1.1086 0.3954 sec/batch\n",
      "Epoch 13/20  Iteration 3485/5640 Training loss: 1.1083 0.3968 sec/batch\n",
      "Epoch 13/20  Iteration 3486/5640 Training loss: 1.1082 0.3948 sec/batch\n",
      "Epoch 13/20  Iteration 3487/5640 Training loss: 1.1080 0.3945 sec/batch\n",
      "Epoch 13/20  Iteration 3488/5640 Training loss: 1.1078 0.3952 sec/batch\n",
      "Epoch 13/20  Iteration 3489/5640 Training loss: 1.1074 0.3941 sec/batch\n",
      "Epoch 13/20  Iteration 3490/5640 Training loss: 1.1071 0.3937 sec/batch\n",
      "Epoch 13/20  Iteration 3491/5640 Training loss: 1.1068 0.3950 sec/batch\n",
      "Epoch 13/20  Iteration 3492/5640 Training loss: 1.1065 0.3932 sec/batch\n",
      "Epoch 13/20  Iteration 3493/5640 Training loss: 1.1063 0.3959 sec/batch\n",
      "Epoch 13/20  Iteration 3494/5640 Training loss: 1.1061 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3495/5640 Training loss: 1.1058 0.4038 sec/batch\n",
      "Epoch 13/20  Iteration 3496/5640 Training loss: 1.1059 0.3957 sec/batch\n",
      "Epoch 13/20  Iteration 3497/5640 Training loss: 1.1056 0.3940 sec/batch\n",
      "Epoch 13/20  Iteration 3498/5640 Training loss: 1.1055 0.3958 sec/batch\n",
      "Epoch 13/20  Iteration 3499/5640 Training loss: 1.1056 0.3974 sec/batch\n",
      "Epoch 13/20  Iteration 3500/5640 Training loss: 1.1056 0.3943 sec/batch\n",
      "Validation loss: 1.10124 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 3501/5640 Training loss: 1.1068 0.3859 sec/batch\n",
      "Epoch 13/20  Iteration 3502/5640 Training loss: 1.1067 0.3825 sec/batch\n",
      "Epoch 13/20  Iteration 3503/5640 Training loss: 1.1065 0.3899 sec/batch\n",
      "Epoch 13/20  Iteration 3504/5640 Training loss: 1.1062 0.3810 sec/batch\n",
      "Epoch 13/20  Iteration 3505/5640 Training loss: 1.1060 0.3835 sec/batch\n",
      "Epoch 13/20  Iteration 3506/5640 Training loss: 1.1060 0.3816 sec/batch\n",
      "Epoch 13/20  Iteration 3507/5640 Training loss: 1.1058 0.3823 sec/batch\n",
      "Epoch 13/20  Iteration 3508/5640 Training loss: 1.1057 0.3823 sec/batch\n",
      "Epoch 13/20  Iteration 3509/5640 Training loss: 1.1060 0.3843 sec/batch\n",
      "Epoch 13/20  Iteration 3510/5640 Training loss: 1.1056 0.3832 sec/batch\n",
      "Epoch 13/20  Iteration 3511/5640 Training loss: 1.1055 0.3894 sec/batch\n",
      "Epoch 13/20  Iteration 3512/5640 Training loss: 1.1051 0.3889 sec/batch\n",
      "Epoch 13/20  Iteration 3513/5640 Training loss: 1.1049 0.3882 sec/batch\n",
      "Epoch 13/20  Iteration 3514/5640 Training loss: 1.1048 0.3893 sec/batch\n",
      "Epoch 13/20  Iteration 3515/5640 Training loss: 1.1047 0.3916 sec/batch\n",
      "Epoch 13/20  Iteration 3516/5640 Training loss: 1.1043 0.3877 sec/batch\n",
      "Epoch 13/20  Iteration 3517/5640 Training loss: 1.1040 0.3902 sec/batch\n",
      "Epoch 13/20  Iteration 3518/5640 Training loss: 1.1037 0.3907 sec/batch\n",
      "Epoch 13/20  Iteration 3519/5640 Training loss: 1.1037 0.4092 sec/batch\n",
      "Epoch 13/20  Iteration 3520/5640 Training loss: 1.1036 0.3995 sec/batch\n",
      "Epoch 13/20  Iteration 3521/5640 Training loss: 1.1037 0.3899 sec/batch\n",
      "Epoch 13/20  Iteration 3522/5640 Training loss: 1.1035 0.3893 sec/batch\n",
      "Epoch 13/20  Iteration 3523/5640 Training loss: 1.1034 0.3918 sec/batch\n",
      "Epoch 13/20  Iteration 3524/5640 Training loss: 1.1034 0.3970 sec/batch\n",
      "Epoch 13/20  Iteration 3525/5640 Training loss: 1.1033 0.3918 sec/batch\n",
      "Epoch 13/20  Iteration 3526/5640 Training loss: 1.1033 0.3904 sec/batch\n",
      "Epoch 13/20  Iteration 3527/5640 Training loss: 1.1033 0.3917 sec/batch\n",
      "Epoch 13/20  Iteration 3528/5640 Training loss: 1.1031 0.3912 sec/batch\n",
      "Epoch 13/20  Iteration 3529/5640 Training loss: 1.1028 0.3933 sec/batch\n",
      "Epoch 13/20  Iteration 3530/5640 Training loss: 1.1026 0.3903 sec/batch\n",
      "Epoch 13/20  Iteration 3531/5640 Training loss: 1.1023 0.3914 sec/batch\n",
      "Epoch 13/20  Iteration 3532/5640 Training loss: 1.1024 0.3942 sec/batch\n",
      "Epoch 13/20  Iteration 3533/5640 Training loss: 1.1024 0.3922 sec/batch\n",
      "Epoch 13/20  Iteration 3534/5640 Training loss: 1.1021 0.3918 sec/batch\n",
      "Epoch 13/20  Iteration 3535/5640 Training loss: 1.1021 0.3892 sec/batch\n",
      "Epoch 13/20  Iteration 3536/5640 Training loss: 1.1021 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3537/5640 Training loss: 1.1023 0.3918 sec/batch\n",
      "Epoch 13/20  Iteration 3538/5640 Training loss: 1.1023 0.3928 sec/batch\n",
      "Epoch 13/20  Iteration 3539/5640 Training loss: 1.1023 0.3937 sec/batch\n",
      "Epoch 13/20  Iteration 3540/5640 Training loss: 1.1024 0.3914 sec/batch\n",
      "Epoch 13/20  Iteration 3541/5640 Training loss: 1.1021 0.3914 sec/batch\n",
      "Epoch 13/20  Iteration 3542/5640 Training loss: 1.1021 0.3927 sec/batch\n",
      "Epoch 13/20  Iteration 3543/5640 Training loss: 1.1020 0.3981 sec/batch\n",
      "Epoch 13/20  Iteration 3544/5640 Training loss: 1.1018 0.3895 sec/batch\n",
      "Epoch 13/20  Iteration 3545/5640 Training loss: 1.1017 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3546/5640 Training loss: 1.1016 0.3975 sec/batch\n",
      "Epoch 13/20  Iteration 3547/5640 Training loss: 1.1015 0.3959 sec/batch\n",
      "Epoch 13/20  Iteration 3548/5640 Training loss: 1.1014 0.3938 sec/batch\n",
      "Epoch 13/20  Iteration 3549/5640 Training loss: 1.1015 0.3946 sec/batch\n",
      "Epoch 13/20  Iteration 3550/5640 Training loss: 1.1017 0.3955 sec/batch\n",
      "Validation loss: 1.09885 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 3551/5640 Training loss: 1.1027 0.3826 sec/batch\n",
      "Epoch 13/20  Iteration 3552/5640 Training loss: 1.1028 0.3820 sec/batch\n",
      "Epoch 13/20  Iteration 3553/5640 Training loss: 1.1028 0.3815 sec/batch\n",
      "Epoch 13/20  Iteration 3554/5640 Training loss: 1.1029 0.3820 sec/batch\n",
      "Epoch 13/20  Iteration 3555/5640 Training loss: 1.1028 0.3820 sec/batch\n",
      "Epoch 13/20  Iteration 3556/5640 Training loss: 1.1027 0.3810 sec/batch\n",
      "Epoch 13/20  Iteration 3557/5640 Training loss: 1.1026 0.3851 sec/batch\n",
      "Epoch 13/20  Iteration 3558/5640 Training loss: 1.1026 0.3890 sec/batch\n",
      "Epoch 13/20  Iteration 3559/5640 Training loss: 1.1025 0.3881 sec/batch\n",
      "Epoch 13/20  Iteration 3560/5640 Training loss: 1.1025 0.4030 sec/batch\n",
      "Epoch 13/20  Iteration 3561/5640 Training loss: 1.1026 0.4210 sec/batch\n",
      "Epoch 13/20  Iteration 3562/5640 Training loss: 1.1024 0.4117 sec/batch\n",
      "Epoch 13/20  Iteration 3563/5640 Training loss: 1.1026 0.4140 sec/batch\n",
      "Epoch 13/20  Iteration 3564/5640 Training loss: 1.1027 0.3944 sec/batch\n",
      "Epoch 13/20  Iteration 3565/5640 Training loss: 1.1027 0.3947 sec/batch\n",
      "Epoch 13/20  Iteration 3566/5640 Training loss: 1.1028 0.3939 sec/batch\n",
      "Epoch 13/20  Iteration 3567/5640 Training loss: 1.1028 0.3980 sec/batch\n",
      "Epoch 13/20  Iteration 3568/5640 Training loss: 1.1026 0.4190 sec/batch\n",
      "Epoch 13/20  Iteration 3569/5640 Training loss: 1.1025 0.3948 sec/batch\n",
      "Epoch 13/20  Iteration 3570/5640 Training loss: 1.1023 0.3930 sec/batch\n",
      "Epoch 13/20  Iteration 3571/5640 Training loss: 1.1020 0.3929 sec/batch\n",
      "Epoch 13/20  Iteration 3572/5640 Training loss: 1.1018 0.3949 sec/batch\n",
      "Epoch 13/20  Iteration 3573/5640 Training loss: 1.1016 0.3950 sec/batch\n",
      "Epoch 13/20  Iteration 3574/5640 Training loss: 1.1014 0.3939 sec/batch\n",
      "Epoch 13/20  Iteration 3575/5640 Training loss: 1.1013 0.4066 sec/batch\n",
      "Epoch 13/20  Iteration 3576/5640 Training loss: 1.1011 0.3954 sec/batch\n",
      "Epoch 13/20  Iteration 3577/5640 Training loss: 1.1010 0.3964 sec/batch\n",
      "Epoch 13/20  Iteration 3578/5640 Training loss: 1.1010 0.3940 sec/batch\n",
      "Epoch 13/20  Iteration 3579/5640 Training loss: 1.1010 0.3965 sec/batch\n",
      "Epoch 13/20  Iteration 3580/5640 Training loss: 1.1011 0.3948 sec/batch\n",
      "Epoch 13/20  Iteration 3581/5640 Training loss: 1.1010 0.4031 sec/batch\n",
      "Epoch 13/20  Iteration 3582/5640 Training loss: 1.1011 0.4403 sec/batch\n",
      "Epoch 13/20  Iteration 3583/5640 Training loss: 1.1010 0.4318 sec/batch\n",
      "Epoch 13/20  Iteration 3584/5640 Training loss: 1.1010 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3585/5640 Training loss: 1.1010 0.3946 sec/batch\n",
      "Epoch 13/20  Iteration 3586/5640 Training loss: 1.1009 0.3966 sec/batch\n",
      "Epoch 13/20  Iteration 3587/5640 Training loss: 1.1008 0.3941 sec/batch\n",
      "Epoch 13/20  Iteration 3588/5640 Training loss: 1.1008 0.3940 sec/batch\n",
      "Epoch 13/20  Iteration 3589/5640 Training loss: 1.1007 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3590/5640 Training loss: 1.1006 0.3927 sec/batch\n",
      "Epoch 13/20  Iteration 3591/5640 Training loss: 1.1004 0.3934 sec/batch\n",
      "Epoch 13/20  Iteration 3592/5640 Training loss: 1.1003 0.3931 sec/batch\n",
      "Epoch 13/20  Iteration 3593/5640 Training loss: 1.1003 0.3931 sec/batch\n",
      "Epoch 13/20  Iteration 3594/5640 Training loss: 1.1002 0.3935 sec/batch\n",
      "Epoch 13/20  Iteration 3595/5640 Training loss: 1.1000 0.3949 sec/batch\n",
      "Epoch 13/20  Iteration 3596/5640 Training loss: 1.1000 0.3939 sec/batch\n",
      "Epoch 13/20  Iteration 3597/5640 Training loss: 1.1000 0.3928 sec/batch\n",
      "Epoch 13/20  Iteration 3598/5640 Training loss: 1.1000 0.3958 sec/batch\n",
      "Epoch 13/20  Iteration 3599/5640 Training loss: 1.0999 0.3936 sec/batch\n",
      "Epoch 13/20  Iteration 3600/5640 Training loss: 1.0999 0.3936 sec/batch\n",
      "Validation loss: 1.09575 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 3601/5640 Training loss: 1.1007 0.3838 sec/batch\n",
      "Epoch 13/20  Iteration 3602/5640 Training loss: 1.1005 0.3833 sec/batch\n",
      "Epoch 13/20  Iteration 3603/5640 Training loss: 1.1004 0.3817 sec/batch\n",
      "Epoch 13/20  Iteration 3604/5640 Training loss: 1.1002 0.3826 sec/batch\n",
      "Epoch 13/20  Iteration 3605/5640 Training loss: 1.1001 0.3831 sec/batch\n",
      "Epoch 13/20  Iteration 3606/5640 Training loss: 1.1001 0.3824 sec/batch\n",
      "Epoch 13/20  Iteration 3607/5640 Training loss: 1.1000 0.3822 sec/batch\n",
      "Epoch 13/20  Iteration 3608/5640 Training loss: 1.0999 0.3809 sec/batch\n",
      "Epoch 13/20  Iteration 3609/5640 Training loss: 1.0999 0.3814 sec/batch\n",
      "Epoch 13/20  Iteration 3610/5640 Training loss: 1.0998 0.3816 sec/batch\n",
      "Epoch 13/20  Iteration 3611/5640 Training loss: 1.0998 0.3839 sec/batch\n",
      "Epoch 13/20  Iteration 3612/5640 Training loss: 1.0999 0.3826 sec/batch\n",
      "Epoch 13/20  Iteration 3613/5640 Training loss: 1.0999 0.3842 sec/batch\n",
      "Epoch 13/20  Iteration 3614/5640 Training loss: 1.0999 0.4157 sec/batch\n",
      "Epoch 13/20  Iteration 3615/5640 Training loss: 1.0998 0.4117 sec/batch\n",
      "Epoch 13/20  Iteration 3616/5640 Training loss: 1.0998 0.4181 sec/batch\n",
      "Epoch 13/20  Iteration 3617/5640 Training loss: 1.0997 0.4069 sec/batch\n",
      "Epoch 13/20  Iteration 3618/5640 Training loss: 1.0996 0.4064 sec/batch\n",
      "Epoch 13/20  Iteration 3619/5640 Training loss: 1.0996 0.3998 sec/batch\n",
      "Epoch 13/20  Iteration 3620/5640 Training loss: 1.0996 0.3967 sec/batch\n",
      "Epoch 13/20  Iteration 3621/5640 Training loss: 1.0995 0.4137 sec/batch\n",
      "Epoch 13/20  Iteration 3622/5640 Training loss: 1.0996 0.4136 sec/batch\n",
      "Epoch 13/20  Iteration 3623/5640 Training loss: 1.0996 0.3987 sec/batch\n",
      "Epoch 13/20  Iteration 3624/5640 Training loss: 1.0995 0.4156 sec/batch\n",
      "Epoch 13/20  Iteration 3625/5640 Training loss: 1.0995 0.4085 sec/batch\n",
      "Epoch 13/20  Iteration 3626/5640 Training loss: 1.0995 0.4077 sec/batch\n",
      "Epoch 13/20  Iteration 3627/5640 Training loss: 1.0994 0.4180 sec/batch\n",
      "Epoch 13/20  Iteration 3628/5640 Training loss: 1.0992 0.4185 sec/batch\n",
      "Epoch 13/20  Iteration 3629/5640 Training loss: 1.0991 0.4096 sec/batch\n",
      "Epoch 13/20  Iteration 3630/5640 Training loss: 1.0990 0.3881 sec/batch\n",
      "Epoch 13/20  Iteration 3631/5640 Training loss: 1.0990 0.3964 sec/batch\n",
      "Epoch 13/20  Iteration 3632/5640 Training loss: 1.0989 0.4025 sec/batch\n",
      "Epoch 13/20  Iteration 3633/5640 Training loss: 1.0990 0.3901 sec/batch\n",
      "Epoch 13/20  Iteration 3634/5640 Training loss: 1.0990 0.3911 sec/batch\n",
      "Epoch 13/20  Iteration 3635/5640 Training loss: 1.0988 0.3882 sec/batch\n",
      "Epoch 13/20  Iteration 3636/5640 Training loss: 1.0988 0.3905 sec/batch\n",
      "Epoch 13/20  Iteration 3637/5640 Training loss: 1.0987 0.3901 sec/batch\n",
      "Epoch 13/20  Iteration 3638/5640 Training loss: 1.0987 0.3925 sec/batch\n",
      "Epoch 13/20  Iteration 3639/5640 Training loss: 1.0986 0.3958 sec/batch\n",
      "Epoch 13/20  Iteration 3640/5640 Training loss: 1.0985 0.3958 sec/batch\n",
      "Epoch 13/20  Iteration 3641/5640 Training loss: 1.0984 0.3974 sec/batch\n",
      "Epoch 13/20  Iteration 3642/5640 Training loss: 1.0984 0.3974 sec/batch\n",
      "Epoch 13/20  Iteration 3643/5640 Training loss: 1.0984 0.3962 sec/batch\n",
      "Epoch 13/20  Iteration 3644/5640 Training loss: 1.0984 0.3963 sec/batch\n",
      "Epoch 13/20  Iteration 3645/5640 Training loss: 1.0984 0.3975 sec/batch\n",
      "Epoch 13/20  Iteration 3646/5640 Training loss: 1.0984 0.3964 sec/batch\n",
      "Epoch 13/20  Iteration 3647/5640 Training loss: 1.0985 0.3977 sec/batch\n",
      "Epoch 13/20  Iteration 3648/5640 Training loss: 1.0984 0.4011 sec/batch\n",
      "Epoch 13/20  Iteration 3649/5640 Training loss: 1.0985 0.3957 sec/batch\n",
      "Epoch 13/20  Iteration 3650/5640 Training loss: 1.0984 0.3986 sec/batch\n",
      "Validation loss: 1.09493 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 3651/5640 Training loss: 1.0990 0.3839 sec/batch\n",
      "Epoch 13/20  Iteration 3652/5640 Training loss: 1.0989 0.3836 sec/batch\n",
      "Epoch 13/20  Iteration 3653/5640 Training loss: 1.0989 0.3818 sec/batch\n",
      "Epoch 13/20  Iteration 3654/5640 Training loss: 1.0988 0.3823 sec/batch\n",
      "Epoch 13/20  Iteration 3655/5640 Training loss: 1.0987 0.3813 sec/batch\n",
      "Epoch 13/20  Iteration 3656/5640 Training loss: 1.0987 0.3822 sec/batch\n",
      "Epoch 13/20  Iteration 3657/5640 Training loss: 1.0987 0.3810 sec/batch\n",
      "Epoch 13/20  Iteration 3658/5640 Training loss: 1.0986 0.3814 sec/batch\n",
      "Epoch 13/20  Iteration 3659/5640 Training loss: 1.0985 0.3816 sec/batch\n",
      "Epoch 13/20  Iteration 3660/5640 Training loss: 1.0985 0.3829 sec/batch\n",
      "Epoch 13/20  Iteration 3661/5640 Training loss: 1.0984 0.3824 sec/batch\n",
      "Epoch 13/20  Iteration 3662/5640 Training loss: 1.0985 0.3848 sec/batch\n",
      "Epoch 13/20  Iteration 3663/5640 Training loss: 1.0986 0.3863 sec/batch\n",
      "Epoch 13/20  Iteration 3664/5640 Training loss: 1.0986 0.3872 sec/batch\n",
      "Epoch 13/20  Iteration 3665/5640 Training loss: 1.0985 0.3877 sec/batch\n",
      "Epoch 13/20  Iteration 3666/5640 Training loss: 1.0984 0.3879 sec/batch\n",
      "Epoch 14/20  Iteration 3667/5640 Training loss: 1.1947 0.3874 sec/batch\n",
      "Epoch 14/20  Iteration 3668/5640 Training loss: 1.1580 0.3887 sec/batch\n",
      "Epoch 14/20  Iteration 3669/5640 Training loss: 1.1362 0.3868 sec/batch\n",
      "Epoch 14/20  Iteration 3670/5640 Training loss: 1.1274 0.3877 sec/batch\n",
      "Epoch 14/20  Iteration 3671/5640 Training loss: 1.1216 0.3874 sec/batch\n",
      "Epoch 14/20  Iteration 3672/5640 Training loss: 1.1205 0.3885 sec/batch\n",
      "Epoch 14/20  Iteration 3673/5640 Training loss: 1.1143 0.3906 sec/batch\n",
      "Epoch 14/20  Iteration 3674/5640 Training loss: 1.1120 0.3925 sec/batch\n",
      "Epoch 14/20  Iteration 3675/5640 Training loss: 1.1077 0.3888 sec/batch\n",
      "Epoch 14/20  Iteration 3676/5640 Training loss: 1.1041 0.3903 sec/batch\n",
      "Epoch 14/20  Iteration 3677/5640 Training loss: 1.1041 0.3887 sec/batch\n",
      "Epoch 14/20  Iteration 3678/5640 Training loss: 1.1020 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 3679/5640 Training loss: 1.1014 0.3912 sec/batch\n",
      "Epoch 14/20  Iteration 3680/5640 Training loss: 1.1009 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 3681/5640 Training loss: 1.0988 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3682/5640 Training loss: 1.0998 0.3910 sec/batch\n",
      "Epoch 14/20  Iteration 3683/5640 Training loss: 1.0999 0.3945 sec/batch\n",
      "Epoch 14/20  Iteration 3684/5640 Training loss: 1.0987 0.3944 sec/batch\n",
      "Epoch 14/20  Iteration 3685/5640 Training loss: 1.0988 0.3944 sec/batch\n",
      "Epoch 14/20  Iteration 3686/5640 Training loss: 1.0973 0.3930 sec/batch\n",
      "Epoch 14/20  Iteration 3687/5640 Training loss: 1.0971 0.3949 sec/batch\n",
      "Epoch 14/20  Iteration 3688/5640 Training loss: 1.0976 0.3932 sec/batch\n",
      "Epoch 14/20  Iteration 3689/5640 Training loss: 1.0968 0.3934 sec/batch\n",
      "Epoch 14/20  Iteration 3690/5640 Training loss: 1.0956 0.3930 sec/batch\n",
      "Epoch 14/20  Iteration 3691/5640 Training loss: 1.0957 0.3953 sec/batch\n",
      "Epoch 14/20  Iteration 3692/5640 Training loss: 1.0956 0.3934 sec/batch\n",
      "Epoch 14/20  Iteration 3693/5640 Training loss: 1.0959 0.3930 sec/batch\n",
      "Epoch 14/20  Iteration 3694/5640 Training loss: 1.0958 0.3935 sec/batch\n",
      "Epoch 14/20  Iteration 3695/5640 Training loss: 1.0953 0.3938 sec/batch\n",
      "Epoch 14/20  Iteration 3696/5640 Training loss: 1.0951 0.3928 sec/batch\n",
      "Epoch 14/20  Iteration 3697/5640 Training loss: 1.0941 0.3931 sec/batch\n",
      "Epoch 14/20  Iteration 3698/5640 Training loss: 1.0935 0.3958 sec/batch\n",
      "Epoch 14/20  Iteration 3699/5640 Training loss: 1.0928 0.3956 sec/batch\n",
      "Epoch 14/20  Iteration 3700/5640 Training loss: 1.0925 0.3926 sec/batch\n",
      "Validation loss: 1.09413 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 3701/5640 Training loss: 1.0984 0.3833 sec/batch\n",
      "Epoch 14/20  Iteration 3702/5640 Training loss: 1.0984 0.3811 sec/batch\n",
      "Epoch 14/20  Iteration 3703/5640 Training loss: 1.0974 0.3817 sec/batch\n",
      "Epoch 14/20  Iteration 3704/5640 Training loss: 1.0962 0.3810 sec/batch\n",
      "Epoch 14/20  Iteration 3705/5640 Training loss: 1.0960 0.3806 sec/batch\n",
      "Epoch 14/20  Iteration 3706/5640 Training loss: 1.0950 0.3840 sec/batch\n",
      "Epoch 14/20  Iteration 3707/5640 Training loss: 1.0942 0.3844 sec/batch\n",
      "Epoch 14/20  Iteration 3708/5640 Training loss: 1.0950 0.3849 sec/batch\n",
      "Epoch 14/20  Iteration 3709/5640 Training loss: 1.0941 0.3859 sec/batch\n",
      "Epoch 14/20  Iteration 3710/5640 Training loss: 1.0939 0.3849 sec/batch\n",
      "Epoch 14/20  Iteration 3711/5640 Training loss: 1.0936 0.3847 sec/batch\n",
      "Epoch 14/20  Iteration 3712/5640 Training loss: 1.0933 0.3901 sec/batch\n",
      "Epoch 14/20  Iteration 3713/5640 Training loss: 1.0932 0.3899 sec/batch\n",
      "Epoch 14/20  Iteration 3714/5640 Training loss: 1.0934 0.3937 sec/batch\n",
      "Epoch 14/20  Iteration 3715/5640 Training loss: 1.0938 0.3940 sec/batch\n",
      "Epoch 14/20  Iteration 3716/5640 Training loss: 1.0936 0.3946 sec/batch\n",
      "Epoch 14/20  Iteration 3717/5640 Training loss: 1.0937 0.3927 sec/batch\n",
      "Epoch 14/20  Iteration 3718/5640 Training loss: 1.0935 0.3936 sec/batch\n",
      "Epoch 14/20  Iteration 3719/5640 Training loss: 1.0933 0.3934 sec/batch\n",
      "Epoch 14/20  Iteration 3720/5640 Training loss: 1.0934 0.3923 sec/batch\n",
      "Epoch 14/20  Iteration 3721/5640 Training loss: 1.0931 0.3943 sec/batch\n",
      "Epoch 14/20  Iteration 3722/5640 Training loss: 1.0928 0.3937 sec/batch\n",
      "Epoch 14/20  Iteration 3723/5640 Training loss: 1.0925 0.3941 sec/batch\n",
      "Epoch 14/20  Iteration 3724/5640 Training loss: 1.0926 0.3962 sec/batch\n",
      "Epoch 14/20  Iteration 3725/5640 Training loss: 1.0929 0.3949 sec/batch\n",
      "Epoch 14/20  Iteration 3726/5640 Training loss: 1.0927 0.3944 sec/batch\n",
      "Epoch 14/20  Iteration 3727/5640 Training loss: 1.0924 0.3932 sec/batch\n",
      "Epoch 14/20  Iteration 3728/5640 Training loss: 1.0920 0.3939 sec/batch\n",
      "Epoch 14/20  Iteration 3729/5640 Training loss: 1.0917 0.3933 sec/batch\n",
      "Epoch 14/20  Iteration 3730/5640 Training loss: 1.0913 0.3931 sec/batch\n",
      "Epoch 14/20  Iteration 3731/5640 Training loss: 1.0914 0.3943 sec/batch\n",
      "Epoch 14/20  Iteration 3732/5640 Training loss: 1.0913 0.3946 sec/batch\n",
      "Epoch 14/20  Iteration 3733/5640 Training loss: 1.0915 0.3933 sec/batch\n",
      "Epoch 14/20  Iteration 3734/5640 Training loss: 1.0913 0.3955 sec/batch\n",
      "Epoch 14/20  Iteration 3735/5640 Training loss: 1.0909 0.3933 sec/batch\n",
      "Epoch 14/20  Iteration 3736/5640 Training loss: 1.0905 0.3935 sec/batch\n",
      "Epoch 14/20  Iteration 3737/5640 Training loss: 1.0903 0.3947 sec/batch\n",
      "Epoch 14/20  Iteration 3738/5640 Training loss: 1.0901 0.3924 sec/batch\n",
      "Epoch 14/20  Iteration 3739/5640 Training loss: 1.0902 0.3944 sec/batch\n",
      "Epoch 14/20  Iteration 3740/5640 Training loss: 1.0901 0.3926 sec/batch\n",
      "Epoch 14/20  Iteration 3741/5640 Training loss: 1.0903 0.3943 sec/batch\n",
      "Epoch 14/20  Iteration 3742/5640 Training loss: 1.0908 0.3944 sec/batch\n",
      "Epoch 14/20  Iteration 3743/5640 Training loss: 1.0908 0.3927 sec/batch\n",
      "Epoch 14/20  Iteration 3744/5640 Training loss: 1.0913 0.3934 sec/batch\n",
      "Epoch 14/20  Iteration 3745/5640 Training loss: 1.0913 0.3930 sec/batch\n",
      "Epoch 14/20  Iteration 3746/5640 Training loss: 1.0915 0.3926 sec/batch\n",
      "Epoch 14/20  Iteration 3747/5640 Training loss: 1.0917 0.3945 sec/batch\n",
      "Epoch 14/20  Iteration 3748/5640 Training loss: 1.0915 0.3927 sec/batch\n",
      "Epoch 14/20  Iteration 3749/5640 Training loss: 1.0913 0.3939 sec/batch\n",
      "Epoch 14/20  Iteration 3750/5640 Training loss: 1.0909 0.4036 sec/batch\n",
      "Validation loss: 1.09188 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 3751/5640 Training loss: 1.0928 0.3824 sec/batch\n",
      "Epoch 14/20  Iteration 3752/5640 Training loss: 1.0926 0.3817 sec/batch\n",
      "Epoch 14/20  Iteration 3753/5640 Training loss: 1.0924 0.3815 sec/batch\n",
      "Epoch 14/20  Iteration 3754/5640 Training loss: 1.0921 0.3821 sec/batch\n",
      "Epoch 14/20  Iteration 3755/5640 Training loss: 1.0918 0.3833 sec/batch\n",
      "Epoch 14/20  Iteration 3756/5640 Training loss: 1.0916 0.3829 sec/batch\n",
      "Epoch 14/20  Iteration 3757/5640 Training loss: 1.0916 0.3838 sec/batch\n",
      "Epoch 14/20  Iteration 3758/5640 Training loss: 1.0916 0.3817 sec/batch\n",
      "Epoch 14/20  Iteration 3759/5640 Training loss: 1.0916 0.3819 sec/batch\n",
      "Epoch 14/20  Iteration 3760/5640 Training loss: 1.0914 0.3821 sec/batch\n",
      "Epoch 14/20  Iteration 3761/5640 Training loss: 1.0913 0.3816 sec/batch\n",
      "Epoch 14/20  Iteration 3762/5640 Training loss: 1.0913 0.3815 sec/batch\n",
      "Epoch 14/20  Iteration 3763/5640 Training loss: 1.0914 0.3827 sec/batch\n",
      "Epoch 14/20  Iteration 3764/5640 Training loss: 1.0915 0.3843 sec/batch\n",
      "Epoch 14/20  Iteration 3765/5640 Training loss: 1.0915 0.3856 sec/batch\n",
      "Epoch 14/20  Iteration 3766/5640 Training loss: 1.0913 0.3848 sec/batch\n",
      "Epoch 14/20  Iteration 3767/5640 Training loss: 1.0910 0.3841 sec/batch\n",
      "Epoch 14/20  Iteration 3768/5640 Training loss: 1.0909 0.3841 sec/batch\n",
      "Epoch 14/20  Iteration 3769/5640 Training loss: 1.0907 0.3863 sec/batch\n",
      "Epoch 14/20  Iteration 3770/5640 Training loss: 1.0904 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 3771/5640 Training loss: 1.0901 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3772/5640 Training loss: 1.0897 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 3773/5640 Training loss: 1.0894 0.3898 sec/batch\n",
      "Epoch 14/20  Iteration 3774/5640 Training loss: 1.0890 0.3893 sec/batch\n",
      "Epoch 14/20  Iteration 3775/5640 Training loss: 1.0888 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3776/5640 Training loss: 1.0885 0.3906 sec/batch\n",
      "Epoch 14/20  Iteration 3777/5640 Training loss: 1.0883 0.3919 sec/batch\n",
      "Epoch 14/20  Iteration 3778/5640 Training loss: 1.0883 0.3892 sec/batch\n",
      "Epoch 14/20  Iteration 3779/5640 Training loss: 1.0881 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3780/5640 Training loss: 1.0880 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 3781/5640 Training loss: 1.0881 0.3894 sec/batch\n",
      "Epoch 14/20  Iteration 3782/5640 Training loss: 1.0880 0.3908 sec/batch\n",
      "Epoch 14/20  Iteration 3783/5640 Training loss: 1.0880 0.3900 sec/batch\n",
      "Epoch 14/20  Iteration 3784/5640 Training loss: 1.0878 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3785/5640 Training loss: 1.0877 0.3895 sec/batch\n",
      "Epoch 14/20  Iteration 3786/5640 Training loss: 1.0874 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 3787/5640 Training loss: 1.0873 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3788/5640 Training loss: 1.0873 0.3908 sec/batch\n",
      "Epoch 14/20  Iteration 3789/5640 Training loss: 1.0871 0.3916 sec/batch\n",
      "Epoch 14/20  Iteration 3790/5640 Training loss: 1.0870 0.3909 sec/batch\n",
      "Epoch 14/20  Iteration 3791/5640 Training loss: 1.0872 0.3893 sec/batch\n",
      "Epoch 14/20  Iteration 3792/5640 Training loss: 1.0869 0.3931 sec/batch\n",
      "Epoch 14/20  Iteration 3793/5640 Training loss: 1.0868 0.3947 sec/batch\n",
      "Epoch 14/20  Iteration 3794/5640 Training loss: 1.0865 0.3958 sec/batch\n",
      "Epoch 14/20  Iteration 3795/5640 Training loss: 1.0864 0.3984 sec/batch\n",
      "Epoch 14/20  Iteration 3796/5640 Training loss: 1.0864 0.3969 sec/batch\n",
      "Epoch 14/20  Iteration 3797/5640 Training loss: 1.0863 0.3968 sec/batch\n",
      "Epoch 14/20  Iteration 3798/5640 Training loss: 1.0860 0.3979 sec/batch\n",
      "Epoch 14/20  Iteration 3799/5640 Training loss: 1.0857 0.3965 sec/batch\n",
      "Epoch 14/20  Iteration 3800/5640 Training loss: 1.0854 0.3975 sec/batch\n",
      "Validation loss: 1.09554 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 3801/5640 Training loss: 1.0867 0.3840 sec/batch\n",
      "Epoch 14/20  Iteration 3802/5640 Training loss: 1.0866 0.3822 sec/batch\n",
      "Epoch 14/20  Iteration 3803/5640 Training loss: 1.0866 0.3817 sec/batch\n",
      "Epoch 14/20  Iteration 3804/5640 Training loss: 1.0865 0.3817 sec/batch\n",
      "Epoch 14/20  Iteration 3805/5640 Training loss: 1.0864 0.3844 sec/batch\n",
      "Epoch 14/20  Iteration 3806/5640 Training loss: 1.0864 0.3820 sec/batch\n",
      "Epoch 14/20  Iteration 3807/5640 Training loss: 1.0862 0.3832 sec/batch\n",
      "Epoch 14/20  Iteration 3808/5640 Training loss: 1.0863 0.3820 sec/batch\n",
      "Epoch 14/20  Iteration 3809/5640 Training loss: 1.0862 0.3814 sec/batch\n",
      "Epoch 14/20  Iteration 3810/5640 Training loss: 1.0861 0.3822 sec/batch\n",
      "Epoch 14/20  Iteration 3811/5640 Training loss: 1.0858 0.3838 sec/batch\n",
      "Epoch 14/20  Iteration 3812/5640 Training loss: 1.0856 0.3866 sec/batch\n",
      "Epoch 14/20  Iteration 3813/5640 Training loss: 1.0853 0.3900 sec/batch\n",
      "Epoch 14/20  Iteration 3814/5640 Training loss: 1.0854 0.3888 sec/batch\n",
      "Epoch 14/20  Iteration 3815/5640 Training loss: 1.0853 0.3867 sec/batch\n",
      "Epoch 14/20  Iteration 3816/5640 Training loss: 1.0851 0.3873 sec/batch\n",
      "Epoch 14/20  Iteration 3817/5640 Training loss: 1.0850 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3818/5640 Training loss: 1.0851 0.3893 sec/batch\n",
      "Epoch 14/20  Iteration 3819/5640 Training loss: 1.0854 0.3903 sec/batch\n",
      "Epoch 14/20  Iteration 3820/5640 Training loss: 1.0854 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3821/5640 Training loss: 1.0854 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 3822/5640 Training loss: 1.0854 0.3894 sec/batch\n",
      "Epoch 14/20  Iteration 3823/5640 Training loss: 1.0852 0.3899 sec/batch\n",
      "Epoch 14/20  Iteration 3824/5640 Training loss: 1.0852 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3825/5640 Training loss: 1.0851 0.3895 sec/batch\n",
      "Epoch 14/20  Iteration 3826/5640 Training loss: 1.0850 0.3923 sec/batch\n",
      "Epoch 14/20  Iteration 3827/5640 Training loss: 1.0849 0.3941 sec/batch\n",
      "Epoch 14/20  Iteration 3828/5640 Training loss: 1.0847 0.3931 sec/batch\n",
      "Epoch 14/20  Iteration 3829/5640 Training loss: 1.0846 0.3942 sec/batch\n",
      "Epoch 14/20  Iteration 3830/5640 Training loss: 1.0845 0.3936 sec/batch\n",
      "Epoch 14/20  Iteration 3831/5640 Training loss: 1.0845 0.3932 sec/batch\n",
      "Epoch 14/20  Iteration 3832/5640 Training loss: 1.0847 0.3927 sec/batch\n",
      "Epoch 14/20  Iteration 3833/5640 Training loss: 1.0846 0.3935 sec/batch\n",
      "Epoch 14/20  Iteration 3834/5640 Training loss: 1.0847 0.3942 sec/batch\n",
      "Epoch 14/20  Iteration 3835/5640 Training loss: 1.0847 0.3946 sec/batch\n",
      "Epoch 14/20  Iteration 3836/5640 Training loss: 1.0847 0.3942 sec/batch\n",
      "Epoch 14/20  Iteration 3837/5640 Training loss: 1.0847 0.3932 sec/batch\n",
      "Epoch 14/20  Iteration 3838/5640 Training loss: 1.0846 0.3927 sec/batch\n",
      "Epoch 14/20  Iteration 3839/5640 Training loss: 1.0845 0.3939 sec/batch\n",
      "Epoch 14/20  Iteration 3840/5640 Training loss: 1.0844 0.3947 sec/batch\n",
      "Epoch 14/20  Iteration 3841/5640 Training loss: 1.0844 0.3937 sec/batch\n",
      "Epoch 14/20  Iteration 3842/5640 Training loss: 1.0842 0.3938 sec/batch\n",
      "Epoch 14/20  Iteration 3843/5640 Training loss: 1.0842 0.3935 sec/batch\n",
      "Epoch 14/20  Iteration 3844/5640 Training loss: 1.0841 0.3950 sec/batch\n",
      "Epoch 14/20  Iteration 3845/5640 Training loss: 1.0842 0.3932 sec/batch\n",
      "Epoch 14/20  Iteration 3846/5640 Training loss: 1.0843 0.3928 sec/batch\n",
      "Epoch 14/20  Iteration 3847/5640 Training loss: 1.0843 0.3934 sec/batch\n",
      "Epoch 14/20  Iteration 3848/5640 Training loss: 1.0843 0.3929 sec/batch\n",
      "Epoch 14/20  Iteration 3849/5640 Training loss: 1.0843 0.3936 sec/batch\n",
      "Epoch 14/20  Iteration 3850/5640 Training loss: 1.0842 0.3938 sec/batch\n",
      "Validation loss: 1.08864 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 3851/5640 Training loss: 1.0850 0.3826 sec/batch\n",
      "Epoch 14/20  Iteration 3852/5640 Training loss: 1.0848 0.3834 sec/batch\n",
      "Epoch 14/20  Iteration 3853/5640 Training loss: 1.0845 0.3807 sec/batch\n",
      "Epoch 14/20  Iteration 3854/5640 Training loss: 1.0843 0.3835 sec/batch\n",
      "Epoch 14/20  Iteration 3855/5640 Training loss: 1.0841 0.3814 sec/batch\n",
      "Epoch 14/20  Iteration 3856/5640 Training loss: 1.0839 0.3815 sec/batch\n",
      "Epoch 14/20  Iteration 3857/5640 Training loss: 1.0839 0.3813 sec/batch\n",
      "Epoch 14/20  Iteration 3858/5640 Training loss: 1.0837 0.3828 sec/batch\n",
      "Epoch 14/20  Iteration 3859/5640 Training loss: 1.0835 0.3829 sec/batch\n",
      "Epoch 14/20  Iteration 3860/5640 Training loss: 1.0835 0.3819 sec/batch\n",
      "Epoch 14/20  Iteration 3861/5640 Training loss: 1.0835 0.3820 sec/batch\n",
      "Epoch 14/20  Iteration 3862/5640 Training loss: 1.0836 0.3828 sec/batch\n",
      "Epoch 14/20  Iteration 3863/5640 Training loss: 1.0835 0.3824 sec/batch\n",
      "Epoch 14/20  Iteration 3864/5640 Training loss: 1.0835 0.3847 sec/batch\n",
      "Epoch 14/20  Iteration 3865/5640 Training loss: 1.0835 0.3874 sec/batch\n",
      "Epoch 14/20  Iteration 3866/5640 Training loss: 1.0834 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3867/5640 Training loss: 1.0834 0.3933 sec/batch\n",
      "Epoch 14/20  Iteration 3868/5640 Training loss: 1.0833 0.3934 sec/batch\n",
      "Epoch 14/20  Iteration 3869/5640 Training loss: 1.0832 0.3954 sec/batch\n",
      "Epoch 14/20  Iteration 3870/5640 Training loss: 1.0832 0.6002 sec/batch\n",
      "Epoch 14/20  Iteration 3871/5640 Training loss: 1.0831 0.3902 sec/batch\n",
      "Epoch 14/20  Iteration 3872/5640 Training loss: 1.0831 0.3893 sec/batch\n",
      "Epoch 14/20  Iteration 3873/5640 Training loss: 1.0829 0.3903 sec/batch\n",
      "Epoch 14/20  Iteration 3874/5640 Training loss: 1.0827 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3875/5640 Training loss: 1.0828 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 3876/5640 Training loss: 1.0826 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 3877/5640 Training loss: 1.0825 0.3925 sec/batch\n",
      "Epoch 14/20  Iteration 3878/5640 Training loss: 1.0825 0.3895 sec/batch\n",
      "Epoch 14/20  Iteration 3879/5640 Training loss: 1.0825 0.3905 sec/batch\n",
      "Epoch 14/20  Iteration 3880/5640 Training loss: 1.0824 0.3894 sec/batch\n",
      "Epoch 14/20  Iteration 3881/5640 Training loss: 1.0824 0.3895 sec/batch\n",
      "Epoch 14/20  Iteration 3882/5640 Training loss: 1.0824 0.3900 sec/batch\n",
      "Epoch 14/20  Iteration 3883/5640 Training loss: 1.0823 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 3884/5640 Training loss: 1.0821 0.3920 sec/batch\n",
      "Epoch 14/20  Iteration 3885/5640 Training loss: 1.0820 0.3900 sec/batch\n",
      "Epoch 14/20  Iteration 3886/5640 Training loss: 1.0818 0.3901 sec/batch\n",
      "Epoch 14/20  Iteration 3887/5640 Training loss: 1.0817 0.3887 sec/batch\n",
      "Epoch 14/20  Iteration 3888/5640 Training loss: 1.0816 0.3901 sec/batch\n",
      "Epoch 14/20  Iteration 3889/5640 Training loss: 1.0816 0.3905 sec/batch\n",
      "Epoch 14/20  Iteration 3890/5640 Training loss: 1.0815 0.3926 sec/batch\n",
      "Epoch 14/20  Iteration 3891/5640 Training loss: 1.0814 0.3895 sec/batch\n",
      "Epoch 14/20  Iteration 3892/5640 Training loss: 1.0814 0.3914 sec/batch\n",
      "Epoch 14/20  Iteration 3893/5640 Training loss: 1.0814 0.3892 sec/batch\n",
      "Epoch 14/20  Iteration 3894/5640 Training loss: 1.0815 0.3900 sec/batch\n",
      "Epoch 14/20  Iteration 3895/5640 Training loss: 1.0815 0.3903 sec/batch\n",
      "Epoch 14/20  Iteration 3896/5640 Training loss: 1.0815 0.3892 sec/batch\n",
      "Epoch 14/20  Iteration 3897/5640 Training loss: 1.0814 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3898/5640 Training loss: 1.0814 0.3897 sec/batch\n",
      "Epoch 14/20  Iteration 3899/5640 Training loss: 1.0813 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 3900/5640 Training loss: 1.0812 0.3919 sec/batch\n",
      "Validation loss: 1.09009 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 3901/5640 Training loss: 1.0820 0.3851 sec/batch\n",
      "Epoch 14/20  Iteration 3902/5640 Training loss: 1.0820 0.3832 sec/batch\n",
      "Epoch 14/20  Iteration 3903/5640 Training loss: 1.0820 0.3822 sec/batch\n",
      "Epoch 14/20  Iteration 3904/5640 Training loss: 1.0821 0.3810 sec/batch\n",
      "Epoch 14/20  Iteration 3905/5640 Training loss: 1.0820 0.3811 sec/batch\n",
      "Epoch 14/20  Iteration 3906/5640 Training loss: 1.0820 0.3826 sec/batch\n",
      "Epoch 14/20  Iteration 3907/5640 Training loss: 1.0819 0.3820 sec/batch\n",
      "Epoch 14/20  Iteration 3908/5640 Training loss: 1.0819 0.3864 sec/batch\n",
      "Epoch 14/20  Iteration 3909/5640 Training loss: 1.0818 0.3870 sec/batch\n",
      "Epoch 14/20  Iteration 3910/5640 Training loss: 1.0816 0.3889 sec/batch\n",
      "Epoch 14/20  Iteration 3911/5640 Training loss: 1.0815 0.3867 sec/batch\n",
      "Epoch 14/20  Iteration 3912/5640 Training loss: 1.0814 0.3870 sec/batch\n",
      "Epoch 14/20  Iteration 3913/5640 Training loss: 1.0813 0.3882 sec/batch\n",
      "Epoch 14/20  Iteration 3914/5640 Training loss: 1.0813 0.3877 sec/batch\n",
      "Epoch 14/20  Iteration 3915/5640 Training loss: 1.0814 0.3891 sec/batch\n",
      "Epoch 14/20  Iteration 3916/5640 Training loss: 1.0813 0.3882 sec/batch\n",
      "Epoch 14/20  Iteration 3917/5640 Training loss: 1.0812 0.3880 sec/batch\n",
      "Epoch 14/20  Iteration 3918/5640 Training loss: 1.0811 0.3870 sec/batch\n",
      "Epoch 14/20  Iteration 3919/5640 Training loss: 1.0810 0.3899 sec/batch\n",
      "Epoch 14/20  Iteration 3920/5640 Training loss: 1.0810 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3921/5640 Training loss: 1.0809 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3922/5640 Training loss: 1.0808 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 3923/5640 Training loss: 1.0807 0.3903 sec/batch\n",
      "Epoch 14/20  Iteration 3924/5640 Training loss: 1.0807 0.3901 sec/batch\n",
      "Epoch 14/20  Iteration 3925/5640 Training loss: 1.0806 0.3915 sec/batch\n",
      "Epoch 14/20  Iteration 3926/5640 Training loss: 1.0807 0.3886 sec/batch\n",
      "Epoch 14/20  Iteration 3927/5640 Training loss: 1.0807 0.3892 sec/batch\n",
      "Epoch 14/20  Iteration 3928/5640 Training loss: 1.0806 0.3913 sec/batch\n",
      "Epoch 14/20  Iteration 3929/5640 Training loss: 1.0806 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3930/5640 Training loss: 1.0806 0.3894 sec/batch\n",
      "Epoch 14/20  Iteration 3931/5640 Training loss: 1.0807 0.3893 sec/batch\n",
      "Epoch 14/20  Iteration 3932/5640 Training loss: 1.0805 0.3904 sec/batch\n",
      "Epoch 14/20  Iteration 3933/5640 Training loss: 1.0805 0.3916 sec/batch\n",
      "Epoch 14/20  Iteration 3934/5640 Training loss: 1.0803 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3935/5640 Training loss: 1.0803 0.3931 sec/batch\n",
      "Epoch 14/20  Iteration 3936/5640 Training loss: 1.0803 0.3880 sec/batch\n",
      "Epoch 14/20  Iteration 3937/5640 Training loss: 1.0802 0.3911 sec/batch\n",
      "Epoch 14/20  Iteration 3938/5640 Training loss: 1.0801 0.3889 sec/batch\n",
      "Epoch 14/20  Iteration 3939/5640 Training loss: 1.0801 0.3892 sec/batch\n",
      "Epoch 14/20  Iteration 3940/5640 Training loss: 1.0801 0.3893 sec/batch\n",
      "Epoch 14/20  Iteration 3941/5640 Training loss: 1.0800 0.3926 sec/batch\n",
      "Epoch 14/20  Iteration 3942/5640 Training loss: 1.0800 0.3894 sec/batch\n",
      "Epoch 14/20  Iteration 3943/5640 Training loss: 1.0799 0.3896 sec/batch\n",
      "Epoch 14/20  Iteration 3944/5640 Training loss: 1.0800 0.3895 sec/batch\n",
      "Epoch 14/20  Iteration 3945/5640 Training loss: 1.0801 0.3903 sec/batch\n",
      "Epoch 14/20  Iteration 3946/5640 Training loss: 1.0801 0.3907 sec/batch\n",
      "Epoch 14/20  Iteration 3947/5640 Training loss: 1.0800 0.3912 sec/batch\n",
      "Epoch 14/20  Iteration 3948/5640 Training loss: 1.0799 0.3916 sec/batch\n",
      "Epoch 15/20  Iteration 3949/5640 Training loss: 1.1747 0.3882 sec/batch\n",
      "Epoch 15/20  Iteration 3950/5640 Training loss: 1.1342 0.3901 sec/batch\n",
      "Validation loss: 1.09529 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 3951/5640 Training loss: 1.1780 0.3842 sec/batch\n",
      "Epoch 15/20  Iteration 3952/5640 Training loss: 1.1551 0.3842 sec/batch\n",
      "Epoch 15/20  Iteration 3953/5640 Training loss: 1.1432 0.3848 sec/batch\n",
      "Epoch 15/20  Iteration 3954/5640 Training loss: 1.1345 0.3862 sec/batch\n",
      "Epoch 15/20  Iteration 3955/5640 Training loss: 1.1239 0.3839 sec/batch\n",
      "Epoch 15/20  Iteration 3956/5640 Training loss: 1.1194 0.3843 sec/batch\n",
      "Epoch 15/20  Iteration 3957/5640 Training loss: 1.1127 0.3835 sec/batch\n",
      "Epoch 15/20  Iteration 3958/5640 Training loss: 1.1063 0.3873 sec/batch\n",
      "Epoch 15/20  Iteration 3959/5640 Training loss: 1.1045 0.3864 sec/batch\n",
      "Epoch 15/20  Iteration 3960/5640 Training loss: 1.1015 0.3894 sec/batch\n",
      "Epoch 15/20  Iteration 3961/5640 Training loss: 1.1007 0.3893 sec/batch\n",
      "Epoch 15/20  Iteration 3962/5640 Training loss: 1.1005 0.3897 sec/batch\n",
      "Epoch 15/20  Iteration 3963/5640 Training loss: 1.0977 0.3897 sec/batch\n",
      "Epoch 15/20  Iteration 3964/5640 Training loss: 1.0981 0.3903 sec/batch\n",
      "Epoch 15/20  Iteration 3965/5640 Training loss: 1.0971 0.3904 sec/batch\n",
      "Epoch 15/20  Iteration 3966/5640 Training loss: 1.0956 0.3922 sec/batch\n",
      "Epoch 15/20  Iteration 3967/5640 Training loss: 1.0946 0.3907 sec/batch\n",
      "Epoch 15/20  Iteration 3968/5640 Training loss: 1.0928 0.3896 sec/batch\n",
      "Epoch 15/20  Iteration 3969/5640 Training loss: 1.0926 0.3899 sec/batch\n",
      "Epoch 15/20  Iteration 3970/5640 Training loss: 1.0921 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 3971/5640 Training loss: 1.0912 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 3972/5640 Training loss: 1.0896 0.3904 sec/batch\n",
      "Epoch 15/20  Iteration 3973/5640 Training loss: 1.0894 0.3900 sec/batch\n",
      "Epoch 15/20  Iteration 3974/5640 Training loss: 1.0889 0.3918 sec/batch\n",
      "Epoch 15/20  Iteration 3975/5640 Training loss: 1.0887 0.3891 sec/batch\n",
      "Epoch 15/20  Iteration 3976/5640 Training loss: 1.0883 0.3897 sec/batch\n",
      "Epoch 15/20  Iteration 3977/5640 Training loss: 1.0873 0.3896 sec/batch\n",
      "Epoch 15/20  Iteration 3978/5640 Training loss: 1.0871 0.3890 sec/batch\n",
      "Epoch 15/20  Iteration 3979/5640 Training loss: 1.0858 0.3915 sec/batch\n",
      "Epoch 15/20  Iteration 3980/5640 Training loss: 1.0851 0.3896 sec/batch\n",
      "Epoch 15/20  Iteration 3981/5640 Training loss: 1.0840 0.3909 sec/batch\n",
      "Epoch 15/20  Iteration 3982/5640 Training loss: 1.0830 0.3891 sec/batch\n",
      "Epoch 15/20  Iteration 3983/5640 Training loss: 1.0833 0.3899 sec/batch\n",
      "Epoch 15/20  Iteration 3984/5640 Training loss: 1.0831 0.3900 sec/batch\n",
      "Epoch 15/20  Iteration 3985/5640 Training loss: 1.0821 0.3896 sec/batch\n",
      "Epoch 15/20  Iteration 3986/5640 Training loss: 1.0813 0.3901 sec/batch\n",
      "Epoch 15/20  Iteration 3987/5640 Training loss: 1.0808 0.3915 sec/batch\n",
      "Epoch 15/20  Iteration 3988/5640 Training loss: 1.0799 0.3905 sec/batch\n",
      "Epoch 15/20  Iteration 3989/5640 Training loss: 1.0791 0.3924 sec/batch\n",
      "Epoch 15/20  Iteration 3990/5640 Training loss: 1.0796 0.3896 sec/batch\n",
      "Epoch 15/20  Iteration 3991/5640 Training loss: 1.0786 0.3894 sec/batch\n",
      "Epoch 15/20  Iteration 3992/5640 Training loss: 1.0786 0.3912 sec/batch\n",
      "Epoch 15/20  Iteration 3993/5640 Training loss: 1.0782 0.3895 sec/batch\n",
      "Epoch 15/20  Iteration 3994/5640 Training loss: 1.0778 0.3901 sec/batch\n",
      "Epoch 15/20  Iteration 3995/5640 Training loss: 1.0779 0.3909 sec/batch\n",
      "Epoch 15/20  Iteration 3996/5640 Training loss: 1.0781 0.3900 sec/batch\n",
      "Epoch 15/20  Iteration 3997/5640 Training loss: 1.0781 0.3914 sec/batch\n",
      "Epoch 15/20  Iteration 3998/5640 Training loss: 1.0777 0.3895 sec/batch\n",
      "Epoch 15/20  Iteration 3999/5640 Training loss: 1.0777 0.3900 sec/batch\n",
      "Epoch 15/20  Iteration 4000/5640 Training loss: 1.0776 0.3902 sec/batch\n",
      "Validation loss: 1.08626 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 4001/5640 Training loss: 1.0808 0.3836 sec/batch\n",
      "Epoch 15/20  Iteration 4002/5640 Training loss: 1.0807 0.3839 sec/batch\n",
      "Epoch 15/20  Iteration 4003/5640 Training loss: 1.0803 0.3812 sec/batch\n",
      "Epoch 15/20  Iteration 4004/5640 Training loss: 1.0800 0.3818 sec/batch\n",
      "Epoch 15/20  Iteration 4005/5640 Training loss: 1.0797 0.3828 sec/batch\n",
      "Epoch 15/20  Iteration 4006/5640 Training loss: 1.0797 0.3833 sec/batch\n",
      "Epoch 15/20  Iteration 4007/5640 Training loss: 1.0796 0.3835 sec/batch\n",
      "Epoch 15/20  Iteration 4008/5640 Training loss: 1.0795 0.3817 sec/batch\n",
      "Epoch 15/20  Iteration 4009/5640 Training loss: 1.0791 0.3816 sec/batch\n",
      "Epoch 15/20  Iteration 4010/5640 Training loss: 1.0786 0.3826 sec/batch\n",
      "Epoch 15/20  Iteration 4011/5640 Training loss: 1.0781 0.3844 sec/batch\n",
      "Epoch 15/20  Iteration 4012/5640 Training loss: 1.0777 0.3879 sec/batch\n",
      "Epoch 15/20  Iteration 4013/5640 Training loss: 1.0780 0.3861 sec/batch\n",
      "Epoch 15/20  Iteration 4014/5640 Training loss: 1.0777 0.3875 sec/batch\n",
      "Epoch 15/20  Iteration 4015/5640 Training loss: 1.0777 0.3886 sec/batch\n",
      "Epoch 15/20  Iteration 4016/5640 Training loss: 1.0774 0.3870 sec/batch\n",
      "Epoch 15/20  Iteration 4017/5640 Training loss: 1.0770 0.3867 sec/batch\n",
      "Epoch 15/20  Iteration 4018/5640 Training loss: 1.0765 0.3886 sec/batch\n",
      "Epoch 15/20  Iteration 4019/5640 Training loss: 1.0763 0.3870 sec/batch\n",
      "Epoch 15/20  Iteration 4020/5640 Training loss: 1.0761 0.3884 sec/batch\n",
      "Epoch 15/20  Iteration 4021/5640 Training loss: 1.0760 0.3888 sec/batch\n",
      "Epoch 15/20  Iteration 4022/5640 Training loss: 1.0760 0.3944 sec/batch\n",
      "Epoch 15/20  Iteration 4023/5640 Training loss: 1.0761 0.3939 sec/batch\n",
      "Epoch 15/20  Iteration 4024/5640 Training loss: 1.0764 0.3924 sec/batch\n",
      "Epoch 15/20  Iteration 4025/5640 Training loss: 1.0764 0.3925 sec/batch\n",
      "Epoch 15/20  Iteration 4026/5640 Training loss: 1.0768 1.1297 sec/batch\n",
      "Epoch 15/20  Iteration 4027/5640 Training loss: 1.0769 0.3808 sec/batch\n",
      "Epoch 15/20  Iteration 4028/5640 Training loss: 1.0770 0.3824 sec/batch\n",
      "Epoch 15/20  Iteration 4029/5640 Training loss: 1.0772 0.3827 sec/batch\n",
      "Epoch 15/20  Iteration 4030/5640 Training loss: 1.0770 0.3821 sec/batch\n",
      "Epoch 15/20  Iteration 4031/5640 Training loss: 1.0767 0.3826 sec/batch\n",
      "Epoch 15/20  Iteration 4032/5640 Training loss: 1.0764 0.3816 sec/batch\n",
      "Epoch 15/20  Iteration 4033/5640 Training loss: 1.0761 0.3813 sec/batch\n",
      "Epoch 15/20  Iteration 4034/5640 Training loss: 1.0759 0.3824 sec/batch\n",
      "Epoch 15/20  Iteration 4035/5640 Training loss: 1.0757 0.3852 sec/batch\n",
      "Epoch 15/20  Iteration 4036/5640 Training loss: 1.0755 0.3887 sec/batch\n",
      "Epoch 15/20  Iteration 4037/5640 Training loss: 1.0753 0.3880 sec/batch\n",
      "Epoch 15/20  Iteration 4038/5640 Training loss: 1.0750 0.3873 sec/batch\n",
      "Epoch 15/20  Iteration 4039/5640 Training loss: 1.0751 0.3863 sec/batch\n",
      "Epoch 15/20  Iteration 4040/5640 Training loss: 1.0751 0.3872 sec/batch\n",
      "Epoch 15/20  Iteration 4041/5640 Training loss: 1.0750 0.3881 sec/batch\n",
      "Epoch 15/20  Iteration 4042/5640 Training loss: 1.0748 0.3909 sec/batch\n",
      "Epoch 15/20  Iteration 4043/5640 Training loss: 1.0747 0.3896 sec/batch\n",
      "Epoch 15/20  Iteration 4044/5640 Training loss: 1.0747 0.3919 sec/batch\n",
      "Epoch 15/20  Iteration 4045/5640 Training loss: 1.0748 0.3889 sec/batch\n",
      "Epoch 15/20  Iteration 4046/5640 Training loss: 1.0748 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 4047/5640 Training loss: 1.0748 0.3903 sec/batch\n",
      "Epoch 15/20  Iteration 4048/5640 Training loss: 1.0746 0.3911 sec/batch\n",
      "Epoch 15/20  Iteration 4049/5640 Training loss: 1.0742 0.3902 sec/batch\n",
      "Epoch 15/20  Iteration 4050/5640 Training loss: 1.0741 0.3912 sec/batch\n",
      "Validation loss: 1.08874 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 4051/5640 Training loss: 1.0757 0.3811 sec/batch\n",
      "Epoch 15/20  Iteration 4052/5640 Training loss: 1.0755 0.3827 sec/batch\n",
      "Epoch 15/20  Iteration 4053/5640 Training loss: 1.0752 0.3830 sec/batch\n",
      "Epoch 15/20  Iteration 4054/5640 Training loss: 1.0749 0.3841 sec/batch\n",
      "Epoch 15/20  Iteration 4055/5640 Training loss: 1.0744 0.3811 sec/batch\n",
      "Epoch 15/20  Iteration 4056/5640 Training loss: 1.0740 0.3812 sec/batch\n",
      "Epoch 15/20  Iteration 4057/5640 Training loss: 1.0739 0.3820 sec/batch\n",
      "Epoch 15/20  Iteration 4058/5640 Training loss: 1.0736 0.3824 sec/batch\n",
      "Epoch 15/20  Iteration 4059/5640 Training loss: 1.0734 0.3818 sec/batch\n",
      "Epoch 15/20  Iteration 4060/5640 Training loss: 1.0734 0.3826 sec/batch\n",
      "Epoch 15/20  Iteration 4061/5640 Training loss: 1.0732 0.3830 sec/batch\n",
      "Epoch 15/20  Iteration 4062/5640 Training loss: 1.0730 0.3833 sec/batch\n",
      "Epoch 15/20  Iteration 4063/5640 Training loss: 1.0730 0.3829 sec/batch\n",
      "Epoch 15/20  Iteration 4064/5640 Training loss: 1.0730 0.3850 sec/batch\n",
      "Epoch 15/20  Iteration 4065/5640 Training loss: 1.0729 0.3848 sec/batch\n",
      "Epoch 15/20  Iteration 4066/5640 Training loss: 1.0728 0.3852 sec/batch\n",
      "Epoch 15/20  Iteration 4067/5640 Training loss: 1.0726 0.3882 sec/batch\n",
      "Epoch 15/20  Iteration 4068/5640 Training loss: 1.0724 0.3892 sec/batch\n",
      "Epoch 15/20  Iteration 4069/5640 Training loss: 1.0723 0.3959 sec/batch\n",
      "Epoch 15/20  Iteration 4070/5640 Training loss: 1.0723 0.3927 sec/batch\n",
      "Epoch 15/20  Iteration 4071/5640 Training loss: 1.0721 0.3927 sec/batch\n",
      "Epoch 15/20  Iteration 4072/5640 Training loss: 1.0721 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4073/5640 Training loss: 1.0723 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4074/5640 Training loss: 1.0720 0.3930 sec/batch\n",
      "Epoch 15/20  Iteration 4075/5640 Training loss: 1.0720 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4076/5640 Training loss: 1.0717 0.3937 sec/batch\n",
      "Epoch 15/20  Iteration 4077/5640 Training loss: 1.0715 0.5436 sec/batch\n",
      "Epoch 15/20  Iteration 4078/5640 Training loss: 1.0714 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 4079/5640 Training loss: 1.0713 0.3899 sec/batch\n",
      "Epoch 15/20  Iteration 4080/5640 Training loss: 1.0710 0.3921 sec/batch\n",
      "Epoch 15/20  Iteration 4081/5640 Training loss: 1.0706 0.3884 sec/batch\n",
      "Epoch 15/20  Iteration 4082/5640 Training loss: 1.0703 0.3907 sec/batch\n",
      "Epoch 15/20  Iteration 4083/5640 Training loss: 1.0702 0.3910 sec/batch\n",
      "Epoch 15/20  Iteration 4084/5640 Training loss: 1.0703 0.3914 sec/batch\n",
      "Epoch 15/20  Iteration 4085/5640 Training loss: 1.0703 0.3892 sec/batch\n",
      "Epoch 15/20  Iteration 4086/5640 Training loss: 1.0701 0.3906 sec/batch\n",
      "Epoch 15/20  Iteration 4087/5640 Training loss: 1.0700 0.3899 sec/batch\n",
      "Epoch 15/20  Iteration 4088/5640 Training loss: 1.0700 0.3899 sec/batch\n",
      "Epoch 15/20  Iteration 4089/5640 Training loss: 1.0699 0.3900 sec/batch\n",
      "Epoch 15/20  Iteration 4090/5640 Training loss: 1.0699 0.3894 sec/batch\n",
      "Epoch 15/20  Iteration 4091/5640 Training loss: 1.0699 0.3895 sec/batch\n",
      "Epoch 15/20  Iteration 4092/5640 Training loss: 1.0698 0.3928 sec/batch\n",
      "Epoch 15/20  Iteration 4093/5640 Training loss: 1.0695 0.3895 sec/batch\n",
      "Epoch 15/20  Iteration 4094/5640 Training loss: 1.0693 0.3917 sec/batch\n",
      "Epoch 15/20  Iteration 4095/5640 Training loss: 1.0690 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 4096/5640 Training loss: 1.0690 0.3908 sec/batch\n",
      "Epoch 15/20  Iteration 4097/5640 Training loss: 1.0690 0.3901 sec/batch\n",
      "Epoch 15/20  Iteration 4098/5640 Training loss: 1.0688 0.3899 sec/batch\n",
      "Epoch 15/20  Iteration 4099/5640 Training loss: 1.0686 0.3900 sec/batch\n",
      "Epoch 15/20  Iteration 4100/5640 Training loss: 1.0687 0.3904 sec/batch\n",
      "Validation loss: 1.08917 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 4101/5640 Training loss: 1.0702 0.3840 sec/batch\n",
      "Epoch 15/20  Iteration 4102/5640 Training loss: 1.0703 0.3852 sec/batch\n",
      "Epoch 15/20  Iteration 4103/5640 Training loss: 1.0702 0.3808 sec/batch\n",
      "Epoch 15/20  Iteration 4104/5640 Training loss: 1.0703 0.3810 sec/batch\n",
      "Epoch 15/20  Iteration 4105/5640 Training loss: 1.0701 0.3833 sec/batch\n",
      "Epoch 15/20  Iteration 4106/5640 Training loss: 1.0701 0.3815 sec/batch\n",
      "Epoch 15/20  Iteration 4107/5640 Training loss: 1.0700 0.3817 sec/batch\n",
      "Epoch 15/20  Iteration 4108/5640 Training loss: 1.0698 0.3815 sec/batch\n",
      "Epoch 15/20  Iteration 4109/5640 Training loss: 1.0698 0.3813 sec/batch\n",
      "Epoch 15/20  Iteration 4110/5640 Training loss: 1.0696 0.3830 sec/batch\n",
      "Epoch 15/20  Iteration 4111/5640 Training loss: 1.0695 0.3843 sec/batch\n",
      "Epoch 15/20  Iteration 4112/5640 Training loss: 1.0693 0.3843 sec/batch\n",
      "Epoch 15/20  Iteration 4113/5640 Training loss: 1.0693 0.3848 sec/batch\n",
      "Epoch 15/20  Iteration 4114/5640 Training loss: 1.0695 0.3845 sec/batch\n",
      "Epoch 15/20  Iteration 4115/5640 Training loss: 1.0694 0.3839 sec/batch\n",
      "Epoch 15/20  Iteration 4116/5640 Training loss: 1.0695 0.3872 sec/batch\n",
      "Epoch 15/20  Iteration 4117/5640 Training loss: 1.0695 0.3883 sec/batch\n",
      "Epoch 15/20  Iteration 4118/5640 Training loss: 1.0695 0.3923 sec/batch\n",
      "Epoch 15/20  Iteration 4119/5640 Training loss: 1.0695 0.3932 sec/batch\n",
      "Epoch 15/20  Iteration 4120/5640 Training loss: 1.0693 0.3950 sec/batch\n",
      "Epoch 15/20  Iteration 4121/5640 Training loss: 1.0692 0.3941 sec/batch\n",
      "Epoch 15/20  Iteration 4122/5640 Training loss: 1.0692 0.3929 sec/batch\n",
      "Epoch 15/20  Iteration 4123/5640 Training loss: 1.0692 0.3949 sec/batch\n",
      "Epoch 15/20  Iteration 4124/5640 Training loss: 1.0691 0.3937 sec/batch\n",
      "Epoch 15/20  Iteration 4125/5640 Training loss: 1.0691 0.3965 sec/batch\n",
      "Epoch 15/20  Iteration 4126/5640 Training loss: 1.0689 0.3932 sec/batch\n",
      "Epoch 15/20  Iteration 4127/5640 Training loss: 1.0691 0.3939 sec/batch\n",
      "Epoch 15/20  Iteration 4128/5640 Training loss: 1.0691 0.3945 sec/batch\n",
      "Epoch 15/20  Iteration 4129/5640 Training loss: 1.0691 0.3944 sec/batch\n",
      "Epoch 15/20  Iteration 4130/5640 Training loss: 1.0691 0.3930 sec/batch\n",
      "Epoch 15/20  Iteration 4131/5640 Training loss: 1.0692 0.3939 sec/batch\n",
      "Epoch 15/20  Iteration 4132/5640 Training loss: 1.0690 0.3937 sec/batch\n",
      "Epoch 15/20  Iteration 4133/5640 Training loss: 1.0690 0.3945 sec/batch\n",
      "Epoch 15/20  Iteration 4134/5640 Training loss: 1.0688 0.3926 sec/batch\n",
      "Epoch 15/20  Iteration 4135/5640 Training loss: 1.0685 0.3925 sec/batch\n",
      "Epoch 15/20  Iteration 4136/5640 Training loss: 1.0682 0.3937 sec/batch\n",
      "Epoch 15/20  Iteration 4137/5640 Training loss: 1.0681 0.3949 sec/batch\n",
      "Epoch 15/20  Iteration 4138/5640 Training loss: 1.0679 0.3937 sec/batch\n",
      "Epoch 15/20  Iteration 4139/5640 Training loss: 1.0678 0.3932 sec/batch\n",
      "Epoch 15/20  Iteration 4140/5640 Training loss: 1.0676 0.3936 sec/batch\n",
      "Epoch 15/20  Iteration 4141/5640 Training loss: 1.0675 0.3943 sec/batch\n",
      "Epoch 15/20  Iteration 4142/5640 Training loss: 1.0675 0.3926 sec/batch\n",
      "Epoch 15/20  Iteration 4143/5640 Training loss: 1.0676 0.3957 sec/batch\n",
      "Epoch 15/20  Iteration 4144/5640 Training loss: 1.0676 0.3930 sec/batch\n",
      "Epoch 15/20  Iteration 4145/5640 Training loss: 1.0675 0.3923 sec/batch\n",
      "Epoch 15/20  Iteration 4146/5640 Training loss: 1.0676 0.3934 sec/batch\n",
      "Epoch 15/20  Iteration 4147/5640 Training loss: 1.0676 0.3930 sec/batch\n",
      "Epoch 15/20  Iteration 4148/5640 Training loss: 1.0675 0.3953 sec/batch\n",
      "Epoch 15/20  Iteration 4149/5640 Training loss: 1.0675 0.3931 sec/batch\n",
      "Epoch 15/20  Iteration 4150/5640 Training loss: 1.0674 0.3933 sec/batch\n",
      "Validation loss: 1.08809 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 4151/5640 Training loss: 1.0683 0.3814 sec/batch\n",
      "Epoch 15/20  Iteration 4152/5640 Training loss: 1.0683 0.3817 sec/batch\n",
      "Epoch 15/20  Iteration 4153/5640 Training loss: 1.0681 0.3816 sec/batch\n",
      "Epoch 15/20  Iteration 4154/5640 Training loss: 1.0681 0.3817 sec/batch\n",
      "Epoch 15/20  Iteration 4155/5640 Training loss: 1.0680 0.3820 sec/batch\n",
      "Epoch 15/20  Iteration 4156/5640 Training loss: 1.0678 0.3830 sec/batch\n",
      "Epoch 15/20  Iteration 4157/5640 Training loss: 1.0679 0.3812 sec/batch\n",
      "Epoch 15/20  Iteration 4158/5640 Training loss: 1.0677 0.3834 sec/batch\n",
      "Epoch 15/20  Iteration 4159/5640 Training loss: 1.0676 0.3810 sec/batch\n",
      "Epoch 15/20  Iteration 4160/5640 Training loss: 1.0675 0.3814 sec/batch\n",
      "Epoch 15/20  Iteration 4161/5640 Training loss: 1.0676 0.3820 sec/batch\n",
      "Epoch 15/20  Iteration 4162/5640 Training loss: 1.0676 0.3824 sec/batch\n",
      "Epoch 15/20  Iteration 4163/5640 Training loss: 1.0675 0.3849 sec/batch\n",
      "Epoch 15/20  Iteration 4164/5640 Training loss: 1.0675 0.3875 sec/batch\n",
      "Epoch 15/20  Iteration 4165/5640 Training loss: 1.0674 0.3891 sec/batch\n",
      "Epoch 15/20  Iteration 4166/5640 Training loss: 1.0672 0.3892 sec/batch\n",
      "Epoch 15/20  Iteration 4167/5640 Training loss: 1.0671 0.3889 sec/batch\n",
      "Epoch 15/20  Iteration 4168/5640 Training loss: 1.0669 0.3869 sec/batch\n",
      "Epoch 15/20  Iteration 4169/5640 Training loss: 1.0668 0.3886 sec/batch\n",
      "Epoch 15/20  Iteration 4170/5640 Training loss: 1.0667 0.3877 sec/batch\n",
      "Epoch 15/20  Iteration 4171/5640 Training loss: 1.0667 0.3875 sec/batch\n",
      "Epoch 15/20  Iteration 4172/5640 Training loss: 1.0666 0.3881 sec/batch\n",
      "Epoch 15/20  Iteration 4173/5640 Training loss: 1.0665 0.3885 sec/batch\n",
      "Epoch 15/20  Iteration 4174/5640 Training loss: 1.0664 0.3901 sec/batch\n",
      "Epoch 15/20  Iteration 4175/5640 Training loss: 1.0665 0.3919 sec/batch\n",
      "Epoch 15/20  Iteration 4176/5640 Training loss: 1.0665 0.3927 sec/batch\n",
      "Epoch 15/20  Iteration 4177/5640 Training loss: 1.0665 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4178/5640 Training loss: 1.0665 0.3933 sec/batch\n",
      "Epoch 15/20  Iteration 4179/5640 Training loss: 1.0664 0.3935 sec/batch\n",
      "Epoch 15/20  Iteration 4180/5640 Training loss: 1.0664 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4181/5640 Training loss: 1.0663 0.3952 sec/batch\n",
      "Epoch 15/20  Iteration 4182/5640 Training loss: 1.0662 0.3948 sec/batch\n",
      "Epoch 15/20  Iteration 4183/5640 Training loss: 1.0661 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4184/5640 Training loss: 1.0661 0.3920 sec/batch\n",
      "Epoch 15/20  Iteration 4185/5640 Training loss: 1.0661 0.3939 sec/batch\n",
      "Epoch 15/20  Iteration 4186/5640 Training loss: 1.0662 0.3940 sec/batch\n",
      "Epoch 15/20  Iteration 4187/5640 Training loss: 1.0661 0.3941 sec/batch\n",
      "Epoch 15/20  Iteration 4188/5640 Training loss: 1.0660 0.3934 sec/batch\n",
      "Epoch 15/20  Iteration 4189/5640 Training loss: 1.0660 0.3945 sec/batch\n",
      "Epoch 15/20  Iteration 4190/5640 Training loss: 1.0660 0.3924 sec/batch\n",
      "Epoch 15/20  Iteration 4191/5640 Training loss: 1.0659 0.3927 sec/batch\n",
      "Epoch 15/20  Iteration 4192/5640 Training loss: 1.0657 0.3933 sec/batch\n",
      "Epoch 15/20  Iteration 4193/5640 Training loss: 1.0656 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4194/5640 Training loss: 1.0655 0.3940 sec/batch\n",
      "Epoch 15/20  Iteration 4195/5640 Training loss: 1.0655 0.3930 sec/batch\n",
      "Epoch 15/20  Iteration 4196/5640 Training loss: 1.0655 0.3932 sec/batch\n",
      "Epoch 15/20  Iteration 4197/5640 Training loss: 1.0655 0.3938 sec/batch\n",
      "Epoch 15/20  Iteration 4198/5640 Training loss: 1.0655 0.3930 sec/batch\n",
      "Epoch 15/20  Iteration 4199/5640 Training loss: 1.0653 0.3935 sec/batch\n",
      "Epoch 15/20  Iteration 4200/5640 Training loss: 1.0652 0.3943 sec/batch\n",
      "Validation loss: 1.09252 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 4201/5640 Training loss: 1.0659 0.3813 sec/batch\n",
      "Epoch 15/20  Iteration 4202/5640 Training loss: 1.0658 0.3806 sec/batch\n",
      "Epoch 15/20  Iteration 4203/5640 Training loss: 1.0658 0.3821 sec/batch\n",
      "Epoch 15/20  Iteration 4204/5640 Training loss: 1.0657 0.3816 sec/batch\n",
      "Epoch 15/20  Iteration 4205/5640 Training loss: 1.0656 0.3821 sec/batch\n",
      "Epoch 15/20  Iteration 4206/5640 Training loss: 1.0656 0.3820 sec/batch\n",
      "Epoch 15/20  Iteration 4207/5640 Training loss: 1.0656 0.3867 sec/batch\n",
      "Epoch 15/20  Iteration 4208/5640 Training loss: 1.0656 0.3815 sec/batch\n",
      "Epoch 15/20  Iteration 4209/5640 Training loss: 1.0656 0.3818 sec/batch\n",
      "Epoch 15/20  Iteration 4210/5640 Training loss: 1.0656 0.3815 sec/batch\n",
      "Epoch 15/20  Iteration 4211/5640 Training loss: 1.0656 0.3821 sec/batch\n",
      "Epoch 15/20  Iteration 4212/5640 Training loss: 1.0656 0.3828 sec/batch\n",
      "Epoch 15/20  Iteration 4213/5640 Training loss: 1.0656 0.3835 sec/batch\n",
      "Epoch 15/20  Iteration 4214/5640 Training loss: 1.0655 0.3826 sec/batch\n",
      "Epoch 15/20  Iteration 4215/5640 Training loss: 1.0654 0.3883 sec/batch\n",
      "Epoch 15/20  Iteration 4216/5640 Training loss: 1.0653 0.3883 sec/batch\n",
      "Epoch 15/20  Iteration 4217/5640 Training loss: 1.0653 0.3897 sec/batch\n",
      "Epoch 15/20  Iteration 4218/5640 Training loss: 1.0653 0.3909 sec/batch\n",
      "Epoch 15/20  Iteration 4219/5640 Training loss: 1.0652 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 4220/5640 Training loss: 1.0651 0.3915 sec/batch\n",
      "Epoch 15/20  Iteration 4221/5640 Training loss: 1.0651 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 4222/5640 Training loss: 1.0650 0.3923 sec/batch\n",
      "Epoch 15/20  Iteration 4223/5640 Training loss: 1.0650 0.3896 sec/batch\n",
      "Epoch 15/20  Iteration 4224/5640 Training loss: 1.0649 0.3909 sec/batch\n",
      "Epoch 15/20  Iteration 4225/5640 Training loss: 1.0649 0.3898 sec/batch\n",
      "Epoch 15/20  Iteration 4226/5640 Training loss: 1.0650 0.3906 sec/batch\n",
      "Epoch 15/20  Iteration 4227/5640 Training loss: 1.0650 0.3894 sec/batch\n",
      "Epoch 15/20  Iteration 4228/5640 Training loss: 1.0650 0.3895 sec/batch\n",
      "Epoch 15/20  Iteration 4229/5640 Training loss: 1.0649 0.3907 sec/batch\n",
      "Epoch 15/20  Iteration 4230/5640 Training loss: 1.0648 0.3910 sec/batch\n",
      "Epoch 16/20  Iteration 4231/5640 Training loss: 1.1740 0.3895 sec/batch\n",
      "Epoch 16/20  Iteration 4232/5640 Training loss: 1.1226 0.3900 sec/batch\n",
      "Epoch 16/20  Iteration 4233/5640 Training loss: 1.1059 0.3906 sec/batch\n",
      "Epoch 16/20  Iteration 4234/5640 Training loss: 1.0959 0.3899 sec/batch\n",
      "Epoch 16/20  Iteration 4235/5640 Training loss: 1.0933 0.3928 sec/batch\n",
      "Epoch 16/20  Iteration 4236/5640 Training loss: 1.0920 0.3938 sec/batch\n",
      "Epoch 16/20  Iteration 4237/5640 Training loss: 1.0843 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 4238/5640 Training loss: 1.0814 0.3942 sec/batch\n",
      "Epoch 16/20  Iteration 4239/5640 Training loss: 1.0760 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 4240/5640 Training loss: 1.0722 0.3926 sec/batch\n",
      "Epoch 16/20  Iteration 4241/5640 Training loss: 1.0719 0.3924 sec/batch\n",
      "Epoch 16/20  Iteration 4242/5640 Training loss: 1.0698 0.3930 sec/batch\n",
      "Epoch 16/20  Iteration 4243/5640 Training loss: 1.0692 0.3936 sec/batch\n",
      "Epoch 16/20  Iteration 4244/5640 Training loss: 1.0694 0.3939 sec/batch\n",
      "Epoch 16/20  Iteration 4245/5640 Training loss: 1.0673 0.3969 sec/batch\n",
      "Epoch 16/20  Iteration 4246/5640 Training loss: 1.0686 0.3933 sec/batch\n",
      "Epoch 16/20  Iteration 4247/5640 Training loss: 1.0686 0.3940 sec/batch\n",
      "Epoch 16/20  Iteration 4248/5640 Training loss: 1.0676 0.3944 sec/batch\n",
      "Epoch 16/20  Iteration 4249/5640 Training loss: 1.0676 0.3933 sec/batch\n",
      "Epoch 16/20  Iteration 4250/5640 Training loss: 1.0660 0.3933 sec/batch\n",
      "Validation loss: 1.08628 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 4251/5640 Training loss: 1.0740 0.3827 sec/batch\n",
      "Epoch 16/20  Iteration 4252/5640 Training loss: 1.0741 0.3838 sec/batch\n",
      "Epoch 16/20  Iteration 4253/5640 Training loss: 1.0736 0.3817 sec/batch\n",
      "Epoch 16/20  Iteration 4254/5640 Training loss: 1.0725 0.3816 sec/batch\n",
      "Epoch 16/20  Iteration 4255/5640 Training loss: 1.0723 0.3841 sec/batch\n",
      "Epoch 16/20  Iteration 4256/5640 Training loss: 1.0716 0.3812 sec/batch\n",
      "Epoch 16/20  Iteration 4257/5640 Training loss: 1.0714 0.3818 sec/batch\n",
      "Epoch 16/20  Iteration 4258/5640 Training loss: 1.0712 0.3827 sec/batch\n",
      "Epoch 16/20  Iteration 4259/5640 Training loss: 1.0704 0.3804 sec/batch\n",
      "Epoch 16/20  Iteration 4260/5640 Training loss: 1.0697 0.3811 sec/batch\n",
      "Epoch 16/20  Iteration 4261/5640 Training loss: 1.0682 0.3839 sec/batch\n",
      "Epoch 16/20  Iteration 4262/5640 Training loss: 1.0678 0.3845 sec/batch\n",
      "Epoch 16/20  Iteration 4263/5640 Training loss: 1.0669 0.3854 sec/batch\n",
      "Epoch 16/20  Iteration 4264/5640 Training loss: 1.0664 0.3842 sec/batch\n",
      "Epoch 16/20  Iteration 4265/5640 Training loss: 1.0666 0.3844 sec/batch\n",
      "Epoch 16/20  Iteration 4266/5640 Training loss: 1.0664 0.3875 sec/batch\n",
      "Epoch 16/20  Iteration 4267/5640 Training loss: 1.0655 0.3873 sec/batch\n",
      "Epoch 16/20  Iteration 4268/5640 Training loss: 1.0644 0.3883 sec/batch\n",
      "Epoch 16/20  Iteration 4269/5640 Training loss: 1.0643 0.3885 sec/batch\n",
      "Epoch 16/20  Iteration 4270/5640 Training loss: 1.0632 0.3877 sec/batch\n",
      "Epoch 16/20  Iteration 4271/5640 Training loss: 1.0624 0.3912 sec/batch\n",
      "Epoch 16/20  Iteration 4272/5640 Training loss: 1.0629 0.3881 sec/batch\n",
      "Epoch 16/20  Iteration 4273/5640 Training loss: 1.0618 0.3872 sec/batch\n",
      "Epoch 16/20  Iteration 4274/5640 Training loss: 1.0616 0.3883 sec/batch\n",
      "Epoch 16/20  Iteration 4275/5640 Training loss: 1.0614 0.3885 sec/batch\n",
      "Epoch 16/20  Iteration 4276/5640 Training loss: 1.0611 0.3896 sec/batch\n",
      "Epoch 16/20  Iteration 4277/5640 Training loss: 1.0610 0.3908 sec/batch\n",
      "Epoch 16/20  Iteration 4278/5640 Training loss: 1.0611 0.3910 sec/batch\n",
      "Epoch 16/20  Iteration 4279/5640 Training loss: 1.0611 0.3935 sec/batch\n",
      "Epoch 16/20  Iteration 4280/5640 Training loss: 1.0605 0.3932 sec/batch\n",
      "Epoch 16/20  Iteration 4281/5640 Training loss: 1.0605 0.3938 sec/batch\n",
      "Epoch 16/20  Iteration 4282/5640 Training loss: 1.0605 0.3949 sec/batch\n",
      "Epoch 16/20  Iteration 4283/5640 Training loss: 1.0605 0.3931 sec/batch\n",
      "Epoch 16/20  Iteration 4284/5640 Training loss: 1.0603 0.3947 sec/batch\n",
      "Epoch 16/20  Iteration 4285/5640 Training loss: 1.0600 0.3926 sec/batch\n",
      "Epoch 16/20  Iteration 4286/5640 Training loss: 1.0598 0.3936 sec/batch\n",
      "Epoch 16/20  Iteration 4287/5640 Training loss: 1.0596 0.3933 sec/batch\n",
      "Epoch 16/20  Iteration 4288/5640 Training loss: 1.0596 0.3931 sec/batch\n",
      "Epoch 16/20  Iteration 4289/5640 Training loss: 1.0596 0.3930 sec/batch\n",
      "Epoch 16/20  Iteration 4290/5640 Training loss: 1.0594 0.3941 sec/batch\n",
      "Epoch 16/20  Iteration 4291/5640 Training loss: 1.0590 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 4292/5640 Training loss: 1.0584 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 4293/5640 Training loss: 1.0579 0.3936 sec/batch\n",
      "Epoch 16/20  Iteration 4294/5640 Training loss: 1.0575 0.3950 sec/batch\n",
      "Epoch 16/20  Iteration 4295/5640 Training loss: 1.0577 0.3938 sec/batch\n",
      "Epoch 16/20  Iteration 4296/5640 Training loss: 1.0575 0.3946 sec/batch\n",
      "Epoch 16/20  Iteration 4297/5640 Training loss: 1.0575 0.3931 sec/batch\n",
      "Epoch 16/20  Iteration 4298/5640 Training loss: 1.0572 0.3929 sec/batch\n",
      "Epoch 16/20  Iteration 4299/5640 Training loss: 1.0569 0.3943 sec/batch\n",
      "Epoch 16/20  Iteration 4300/5640 Training loss: 1.0565 0.3930 sec/batch\n",
      "Validation loss: 1.08417 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 4301/5640 Training loss: 1.0590 0.3822 sec/batch\n",
      "Epoch 16/20  Iteration 4302/5640 Training loss: 1.0589 0.3840 sec/batch\n",
      "Epoch 16/20  Iteration 4303/5640 Training loss: 1.0588 0.3833 sec/batch\n",
      "Epoch 16/20  Iteration 4304/5640 Training loss: 1.0587 0.3833 sec/batch\n",
      "Epoch 16/20  Iteration 4305/5640 Training loss: 1.0587 0.3810 sec/batch\n",
      "Epoch 16/20  Iteration 4306/5640 Training loss: 1.0592 0.3822 sec/batch\n",
      "Epoch 16/20  Iteration 4307/5640 Training loss: 1.0591 0.3816 sec/batch\n",
      "Epoch 16/20  Iteration 4308/5640 Training loss: 1.0597 0.3818 sec/batch\n",
      "Epoch 16/20  Iteration 4309/5640 Training loss: 1.0598 0.3840 sec/batch\n",
      "Epoch 16/20  Iteration 4310/5640 Training loss: 1.0598 0.3811 sec/batch\n",
      "Epoch 16/20  Iteration 4311/5640 Training loss: 1.0601 0.3811 sec/batch\n",
      "Epoch 16/20  Iteration 4312/5640 Training loss: 1.0599 0.3859 sec/batch\n",
      "Epoch 16/20  Iteration 4313/5640 Training loss: 1.0597 0.3862 sec/batch\n",
      "Epoch 16/20  Iteration 4314/5640 Training loss: 1.0595 0.3894 sec/batch\n",
      "Epoch 16/20  Iteration 4315/5640 Training loss: 1.0593 0.3881 sec/batch\n",
      "Epoch 16/20  Iteration 4316/5640 Training loss: 1.0591 0.3878 sec/batch\n",
      "Epoch 16/20  Iteration 4317/5640 Training loss: 1.0589 0.3878 sec/batch\n",
      "Epoch 16/20  Iteration 4318/5640 Training loss: 1.0586 0.3879 sec/batch\n",
      "Epoch 16/20  Iteration 4319/5640 Training loss: 1.0584 0.3886 sec/batch\n",
      "Epoch 16/20  Iteration 4320/5640 Training loss: 1.0582 0.3866 sec/batch\n",
      "Epoch 16/20  Iteration 4321/5640 Training loss: 1.0582 0.3893 sec/batch\n",
      "Epoch 16/20  Iteration 4322/5640 Training loss: 1.0581 0.3917 sec/batch\n",
      "Epoch 16/20  Iteration 4323/5640 Training loss: 1.0581 0.3923 sec/batch\n",
      "Epoch 16/20  Iteration 4324/5640 Training loss: 1.0580 0.3923 sec/batch\n",
      "Epoch 16/20  Iteration 4325/5640 Training loss: 1.0578 0.3941 sec/batch\n",
      "Epoch 16/20  Iteration 4326/5640 Training loss: 1.0577 0.3939 sec/batch\n",
      "Epoch 16/20  Iteration 4327/5640 Training loss: 1.0578 0.3953 sec/batch\n",
      "Epoch 16/20  Iteration 4328/5640 Training loss: 1.0578 0.3929 sec/batch\n",
      "Epoch 16/20  Iteration 4329/5640 Training loss: 1.0577 0.3937 sec/batch\n",
      "Epoch 16/20  Iteration 4330/5640 Training loss: 1.0576 0.3932 sec/batch\n",
      "Epoch 16/20  Iteration 4331/5640 Training loss: 1.0573 0.3941 sec/batch\n",
      "Epoch 16/20  Iteration 4332/5640 Training loss: 1.0572 0.3948 sec/batch\n",
      "Epoch 16/20  Iteration 4333/5640 Training loss: 1.0571 0.3935 sec/batch\n",
      "Epoch 16/20  Iteration 4334/5640 Training loss: 1.0569 0.3952 sec/batch\n",
      "Epoch 16/20  Iteration 4335/5640 Training loss: 1.0566 0.3958 sec/batch\n",
      "Epoch 16/20  Iteration 4336/5640 Training loss: 1.0564 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 4337/5640 Training loss: 1.0561 0.3925 sec/batch\n",
      "Epoch 16/20  Iteration 4338/5640 Training loss: 1.0558 0.3950 sec/batch\n",
      "Epoch 16/20  Iteration 4339/5640 Training loss: 1.0556 0.3945 sec/batch\n",
      "Epoch 16/20  Iteration 4340/5640 Training loss: 1.0554 0.3933 sec/batch\n",
      "Epoch 16/20  Iteration 4341/5640 Training loss: 1.0551 0.3942 sec/batch\n",
      "Epoch 16/20  Iteration 4342/5640 Training loss: 1.0551 0.3951 sec/batch\n",
      "Epoch 16/20  Iteration 4343/5640 Training loss: 1.0549 0.3939 sec/batch\n",
      "Epoch 16/20  Iteration 4344/5640 Training loss: 1.0548 0.3926 sec/batch\n",
      "Epoch 16/20  Iteration 4345/5640 Training loss: 1.0549 0.3939 sec/batch\n",
      "Epoch 16/20  Iteration 4346/5640 Training loss: 1.0547 0.3937 sec/batch\n",
      "Epoch 16/20  Iteration 4347/5640 Training loss: 1.0547 0.3953 sec/batch\n",
      "Epoch 16/20  Iteration 4348/5640 Training loss: 1.0546 0.3928 sec/batch\n",
      "Epoch 16/20  Iteration 4349/5640 Training loss: 1.0545 0.3940 sec/batch\n",
      "Epoch 16/20  Iteration 4350/5640 Training loss: 1.0542 0.3950 sec/batch\n",
      "Validation loss: 1.08842 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 4351/5640 Training loss: 1.0558 0.3820 sec/batch\n",
      "Epoch 16/20  Iteration 4352/5640 Training loss: 1.0558 0.3827 sec/batch\n",
      "Epoch 16/20  Iteration 4353/5640 Training loss: 1.0555 0.3829 sec/batch\n",
      "Epoch 16/20  Iteration 4354/5640 Training loss: 1.0555 0.3817 sec/batch\n",
      "Epoch 16/20  Iteration 4355/5640 Training loss: 1.0558 0.3814 sec/batch\n",
      "Epoch 16/20  Iteration 4356/5640 Training loss: 1.0555 0.3815 sec/batch\n",
      "Epoch 16/20  Iteration 4357/5640 Training loss: 1.0556 0.3809 sec/batch\n",
      "Epoch 16/20  Iteration 4358/5640 Training loss: 1.0553 0.3838 sec/batch\n",
      "Epoch 16/20  Iteration 4359/5640 Training loss: 1.0550 0.3870 sec/batch\n",
      "Epoch 16/20  Iteration 4360/5640 Training loss: 1.0548 0.3905 sec/batch\n",
      "Epoch 16/20  Iteration 4361/5640 Training loss: 1.0547 0.3873 sec/batch\n",
      "Epoch 16/20  Iteration 4362/5640 Training loss: 1.0544 0.3866 sec/batch\n",
      "Epoch 16/20  Iteration 4363/5640 Training loss: 1.0541 0.3868 sec/batch\n",
      "Epoch 16/20  Iteration 4364/5640 Training loss: 1.0539 0.3878 sec/batch\n",
      "Epoch 16/20  Iteration 4365/5640 Training loss: 1.0538 0.3888 sec/batch\n",
      "Epoch 16/20  Iteration 4366/5640 Training loss: 1.0539 0.3864 sec/batch\n",
      "Epoch 16/20  Iteration 4367/5640 Training loss: 1.0538 0.3886 sec/batch\n",
      "Epoch 16/20  Iteration 4368/5640 Training loss: 1.0536 0.3911 sec/batch\n",
      "Epoch 16/20  Iteration 4369/5640 Training loss: 1.0536 0.3899 sec/batch\n",
      "Epoch 16/20  Iteration 4370/5640 Training loss: 1.0536 0.3903 sec/batch\n",
      "Epoch 16/20  Iteration 4371/5640 Training loss: 1.0534 0.3896 sec/batch\n",
      "Epoch 16/20  Iteration 4372/5640 Training loss: 1.0534 0.3902 sec/batch\n",
      "Epoch 16/20  Iteration 4373/5640 Training loss: 1.0535 0.3912 sec/batch\n",
      "Epoch 16/20  Iteration 4374/5640 Training loss: 1.0533 0.3910 sec/batch\n",
      "Epoch 16/20  Iteration 4375/5640 Training loss: 1.0530 0.3913 sec/batch\n",
      "Epoch 16/20  Iteration 4376/5640 Training loss: 1.0528 0.3905 sec/batch\n",
      "Epoch 16/20  Iteration 4377/5640 Training loss: 1.0525 0.3903 sec/batch\n",
      "Epoch 16/20  Iteration 4378/5640 Training loss: 1.0526 0.3921 sec/batch\n",
      "Epoch 16/20  Iteration 4379/5640 Training loss: 1.0526 0.3927 sec/batch\n",
      "Epoch 16/20  Iteration 4380/5640 Training loss: 1.0523 0.3933 sec/batch\n",
      "Epoch 16/20  Iteration 4381/5640 Training loss: 1.0523 0.3935 sec/batch\n",
      "Epoch 16/20  Iteration 4382/5640 Training loss: 1.0523 0.3931 sec/batch\n",
      "Epoch 16/20  Iteration 4383/5640 Training loss: 1.0525 0.3950 sec/batch\n",
      "Epoch 16/20  Iteration 4384/5640 Training loss: 1.0525 0.3919 sec/batch\n",
      "Epoch 16/20  Iteration 4385/5640 Training loss: 1.0525 0.3948 sec/batch\n",
      "Epoch 16/20  Iteration 4386/5640 Training loss: 1.0525 0.3920 sec/batch\n",
      "Epoch 16/20  Iteration 4387/5640 Training loss: 1.0523 0.3935 sec/batch\n",
      "Epoch 16/20  Iteration 4388/5640 Training loss: 1.0523 0.3950 sec/batch\n",
      "Epoch 16/20  Iteration 4389/5640 Training loss: 1.0522 0.3941 sec/batch\n",
      "Epoch 16/20  Iteration 4390/5640 Training loss: 1.0520 0.3935 sec/batch\n",
      "Epoch 16/20  Iteration 4391/5640 Training loss: 1.0520 0.3945 sec/batch\n",
      "Epoch 16/20  Iteration 4392/5640 Training loss: 1.0518 0.3936 sec/batch\n",
      "Epoch 16/20  Iteration 4393/5640 Training loss: 1.0517 0.3930 sec/batch\n",
      "Epoch 16/20  Iteration 4394/5640 Training loss: 1.0516 0.3928 sec/batch\n",
      "Epoch 16/20  Iteration 4395/5640 Training loss: 1.0516 0.3942 sec/batch\n",
      "Epoch 16/20  Iteration 4396/5640 Training loss: 1.0517 0.3936 sec/batch\n",
      "Epoch 16/20  Iteration 4397/5640 Training loss: 1.0516 0.3936 sec/batch\n",
      "Epoch 16/20  Iteration 4398/5640 Training loss: 1.0516 0.3971 sec/batch\n",
      "Epoch 16/20  Iteration 4399/5640 Training loss: 1.0516 0.3924 sec/batch\n",
      "Epoch 16/20  Iteration 4400/5640 Training loss: 1.0516 0.3933 sec/batch\n",
      "Validation loss: 1.08578 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 4401/5640 Training loss: 1.0529 0.3811 sec/batch\n",
      "Epoch 16/20  Iteration 4402/5640 Training loss: 1.0528 0.3819 sec/batch\n",
      "Epoch 16/20  Iteration 4403/5640 Training loss: 1.0527 0.3815 sec/batch\n",
      "Epoch 16/20  Iteration 4404/5640 Training loss: 1.0527 0.3829 sec/batch\n",
      "Epoch 16/20  Iteration 4405/5640 Training loss: 1.0527 0.3818 sec/batch\n",
      "Epoch 16/20  Iteration 4406/5640 Training loss: 1.0526 0.3820 sec/batch\n",
      "Epoch 16/20  Iteration 4407/5640 Training loss: 1.0526 0.3812 sec/batch\n",
      "Epoch 16/20  Iteration 4408/5640 Training loss: 1.0525 0.3839 sec/batch\n",
      "Epoch 16/20  Iteration 4409/5640 Training loss: 1.0526 0.3807 sec/batch\n",
      "Epoch 16/20  Iteration 4410/5640 Training loss: 1.0527 0.3811 sec/batch\n",
      "Epoch 16/20  Iteration 4411/5640 Training loss: 1.0528 0.3825 sec/batch\n",
      "Epoch 16/20  Iteration 4412/5640 Training loss: 1.0528 0.3816 sec/batch\n",
      "Epoch 16/20  Iteration 4413/5640 Training loss: 1.0528 0.3832 sec/batch\n",
      "Epoch 16/20  Iteration 4414/5640 Training loss: 1.0527 0.3857 sec/batch\n",
      "Epoch 16/20  Iteration 4415/5640 Training loss: 1.0528 0.3882 sec/batch\n",
      "Epoch 16/20  Iteration 4416/5640 Training loss: 1.0525 0.3885 sec/batch\n",
      "Epoch 16/20  Iteration 4417/5640 Training loss: 1.0522 0.3870 sec/batch\n",
      "Epoch 16/20  Iteration 4418/5640 Training loss: 1.0520 0.3884 sec/batch\n",
      "Epoch 16/20  Iteration 4419/5640 Training loss: 1.0519 0.3919 sec/batch\n",
      "Epoch 16/20  Iteration 4420/5640 Training loss: 1.0517 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 4421/5640 Training loss: 1.0517 0.3953 sec/batch\n",
      "Epoch 16/20  Iteration 4422/5640 Training loss: 1.0515 0.3976 sec/batch\n",
      "Epoch 16/20  Iteration 4423/5640 Training loss: 1.0514 0.3969 sec/batch\n",
      "Epoch 16/20  Iteration 4424/5640 Training loss: 1.0514 0.3999 sec/batch\n",
      "Epoch 16/20  Iteration 4425/5640 Training loss: 1.0515 0.3962 sec/batch\n",
      "Epoch 16/20  Iteration 4426/5640 Training loss: 1.0515 0.3964 sec/batch\n",
      "Epoch 16/20  Iteration 4427/5640 Training loss: 1.0514 0.3971 sec/batch\n",
      "Epoch 16/20  Iteration 4428/5640 Training loss: 1.0515 0.3977 sec/batch\n",
      "Epoch 16/20  Iteration 4429/5640 Training loss: 1.0514 0.3977 sec/batch\n",
      "Epoch 16/20  Iteration 4430/5640 Training loss: 1.0514 0.3969 sec/batch\n",
      "Epoch 16/20  Iteration 4431/5640 Training loss: 1.0514 0.3983 sec/batch\n",
      "Epoch 16/20  Iteration 4432/5640 Training loss: 1.0514 0.3973 sec/batch\n",
      "Epoch 16/20  Iteration 4433/5640 Training loss: 1.0513 0.3971 sec/batch\n",
      "Epoch 16/20  Iteration 4434/5640 Training loss: 1.0513 0.3969 sec/batch\n",
      "Epoch 16/20  Iteration 4435/5640 Training loss: 1.0512 0.3966 sec/batch\n",
      "Epoch 16/20  Iteration 4436/5640 Training loss: 1.0511 0.3981 sec/batch\n",
      "Epoch 16/20  Iteration 4437/5640 Training loss: 1.0510 0.3956 sec/batch\n",
      "Epoch 16/20  Iteration 4438/5640 Training loss: 1.0508 0.3962 sec/batch\n",
      "Epoch 16/20  Iteration 4439/5640 Training loss: 1.0509 0.3979 sec/batch\n",
      "Epoch 16/20  Iteration 4440/5640 Training loss: 1.0508 0.3952 sec/batch\n",
      "Epoch 16/20  Iteration 4441/5640 Training loss: 1.0506 0.3964 sec/batch\n",
      "Epoch 16/20  Iteration 4442/5640 Training loss: 1.0505 0.3964 sec/batch\n",
      "Epoch 16/20  Iteration 4443/5640 Training loss: 1.0506 0.3968 sec/batch\n",
      "Epoch 16/20  Iteration 4444/5640 Training loss: 1.0505 0.3972 sec/batch\n",
      "Epoch 16/20  Iteration 4445/5640 Training loss: 1.0505 0.3967 sec/batch\n",
      "Epoch 16/20  Iteration 4446/5640 Training loss: 1.0505 0.3973 sec/batch\n",
      "Epoch 16/20  Iteration 4447/5640 Training loss: 1.0504 0.3974 sec/batch\n",
      "Epoch 16/20  Iteration 4448/5640 Training loss: 1.0502 0.3961 sec/batch\n",
      "Epoch 16/20  Iteration 4449/5640 Training loss: 1.0500 0.3978 sec/batch\n",
      "Epoch 16/20  Iteration 4450/5640 Training loss: 1.0499 0.3968 sec/batch\n",
      "Validation loss: 1.08754 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 4451/5640 Training loss: 1.0508 0.3814 sec/batch\n",
      "Epoch 16/20  Iteration 4452/5640 Training loss: 1.0507 0.3826 sec/batch\n",
      "Epoch 16/20  Iteration 4453/5640 Training loss: 1.0506 0.3824 sec/batch\n",
      "Epoch 16/20  Iteration 4454/5640 Training loss: 1.0506 0.3822 sec/batch\n",
      "Epoch 16/20  Iteration 4455/5640 Training loss: 1.0505 0.3829 sec/batch\n",
      "Epoch 16/20  Iteration 4456/5640 Training loss: 1.0505 0.3817 sec/batch\n",
      "Epoch 16/20  Iteration 4457/5640 Training loss: 1.0505 0.3819 sec/batch\n",
      "Epoch 16/20  Iteration 4458/5640 Training loss: 1.0506 0.3814 sec/batch\n",
      "Epoch 16/20  Iteration 4459/5640 Training loss: 1.0506 0.3816 sec/batch\n",
      "Epoch 16/20  Iteration 4460/5640 Training loss: 1.0506 0.3824 sec/batch\n",
      "Epoch 16/20  Iteration 4461/5640 Training loss: 1.0505 0.3828 sec/batch\n",
      "Epoch 16/20  Iteration 4462/5640 Training loss: 1.0506 0.3828 sec/batch\n",
      "Epoch 16/20  Iteration 4463/5640 Training loss: 1.0505 0.3817 sec/batch\n",
      "Epoch 16/20  Iteration 4464/5640 Training loss: 1.0503 0.3827 sec/batch\n",
      "Epoch 16/20  Iteration 4465/5640 Training loss: 1.0503 0.3820 sec/batch\n",
      "Epoch 16/20  Iteration 4466/5640 Training loss: 1.0504 0.3847 sec/batch\n",
      "Epoch 16/20  Iteration 4467/5640 Training loss: 1.0503 0.3845 sec/batch\n",
      "Epoch 16/20  Iteration 4468/5640 Training loss: 1.0504 0.3872 sec/batch\n",
      "Epoch 16/20  Iteration 4469/5640 Training loss: 1.0503 0.3888 sec/batch\n",
      "Epoch 16/20  Iteration 4470/5640 Training loss: 1.0503 0.3888 sec/batch\n",
      "Epoch 16/20  Iteration 4471/5640 Training loss: 1.0503 0.3909 sec/batch\n",
      "Epoch 16/20  Iteration 4472/5640 Training loss: 1.0502 0.3917 sec/batch\n",
      "Epoch 16/20  Iteration 4473/5640 Training loss: 1.0502 0.3897 sec/batch\n",
      "Epoch 16/20  Iteration 4474/5640 Training loss: 1.0500 0.3924 sec/batch\n",
      "Epoch 16/20  Iteration 4475/5640 Training loss: 1.0499 0.3902 sec/batch\n",
      "Epoch 16/20  Iteration 4476/5640 Training loss: 1.0498 0.3900 sec/batch\n",
      "Epoch 16/20  Iteration 4477/5640 Training loss: 1.0498 0.3903 sec/batch\n",
      "Epoch 16/20  Iteration 4478/5640 Training loss: 1.0498 0.3903 sec/batch\n",
      "Epoch 16/20  Iteration 4479/5640 Training loss: 1.0498 0.3927 sec/batch\n",
      "Epoch 16/20  Iteration 4480/5640 Training loss: 1.0498 0.3949 sec/batch\n",
      "Epoch 16/20  Iteration 4481/5640 Training loss: 1.0497 0.3926 sec/batch\n",
      "Epoch 16/20  Iteration 4482/5640 Training loss: 1.0496 0.3930 sec/batch\n",
      "Epoch 16/20  Iteration 4483/5640 Training loss: 1.0495 0.3936 sec/batch\n",
      "Epoch 16/20  Iteration 4484/5640 Training loss: 1.0495 0.3928 sec/batch\n",
      "Epoch 16/20  Iteration 4485/5640 Training loss: 1.0494 0.3930 sec/batch\n",
      "Epoch 16/20  Iteration 4486/5640 Training loss: 1.0494 0.3931 sec/batch\n",
      "Epoch 16/20  Iteration 4487/5640 Training loss: 1.0493 0.3965 sec/batch\n",
      "Epoch 16/20  Iteration 4488/5640 Training loss: 1.0493 0.3928 sec/batch\n",
      "Epoch 16/20  Iteration 4489/5640 Training loss: 1.0493 0.3926 sec/batch\n",
      "Epoch 16/20  Iteration 4490/5640 Training loss: 1.0493 0.3934 sec/batch\n",
      "Epoch 16/20  Iteration 4491/5640 Training loss: 1.0493 0.3945 sec/batch\n",
      "Epoch 16/20  Iteration 4492/5640 Training loss: 1.0493 0.3932 sec/batch\n",
      "Epoch 16/20  Iteration 4493/5640 Training loss: 1.0493 0.3929 sec/batch\n",
      "Epoch 16/20  Iteration 4494/5640 Training loss: 1.0493 0.3957 sec/batch\n",
      "Epoch 16/20  Iteration 4495/5640 Training loss: 1.0493 0.3976 sec/batch\n",
      "Epoch 16/20  Iteration 4496/5640 Training loss: 1.0493 0.3962 sec/batch\n",
      "Epoch 16/20  Iteration 4497/5640 Training loss: 1.0492 0.3969 sec/batch\n",
      "Epoch 16/20  Iteration 4498/5640 Training loss: 1.0491 0.3960 sec/batch\n",
      "Epoch 16/20  Iteration 4499/5640 Training loss: 1.0491 0.3972 sec/batch\n",
      "Epoch 16/20  Iteration 4500/5640 Training loss: 1.0491 0.3981 sec/batch\n",
      "Validation loss: 1.08676 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 4501/5640 Training loss: 1.0497 0.3811 sec/batch\n",
      "Epoch 16/20  Iteration 4502/5640 Training loss: 1.0496 0.3810 sec/batch\n",
      "Epoch 16/20  Iteration 4503/5640 Training loss: 1.0496 0.3813 sec/batch\n",
      "Epoch 16/20  Iteration 4504/5640 Training loss: 1.0495 0.3852 sec/batch\n",
      "Epoch 16/20  Iteration 4505/5640 Training loss: 1.0495 0.3837 sec/batch\n",
      "Epoch 16/20  Iteration 4506/5640 Training loss: 1.0494 0.3822 sec/batch\n",
      "Epoch 16/20  Iteration 4507/5640 Training loss: 1.0494 0.3812 sec/batch\n",
      "Epoch 16/20  Iteration 4508/5640 Training loss: 1.0496 0.3810 sec/batch\n",
      "Epoch 16/20  Iteration 4509/5640 Training loss: 1.0496 0.3818 sec/batch\n",
      "Epoch 16/20  Iteration 4510/5640 Training loss: 1.0496 0.3816 sec/batch\n",
      "Epoch 16/20  Iteration 4511/5640 Training loss: 1.0495 0.3822 sec/batch\n",
      "Epoch 16/20  Iteration 4512/5640 Training loss: 1.0494 0.3834 sec/batch\n",
      "Epoch 17/20  Iteration 4513/5640 Training loss: 1.1472 0.3832 sec/batch\n",
      "Epoch 17/20  Iteration 4514/5640 Training loss: 1.1059 0.3854 sec/batch\n",
      "Epoch 17/20  Iteration 4515/5640 Training loss: 1.0909 0.3864 sec/batch\n",
      "Epoch 17/20  Iteration 4516/5640 Training loss: 1.0791 0.3882 sec/batch\n",
      "Epoch 17/20  Iteration 4517/5640 Training loss: 1.0740 0.3899 sec/batch\n",
      "Epoch 17/20  Iteration 4518/5640 Training loss: 1.0728 0.3898 sec/batch\n",
      "Epoch 17/20  Iteration 4519/5640 Training loss: 1.0672 0.3892 sec/batch\n",
      "Epoch 17/20  Iteration 4520/5640 Training loss: 1.0647 0.3898 sec/batch\n",
      "Epoch 17/20  Iteration 4521/5640 Training loss: 1.0602 0.3896 sec/batch\n",
      "Epoch 17/20  Iteration 4522/5640 Training loss: 1.0561 0.3894 sec/batch\n",
      "Epoch 17/20  Iteration 4523/5640 Training loss: 1.0563 0.3897 sec/batch\n",
      "Epoch 17/20  Iteration 4524/5640 Training loss: 1.0548 0.3897 sec/batch\n",
      "Epoch 17/20  Iteration 4525/5640 Training loss: 1.0537 0.3912 sec/batch\n",
      "Epoch 17/20  Iteration 4526/5640 Training loss: 1.0542 0.3894 sec/batch\n",
      "Epoch 17/20  Iteration 4527/5640 Training loss: 1.0522 0.3898 sec/batch\n",
      "Epoch 17/20  Iteration 4528/5640 Training loss: 1.0532 0.3907 sec/batch\n",
      "Epoch 17/20  Iteration 4529/5640 Training loss: 1.0533 0.3901 sec/batch\n",
      "Epoch 17/20  Iteration 4530/5640 Training loss: 1.0518 0.3905 sec/batch\n",
      "Epoch 17/20  Iteration 4531/5640 Training loss: 1.0511 0.3905 sec/batch\n",
      "Epoch 17/20  Iteration 4532/5640 Training loss: 1.0492 0.3907 sec/batch\n",
      "Epoch 17/20  Iteration 4533/5640 Training loss: 1.0495 0.3920 sec/batch\n",
      "Epoch 17/20  Iteration 4534/5640 Training loss: 1.0493 0.3906 sec/batch\n",
      "Epoch 17/20  Iteration 4535/5640 Training loss: 1.0489 0.3901 sec/batch\n",
      "Epoch 17/20  Iteration 4536/5640 Training loss: 1.0475 0.3904 sec/batch\n",
      "Epoch 17/20  Iteration 4537/5640 Training loss: 1.0474 0.3904 sec/batch\n",
      "Epoch 17/20  Iteration 4538/5640 Training loss: 1.0474 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4539/5640 Training loss: 1.0475 0.3916 sec/batch\n",
      "Epoch 17/20  Iteration 4540/5640 Training loss: 1.0475 0.3927 sec/batch\n",
      "Epoch 17/20  Iteration 4541/5640 Training loss: 1.0469 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4542/5640 Training loss: 1.0466 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4543/5640 Training loss: 1.0459 0.3941 sec/batch\n",
      "Epoch 17/20  Iteration 4544/5640 Training loss: 1.0456 0.3929 sec/batch\n",
      "Epoch 17/20  Iteration 4545/5640 Training loss: 1.0449 0.3931 sec/batch\n",
      "Epoch 17/20  Iteration 4546/5640 Training loss: 1.0444 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 4547/5640 Training loss: 1.0450 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4548/5640 Training loss: 1.0447 0.3926 sec/batch\n",
      "Epoch 17/20  Iteration 4549/5640 Training loss: 1.0438 0.3925 sec/batch\n",
      "Epoch 17/20  Iteration 4550/5640 Training loss: 1.0427 0.3935 sec/batch\n",
      "Validation loss: 1.0873 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 4551/5640 Training loss: 1.0473 0.3814 sec/batch\n",
      "Epoch 17/20  Iteration 4552/5640 Training loss: 1.0465 0.3822 sec/batch\n",
      "Epoch 17/20  Iteration 4553/5640 Training loss: 1.0460 0.3830 sec/batch\n",
      "Epoch 17/20  Iteration 4554/5640 Training loss: 1.0463 0.3817 sec/batch\n",
      "Epoch 17/20  Iteration 4555/5640 Training loss: 1.0451 0.3820 sec/batch\n",
      "Epoch 17/20  Iteration 4556/5640 Training loss: 1.0451 0.3830 sec/batch\n",
      "Epoch 17/20  Iteration 4557/5640 Training loss: 1.0449 0.3814 sec/batch\n",
      "Epoch 17/20  Iteration 4558/5640 Training loss: 1.0446 0.3852 sec/batch\n",
      "Epoch 17/20  Iteration 4559/5640 Training loss: 1.0447 0.3844 sec/batch\n",
      "Epoch 17/20  Iteration 4560/5640 Training loss: 1.0448 0.3866 sec/batch\n",
      "Epoch 17/20  Iteration 4561/5640 Training loss: 1.0450 0.3891 sec/batch\n",
      "Epoch 17/20  Iteration 4562/5640 Training loss: 1.0447 0.3869 sec/batch\n",
      "Epoch 17/20  Iteration 4563/5640 Training loss: 1.0447 0.3885 sec/batch\n",
      "Epoch 17/20  Iteration 4564/5640 Training loss: 1.0446 0.3873 sec/batch\n",
      "Epoch 17/20  Iteration 4565/5640 Training loss: 1.0446 0.3873 sec/batch\n",
      "Epoch 17/20  Iteration 4566/5640 Training loss: 1.0446 0.3874 sec/batch\n",
      "Epoch 17/20  Iteration 4567/5640 Training loss: 1.0442 0.3875 sec/batch\n",
      "Epoch 17/20  Iteration 4568/5640 Training loss: 1.0443 0.3894 sec/batch\n",
      "Epoch 17/20  Iteration 4569/5640 Training loss: 1.0442 0.3911 sec/batch\n",
      "Epoch 17/20  Iteration 4570/5640 Training loss: 1.0440 0.3898 sec/batch\n",
      "Epoch 17/20  Iteration 4571/5640 Training loss: 1.0439 0.3902 sec/batch\n",
      "Epoch 17/20  Iteration 4572/5640 Training loss: 1.0436 0.3914 sec/batch\n",
      "Epoch 17/20  Iteration 4573/5640 Training loss: 1.0433 0.3905 sec/batch\n",
      "Epoch 17/20  Iteration 4574/5640 Training loss: 1.0429 0.3894 sec/batch\n",
      "Epoch 17/20  Iteration 4575/5640 Training loss: 1.0425 0.3902 sec/batch\n",
      "Epoch 17/20  Iteration 4576/5640 Training loss: 1.0420 0.3913 sec/batch\n",
      "Epoch 17/20  Iteration 4577/5640 Training loss: 1.0422 0.3903 sec/batch\n",
      "Epoch 17/20  Iteration 4578/5640 Training loss: 1.0421 0.3884 sec/batch\n",
      "Epoch 17/20  Iteration 4579/5640 Training loss: 1.0421 0.3892 sec/batch\n",
      "Epoch 17/20  Iteration 4580/5640 Training loss: 1.0419 0.3921 sec/batch\n",
      "Epoch 17/20  Iteration 4581/5640 Training loss: 1.0417 0.3941 sec/batch\n",
      "Epoch 17/20  Iteration 4582/5640 Training loss: 1.0412 0.3942 sec/batch\n",
      "Epoch 17/20  Iteration 4583/5640 Training loss: 1.0411 0.3924 sec/batch\n",
      "Epoch 17/20  Iteration 4584/5640 Training loss: 1.0410 0.3941 sec/batch\n",
      "Epoch 17/20  Iteration 4585/5640 Training loss: 1.0410 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4586/5640 Training loss: 1.0411 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4587/5640 Training loss: 1.0413 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4588/5640 Training loss: 1.0418 0.3936 sec/batch\n",
      "Epoch 17/20  Iteration 4589/5640 Training loss: 1.0418 0.3958 sec/batch\n",
      "Epoch 17/20  Iteration 4590/5640 Training loss: 1.0423 0.3921 sec/batch\n",
      "Epoch 17/20  Iteration 4591/5640 Training loss: 1.0424 0.3936 sec/batch\n",
      "Epoch 17/20  Iteration 4592/5640 Training loss: 1.0426 0.3946 sec/batch\n",
      "Epoch 17/20  Iteration 4593/5640 Training loss: 1.0429 0.3925 sec/batch\n",
      "Epoch 17/20  Iteration 4594/5640 Training loss: 1.0427 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4595/5640 Training loss: 1.0425 0.3933 sec/batch\n",
      "Epoch 17/20  Iteration 4596/5640 Training loss: 1.0424 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4597/5640 Training loss: 1.0423 0.3943 sec/batch\n",
      "Epoch 17/20  Iteration 4598/5640 Training loss: 1.0421 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4599/5640 Training loss: 1.0418 0.3949 sec/batch\n",
      "Epoch 17/20  Iteration 4600/5640 Training loss: 1.0417 0.3926 sec/batch\n",
      "Validation loss: 1.08807 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 4601/5640 Training loss: 1.0436 0.3841 sec/batch\n",
      "Epoch 17/20  Iteration 4602/5640 Training loss: 1.0433 0.3826 sec/batch\n",
      "Epoch 17/20  Iteration 4603/5640 Training loss: 1.0434 0.3814 sec/batch\n",
      "Epoch 17/20  Iteration 4604/5640 Training loss: 1.0433 0.3827 sec/batch\n",
      "Epoch 17/20  Iteration 4605/5640 Training loss: 1.0433 0.3824 sec/batch\n",
      "Epoch 17/20  Iteration 4606/5640 Training loss: 1.0433 0.3814 sec/batch\n",
      "Epoch 17/20  Iteration 4607/5640 Training loss: 1.0431 0.3824 sec/batch\n",
      "Epoch 17/20  Iteration 4608/5640 Training loss: 1.0430 0.3820 sec/batch\n",
      "Epoch 17/20  Iteration 4609/5640 Training loss: 1.0431 0.3823 sec/batch\n",
      "Epoch 17/20  Iteration 4610/5640 Training loss: 1.0431 0.3845 sec/batch\n",
      "Epoch 17/20  Iteration 4611/5640 Training loss: 1.0431 0.3840 sec/batch\n",
      "Epoch 17/20  Iteration 4612/5640 Training loss: 1.0429 0.3867 sec/batch\n",
      "Epoch 17/20  Iteration 4613/5640 Training loss: 1.0425 0.3877 sec/batch\n",
      "Epoch 17/20  Iteration 4614/5640 Training loss: 1.0425 0.3913 sec/batch\n",
      "Epoch 17/20  Iteration 4615/5640 Training loss: 1.0424 0.3926 sec/batch\n",
      "Epoch 17/20  Iteration 4616/5640 Training loss: 1.0422 0.3925 sec/batch\n",
      "Epoch 17/20  Iteration 4617/5640 Training loss: 1.0419 0.3953 sec/batch\n",
      "Epoch 17/20  Iteration 4618/5640 Training loss: 1.0416 0.3924 sec/batch\n",
      "Epoch 17/20  Iteration 4619/5640 Training loss: 1.0413 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4620/5640 Training loss: 1.0410 0.3932 sec/batch\n",
      "Epoch 17/20  Iteration 4621/5640 Training loss: 1.0408 0.3933 sec/batch\n",
      "Epoch 17/20  Iteration 4622/5640 Training loss: 1.0406 0.3944 sec/batch\n",
      "Epoch 17/20  Iteration 4623/5640 Training loss: 1.0403 0.3934 sec/batch\n",
      "Epoch 17/20  Iteration 4624/5640 Training loss: 1.0403 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 4625/5640 Training loss: 1.0401 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4626/5640 Training loss: 1.0400 0.3932 sec/batch\n",
      "Epoch 17/20  Iteration 4627/5640 Training loss: 1.0401 0.3945 sec/batch\n",
      "Epoch 17/20  Iteration 4628/5640 Training loss: 1.0400 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4629/5640 Training loss: 1.0399 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4630/5640 Training loss: 1.0398 0.3932 sec/batch\n",
      "Epoch 17/20  Iteration 4631/5640 Training loss: 1.0396 0.3932 sec/batch\n",
      "Epoch 17/20  Iteration 4632/5640 Training loss: 1.0393 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 4633/5640 Training loss: 1.0393 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4634/5640 Training loss: 1.0392 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4635/5640 Training loss: 1.0391 0.3933 sec/batch\n",
      "Epoch 17/20  Iteration 4636/5640 Training loss: 1.0391 0.3944 sec/batch\n",
      "Epoch 17/20  Iteration 4637/5640 Training loss: 1.0394 0.3971 sec/batch\n",
      "Epoch 17/20  Iteration 4638/5640 Training loss: 1.0391 0.3943 sec/batch\n",
      "Epoch 17/20  Iteration 4639/5640 Training loss: 1.0390 0.3936 sec/batch\n",
      "Epoch 17/20  Iteration 4640/5640 Training loss: 1.0388 0.3958 sec/batch\n",
      "Epoch 17/20  Iteration 4641/5640 Training loss: 1.0385 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4642/5640 Training loss: 1.0384 0.3941 sec/batch\n",
      "Epoch 17/20  Iteration 4643/5640 Training loss: 1.0383 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4644/5640 Training loss: 1.0380 0.3946 sec/batch\n",
      "Epoch 17/20  Iteration 4645/5640 Training loss: 1.0378 0.3933 sec/batch\n",
      "Epoch 17/20  Iteration 4646/5640 Training loss: 1.0375 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4647/5640 Training loss: 1.0375 0.3944 sec/batch\n",
      "Epoch 17/20  Iteration 4648/5640 Training loss: 1.0375 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4649/5640 Training loss: 1.0374 0.3934 sec/batch\n",
      "Epoch 17/20  Iteration 4650/5640 Training loss: 1.0372 0.3931 sec/batch\n",
      "Validation loss: 1.08429 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 4651/5640 Training loss: 1.0387 0.3808 sec/batch\n",
      "Epoch 17/20  Iteration 4652/5640 Training loss: 1.0387 0.3843 sec/batch\n",
      "Epoch 17/20  Iteration 4653/5640 Training loss: 1.0386 0.3817 sec/batch\n",
      "Epoch 17/20  Iteration 4654/5640 Training loss: 1.0386 0.3821 sec/batch\n",
      "Epoch 17/20  Iteration 4655/5640 Training loss: 1.0387 0.3822 sec/batch\n",
      "Epoch 17/20  Iteration 4656/5640 Training loss: 1.0386 0.3820 sec/batch\n",
      "Epoch 17/20  Iteration 4657/5640 Training loss: 1.0382 0.3826 sec/batch\n",
      "Epoch 17/20  Iteration 4658/5640 Training loss: 1.0381 0.3862 sec/batch\n",
      "Epoch 17/20  Iteration 4659/5640 Training loss: 1.0378 0.3863 sec/batch\n",
      "Epoch 17/20  Iteration 4660/5640 Training loss: 1.0378 0.3884 sec/batch\n",
      "Epoch 17/20  Iteration 4661/5640 Training loss: 1.0377 0.3901 sec/batch\n",
      "Epoch 17/20  Iteration 4662/5640 Training loss: 1.0376 0.3903 sec/batch\n",
      "Epoch 17/20  Iteration 4663/5640 Training loss: 1.0375 0.3918 sec/batch\n",
      "Epoch 17/20  Iteration 4664/5640 Training loss: 1.0375 0.3903 sec/batch\n",
      "Epoch 17/20  Iteration 4665/5640 Training loss: 1.0377 0.3919 sec/batch\n",
      "Epoch 17/20  Iteration 4666/5640 Training loss: 1.0377 0.3913 sec/batch\n",
      "Epoch 17/20  Iteration 4667/5640 Training loss: 1.0377 0.3894 sec/batch\n",
      "Epoch 17/20  Iteration 4668/5640 Training loss: 1.0378 0.3900 sec/batch\n",
      "Epoch 17/20  Iteration 4669/5640 Training loss: 1.0376 0.3899 sec/batch\n",
      "Epoch 17/20  Iteration 4670/5640 Training loss: 1.0377 0.3889 sec/batch\n",
      "Epoch 17/20  Iteration 4671/5640 Training loss: 1.0375 0.3904 sec/batch\n",
      "Epoch 17/20  Iteration 4672/5640 Training loss: 1.0374 0.3904 sec/batch\n",
      "Epoch 17/20  Iteration 4673/5640 Training loss: 1.0373 0.3910 sec/batch\n",
      "Epoch 17/20  Iteration 4674/5640 Training loss: 1.0372 0.3895 sec/batch\n",
      "Epoch 17/20  Iteration 4675/5640 Training loss: 1.0371 0.3905 sec/batch\n",
      "Epoch 17/20  Iteration 4676/5640 Training loss: 1.0369 0.3896 sec/batch\n",
      "Epoch 17/20  Iteration 4677/5640 Training loss: 1.0370 0.3900 sec/batch\n",
      "Epoch 17/20  Iteration 4678/5640 Training loss: 1.0371 0.3910 sec/batch\n",
      "Epoch 17/20  Iteration 4679/5640 Training loss: 1.0371 0.3899 sec/batch\n",
      "Epoch 17/20  Iteration 4680/5640 Training loss: 1.0371 0.3897 sec/batch\n",
      "Epoch 17/20  Iteration 4681/5640 Training loss: 1.0371 0.3915 sec/batch\n",
      "Epoch 17/20  Iteration 4682/5640 Training loss: 1.0371 0.3897 sec/batch\n",
      "Epoch 17/20  Iteration 4683/5640 Training loss: 1.0371 0.3898 sec/batch\n",
      "Epoch 17/20  Iteration 4684/5640 Training loss: 1.0370 0.3901 sec/batch\n",
      "Epoch 17/20  Iteration 4685/5640 Training loss: 1.0369 0.3907 sec/batch\n",
      "Epoch 17/20  Iteration 4686/5640 Training loss: 1.0369 0.3897 sec/batch\n",
      "Epoch 17/20  Iteration 4687/5640 Training loss: 1.0369 0.3897 sec/batch\n",
      "Epoch 17/20  Iteration 4688/5640 Training loss: 1.0368 0.3890 sec/batch\n",
      "Epoch 17/20  Iteration 4689/5640 Training loss: 1.0368 0.3913 sec/batch\n",
      "Epoch 17/20  Iteration 4690/5640 Training loss: 1.0367 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 4691/5640 Training loss: 1.0368 0.3950 sec/batch\n",
      "Epoch 17/20  Iteration 4692/5640 Training loss: 1.0369 0.3946 sec/batch\n",
      "Epoch 17/20  Iteration 4693/5640 Training loss: 1.0370 0.3936 sec/batch\n",
      "Epoch 17/20  Iteration 4694/5640 Training loss: 1.0370 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4695/5640 Training loss: 1.0370 0.3934 sec/batch\n",
      "Epoch 17/20  Iteration 4696/5640 Training loss: 1.0369 0.3953 sec/batch\n",
      "Epoch 17/20  Iteration 4697/5640 Training loss: 1.0369 0.3928 sec/batch\n",
      "Epoch 17/20  Iteration 4698/5640 Training loss: 1.0367 0.3931 sec/batch\n",
      "Epoch 17/20  Iteration 4699/5640 Training loss: 1.0365 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4700/5640 Training loss: 1.0362 0.3946 sec/batch\n",
      "Validation loss: 1.08471 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 4701/5640 Training loss: 1.0372 0.3820 sec/batch\n",
      "Epoch 17/20  Iteration 4702/5640 Training loss: 1.0370 0.3837 sec/batch\n",
      "Epoch 17/20  Iteration 4703/5640 Training loss: 1.0370 0.3831 sec/batch\n",
      "Epoch 17/20  Iteration 4704/5640 Training loss: 1.0369 0.3821 sec/batch\n",
      "Epoch 17/20  Iteration 4705/5640 Training loss: 1.0367 0.3812 sec/batch\n",
      "Epoch 17/20  Iteration 4706/5640 Training loss: 1.0368 0.3831 sec/batch\n",
      "Epoch 17/20  Iteration 4707/5640 Training loss: 1.0369 0.3811 sec/batch\n",
      "Epoch 17/20  Iteration 4708/5640 Training loss: 1.0369 0.3807 sec/batch\n",
      "Epoch 17/20  Iteration 4709/5640 Training loss: 1.0368 0.3823 sec/batch\n",
      "Epoch 17/20  Iteration 4710/5640 Training loss: 1.0369 0.3842 sec/batch\n",
      "Epoch 17/20  Iteration 4711/5640 Training loss: 1.0369 0.3871 sec/batch\n",
      "Epoch 17/20  Iteration 4712/5640 Training loss: 1.0368 0.3891 sec/batch\n",
      "Epoch 17/20  Iteration 4713/5640 Training loss: 1.0368 0.3924 sec/batch\n",
      "Epoch 17/20  Iteration 4714/5640 Training loss: 1.0368 0.3948 sec/batch\n",
      "Epoch 17/20  Iteration 4715/5640 Training loss: 1.0367 0.3924 sec/batch\n",
      "Epoch 17/20  Iteration 4716/5640 Training loss: 1.0368 0.3960 sec/batch\n",
      "Epoch 17/20  Iteration 4717/5640 Training loss: 1.0366 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 4718/5640 Training loss: 1.0366 0.3929 sec/batch\n",
      "Epoch 17/20  Iteration 4719/5640 Training loss: 1.0365 0.3927 sec/batch\n",
      "Epoch 17/20  Iteration 4720/5640 Training loss: 1.0364 0.3938 sec/batch\n",
      "Epoch 17/20  Iteration 4721/5640 Training loss: 1.0365 0.3938 sec/batch\n",
      "Epoch 17/20  Iteration 4722/5640 Training loss: 1.0364 0.3958 sec/batch\n",
      "Epoch 17/20  Iteration 4723/5640 Training loss: 1.0362 0.3932 sec/batch\n",
      "Epoch 17/20  Iteration 4724/5640 Training loss: 1.0362 0.3942 sec/batch\n",
      "Epoch 17/20  Iteration 4725/5640 Training loss: 1.0362 0.3936 sec/batch\n",
      "Epoch 17/20  Iteration 4726/5640 Training loss: 1.0362 0.3950 sec/batch\n",
      "Epoch 17/20  Iteration 4727/5640 Training loss: 1.0361 0.3933 sec/batch\n",
      "Epoch 17/20  Iteration 4728/5640 Training loss: 1.0361 0.3941 sec/batch\n",
      "Epoch 17/20  Iteration 4729/5640 Training loss: 1.0361 0.3972 sec/batch\n",
      "Epoch 17/20  Iteration 4730/5640 Training loss: 1.0359 0.3942 sec/batch\n",
      "Epoch 17/20  Iteration 4731/5640 Training loss: 1.0358 0.3931 sec/batch\n",
      "Epoch 17/20  Iteration 4732/5640 Training loss: 1.0357 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4733/5640 Training loss: 1.0356 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4734/5640 Training loss: 1.0355 0.3938 sec/batch\n",
      "Epoch 17/20  Iteration 4735/5640 Training loss: 1.0355 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4736/5640 Training loss: 1.0355 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4737/5640 Training loss: 1.0354 0.3954 sec/batch\n",
      "Epoch 17/20  Iteration 4738/5640 Training loss: 1.0354 0.3935 sec/batch\n",
      "Epoch 17/20  Iteration 4739/5640 Training loss: 1.0354 0.3956 sec/batch\n",
      "Epoch 17/20  Iteration 4740/5640 Training loss: 1.0355 0.3937 sec/batch\n",
      "Epoch 17/20  Iteration 4741/5640 Training loss: 1.0354 0.3949 sec/batch\n",
      "Epoch 17/20  Iteration 4742/5640 Training loss: 1.0354 0.3947 sec/batch\n",
      "Epoch 17/20  Iteration 4743/5640 Training loss: 1.0354 0.3922 sec/batch\n",
      "Epoch 17/20  Iteration 4744/5640 Training loss: 1.0354 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4745/5640 Training loss: 1.0354 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4746/5640 Training loss: 1.0353 0.3903 sec/batch\n",
      "Epoch 17/20  Iteration 4747/5640 Training loss: 1.0352 0.3909 sec/batch\n",
      "Epoch 17/20  Iteration 4748/5640 Training loss: 1.0352 0.3896 sec/batch\n",
      "Epoch 17/20  Iteration 4749/5640 Training loss: 1.0352 0.3904 sec/batch\n",
      "Epoch 17/20  Iteration 4750/5640 Training loss: 1.0353 0.3895 sec/batch\n",
      "Validation loss: 1.08441 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 4751/5640 Training loss: 1.0361 0.3820 sec/batch\n",
      "Epoch 17/20  Iteration 4752/5640 Training loss: 1.0361 0.3826 sec/batch\n",
      "Epoch 17/20  Iteration 4753/5640 Training loss: 1.0361 0.3838 sec/batch\n",
      "Epoch 17/20  Iteration 4754/5640 Training loss: 1.0361 0.3863 sec/batch\n",
      "Epoch 17/20  Iteration 4755/5640 Training loss: 1.0360 0.3819 sec/batch\n",
      "Epoch 17/20  Iteration 4756/5640 Training loss: 1.0359 0.3816 sec/batch\n",
      "Epoch 17/20  Iteration 4757/5640 Training loss: 1.0358 0.3817 sec/batch\n",
      "Epoch 17/20  Iteration 4758/5640 Training loss: 1.0358 0.3817 sec/batch\n",
      "Epoch 17/20  Iteration 4759/5640 Training loss: 1.0358 0.3821 sec/batch\n",
      "Epoch 17/20  Iteration 4760/5640 Training loss: 1.0358 0.3813 sec/batch\n",
      "Epoch 17/20  Iteration 4761/5640 Training loss: 1.0358 0.3829 sec/batch\n",
      "Epoch 17/20  Iteration 4762/5640 Training loss: 1.0358 0.3843 sec/batch\n",
      "Epoch 17/20  Iteration 4763/5640 Training loss: 1.0357 0.3860 sec/batch\n",
      "Epoch 17/20  Iteration 4764/5640 Training loss: 1.0356 0.3842 sec/batch\n",
      "Epoch 17/20  Iteration 4765/5640 Training loss: 1.0356 0.3845 sec/batch\n",
      "Epoch 17/20  Iteration 4766/5640 Training loss: 1.0355 0.3844 sec/batch\n",
      "Epoch 17/20  Iteration 4767/5640 Training loss: 1.0354 0.3874 sec/batch\n",
      "Epoch 17/20  Iteration 4768/5640 Training loss: 1.0354 0.3881 sec/batch\n",
      "Epoch 17/20  Iteration 4769/5640 Training loss: 1.0353 0.3926 sec/batch\n",
      "Epoch 17/20  Iteration 4770/5640 Training loss: 1.0353 0.3939 sec/batch\n",
      "Epoch 17/20  Iteration 4771/5640 Training loss: 1.0353 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4772/5640 Training loss: 1.0353 0.3934 sec/batch\n",
      "Epoch 17/20  Iteration 4773/5640 Training loss: 1.0353 0.3916 sec/batch\n",
      "Epoch 17/20  Iteration 4774/5640 Training loss: 1.0353 0.3934 sec/batch\n",
      "Epoch 17/20  Iteration 4775/5640 Training loss: 1.0354 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4776/5640 Training loss: 1.0354 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4777/5640 Training loss: 1.0354 0.3954 sec/batch\n",
      "Epoch 17/20  Iteration 4778/5640 Training loss: 1.0353 0.3949 sec/batch\n",
      "Epoch 17/20  Iteration 4779/5640 Training loss: 1.0352 0.3933 sec/batch\n",
      "Epoch 17/20  Iteration 4780/5640 Training loss: 1.0351 0.3945 sec/batch\n",
      "Epoch 17/20  Iteration 4781/5640 Training loss: 1.0351 0.3934 sec/batch\n",
      "Epoch 17/20  Iteration 4782/5640 Training loss: 1.0351 0.3931 sec/batch\n",
      "Epoch 17/20  Iteration 4783/5640 Training loss: 1.0350 0.3931 sec/batch\n",
      "Epoch 17/20  Iteration 4784/5640 Training loss: 1.0349 0.3945 sec/batch\n",
      "Epoch 17/20  Iteration 4785/5640 Training loss: 1.0349 0.3949 sec/batch\n",
      "Epoch 17/20  Iteration 4786/5640 Training loss: 1.0349 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4787/5640 Training loss: 1.0349 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4788/5640 Training loss: 1.0348 0.3924 sec/batch\n",
      "Epoch 17/20  Iteration 4789/5640 Training loss: 1.0348 0.3928 sec/batch\n",
      "Epoch 17/20  Iteration 4790/5640 Training loss: 1.0349 0.3940 sec/batch\n",
      "Epoch 17/20  Iteration 4791/5640 Training loss: 1.0350 0.3930 sec/batch\n",
      "Epoch 17/20  Iteration 4792/5640 Training loss: 1.0350 0.3943 sec/batch\n",
      "Epoch 17/20  Iteration 4793/5640 Training loss: 1.0350 0.3942 sec/batch\n",
      "Epoch 17/20  Iteration 4794/5640 Training loss: 1.0349 0.3931 sec/batch\n",
      "Epoch 18/20  Iteration 4795/5640 Training loss: 1.1394 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 4796/5640 Training loss: 1.0989 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 4797/5640 Training loss: 1.0785 0.3928 sec/batch\n",
      "Epoch 18/20  Iteration 4798/5640 Training loss: 1.0706 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 4799/5640 Training loss: 1.0667 0.3938 sec/batch\n",
      "Epoch 18/20  Iteration 4800/5640 Training loss: 1.0648 0.3942 sec/batch\n",
      "Validation loss: 1.08507 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 4801/5640 Training loss: 1.0863 0.3827 sec/batch\n",
      "Epoch 18/20  Iteration 4802/5640 Training loss: 1.0800 0.3836 sec/batch\n",
      "Epoch 18/20  Iteration 4803/5640 Training loss: 1.0727 0.3836 sec/batch\n",
      "Epoch 18/20  Iteration 4804/5640 Training loss: 1.0658 0.3814 sec/batch\n",
      "Epoch 18/20  Iteration 4805/5640 Training loss: 1.0642 0.3846 sec/batch\n",
      "Epoch 18/20  Iteration 4806/5640 Training loss: 1.0605 0.3819 sec/batch\n",
      "Epoch 18/20  Iteration 4807/5640 Training loss: 1.0583 0.3822 sec/batch\n",
      "Epoch 18/20  Iteration 4808/5640 Training loss: 1.0577 0.3811 sec/batch\n",
      "Epoch 18/20  Iteration 4809/5640 Training loss: 1.0547 0.3823 sec/batch\n",
      "Epoch 18/20  Iteration 4810/5640 Training loss: 1.0546 0.3808 sec/batch\n",
      "Epoch 18/20  Iteration 4811/5640 Training loss: 1.0541 0.3818 sec/batch\n",
      "Epoch 18/20  Iteration 4812/5640 Training loss: 1.0517 0.3814 sec/batch\n",
      "Epoch 18/20  Iteration 4813/5640 Training loss: 1.0507 0.3826 sec/batch\n",
      "Epoch 18/20  Iteration 4814/5640 Training loss: 1.0488 0.3809 sec/batch\n",
      "Epoch 18/20  Iteration 4815/5640 Training loss: 1.0483 0.3837 sec/batch\n",
      "Epoch 18/20  Iteration 4816/5640 Training loss: 1.0480 0.3842 sec/batch\n",
      "Epoch 18/20  Iteration 4817/5640 Training loss: 1.0472 0.3850 sec/batch\n",
      "Epoch 18/20  Iteration 4818/5640 Training loss: 1.0453 0.3861 sec/batch\n",
      "Epoch 18/20  Iteration 4819/5640 Training loss: 1.0449 0.3861 sec/batch\n",
      "Epoch 18/20  Iteration 4820/5640 Training loss: 1.0444 0.3848 sec/batch\n",
      "Epoch 18/20  Iteration 4821/5640 Training loss: 1.0440 0.3867 sec/batch\n",
      "Epoch 18/20  Iteration 4822/5640 Training loss: 1.0438 0.3892 sec/batch\n",
      "Epoch 18/20  Iteration 4823/5640 Training loss: 1.0428 0.3894 sec/batch\n",
      "Epoch 18/20  Iteration 4824/5640 Training loss: 1.0422 0.3927 sec/batch\n",
      "Epoch 18/20  Iteration 4825/5640 Training loss: 1.0409 0.3949 sec/batch\n",
      "Epoch 18/20  Iteration 4826/5640 Training loss: 1.0403 0.3958 sec/batch\n",
      "Epoch 18/20  Iteration 4827/5640 Training loss: 1.0395 0.3928 sec/batch\n",
      "Epoch 18/20  Iteration 4828/5640 Training loss: 1.0386 0.3920 sec/batch\n",
      "Epoch 18/20  Iteration 4829/5640 Training loss: 1.0388 0.3933 sec/batch\n",
      "Epoch 18/20  Iteration 4830/5640 Training loss: 1.0385 0.3938 sec/batch\n",
      "Epoch 18/20  Iteration 4831/5640 Training loss: 1.0375 0.3945 sec/batch\n",
      "Epoch 18/20  Iteration 4832/5640 Training loss: 1.0364 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 4833/5640 Training loss: 1.0363 0.3930 sec/batch\n",
      "Epoch 18/20  Iteration 4834/5640 Training loss: 1.0356 0.3958 sec/batch\n",
      "Epoch 18/20  Iteration 4835/5640 Training loss: 1.0349 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 4836/5640 Training loss: 1.0354 0.3932 sec/batch\n",
      "Epoch 18/20  Iteration 4837/5640 Training loss: 1.0342 0.3938 sec/batch\n",
      "Epoch 18/20  Iteration 4838/5640 Training loss: 1.0342 0.3951 sec/batch\n",
      "Epoch 18/20  Iteration 4839/5640 Training loss: 1.0337 0.3937 sec/batch\n",
      "Epoch 18/20  Iteration 4840/5640 Training loss: 1.0333 0.3948 sec/batch\n",
      "Epoch 18/20  Iteration 4841/5640 Training loss: 1.0333 0.3936 sec/batch\n",
      "Epoch 18/20  Iteration 4842/5640 Training loss: 1.0334 0.3938 sec/batch\n",
      "Epoch 18/20  Iteration 4843/5640 Training loss: 1.0335 0.3960 sec/batch\n",
      "Epoch 18/20  Iteration 4844/5640 Training loss: 1.0332 0.3925 sec/batch\n",
      "Epoch 18/20  Iteration 4845/5640 Training loss: 1.0331 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 4846/5640 Training loss: 1.0331 0.3924 sec/batch\n",
      "Epoch 18/20  Iteration 4847/5640 Training loss: 1.0328 0.3939 sec/batch\n",
      "Epoch 18/20  Iteration 4848/5640 Training loss: 1.0327 0.3943 sec/batch\n",
      "Epoch 18/20  Iteration 4849/5640 Training loss: 1.0323 0.3960 sec/batch\n",
      "Epoch 18/20  Iteration 4850/5640 Training loss: 1.0321 0.3940 sec/batch\n",
      "Validation loss: 1.08066 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 4851/5640 Training loss: 1.0355 0.3850 sec/batch\n",
      "Epoch 18/20  Iteration 4852/5640 Training loss: 1.0354 0.3816 sec/batch\n",
      "Epoch 18/20  Iteration 4853/5640 Training loss: 1.0353 0.3816 sec/batch\n",
      "Epoch 18/20  Iteration 4854/5640 Training loss: 1.0349 0.3813 sec/batch\n",
      "Epoch 18/20  Iteration 4855/5640 Training loss: 1.0346 0.3815 sec/batch\n",
      "Epoch 18/20  Iteration 4856/5640 Training loss: 1.0342 0.3838 sec/batch\n",
      "Epoch 18/20  Iteration 4857/5640 Training loss: 1.0335 0.3804 sec/batch\n",
      "Epoch 18/20  Iteration 4858/5640 Training loss: 1.0330 0.3812 sec/batch\n",
      "Epoch 18/20  Iteration 4859/5640 Training loss: 1.0332 0.3858 sec/batch\n",
      "Epoch 18/20  Iteration 4860/5640 Training loss: 1.0329 0.3844 sec/batch\n",
      "Epoch 18/20  Iteration 4861/5640 Training loss: 1.0329 0.3836 sec/batch\n",
      "Epoch 18/20  Iteration 4862/5640 Training loss: 1.0326 0.3850 sec/batch\n",
      "Epoch 18/20  Iteration 4863/5640 Training loss: 1.0324 0.3844 sec/batch\n",
      "Epoch 18/20  Iteration 4864/5640 Training loss: 1.0320 0.3846 sec/batch\n",
      "Epoch 18/20  Iteration 4865/5640 Training loss: 1.0317 0.3840 sec/batch\n",
      "Epoch 18/20  Iteration 4866/5640 Training loss: 1.0316 0.3874 sec/batch\n",
      "Epoch 18/20  Iteration 4867/5640 Training loss: 1.0317 0.3910 sec/batch\n",
      "Epoch 18/20  Iteration 4868/5640 Training loss: 1.0316 0.3896 sec/batch\n",
      "Epoch 18/20  Iteration 4869/5640 Training loss: 1.0316 0.3915 sec/batch\n",
      "Epoch 18/20  Iteration 4870/5640 Training loss: 1.0321 0.3901 sec/batch\n",
      "Epoch 18/20  Iteration 4871/5640 Training loss: 1.0320 0.3892 sec/batch\n",
      "Epoch 18/20  Iteration 4872/5640 Training loss: 1.0325 0.3894 sec/batch\n",
      "Epoch 18/20  Iteration 4873/5640 Training loss: 1.0324 0.3905 sec/batch\n",
      "Epoch 18/20  Iteration 4874/5640 Training loss: 1.0324 0.3894 sec/batch\n",
      "Epoch 18/20  Iteration 4875/5640 Training loss: 1.0326 0.3906 sec/batch\n",
      "Epoch 18/20  Iteration 4876/5640 Training loss: 1.0325 0.3894 sec/batch\n",
      "Epoch 18/20  Iteration 4877/5640 Training loss: 1.0322 0.4341 sec/batch\n",
      "Epoch 18/20  Iteration 4878/5640 Training loss: 1.0320 0.3896 sec/batch\n",
      "Epoch 18/20  Iteration 4879/5640 Training loss: 1.0317 0.3898 sec/batch\n",
      "Epoch 18/20  Iteration 4880/5640 Training loss: 1.0315 0.3897 sec/batch\n",
      "Epoch 18/20  Iteration 4881/5640 Training loss: 1.0312 0.3908 sec/batch\n",
      "Epoch 18/20  Iteration 4882/5640 Training loss: 1.0309 0.3907 sec/batch\n",
      "Epoch 18/20  Iteration 4883/5640 Training loss: 1.0307 0.3886 sec/batch\n",
      "Epoch 18/20  Iteration 4884/5640 Training loss: 1.0305 0.3891 sec/batch\n",
      "Epoch 18/20  Iteration 4885/5640 Training loss: 1.0305 0.3889 sec/batch\n",
      "Epoch 18/20  Iteration 4886/5640 Training loss: 1.0305 0.3909 sec/batch\n",
      "Epoch 18/20  Iteration 4887/5640 Training loss: 1.0306 0.3904 sec/batch\n",
      "Epoch 18/20  Iteration 4888/5640 Training loss: 1.0304 0.3897 sec/batch\n",
      "Epoch 18/20  Iteration 4889/5640 Training loss: 1.0302 0.3937 sec/batch\n",
      "Epoch 18/20  Iteration 4890/5640 Training loss: 1.0301 0.3956 sec/batch\n",
      "Epoch 18/20  Iteration 4891/5640 Training loss: 1.0301 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 4892/5640 Training loss: 1.0302 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 4893/5640 Training loss: 1.0301 0.3931 sec/batch\n",
      "Epoch 18/20  Iteration 4894/5640 Training loss: 1.0299 0.3942 sec/batch\n",
      "Epoch 18/20  Iteration 4895/5640 Training loss: 1.0297 0.3938 sec/batch\n",
      "Epoch 18/20  Iteration 4896/5640 Training loss: 1.0296 0.3928 sec/batch\n",
      "Epoch 18/20  Iteration 4897/5640 Training loss: 1.0295 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 4898/5640 Training loss: 1.0293 0.3951 sec/batch\n",
      "Epoch 18/20  Iteration 4899/5640 Training loss: 1.0290 0.3932 sec/batch\n",
      "Epoch 18/20  Iteration 4900/5640 Training loss: 1.0286 0.3948 sec/batch\n",
      "Validation loss: 1.0914 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 4901/5640 Training loss: 1.0303 0.3822 sec/batch\n",
      "Epoch 18/20  Iteration 4902/5640 Training loss: 1.0300 0.3823 sec/batch\n",
      "Epoch 18/20  Iteration 4903/5640 Training loss: 1.0298 0.3820 sec/batch\n",
      "Epoch 18/20  Iteration 4904/5640 Training loss: 1.0296 0.3818 sec/batch\n",
      "Epoch 18/20  Iteration 4905/5640 Training loss: 1.0293 0.3818 sec/batch\n",
      "Epoch 18/20  Iteration 4906/5640 Training loss: 1.0293 0.3825 sec/batch\n",
      "Epoch 18/20  Iteration 4907/5640 Training loss: 1.0291 0.3839 sec/batch\n",
      "Epoch 18/20  Iteration 4908/5640 Training loss: 1.0290 0.3831 sec/batch\n",
      "Epoch 18/20  Iteration 4909/5640 Training loss: 1.0290 0.3816 sec/batch\n",
      "Epoch 18/20  Iteration 4910/5640 Training loss: 1.0289 0.3816 sec/batch\n",
      "Epoch 18/20  Iteration 4911/5640 Training loss: 1.0289 0.3821 sec/batch\n",
      "Epoch 18/20  Iteration 4912/5640 Training loss: 1.0287 0.3818 sec/batch\n",
      "Epoch 18/20  Iteration 4913/5640 Training loss: 1.0286 0.3829 sec/batch\n",
      "Epoch 18/20  Iteration 4914/5640 Training loss: 1.0284 0.3828 sec/batch\n",
      "Epoch 18/20  Iteration 4915/5640 Training loss: 1.0283 0.3827 sec/batch\n",
      "Epoch 18/20  Iteration 4916/5640 Training loss: 1.0283 0.3817 sec/batch\n",
      "Epoch 18/20  Iteration 4917/5640 Training loss: 1.0282 0.3843 sec/batch\n",
      "Epoch 18/20  Iteration 4918/5640 Training loss: 1.0282 0.3839 sec/batch\n",
      "Epoch 18/20  Iteration 4919/5640 Training loss: 1.0285 0.3847 sec/batch\n",
      "Epoch 18/20  Iteration 4920/5640 Training loss: 1.0281 0.3864 sec/batch\n",
      "Epoch 18/20  Iteration 4921/5640 Training loss: 1.0280 0.3873 sec/batch\n",
      "Epoch 18/20  Iteration 4922/5640 Training loss: 1.0278 0.3878 sec/batch\n",
      "Epoch 18/20  Iteration 4923/5640 Training loss: 1.0276 0.3890 sec/batch\n",
      "Epoch 18/20  Iteration 4924/5640 Training loss: 1.0275 0.3896 sec/batch\n",
      "Epoch 18/20  Iteration 4925/5640 Training loss: 1.0273 0.3890 sec/batch\n",
      "Epoch 18/20  Iteration 4926/5640 Training loss: 1.0270 0.3904 sec/batch\n",
      "Epoch 18/20  Iteration 4927/5640 Training loss: 1.0267 0.3892 sec/batch\n",
      "Epoch 18/20  Iteration 4928/5640 Training loss: 1.0265 0.3903 sec/batch\n",
      "Epoch 18/20  Iteration 4929/5640 Training loss: 1.0265 0.3913 sec/batch\n",
      "Epoch 18/20  Iteration 4930/5640 Training loss: 1.0264 0.3904 sec/batch\n",
      "Epoch 18/20  Iteration 4931/5640 Training loss: 1.0264 0.3917 sec/batch\n",
      "Epoch 18/20  Iteration 4932/5640 Training loss: 1.0262 0.3914 sec/batch\n",
      "Epoch 18/20  Iteration 4933/5640 Training loss: 1.0261 0.3899 sec/batch\n",
      "Epoch 18/20  Iteration 4934/5640 Training loss: 1.0261 0.3898 sec/batch\n",
      "Epoch 18/20  Iteration 4935/5640 Training loss: 1.0260 0.3932 sec/batch\n",
      "Epoch 18/20  Iteration 4936/5640 Training loss: 1.0261 0.3938 sec/batch\n",
      "Epoch 18/20  Iteration 4937/5640 Training loss: 1.0261 0.3941 sec/batch\n",
      "Epoch 18/20  Iteration 4938/5640 Training loss: 1.0259 0.3948 sec/batch\n",
      "Epoch 18/20  Iteration 4939/5640 Training loss: 1.0256 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 4940/5640 Training loss: 1.0253 0.3930 sec/batch\n",
      "Epoch 18/20  Iteration 4941/5640 Training loss: 1.0251 0.3928 sec/batch\n",
      "Epoch 18/20  Iteration 4942/5640 Training loss: 1.0251 0.3937 sec/batch\n",
      "Epoch 18/20  Iteration 4943/5640 Training loss: 1.0250 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 4944/5640 Training loss: 1.0248 0.3938 sec/batch\n",
      "Epoch 18/20  Iteration 4945/5640 Training loss: 1.0247 0.3946 sec/batch\n",
      "Epoch 18/20  Iteration 4946/5640 Training loss: 1.0247 0.3944 sec/batch\n",
      "Epoch 18/20  Iteration 4947/5640 Training loss: 1.0249 0.3933 sec/batch\n",
      "Epoch 18/20  Iteration 4948/5640 Training loss: 1.0250 0.3936 sec/batch\n",
      "Epoch 18/20  Iteration 4949/5640 Training loss: 1.0250 0.3944 sec/batch\n",
      "Epoch 18/20  Iteration 4950/5640 Training loss: 1.0250 0.3928 sec/batch\n",
      "Validation loss: 1.0837 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 4951/5640 Training loss: 1.0260 0.3814 sec/batch\n",
      "Epoch 18/20  Iteration 4952/5640 Training loss: 1.0260 0.3823 sec/batch\n",
      "Epoch 18/20  Iteration 4953/5640 Training loss: 1.0259 0.3822 sec/batch\n",
      "Epoch 18/20  Iteration 4954/5640 Training loss: 1.0258 0.3834 sec/batch\n",
      "Epoch 18/20  Iteration 4955/5640 Training loss: 1.0257 0.3817 sec/batch\n",
      "Epoch 18/20  Iteration 4956/5640 Training loss: 1.0256 0.3827 sec/batch\n",
      "Epoch 18/20  Iteration 4957/5640 Training loss: 1.0255 0.3810 sec/batch\n",
      "Epoch 18/20  Iteration 4958/5640 Training loss: 1.0254 0.3860 sec/batch\n",
      "Epoch 18/20  Iteration 4959/5640 Training loss: 1.0254 0.3868 sec/batch\n",
      "Epoch 18/20  Iteration 4960/5640 Training loss: 1.0255 0.3866 sec/batch\n",
      "Epoch 18/20  Iteration 4961/5640 Training loss: 1.0254 0.3864 sec/batch\n",
      "Epoch 18/20  Iteration 4962/5640 Training loss: 1.0255 0.3875 sec/batch\n",
      "Epoch 18/20  Iteration 4963/5640 Training loss: 1.0255 0.3870 sec/batch\n",
      "Epoch 18/20  Iteration 4964/5640 Training loss: 1.0255 0.3885 sec/batch\n",
      "Epoch 18/20  Iteration 4965/5640 Training loss: 1.0255 0.3865 sec/batch\n",
      "Epoch 18/20  Iteration 4966/5640 Training loss: 1.0254 0.3893 sec/batch\n",
      "Epoch 18/20  Iteration 4967/5640 Training loss: 1.0253 0.3901 sec/batch\n",
      "Epoch 18/20  Iteration 4968/5640 Training loss: 1.0253 0.3899 sec/batch\n",
      "Epoch 18/20  Iteration 4969/5640 Training loss: 1.0253 0.3895 sec/batch\n",
      "Epoch 18/20  Iteration 4970/5640 Training loss: 1.0253 0.3907 sec/batch\n",
      "Epoch 18/20  Iteration 4971/5640 Training loss: 1.0253 0.3927 sec/batch\n",
      "Epoch 18/20  Iteration 4972/5640 Training loss: 1.0251 0.3897 sec/batch\n",
      "Epoch 18/20  Iteration 4973/5640 Training loss: 1.0253 0.3892 sec/batch\n",
      "Epoch 18/20  Iteration 4974/5640 Training loss: 1.0253 0.3899 sec/batch\n",
      "Epoch 18/20  Iteration 4975/5640 Training loss: 1.0253 0.3898 sec/batch\n",
      "Epoch 18/20  Iteration 4976/5640 Training loss: 1.0254 0.3895 sec/batch\n",
      "Epoch 18/20  Iteration 4977/5640 Training loss: 1.0254 0.3905 sec/batch\n",
      "Epoch 18/20  Iteration 4978/5640 Training loss: 1.0252 0.3898 sec/batch\n",
      "Epoch 18/20  Iteration 4979/5640 Training loss: 1.0252 0.3936 sec/batch\n",
      "Epoch 18/20  Iteration 4980/5640 Training loss: 1.0250 0.3924 sec/batch\n",
      "Epoch 18/20  Iteration 4981/5640 Training loss: 1.0247 0.3931 sec/batch\n",
      "Epoch 18/20  Iteration 4982/5640 Training loss: 1.0245 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 4983/5640 Training loss: 1.0244 0.3950 sec/batch\n",
      "Epoch 18/20  Iteration 4984/5640 Training loss: 1.0242 0.3921 sec/batch\n",
      "Epoch 18/20  Iteration 4985/5640 Training loss: 1.0242 0.3914 sec/batch\n",
      "Epoch 18/20  Iteration 4986/5640 Training loss: 1.0241 0.3897 sec/batch\n",
      "Epoch 18/20  Iteration 4987/5640 Training loss: 1.0239 0.3911 sec/batch\n",
      "Epoch 18/20  Iteration 4988/5640 Training loss: 1.0239 0.3897 sec/batch\n",
      "Epoch 18/20  Iteration 4989/5640 Training loss: 1.0240 0.3896 sec/batch\n",
      "Epoch 18/20  Iteration 4990/5640 Training loss: 1.0240 0.3900 sec/batch\n",
      "Epoch 18/20  Iteration 4991/5640 Training loss: 1.0239 0.3908 sec/batch\n",
      "Epoch 18/20  Iteration 4992/5640 Training loss: 1.0240 0.3899 sec/batch\n",
      "Epoch 18/20  Iteration 4993/5640 Training loss: 1.0239 0.3901 sec/batch\n",
      "Epoch 18/20  Iteration 4994/5640 Training loss: 1.0238 0.3900 sec/batch\n",
      "Epoch 18/20  Iteration 4995/5640 Training loss: 1.0238 0.3901 sec/batch\n",
      "Epoch 18/20  Iteration 4996/5640 Training loss: 1.0237 0.3905 sec/batch\n",
      "Epoch 18/20  Iteration 4997/5640 Training loss: 1.0237 0.3901 sec/batch\n",
      "Epoch 18/20  Iteration 4998/5640 Training loss: 1.0237 0.3907 sec/batch\n",
      "Epoch 18/20  Iteration 4999/5640 Training loss: 1.0235 0.3895 sec/batch\n",
      "Epoch 18/20  Iteration 5000/5640 Training loss: 1.0235 0.3906 sec/batch\n",
      "Validation loss: 1.0882 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 5001/5640 Training loss: 1.0243 0.3824 sec/batch\n",
      "Epoch 18/20  Iteration 5002/5640 Training loss: 1.0242 0.3835 sec/batch\n",
      "Epoch 18/20  Iteration 5003/5640 Training loss: 1.0243 0.3829 sec/batch\n",
      "Epoch 18/20  Iteration 5004/5640 Training loss: 1.0242 0.3853 sec/batch\n",
      "Epoch 18/20  Iteration 5005/5640 Training loss: 1.0240 0.3810 sec/batch\n",
      "Epoch 18/20  Iteration 5006/5640 Training loss: 1.0240 0.3823 sec/batch\n",
      "Epoch 18/20  Iteration 5007/5640 Training loss: 1.0240 0.3818 sec/batch\n",
      "Epoch 18/20  Iteration 5008/5640 Training loss: 1.0240 0.3826 sec/batch\n",
      "Epoch 18/20  Iteration 5009/5640 Training loss: 1.0239 0.3862 sec/batch\n",
      "Epoch 18/20  Iteration 5010/5640 Training loss: 1.0239 0.3877 sec/batch\n",
      "Epoch 18/20  Iteration 5011/5640 Training loss: 1.0238 0.3878 sec/batch\n",
      "Epoch 18/20  Iteration 5012/5640 Training loss: 1.0236 0.3886 sec/batch\n",
      "Epoch 18/20  Iteration 5013/5640 Training loss: 1.0235 0.3861 sec/batch\n",
      "Epoch 18/20  Iteration 5014/5640 Training loss: 1.0234 0.3881 sec/batch\n",
      "Epoch 18/20  Iteration 5015/5640 Training loss: 1.0233 0.3865 sec/batch\n",
      "Epoch 18/20  Iteration 5016/5640 Training loss: 1.0232 0.3878 sec/batch\n",
      "Epoch 18/20  Iteration 5017/5640 Training loss: 1.0231 0.3896 sec/batch\n",
      "Epoch 18/20  Iteration 5018/5640 Training loss: 1.0232 0.3901 sec/batch\n",
      "Epoch 18/20  Iteration 5019/5640 Training loss: 1.0231 0.3927 sec/batch\n",
      "Epoch 18/20  Iteration 5020/5640 Training loss: 1.0231 0.3951 sec/batch\n",
      "Epoch 18/20  Iteration 5021/5640 Training loss: 1.0231 0.3927 sec/batch\n",
      "Epoch 18/20  Iteration 5022/5640 Training loss: 1.0232 0.3961 sec/batch\n",
      "Epoch 18/20  Iteration 5023/5640 Training loss: 1.0231 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 5024/5640 Training loss: 1.0231 0.3945 sec/batch\n",
      "Epoch 18/20  Iteration 5025/5640 Training loss: 1.0231 0.3947 sec/batch\n",
      "Epoch 18/20  Iteration 5026/5640 Training loss: 1.0230 0.3936 sec/batch\n",
      "Epoch 18/20  Iteration 5027/5640 Training loss: 1.0230 0.3940 sec/batch\n",
      "Epoch 18/20  Iteration 5028/5640 Training loss: 1.0229 0.3941 sec/batch\n",
      "Epoch 18/20  Iteration 5029/5640 Training loss: 1.0228 0.3935 sec/batch\n",
      "Epoch 18/20  Iteration 5030/5640 Training loss: 1.0229 0.3925 sec/batch\n",
      "Epoch 18/20  Iteration 5031/5640 Training loss: 1.0229 0.3931 sec/batch\n",
      "Epoch 18/20  Iteration 5032/5640 Training loss: 1.0229 0.3935 sec/batch\n",
      "Epoch 18/20  Iteration 5033/5640 Training loss: 1.0229 0.3949 sec/batch\n",
      "Epoch 18/20  Iteration 5034/5640 Training loss: 1.0228 0.3954 sec/batch\n",
      "Epoch 18/20  Iteration 5035/5640 Training loss: 1.0228 0.3945 sec/batch\n",
      "Epoch 18/20  Iteration 5036/5640 Training loss: 1.0228 0.3923 sec/batch\n",
      "Epoch 18/20  Iteration 5037/5640 Training loss: 1.0227 0.3937 sec/batch\n",
      "Epoch 18/20  Iteration 5038/5640 Training loss: 1.0226 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 5039/5640 Training loss: 1.0225 0.3939 sec/batch\n",
      "Epoch 18/20  Iteration 5040/5640 Training loss: 1.0225 0.3936 sec/batch\n",
      "Epoch 18/20  Iteration 5041/5640 Training loss: 1.0224 0.3934 sec/batch\n",
      "Epoch 18/20  Iteration 5042/5640 Training loss: 1.0224 0.3932 sec/batch\n",
      "Epoch 18/20  Iteration 5043/5640 Training loss: 1.0224 0.3956 sec/batch\n",
      "Epoch 18/20  Iteration 5044/5640 Training loss: 1.0224 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 5045/5640 Training loss: 1.0223 0.3931 sec/batch\n",
      "Epoch 18/20  Iteration 5046/5640 Training loss: 1.0222 0.3937 sec/batch\n",
      "Epoch 18/20  Iteration 5047/5640 Training loss: 1.0221 0.3951 sec/batch\n",
      "Epoch 18/20  Iteration 5048/5640 Training loss: 1.0221 0.3923 sec/batch\n",
      "Epoch 18/20  Iteration 5049/5640 Training loss: 1.0220 0.3939 sec/batch\n",
      "Epoch 18/20  Iteration 5050/5640 Training loss: 1.0219 0.3933 sec/batch\n",
      "Validation loss: 1.09036 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 5051/5640 Training loss: 1.0227 0.3819 sec/batch\n",
      "Epoch 18/20  Iteration 5052/5640 Training loss: 1.0227 0.3816 sec/batch\n",
      "Epoch 18/20  Iteration 5053/5640 Training loss: 1.0227 0.3831 sec/batch\n",
      "Epoch 18/20  Iteration 5054/5640 Training loss: 1.0227 0.3813 sec/batch\n",
      "Epoch 18/20  Iteration 5055/5640 Training loss: 1.0227 0.3812 sec/batch\n",
      "Epoch 18/20  Iteration 5056/5640 Training loss: 1.0227 0.3822 sec/batch\n",
      "Epoch 18/20  Iteration 5057/5640 Training loss: 1.0227 0.3814 sec/batch\n",
      "Epoch 18/20  Iteration 5058/5640 Training loss: 1.0227 0.3838 sec/batch\n",
      "Epoch 18/20  Iteration 5059/5640 Training loss: 1.0227 0.3845 sec/batch\n",
      "Epoch 18/20  Iteration 5060/5640 Training loss: 1.0227 0.3870 sec/batch\n",
      "Epoch 18/20  Iteration 5061/5640 Training loss: 1.0227 0.3854 sec/batch\n",
      "Epoch 18/20  Iteration 5062/5640 Training loss: 1.0225 0.3844 sec/batch\n",
      "Epoch 18/20  Iteration 5063/5640 Training loss: 1.0225 0.3843 sec/batch\n",
      "Epoch 18/20  Iteration 5064/5640 Training loss: 1.0225 0.3856 sec/batch\n",
      "Epoch 18/20  Iteration 5065/5640 Training loss: 1.0224 0.3865 sec/batch\n",
      "Epoch 18/20  Iteration 5066/5640 Training loss: 1.0223 0.3892 sec/batch\n",
      "Epoch 18/20  Iteration 5067/5640 Training loss: 1.0223 0.3914 sec/batch\n",
      "Epoch 18/20  Iteration 5068/5640 Training loss: 1.0222 0.3922 sec/batch\n",
      "Epoch 18/20  Iteration 5069/5640 Training loss: 1.0222 0.3898 sec/batch\n",
      "Epoch 18/20  Iteration 5070/5640 Training loss: 1.0222 0.3929 sec/batch\n",
      "Epoch 18/20  Iteration 5071/5640 Training loss: 1.0222 0.3896 sec/batch\n",
      "Epoch 18/20  Iteration 5072/5640 Training loss: 1.0223 0.3911 sec/batch\n",
      "Epoch 18/20  Iteration 5073/5640 Training loss: 1.0224 0.3885 sec/batch\n",
      "Epoch 18/20  Iteration 5074/5640 Training loss: 1.0224 0.3904 sec/batch\n",
      "Epoch 18/20  Iteration 5075/5640 Training loss: 1.0223 0.3902 sec/batch\n",
      "Epoch 18/20  Iteration 5076/5640 Training loss: 1.0223 0.3910 sec/batch\n",
      "Epoch 19/20  Iteration 5077/5640 Training loss: 1.1350 0.3892 sec/batch\n",
      "Epoch 19/20  Iteration 5078/5640 Training loss: 1.0862 0.3899 sec/batch\n",
      "Epoch 19/20  Iteration 5079/5640 Training loss: 1.0658 0.3896 sec/batch\n",
      "Epoch 19/20  Iteration 5080/5640 Training loss: 1.0541 0.3896 sec/batch\n",
      "Epoch 19/20  Iteration 5081/5640 Training loss: 1.0490 0.3901 sec/batch\n",
      "Epoch 19/20  Iteration 5082/5640 Training loss: 1.0484 0.3898 sec/batch\n",
      "Epoch 19/20  Iteration 5083/5640 Training loss: 1.0426 0.3908 sec/batch\n",
      "Epoch 19/20  Iteration 5084/5640 Training loss: 1.0394 0.3914 sec/batch\n",
      "Epoch 19/20  Iteration 5085/5640 Training loss: 1.0350 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5086/5640 Training loss: 1.0313 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5087/5640 Training loss: 1.0321 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5088/5640 Training loss: 1.0310 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5089/5640 Training loss: 1.0298 0.3942 sec/batch\n",
      "Epoch 19/20  Iteration 5090/5640 Training loss: 1.0303 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5091/5640 Training loss: 1.0288 0.3952 sec/batch\n",
      "Epoch 19/20  Iteration 5092/5640 Training loss: 1.0303 0.3929 sec/batch\n",
      "Epoch 19/20  Iteration 5093/5640 Training loss: 1.0301 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5094/5640 Training loss: 1.0286 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5095/5640 Training loss: 1.0282 0.3928 sec/batch\n",
      "Epoch 19/20  Iteration 5096/5640 Training loss: 1.0261 0.3945 sec/batch\n",
      "Epoch 19/20  Iteration 5097/5640 Training loss: 1.0261 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5098/5640 Training loss: 1.0260 0.3952 sec/batch\n",
      "Epoch 19/20  Iteration 5099/5640 Training loss: 1.0255 0.3946 sec/batch\n",
      "Epoch 19/20  Iteration 5100/5640 Training loss: 1.0241 0.3935 sec/batch\n",
      "Validation loss: 1.08318 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 5101/5640 Training loss: 1.0320 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 5102/5640 Training loss: 1.0313 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5103/5640 Training loss: 1.0311 0.3821 sec/batch\n",
      "Epoch 19/20  Iteration 5104/5640 Training loss: 1.0307 0.3818 sec/batch\n",
      "Epoch 19/20  Iteration 5105/5640 Training loss: 1.0300 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5106/5640 Training loss: 1.0296 0.3822 sec/batch\n",
      "Epoch 19/20  Iteration 5107/5640 Training loss: 1.0288 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 5108/5640 Training loss: 1.0285 0.3831 sec/batch\n",
      "Epoch 19/20  Iteration 5109/5640 Training loss: 1.0277 0.3853 sec/batch\n",
      "Epoch 19/20  Iteration 5110/5640 Training loss: 1.0270 0.3839 sec/batch\n",
      "Epoch 19/20  Iteration 5111/5640 Training loss: 1.0273 0.3858 sec/batch\n",
      "Epoch 19/20  Iteration 5112/5640 Training loss: 1.0270 0.3843 sec/batch\n",
      "Epoch 19/20  Iteration 5113/5640 Training loss: 1.0260 0.3844 sec/batch\n",
      "Epoch 19/20  Iteration 5114/5640 Training loss: 1.0250 0.3871 sec/batch\n",
      "Epoch 19/20  Iteration 5115/5640 Training loss: 1.0247 0.3872 sec/batch\n",
      "Epoch 19/20  Iteration 5116/5640 Training loss: 1.0237 0.3880 sec/batch\n",
      "Epoch 19/20  Iteration 5117/5640 Training loss: 1.0230 0.3909 sec/batch\n",
      "Epoch 19/20  Iteration 5118/5640 Training loss: 1.0234 0.3890 sec/batch\n",
      "Epoch 19/20  Iteration 5119/5640 Training loss: 1.0225 0.3924 sec/batch\n",
      "Epoch 19/20  Iteration 5120/5640 Training loss: 1.0224 0.3929 sec/batch\n",
      "Epoch 19/20  Iteration 5121/5640 Training loss: 1.0221 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5122/5640 Training loss: 1.0221 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5123/5640 Training loss: 1.0221 0.3948 sec/batch\n",
      "Epoch 19/20  Iteration 5124/5640 Training loss: 1.0223 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5125/5640 Training loss: 1.0223 0.3952 sec/batch\n",
      "Epoch 19/20  Iteration 5126/5640 Training loss: 1.0219 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5127/5640 Training loss: 1.0217 0.3931 sec/batch\n",
      "Epoch 19/20  Iteration 5128/5640 Training loss: 1.0216 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5129/5640 Training loss: 1.0214 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5130/5640 Training loss: 1.0213 0.3944 sec/batch\n",
      "Epoch 19/20  Iteration 5131/5640 Training loss: 1.0210 0.3950 sec/batch\n",
      "Epoch 19/20  Iteration 5132/5640 Training loss: 1.0208 0.3941 sec/batch\n",
      "Epoch 19/20  Iteration 5133/5640 Training loss: 1.0208 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5134/5640 Training loss: 1.0207 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5135/5640 Training loss: 1.0206 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5136/5640 Training loss: 1.0203 0.3946 sec/batch\n",
      "Epoch 19/20  Iteration 5137/5640 Training loss: 1.0201 0.3929 sec/batch\n",
      "Epoch 19/20  Iteration 5138/5640 Training loss: 1.0197 0.3940 sec/batch\n",
      "Epoch 19/20  Iteration 5139/5640 Training loss: 1.0192 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5140/5640 Training loss: 1.0185 0.3941 sec/batch\n",
      "Epoch 19/20  Iteration 5141/5640 Training loss: 1.0186 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5142/5640 Training loss: 1.0183 0.3945 sec/batch\n",
      "Epoch 19/20  Iteration 5143/5640 Training loss: 1.0184 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5144/5640 Training loss: 1.0180 0.3926 sec/batch\n",
      "Epoch 19/20  Iteration 5145/5640 Training loss: 1.0177 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5146/5640 Training loss: 1.0173 0.3944 sec/batch\n",
      "Epoch 19/20  Iteration 5147/5640 Training loss: 1.0170 0.3946 sec/batch\n",
      "Epoch 19/20  Iteration 5148/5640 Training loss: 1.0170 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5149/5640 Training loss: 1.0170 0.3946 sec/batch\n",
      "Epoch 19/20  Iteration 5150/5640 Training loss: 1.0169 0.3940 sec/batch\n",
      "Validation loss: 1.08183 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 5151/5640 Training loss: 1.0199 0.3812 sec/batch\n",
      "Epoch 19/20  Iteration 5152/5640 Training loss: 1.0203 0.3824 sec/batch\n",
      "Epoch 19/20  Iteration 5153/5640 Training loss: 1.0204 0.3823 sec/batch\n",
      "Epoch 19/20  Iteration 5154/5640 Training loss: 1.0208 0.3830 sec/batch\n",
      "Epoch 19/20  Iteration 5155/5640 Training loss: 1.0208 0.3820 sec/batch\n",
      "Epoch 19/20  Iteration 5156/5640 Training loss: 1.0208 0.3817 sec/batch\n",
      "Epoch 19/20  Iteration 5157/5640 Training loss: 1.0211 0.3834 sec/batch\n",
      "Epoch 19/20  Iteration 5158/5640 Training loss: 1.0209 0.3829 sec/batch\n",
      "Epoch 19/20  Iteration 5159/5640 Training loss: 1.0207 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5160/5640 Training loss: 1.0205 0.3810 sec/batch\n",
      "Epoch 19/20  Iteration 5161/5640 Training loss: 1.0203 0.3820 sec/batch\n",
      "Epoch 19/20  Iteration 5162/5640 Training loss: 1.0202 0.3849 sec/batch\n",
      "Epoch 19/20  Iteration 5163/5640 Training loss: 1.0199 0.3837 sec/batch\n",
      "Epoch 19/20  Iteration 5164/5640 Training loss: 1.0197 0.3846 sec/batch\n",
      "Epoch 19/20  Iteration 5165/5640 Training loss: 1.0195 0.3854 sec/batch\n",
      "Epoch 19/20  Iteration 5166/5640 Training loss: 1.0192 0.3849 sec/batch\n",
      "Epoch 19/20  Iteration 5167/5640 Training loss: 1.0193 0.3844 sec/batch\n",
      "Epoch 19/20  Iteration 5168/5640 Training loss: 1.0192 0.3875 sec/batch\n",
      "Epoch 19/20  Iteration 5169/5640 Training loss: 1.0192 0.3879 sec/batch\n",
      "Epoch 19/20  Iteration 5170/5640 Training loss: 1.0190 0.3897 sec/batch\n",
      "Epoch 19/20  Iteration 5171/5640 Training loss: 1.0188 0.3927 sec/batch\n",
      "Epoch 19/20  Iteration 5172/5640 Training loss: 1.0188 0.3942 sec/batch\n",
      "Epoch 19/20  Iteration 5173/5640 Training loss: 1.0188 0.3952 sec/batch\n",
      "Epoch 19/20  Iteration 5174/5640 Training loss: 1.0188 0.3947 sec/batch\n",
      "Epoch 19/20  Iteration 5175/5640 Training loss: 1.0188 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5176/5640 Training loss: 1.0186 0.3931 sec/batch\n",
      "Epoch 19/20  Iteration 5177/5640 Training loss: 1.0184 0.3947 sec/batch\n",
      "Epoch 19/20  Iteration 5178/5640 Training loss: 1.0184 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5179/5640 Training loss: 1.0182 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5180/5640 Training loss: 1.0181 0.3940 sec/batch\n",
      "Epoch 19/20  Iteration 5181/5640 Training loss: 1.0178 0.3940 sec/batch\n",
      "Epoch 19/20  Iteration 5182/5640 Training loss: 1.0175 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5183/5640 Training loss: 1.0173 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5184/5640 Training loss: 1.0170 0.3945 sec/batch\n",
      "Epoch 19/20  Iteration 5185/5640 Training loss: 1.0169 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5186/5640 Training loss: 1.0166 0.3936 sec/batch\n",
      "Epoch 19/20  Iteration 5187/5640 Training loss: 1.0164 0.3960 sec/batch\n",
      "Epoch 19/20  Iteration 5188/5640 Training loss: 1.0164 0.3947 sec/batch\n",
      "Epoch 19/20  Iteration 5189/5640 Training loss: 1.0162 0.3941 sec/batch\n",
      "Epoch 19/20  Iteration 5190/5640 Training loss: 1.0161 0.3931 sec/batch\n",
      "Epoch 19/20  Iteration 5191/5640 Training loss: 1.0162 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5192/5640 Training loss: 1.0161 0.3929 sec/batch\n",
      "Epoch 19/20  Iteration 5193/5640 Training loss: 1.0160 0.3929 sec/batch\n",
      "Epoch 19/20  Iteration 5194/5640 Training loss: 1.0158 0.3938 sec/batch\n",
      "Epoch 19/20  Iteration 5195/5640 Training loss: 1.0157 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5196/5640 Training loss: 1.0154 0.3957 sec/batch\n",
      "Epoch 19/20  Iteration 5197/5640 Training loss: 1.0153 0.3929 sec/batch\n",
      "Epoch 19/20  Iteration 5198/5640 Training loss: 1.0153 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5199/5640 Training loss: 1.0152 0.3944 sec/batch\n",
      "Epoch 19/20  Iteration 5200/5640 Training loss: 1.0151 0.3957 sec/batch\n",
      "Validation loss: 1.08813 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 5201/5640 Training loss: 1.0169 0.3824 sec/batch\n",
      "Epoch 19/20  Iteration 5202/5640 Training loss: 1.0166 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5203/5640 Training loss: 1.0166 0.3825 sec/batch\n",
      "Epoch 19/20  Iteration 5204/5640 Training loss: 1.0162 0.3819 sec/batch\n",
      "Epoch 19/20  Iteration 5205/5640 Training loss: 1.0160 0.3829 sec/batch\n",
      "Epoch 19/20  Iteration 5206/5640 Training loss: 1.0158 0.3825 sec/batch\n",
      "Epoch 19/20  Iteration 5207/5640 Training loss: 1.0157 0.3823 sec/batch\n",
      "Epoch 19/20  Iteration 5208/5640 Training loss: 1.0154 0.3817 sec/batch\n",
      "Epoch 19/20  Iteration 5209/5640 Training loss: 1.0151 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 5210/5640 Training loss: 1.0148 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 5211/5640 Training loss: 1.0147 0.3819 sec/batch\n",
      "Epoch 19/20  Iteration 5212/5640 Training loss: 1.0147 0.3836 sec/batch\n",
      "Epoch 19/20  Iteration 5213/5640 Training loss: 1.0146 0.3839 sec/batch\n",
      "Epoch 19/20  Iteration 5214/5640 Training loss: 1.0144 0.3859 sec/batch\n",
      "Epoch 19/20  Iteration 5215/5640 Training loss: 1.0143 0.3873 sec/batch\n",
      "Epoch 19/20  Iteration 5216/5640 Training loss: 1.0143 0.3883 sec/batch\n",
      "Epoch 19/20  Iteration 5217/5640 Training loss: 1.0142 0.3904 sec/batch\n",
      "Epoch 19/20  Iteration 5218/5640 Training loss: 1.0142 0.3904 sec/batch\n",
      "Epoch 19/20  Iteration 5219/5640 Training loss: 1.0143 0.3900 sec/batch\n",
      "Epoch 19/20  Iteration 5220/5640 Training loss: 1.0142 0.3908 sec/batch\n",
      "Epoch 19/20  Iteration 5221/5640 Training loss: 1.0138 0.3907 sec/batch\n",
      "Epoch 19/20  Iteration 5222/5640 Training loss: 1.0136 0.3903 sec/batch\n",
      "Epoch 19/20  Iteration 5223/5640 Training loss: 1.0133 0.3898 sec/batch\n",
      "Epoch 19/20  Iteration 5224/5640 Training loss: 1.0133 0.3902 sec/batch\n",
      "Epoch 19/20  Iteration 5225/5640 Training loss: 1.0132 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5226/5640 Training loss: 1.0130 0.3895 sec/batch\n",
      "Epoch 19/20  Iteration 5227/5640 Training loss: 1.0129 0.3900 sec/batch\n",
      "Epoch 19/20  Iteration 5228/5640 Training loss: 1.0130 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5229/5640 Training loss: 1.0132 0.3948 sec/batch\n",
      "Epoch 19/20  Iteration 5230/5640 Training loss: 1.0131 0.3947 sec/batch\n",
      "Epoch 19/20  Iteration 5231/5640 Training loss: 1.0132 0.3926 sec/batch\n",
      "Epoch 19/20  Iteration 5232/5640 Training loss: 1.0132 0.3943 sec/batch\n",
      "Epoch 19/20  Iteration 5233/5640 Training loss: 1.0130 0.3945 sec/batch\n",
      "Epoch 19/20  Iteration 5234/5640 Training loss: 1.0131 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5235/5640 Training loss: 1.0130 0.3940 sec/batch\n",
      "Epoch 19/20  Iteration 5236/5640 Training loss: 1.0128 0.3942 sec/batch\n",
      "Epoch 19/20  Iteration 5237/5640 Training loss: 1.0128 0.3955 sec/batch\n",
      "Epoch 19/20  Iteration 5238/5640 Training loss: 1.0126 0.3954 sec/batch\n",
      "Epoch 19/20  Iteration 5239/5640 Training loss: 1.0125 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5240/5640 Training loss: 1.0125 0.3928 sec/batch\n",
      "Epoch 19/20  Iteration 5241/5640 Training loss: 1.0125 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5242/5640 Training loss: 1.0127 0.3938 sec/batch\n",
      "Epoch 19/20  Iteration 5243/5640 Training loss: 1.0126 0.3945 sec/batch\n",
      "Epoch 19/20  Iteration 5244/5640 Training loss: 1.0126 0.3942 sec/batch\n",
      "Epoch 19/20  Iteration 5245/5640 Training loss: 1.0127 0.3919 sec/batch\n",
      "Epoch 19/20  Iteration 5246/5640 Training loss: 1.0127 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5247/5640 Training loss: 1.0126 0.3997 sec/batch\n",
      "Epoch 19/20  Iteration 5248/5640 Training loss: 1.0125 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5249/5640 Training loss: 1.0124 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5250/5640 Training loss: 1.0124 0.3930 sec/batch\n",
      "Validation loss: 1.08306 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 5251/5640 Training loss: 1.0136 0.3814 sec/batch\n",
      "Epoch 19/20  Iteration 5252/5640 Training loss: 1.0136 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5253/5640 Training loss: 1.0136 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5254/5640 Training loss: 1.0135 0.3830 sec/batch\n",
      "Epoch 19/20  Iteration 5255/5640 Training loss: 1.0136 0.3819 sec/batch\n",
      "Epoch 19/20  Iteration 5256/5640 Training loss: 1.0136 0.3829 sec/batch\n",
      "Epoch 19/20  Iteration 5257/5640 Training loss: 1.0137 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 5258/5640 Training loss: 1.0137 0.3820 sec/batch\n",
      "Epoch 19/20  Iteration 5259/5640 Training loss: 1.0137 0.3814 sec/batch\n",
      "Epoch 19/20  Iteration 5260/5640 Training loss: 1.0136 0.3821 sec/batch\n",
      "Epoch 19/20  Iteration 5261/5640 Training loss: 1.0136 0.3852 sec/batch\n",
      "Epoch 19/20  Iteration 5262/5640 Training loss: 1.0134 0.3855 sec/batch\n",
      "Epoch 19/20  Iteration 5263/5640 Training loss: 1.0131 0.3858 sec/batch\n",
      "Epoch 19/20  Iteration 5264/5640 Training loss: 1.0129 0.3841 sec/batch\n",
      "Epoch 19/20  Iteration 5265/5640 Training loss: 1.0128 0.3847 sec/batch\n",
      "Epoch 19/20  Iteration 5266/5640 Training loss: 1.0126 0.3872 sec/batch\n",
      "Epoch 19/20  Iteration 5267/5640 Training loss: 1.0125 0.3868 sec/batch\n",
      "Epoch 19/20  Iteration 5268/5640 Training loss: 1.0124 0.3877 sec/batch\n",
      "Epoch 19/20  Iteration 5269/5640 Training loss: 1.0122 0.3880 sec/batch\n",
      "Epoch 19/20  Iteration 5270/5640 Training loss: 1.0122 0.3918 sec/batch\n",
      "Epoch 19/20  Iteration 5271/5640 Training loss: 1.0123 0.3889 sec/batch\n",
      "Epoch 19/20  Iteration 5272/5640 Training loss: 1.0123 0.3898 sec/batch\n",
      "Epoch 19/20  Iteration 5273/5640 Training loss: 1.0122 0.3899 sec/batch\n",
      "Epoch 19/20  Iteration 5274/5640 Training loss: 1.0123 0.3916 sec/batch\n",
      "Epoch 19/20  Iteration 5275/5640 Training loss: 1.0122 0.3909 sec/batch\n",
      "Epoch 19/20  Iteration 5276/5640 Training loss: 1.0121 0.3922 sec/batch\n",
      "Epoch 19/20  Iteration 5277/5640 Training loss: 1.0121 0.3899 sec/batch\n",
      "Epoch 19/20  Iteration 5278/5640 Training loss: 1.0120 0.3948 sec/batch\n",
      "Epoch 19/20  Iteration 5279/5640 Training loss: 1.0120 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5280/5640 Training loss: 1.0120 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5281/5640 Training loss: 1.0118 0.3935 sec/batch\n",
      "Epoch 19/20  Iteration 5282/5640 Training loss: 1.0118 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5283/5640 Training loss: 1.0117 0.3949 sec/batch\n",
      "Epoch 19/20  Iteration 5284/5640 Training loss: 1.0116 0.3950 sec/batch\n",
      "Epoch 19/20  Iteration 5285/5640 Training loss: 1.0116 0.3948 sec/batch\n",
      "Epoch 19/20  Iteration 5286/5640 Training loss: 1.0115 0.3935 sec/batch\n",
      "Epoch 19/20  Iteration 5287/5640 Training loss: 1.0114 0.3929 sec/batch\n",
      "Epoch 19/20  Iteration 5288/5640 Training loss: 1.0113 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5289/5640 Training loss: 1.0113 0.3954 sec/batch\n",
      "Epoch 19/20  Iteration 5290/5640 Training loss: 1.0112 0.3930 sec/batch\n",
      "Epoch 19/20  Iteration 5291/5640 Training loss: 1.0112 0.3926 sec/batch\n",
      "Epoch 19/20  Iteration 5292/5640 Training loss: 1.0111 0.3935 sec/batch\n",
      "Epoch 19/20  Iteration 5293/5640 Training loss: 1.0111 0.3948 sec/batch\n",
      "Epoch 19/20  Iteration 5294/5640 Training loss: 1.0109 0.3926 sec/batch\n",
      "Epoch 19/20  Iteration 5295/5640 Training loss: 1.0108 0.3944 sec/batch\n",
      "Epoch 19/20  Iteration 5296/5640 Training loss: 1.0107 0.3931 sec/batch\n",
      "Epoch 19/20  Iteration 5297/5640 Training loss: 1.0107 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5298/5640 Training loss: 1.0106 0.3938 sec/batch\n",
      "Epoch 19/20  Iteration 5299/5640 Training loss: 1.0106 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5300/5640 Training loss: 1.0106 0.3947 sec/batch\n",
      "Validation loss: 1.08838 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 5301/5640 Training loss: 1.0115 0.3852 sec/batch\n",
      "Epoch 19/20  Iteration 5302/5640 Training loss: 1.0114 0.3817 sec/batch\n",
      "Epoch 19/20  Iteration 5303/5640 Training loss: 1.0115 0.3828 sec/batch\n",
      "Epoch 19/20  Iteration 5304/5640 Training loss: 1.0115 0.3821 sec/batch\n",
      "Epoch 19/20  Iteration 5305/5640 Training loss: 1.0115 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5306/5640 Training loss: 1.0115 0.3825 sec/batch\n",
      "Epoch 19/20  Iteration 5307/5640 Training loss: 1.0114 0.3822 sec/batch\n",
      "Epoch 19/20  Iteration 5308/5640 Training loss: 1.0114 0.3825 sec/batch\n",
      "Epoch 19/20  Iteration 5309/5640 Training loss: 1.0113 0.3822 sec/batch\n",
      "Epoch 19/20  Iteration 5310/5640 Training loss: 1.0112 0.3821 sec/batch\n",
      "Epoch 19/20  Iteration 5311/5640 Training loss: 1.0112 0.3832 sec/batch\n",
      "Epoch 19/20  Iteration 5312/5640 Training loss: 1.0112 0.3816 sec/batch\n",
      "Epoch 19/20  Iteration 5313/5640 Training loss: 1.0112 0.3821 sec/batch\n",
      "Epoch 19/20  Iteration 5314/5640 Training loss: 1.0113 0.3830 sec/batch\n",
      "Epoch 19/20  Iteration 5315/5640 Training loss: 1.0112 0.3840 sec/batch\n",
      "Epoch 19/20  Iteration 5316/5640 Training loss: 1.0112 0.3846 sec/batch\n",
      "Epoch 19/20  Iteration 5317/5640 Training loss: 1.0112 0.3844 sec/batch\n",
      "Epoch 19/20  Iteration 5318/5640 Training loss: 1.0111 0.3882 sec/batch\n",
      "Epoch 19/20  Iteration 5319/5640 Training loss: 1.0110 0.3840 sec/batch\n",
      "Epoch 19/20  Iteration 5320/5640 Training loss: 1.0109 0.3871 sec/batch\n",
      "Epoch 19/20  Iteration 5321/5640 Training loss: 1.0108 0.3884 sec/batch\n",
      "Epoch 19/20  Iteration 5322/5640 Training loss: 1.0107 0.3896 sec/batch\n",
      "Epoch 19/20  Iteration 5323/5640 Training loss: 1.0107 0.3926 sec/batch\n",
      "Epoch 19/20  Iteration 5324/5640 Training loss: 1.0107 0.3944 sec/batch\n",
      "Epoch 19/20  Iteration 5325/5640 Training loss: 1.0107 0.3936 sec/batch\n",
      "Epoch 19/20  Iteration 5326/5640 Training loss: 1.0107 0.3948 sec/batch\n",
      "Epoch 19/20  Iteration 5327/5640 Training loss: 1.0106 0.3967 sec/batch\n",
      "Epoch 19/20  Iteration 5328/5640 Training loss: 1.0105 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5329/5640 Training loss: 1.0104 0.3941 sec/batch\n",
      "Epoch 19/20  Iteration 5330/5640 Training loss: 1.0104 0.3927 sec/batch\n",
      "Epoch 19/20  Iteration 5331/5640 Training loss: 1.0104 0.3933 sec/batch\n",
      "Epoch 19/20  Iteration 5332/5640 Training loss: 1.0103 0.3938 sec/batch\n",
      "Epoch 19/20  Iteration 5333/5640 Training loss: 1.0103 0.3932 sec/batch\n",
      "Epoch 19/20  Iteration 5334/5640 Training loss: 1.0103 0.3946 sec/batch\n",
      "Epoch 19/20  Iteration 5335/5640 Training loss: 1.0103 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5336/5640 Training loss: 1.0103 0.3925 sec/batch\n",
      "Epoch 19/20  Iteration 5337/5640 Training loss: 1.0103 0.3927 sec/batch\n",
      "Epoch 19/20  Iteration 5338/5640 Training loss: 1.0103 0.3940 sec/batch\n",
      "Epoch 19/20  Iteration 5339/5640 Training loss: 1.0103 0.3941 sec/batch\n",
      "Epoch 19/20  Iteration 5340/5640 Training loss: 1.0103 0.3955 sec/batch\n",
      "Epoch 19/20  Iteration 5341/5640 Training loss: 1.0104 0.3961 sec/batch\n",
      "Epoch 19/20  Iteration 5342/5640 Training loss: 1.0103 0.3937 sec/batch\n",
      "Epoch 19/20  Iteration 5343/5640 Training loss: 1.0102 0.3931 sec/batch\n",
      "Epoch 19/20  Iteration 5344/5640 Training loss: 1.0101 0.3943 sec/batch\n",
      "Epoch 19/20  Iteration 5345/5640 Training loss: 1.0100 0.3939 sec/batch\n",
      "Epoch 19/20  Iteration 5346/5640 Training loss: 1.0100 0.3935 sec/batch\n",
      "Epoch 19/20  Iteration 5347/5640 Training loss: 1.0099 0.3934 sec/batch\n",
      "Epoch 19/20  Iteration 5348/5640 Training loss: 1.0098 0.3945 sec/batch\n",
      "Epoch 19/20  Iteration 5349/5640 Training loss: 1.0098 0.3953 sec/batch\n",
      "Epoch 19/20  Iteration 5350/5640 Training loss: 1.0097 0.3932 sec/batch\n",
      "Validation loss: 1.08971 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 5351/5640 Training loss: 1.0104 0.3840 sec/batch\n",
      "Epoch 19/20  Iteration 5352/5640 Training loss: 1.0104 0.3832 sec/batch\n",
      "Epoch 19/20  Iteration 5353/5640 Training loss: 1.0104 0.3814 sec/batch\n",
      "Epoch 19/20  Iteration 5354/5640 Training loss: 1.0105 0.3804 sec/batch\n",
      "Epoch 19/20  Iteration 5355/5640 Training loss: 1.0106 0.3821 sec/batch\n",
      "Epoch 19/20  Iteration 5356/5640 Training loss: 1.0106 0.3819 sec/batch\n",
      "Epoch 19/20  Iteration 5357/5640 Training loss: 1.0106 0.3817 sec/batch\n",
      "Epoch 19/20  Iteration 5358/5640 Training loss: 1.0105 0.3808 sec/batch\n",
      "Epoch 20/20  Iteration 5359/5640 Training loss: 1.1162 0.3850 sec/batch\n",
      "Epoch 20/20  Iteration 5360/5640 Training loss: 1.0749 0.3861 sec/batch\n",
      "Epoch 20/20  Iteration 5361/5640 Training loss: 1.0582 0.3867 sec/batch\n",
      "Epoch 20/20  Iteration 5362/5640 Training loss: 1.0479 0.3864 sec/batch\n",
      "Epoch 20/20  Iteration 5363/5640 Training loss: 1.0416 0.3869 sec/batch\n",
      "Epoch 20/20  Iteration 5364/5640 Training loss: 1.0406 0.3878 sec/batch\n",
      "Epoch 20/20  Iteration 5365/5640 Training loss: 1.0345 0.3889 sec/batch\n",
      "Epoch 20/20  Iteration 5366/5640 Training loss: 1.0313 0.3874 sec/batch\n",
      "Epoch 20/20  Iteration 5367/5640 Training loss: 1.0265 0.3901 sec/batch\n",
      "Epoch 20/20  Iteration 5368/5640 Training loss: 1.0228 0.3892 sec/batch\n",
      "Epoch 20/20  Iteration 5369/5640 Training loss: 1.0230 0.3897 sec/batch\n",
      "Epoch 20/20  Iteration 5370/5640 Training loss: 1.0220 0.3893 sec/batch\n",
      "Epoch 20/20  Iteration 5371/5640 Training loss: 1.0211 0.3904 sec/batch\n",
      "Epoch 20/20  Iteration 5372/5640 Training loss: 1.0217 0.3906 sec/batch\n",
      "Epoch 20/20  Iteration 5373/5640 Training loss: 1.0192 0.3898 sec/batch\n",
      "Epoch 20/20  Iteration 5374/5640 Training loss: 1.0198 0.3889 sec/batch\n",
      "Epoch 20/20  Iteration 5375/5640 Training loss: 1.0196 0.3892 sec/batch\n",
      "Epoch 20/20  Iteration 5376/5640 Training loss: 1.0184 0.3897 sec/batch\n",
      "Epoch 20/20  Iteration 5377/5640 Training loss: 1.0178 0.3898 sec/batch\n",
      "Epoch 20/20  Iteration 5378/5640 Training loss: 1.0164 0.3914 sec/batch\n",
      "Epoch 20/20  Iteration 5379/5640 Training loss: 1.0162 0.3920 sec/batch\n",
      "Epoch 20/20  Iteration 5380/5640 Training loss: 1.0160 0.3930 sec/batch\n",
      "Epoch 20/20  Iteration 5381/5640 Training loss: 1.0156 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5382/5640 Training loss: 1.0144 0.3951 sec/batch\n",
      "Epoch 20/20  Iteration 5383/5640 Training loss: 1.0148 0.3934 sec/batch\n",
      "Epoch 20/20  Iteration 5384/5640 Training loss: 1.0146 0.3928 sec/batch\n",
      "Epoch 20/20  Iteration 5385/5640 Training loss: 1.0147 0.3932 sec/batch\n",
      "Epoch 20/20  Iteration 5386/5640 Training loss: 1.0148 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5387/5640 Training loss: 1.0140 0.3935 sec/batch\n",
      "Epoch 20/20  Iteration 5388/5640 Training loss: 1.0139 0.3943 sec/batch\n",
      "Epoch 20/20  Iteration 5389/5640 Training loss: 1.0128 0.3941 sec/batch\n",
      "Epoch 20/20  Iteration 5390/5640 Training loss: 1.0123 0.3958 sec/batch\n",
      "Epoch 20/20  Iteration 5391/5640 Training loss: 1.0117 0.3937 sec/batch\n",
      "Epoch 20/20  Iteration 5392/5640 Training loss: 1.0111 0.3936 sec/batch\n",
      "Epoch 20/20  Iteration 5393/5640 Training loss: 1.0112 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5394/5640 Training loss: 1.0111 0.3935 sec/batch\n",
      "Epoch 20/20  Iteration 5395/5640 Training loss: 1.0099 0.3940 sec/batch\n",
      "Epoch 20/20  Iteration 5396/5640 Training loss: 1.0088 0.3937 sec/batch\n",
      "Epoch 20/20  Iteration 5397/5640 Training loss: 1.0085 0.3949 sec/batch\n",
      "Epoch 20/20  Iteration 5398/5640 Training loss: 1.0074 0.3937 sec/batch\n",
      "Epoch 20/20  Iteration 5399/5640 Training loss: 1.0068 0.3937 sec/batch\n",
      "Epoch 20/20  Iteration 5400/5640 Training loss: 1.0072 0.3937 sec/batch\n",
      "Validation loss: 1.09157 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 5401/5640 Training loss: 1.0116 0.3822 sec/batch\n",
      "Epoch 20/20  Iteration 5402/5640 Training loss: 1.0116 0.3821 sec/batch\n",
      "Epoch 20/20  Iteration 5403/5640 Training loss: 1.0114 0.3839 sec/batch\n",
      "Epoch 20/20  Iteration 5404/5640 Training loss: 1.0113 0.3817 sec/batch\n",
      "Epoch 20/20  Iteration 5405/5640 Training loss: 1.0112 0.3816 sec/batch\n",
      "Epoch 20/20  Iteration 5406/5640 Training loss: 1.0112 0.3809 sec/batch\n",
      "Epoch 20/20  Iteration 5407/5640 Training loss: 1.0114 0.3831 sec/batch\n",
      "Epoch 20/20  Iteration 5408/5640 Training loss: 1.0112 0.3816 sec/batch\n",
      "Epoch 20/20  Iteration 5409/5640 Training loss: 1.0111 0.3811 sec/batch\n",
      "Epoch 20/20  Iteration 5410/5640 Training loss: 1.0110 0.3831 sec/batch\n",
      "Epoch 20/20  Iteration 5411/5640 Training loss: 1.0107 0.3822 sec/batch\n",
      "Epoch 20/20  Iteration 5412/5640 Training loss: 1.0107 0.3851 sec/batch\n",
      "Epoch 20/20  Iteration 5413/5640 Training loss: 1.0104 0.3885 sec/batch\n",
      "Epoch 20/20  Iteration 5414/5640 Training loss: 1.0103 0.3885 sec/batch\n",
      "Epoch 20/20  Iteration 5415/5640 Training loss: 1.0101 0.3905 sec/batch\n",
      "Epoch 20/20  Iteration 5416/5640 Training loss: 1.0100 0.3887 sec/batch\n",
      "Epoch 20/20  Iteration 5417/5640 Training loss: 1.0097 0.3867 sec/batch\n",
      "Epoch 20/20  Iteration 5418/5640 Training loss: 1.0095 0.3869 sec/batch\n",
      "Epoch 20/20  Iteration 5419/5640 Training loss: 1.0092 0.3869 sec/batch\n",
      "Epoch 20/20  Iteration 5420/5640 Training loss: 1.0087 0.3894 sec/batch\n",
      "Epoch 20/20  Iteration 5421/5640 Training loss: 1.0081 0.3904 sec/batch\n",
      "Epoch 20/20  Iteration 5422/5640 Training loss: 1.0075 0.3897 sec/batch\n",
      "Epoch 20/20  Iteration 5423/5640 Training loss: 1.0075 0.3913 sec/batch\n",
      "Epoch 20/20  Iteration 5424/5640 Training loss: 1.0072 0.3893 sec/batch\n",
      "Epoch 20/20  Iteration 5425/5640 Training loss: 1.0072 0.3902 sec/batch\n",
      "Epoch 20/20  Iteration 5426/5640 Training loss: 1.0070 0.3898 sec/batch\n",
      "Epoch 20/20  Iteration 5427/5640 Training loss: 1.0066 0.3900 sec/batch\n",
      "Epoch 20/20  Iteration 5428/5640 Training loss: 1.0060 0.3906 sec/batch\n",
      "Epoch 20/20  Iteration 5429/5640 Training loss: 1.0058 0.3909 sec/batch\n",
      "Epoch 20/20  Iteration 5430/5640 Training loss: 1.0057 0.3904 sec/batch\n",
      "Epoch 20/20  Iteration 5431/5640 Training loss: 1.0056 0.3922 sec/batch\n",
      "Epoch 20/20  Iteration 5432/5640 Training loss: 1.0056 0.3928 sec/batch\n",
      "Epoch 20/20  Iteration 5433/5640 Training loss: 1.0058 0.3961 sec/batch\n",
      "Epoch 20/20  Iteration 5434/5640 Training loss: 1.0061 0.3936 sec/batch\n",
      "Epoch 20/20  Iteration 5435/5640 Training loss: 1.0061 0.3939 sec/batch\n",
      "Epoch 20/20  Iteration 5436/5640 Training loss: 1.0067 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5437/5640 Training loss: 1.0067 0.3941 sec/batch\n",
      "Epoch 20/20  Iteration 5438/5640 Training loss: 1.0068 0.3953 sec/batch\n",
      "Epoch 20/20  Iteration 5439/5640 Training loss: 1.0072 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5440/5640 Training loss: 1.0069 0.3930 sec/batch\n",
      "Epoch 20/20  Iteration 5441/5640 Training loss: 1.0067 0.3945 sec/batch\n",
      "Epoch 20/20  Iteration 5442/5640 Training loss: 1.0066 0.3960 sec/batch\n",
      "Epoch 20/20  Iteration 5443/5640 Training loss: 1.0064 0.3925 sec/batch\n",
      "Epoch 20/20  Iteration 5444/5640 Training loss: 1.0062 0.3927 sec/batch\n",
      "Epoch 20/20  Iteration 5445/5640 Training loss: 1.0061 0.3934 sec/batch\n",
      "Epoch 20/20  Iteration 5446/5640 Training loss: 1.0060 0.3948 sec/batch\n",
      "Epoch 20/20  Iteration 5447/5640 Training loss: 1.0058 0.3923 sec/batch\n",
      "Epoch 20/20  Iteration 5448/5640 Training loss: 1.0055 0.3926 sec/batch\n",
      "Epoch 20/20  Iteration 5449/5640 Training loss: 1.0055 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5450/5640 Training loss: 1.0053 0.3948 sec/batch\n",
      "Validation loss: 1.09418 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 5451/5640 Training loss: 1.0078 0.3817 sec/batch\n",
      "Epoch 20/20  Iteration 5452/5640 Training loss: 1.0077 0.3819 sec/batch\n",
      "Epoch 20/20  Iteration 5453/5640 Training loss: 1.0074 0.3856 sec/batch\n",
      "Epoch 20/20  Iteration 5454/5640 Training loss: 1.0073 0.3857 sec/batch\n",
      "Epoch 20/20  Iteration 5455/5640 Training loss: 1.0073 0.3824 sec/batch\n",
      "Epoch 20/20  Iteration 5456/5640 Training loss: 1.0073 0.3837 sec/batch\n",
      "Epoch 20/20  Iteration 5457/5640 Training loss: 1.0073 0.3818 sec/batch\n",
      "Epoch 20/20  Iteration 5458/5640 Training loss: 1.0071 0.3810 sec/batch\n",
      "Epoch 20/20  Iteration 5459/5640 Training loss: 1.0068 0.3826 sec/batch\n",
      "Epoch 20/20  Iteration 5460/5640 Training loss: 1.0067 0.3835 sec/batch\n",
      "Epoch 20/20  Iteration 5461/5640 Training loss: 1.0065 0.3848 sec/batch\n",
      "Epoch 20/20  Iteration 5462/5640 Training loss: 1.0063 0.3842 sec/batch\n",
      "Epoch 20/20  Iteration 5463/5640 Training loss: 1.0059 0.3878 sec/batch\n",
      "Epoch 20/20  Iteration 5464/5640 Training loss: 1.0057 0.3897 sec/batch\n",
      "Epoch 20/20  Iteration 5465/5640 Training loss: 1.0054 0.3892 sec/batch\n",
      "Epoch 20/20  Iteration 5466/5640 Training loss: 1.0051 0.3897 sec/batch\n",
      "Epoch 20/20  Iteration 5467/5640 Training loss: 1.0050 0.3921 sec/batch\n",
      "Epoch 20/20  Iteration 5468/5640 Training loss: 1.0048 0.3894 sec/batch\n",
      "Epoch 20/20  Iteration 5469/5640 Training loss: 1.0045 0.3905 sec/batch\n",
      "Epoch 20/20  Iteration 5470/5640 Training loss: 1.0045 0.3904 sec/batch\n",
      "Epoch 20/20  Iteration 5471/5640 Training loss: 1.0043 0.3915 sec/batch\n",
      "Epoch 20/20  Iteration 5472/5640 Training loss: 1.0041 0.3901 sec/batch\n",
      "Epoch 20/20  Iteration 5473/5640 Training loss: 1.0041 0.3903 sec/batch\n",
      "Epoch 20/20  Iteration 5474/5640 Training loss: 1.0041 0.3884 sec/batch\n",
      "Epoch 20/20  Iteration 5475/5640 Training loss: 1.0040 0.3900 sec/batch\n",
      "Epoch 20/20  Iteration 5476/5640 Training loss: 1.0039 0.3889 sec/batch\n",
      "Epoch 20/20  Iteration 5477/5640 Training loss: 1.0037 0.3900 sec/batch\n",
      "Epoch 20/20  Iteration 5478/5640 Training loss: 1.0034 0.3891 sec/batch\n",
      "Epoch 20/20  Iteration 5479/5640 Training loss: 1.0034 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 5480/5640 Training loss: 1.0034 0.3912 sec/batch\n",
      "Epoch 20/20  Iteration 5481/5640 Training loss: 1.0032 0.3885 sec/batch\n",
      "Epoch 20/20  Iteration 5482/5640 Training loss: 1.0031 0.3895 sec/batch\n",
      "Epoch 20/20  Iteration 5483/5640 Training loss: 1.0034 0.3912 sec/batch\n",
      "Epoch 20/20  Iteration 5484/5640 Training loss: 1.0031 0.3894 sec/batch\n",
      "Epoch 20/20  Iteration 5485/5640 Training loss: 1.0031 0.3893 sec/batch\n",
      "Epoch 20/20  Iteration 5486/5640 Training loss: 1.0029 0.3893 sec/batch\n",
      "Epoch 20/20  Iteration 5487/5640 Training loss: 1.0027 0.3915 sec/batch\n",
      "Epoch 20/20  Iteration 5488/5640 Training loss: 1.0026 0.3893 sec/batch\n",
      "Epoch 20/20  Iteration 5489/5640 Training loss: 1.0025 0.3895 sec/batch\n",
      "Epoch 20/20  Iteration 5490/5640 Training loss: 1.0021 0.3894 sec/batch\n",
      "Epoch 20/20  Iteration 5491/5640 Training loss: 1.0019 0.3898 sec/batch\n",
      "Epoch 20/20  Iteration 5492/5640 Training loss: 1.0017 0.3900 sec/batch\n",
      "Epoch 20/20  Iteration 5493/5640 Training loss: 1.0016 0.3924 sec/batch\n",
      "Epoch 20/20  Iteration 5494/5640 Training loss: 1.0015 0.3902 sec/batch\n",
      "Epoch 20/20  Iteration 5495/5640 Training loss: 1.0014 0.3932 sec/batch\n",
      "Epoch 20/20  Iteration 5496/5640 Training loss: 1.0012 0.3930 sec/batch\n",
      "Epoch 20/20  Iteration 5497/5640 Training loss: 1.0011 0.3936 sec/batch\n",
      "Epoch 20/20  Iteration 5498/5640 Training loss: 1.0011 0.3937 sec/batch\n",
      "Epoch 20/20  Iteration 5499/5640 Training loss: 1.0009 0.3932 sec/batch\n",
      "Epoch 20/20  Iteration 5500/5640 Training loss: 1.0010 0.3931 sec/batch\n",
      "Validation loss: 1.08779 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 5501/5640 Training loss: 1.0025 0.3839 sec/batch\n",
      "Epoch 20/20  Iteration 5502/5640 Training loss: 1.0024 0.3829 sec/batch\n",
      "Epoch 20/20  Iteration 5503/5640 Training loss: 1.0021 0.3825 sec/batch\n",
      "Epoch 20/20  Iteration 5504/5640 Training loss: 1.0019 0.3846 sec/batch\n",
      "Epoch 20/20  Iteration 5505/5640 Training loss: 1.0016 0.3831 sec/batch\n",
      "Epoch 20/20  Iteration 5506/5640 Training loss: 1.0016 0.3810 sec/batch\n",
      "Epoch 20/20  Iteration 5507/5640 Training loss: 1.0015 0.3841 sec/batch\n",
      "Epoch 20/20  Iteration 5508/5640 Training loss: 1.0013 0.3846 sec/batch\n",
      "Epoch 20/20  Iteration 5509/5640 Training loss: 1.0012 0.3869 sec/batch\n",
      "Epoch 20/20  Iteration 5510/5640 Training loss: 1.0012 0.3886 sec/batch\n",
      "Epoch 20/20  Iteration 5511/5640 Training loss: 1.0013 0.3875 sec/batch\n",
      "Epoch 20/20  Iteration 5512/5640 Training loss: 1.0012 0.3898 sec/batch\n",
      "Epoch 20/20  Iteration 5513/5640 Training loss: 1.0012 0.3871 sec/batch\n",
      "Epoch 20/20  Iteration 5514/5640 Training loss: 1.0012 0.3872 sec/batch\n",
      "Epoch 20/20  Iteration 5515/5640 Training loss: 1.0011 0.3876 sec/batch\n",
      "Epoch 20/20  Iteration 5516/5640 Training loss: 1.0011 0.3900 sec/batch\n",
      "Epoch 20/20  Iteration 5517/5640 Training loss: 1.0011 0.3870 sec/batch\n",
      "Epoch 20/20  Iteration 5518/5640 Training loss: 1.0010 0.3881 sec/batch\n",
      "Epoch 20/20  Iteration 5519/5640 Training loss: 1.0010 0.3918 sec/batch\n",
      "Epoch 20/20  Iteration 5520/5640 Training loss: 1.0008 0.3927 sec/batch\n",
      "Epoch 20/20  Iteration 5521/5640 Training loss: 1.0007 0.3921 sec/batch\n",
      "Epoch 20/20  Iteration 5522/5640 Training loss: 1.0007 0.3948 sec/batch\n",
      "Epoch 20/20  Iteration 5523/5640 Training loss: 1.0008 0.3930 sec/batch\n",
      "Epoch 20/20  Iteration 5524/5640 Training loss: 1.0009 0.3928 sec/batch\n",
      "Epoch 20/20  Iteration 5525/5640 Training loss: 1.0009 0.3923 sec/batch\n",
      "Epoch 20/20  Iteration 5526/5640 Training loss: 1.0009 0.3930 sec/batch\n",
      "Epoch 20/20  Iteration 5527/5640 Training loss: 1.0010 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5528/5640 Training loss: 1.0009 0.3943 sec/batch\n",
      "Epoch 20/20  Iteration 5529/5640 Training loss: 1.0009 0.3923 sec/batch\n",
      "Epoch 20/20  Iteration 5530/5640 Training loss: 1.0008 0.3932 sec/batch\n",
      "Epoch 20/20  Iteration 5531/5640 Training loss: 1.0007 0.3953 sec/batch\n",
      "Epoch 20/20  Iteration 5532/5640 Training loss: 1.0007 0.3930 sec/batch\n",
      "Epoch 20/20  Iteration 5533/5640 Training loss: 1.0008 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5534/5640 Training loss: 1.0008 0.3942 sec/batch\n",
      "Epoch 20/20  Iteration 5535/5640 Training loss: 1.0009 0.3949 sec/batch\n",
      "Epoch 20/20  Iteration 5536/5640 Training loss: 1.0008 0.3938 sec/batch\n",
      "Epoch 20/20  Iteration 5537/5640 Training loss: 1.0009 0.3937 sec/batch\n",
      "Epoch 20/20  Iteration 5538/5640 Training loss: 1.0009 0.3935 sec/batch\n",
      "Epoch 20/20  Iteration 5539/5640 Training loss: 1.0010 0.3936 sec/batch\n",
      "Epoch 20/20  Iteration 5540/5640 Training loss: 1.0010 0.3939 sec/batch\n",
      "Epoch 20/20  Iteration 5541/5640 Training loss: 1.0011 0.3966 sec/batch\n",
      "Epoch 20/20  Iteration 5542/5640 Training loss: 1.0010 0.3926 sec/batch\n",
      "Epoch 20/20  Iteration 5543/5640 Training loss: 1.0009 0.3939 sec/batch\n",
      "Epoch 20/20  Iteration 5544/5640 Training loss: 1.0007 0.3948 sec/batch\n",
      "Epoch 20/20  Iteration 5545/5640 Training loss: 1.0005 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 5546/5640 Training loss: 1.0003 0.3922 sec/batch\n",
      "Epoch 20/20  Iteration 5547/5640 Training loss: 1.0002 0.3946 sec/batch\n",
      "Epoch 20/20  Iteration 5548/5640 Training loss: 1.0000 0.3931 sec/batch\n",
      "Epoch 20/20  Iteration 5549/5640 Training loss: 1.0000 0.3941 sec/batch\n",
      "Epoch 20/20  Iteration 5550/5640 Training loss: 0.9999 0.3950 sec/batch\n",
      "Validation loss: 1.08471 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 5551/5640 Training loss: 1.0009 0.3819 sec/batch\n",
      "Epoch 20/20  Iteration 5552/5640 Training loss: 1.0009 0.3827 sec/batch\n",
      "Epoch 20/20  Iteration 5553/5640 Training loss: 1.0010 0.3830 sec/batch\n",
      "Epoch 20/20  Iteration 5554/5640 Training loss: 1.0010 0.3815 sec/batch\n",
      "Epoch 20/20  Iteration 5555/5640 Training loss: 1.0008 0.3816 sec/batch\n",
      "Epoch 20/20  Iteration 5556/5640 Training loss: 1.0009 0.3830 sec/batch\n",
      "Epoch 20/20  Iteration 5557/5640 Training loss: 1.0009 0.3810 sec/batch\n",
      "Epoch 20/20  Iteration 5558/5640 Training loss: 1.0008 0.3824 sec/batch\n",
      "Epoch 20/20  Iteration 5559/5640 Training loss: 1.0008 0.3809 sec/batch\n",
      "Epoch 20/20  Iteration 5560/5640 Training loss: 1.0007 0.3819 sec/batch\n",
      "Epoch 20/20  Iteration 5561/5640 Training loss: 1.0006 0.3839 sec/batch\n",
      "Epoch 20/20  Iteration 5562/5640 Training loss: 1.0007 0.3806 sec/batch\n",
      "Epoch 20/20  Iteration 5563/5640 Training loss: 1.0006 0.3814 sec/batch\n",
      "Epoch 20/20  Iteration 5564/5640 Training loss: 1.0006 0.3855 sec/batch\n",
      "Epoch 20/20  Iteration 5565/5640 Training loss: 1.0005 0.3851 sec/batch\n",
      "Epoch 20/20  Iteration 5566/5640 Training loss: 1.0004 0.3852 sec/batch\n",
      "Epoch 20/20  Iteration 5567/5640 Training loss: 1.0004 0.3841 sec/batch\n",
      "Epoch 20/20  Iteration 5568/5640 Training loss: 1.0003 0.3865 sec/batch\n",
      "Epoch 20/20  Iteration 5569/5640 Training loss: 1.0002 0.3872 sec/batch\n",
      "Epoch 20/20  Iteration 5570/5640 Training loss: 1.0001 0.3872 sec/batch\n",
      "Epoch 20/20  Iteration 5571/5640 Training loss: 1.0001 0.3896 sec/batch\n",
      "Epoch 20/20  Iteration 5572/5640 Training loss: 1.0001 0.3948 sec/batch\n",
      "Epoch 20/20  Iteration 5573/5640 Training loss: 1.0001 0.3937 sec/batch\n",
      "Epoch 20/20  Iteration 5574/5640 Training loss: 1.0000 0.3936 sec/batch\n",
      "Epoch 20/20  Iteration 5575/5640 Training loss: 1.0000 0.3932 sec/batch\n",
      "Epoch 20/20  Iteration 5576/5640 Training loss: 0.9998 0.3943 sec/batch\n",
      "Epoch 20/20  Iteration 5577/5640 Training loss: 0.9997 0.3931 sec/batch\n",
      "Epoch 20/20  Iteration 5578/5640 Training loss: 0.9996 0.3931 sec/batch\n",
      "Epoch 20/20  Iteration 5579/5640 Training loss: 0.9996 0.3940 sec/batch\n",
      "Epoch 20/20  Iteration 5580/5640 Training loss: 0.9995 0.3932 sec/batch\n",
      "Epoch 20/20  Iteration 5581/5640 Training loss: 0.9995 0.3945 sec/batch\n",
      "Epoch 20/20  Iteration 5582/5640 Training loss: 0.9995 0.3959 sec/batch\n",
      "Epoch 20/20  Iteration 5583/5640 Training loss: 0.9994 0.3935 sec/batch\n",
      "Epoch 20/20  Iteration 5584/5640 Training loss: 0.9994 0.3944 sec/batch\n",
      "Epoch 20/20  Iteration 5585/5640 Training loss: 0.9995 0.3931 sec/batch\n",
      "Epoch 20/20  Iteration 5586/5640 Training loss: 0.9995 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 5587/5640 Training loss: 0.9995 0.3938 sec/batch\n",
      "Epoch 20/20  Iteration 5588/5640 Training loss: 0.9995 0.3950 sec/batch\n",
      "Epoch 20/20  Iteration 5589/5640 Training loss: 0.9994 0.3917 sec/batch\n",
      "Epoch 20/20  Iteration 5590/5640 Training loss: 0.9994 0.3933 sec/batch\n",
      "Epoch 20/20  Iteration 5591/5640 Training loss: 0.9993 0.3952 sec/batch\n",
      "Epoch 20/20  Iteration 5592/5640 Training loss: 0.9992 0.3936 sec/batch\n",
      "Epoch 20/20  Iteration 5593/5640 Training loss: 0.9992 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 5594/5640 Training loss: 0.9992 0.3951 sec/batch\n",
      "Epoch 20/20  Iteration 5595/5640 Training loss: 0.9991 0.3957 sec/batch\n",
      "Epoch 20/20  Iteration 5596/5640 Training loss: 0.9992 0.3967 sec/batch\n",
      "Epoch 20/20  Iteration 5597/5640 Training loss: 0.9991 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 5598/5640 Training loss: 0.9991 0.3929 sec/batch\n",
      "Epoch 20/20  Iteration 5599/5640 Training loss: 0.9990 0.3952 sec/batch\n",
      "Epoch 20/20  Iteration 5600/5640 Training loss: 0.9990 0.3933 sec/batch\n",
      "Validation loss: 1.085 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 5601/5640 Training loss: 0.9999 0.3850 sec/batch\n",
      "Epoch 20/20  Iteration 5602/5640 Training loss: 0.9998 0.3809 sec/batch\n",
      "Epoch 20/20  Iteration 5603/5640 Training loss: 0.9996 0.3815 sec/batch\n",
      "Epoch 20/20  Iteration 5604/5640 Training loss: 0.9996 0.3817 sec/batch\n",
      "Epoch 20/20  Iteration 5605/5640 Training loss: 0.9995 0.3824 sec/batch\n",
      "Epoch 20/20  Iteration 5606/5640 Training loss: 0.9995 0.3813 sec/batch\n",
      "Epoch 20/20  Iteration 5607/5640 Training loss: 0.9996 0.3841 sec/batch\n",
      "Epoch 20/20  Iteration 5608/5640 Training loss: 0.9996 0.3818 sec/batch\n",
      "Epoch 20/20  Iteration 5609/5640 Training loss: 0.9994 0.3828 sec/batch\n",
      "Epoch 20/20  Iteration 5610/5640 Training loss: 0.9993 0.3819 sec/batch\n",
      "Epoch 20/20  Iteration 5611/5640 Training loss: 0.9992 0.3814 sec/batch\n",
      "Epoch 20/20  Iteration 5612/5640 Training loss: 0.9991 0.3819 sec/batch\n",
      "Epoch 20/20  Iteration 5613/5640 Training loss: 0.9991 0.3816 sec/batch\n",
      "Epoch 20/20  Iteration 5614/5640 Training loss: 0.9990 0.3849 sec/batch\n",
      "Epoch 20/20  Iteration 5615/5640 Training loss: 0.9989 0.3872 sec/batch\n",
      "Epoch 20/20  Iteration 5616/5640 Training loss: 0.9989 0.3885 sec/batch\n",
      "Epoch 20/20  Iteration 5617/5640 Training loss: 0.9989 0.3916 sec/batch\n",
      "Epoch 20/20  Iteration 5618/5640 Training loss: 0.9989 0.3898 sec/batch\n",
      "Epoch 20/20  Iteration 5619/5640 Training loss: 0.9989 0.3889 sec/batch\n",
      "Epoch 20/20  Iteration 5620/5640 Training loss: 0.9989 0.3914 sec/batch\n",
      "Epoch 20/20  Iteration 5621/5640 Training loss: 0.9989 0.3900 sec/batch\n",
      "Epoch 20/20  Iteration 5622/5640 Training loss: 0.9988 0.3910 sec/batch\n",
      "Epoch 20/20  Iteration 5623/5640 Training loss: 0.9989 0.8056 sec/batch\n",
      "Epoch 20/20  Iteration 5624/5640 Training loss: 0.9988 0.3814 sec/batch\n",
      "Epoch 20/20  Iteration 5625/5640 Training loss: 0.9987 0.3806 sec/batch\n",
      "Epoch 20/20  Iteration 5626/5640 Training loss: 0.9986 0.3840 sec/batch\n",
      "Epoch 20/20  Iteration 5627/5640 Training loss: 0.9986 0.3897 sec/batch\n",
      "Epoch 20/20  Iteration 5628/5640 Training loss: 0.9985 0.3886 sec/batch\n",
      "Epoch 20/20  Iteration 5629/5640 Training loss: 0.9985 0.3866 sec/batch\n",
      "Epoch 20/20  Iteration 5630/5640 Training loss: 0.9984 0.3869 sec/batch\n",
      "Epoch 20/20  Iteration 5631/5640 Training loss: 0.9984 0.3893 sec/batch\n",
      "Epoch 20/20  Iteration 5632/5640 Training loss: 0.9983 0.3880 sec/batch\n",
      "Epoch 20/20  Iteration 5633/5640 Training loss: 0.9983 0.3870 sec/batch\n",
      "Epoch 20/20  Iteration 5634/5640 Training loss: 0.9983 0.3900 sec/batch\n",
      "Epoch 20/20  Iteration 5635/5640 Training loss: 0.9983 0.3884 sec/batch\n",
      "Epoch 20/20  Iteration 5636/5640 Training loss: 0.9984 0.3870 sec/batch\n",
      "Epoch 20/20  Iteration 5637/5640 Training loss: 0.9984 0.3885 sec/batch\n",
      "Epoch 20/20  Iteration 5638/5640 Training loss: 0.9984 0.3878 sec/batch\n",
      "Epoch 20/20  Iteration 5639/5640 Training loss: 0.9984 0.3895 sec/batch\n",
      "Epoch 20/20  Iteration 5640/5640 Training loss: 0.9983 0.3871 sec/batch\n",
      "Validation loss: 1.08327 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, **params)\n",
    "model = define_rnn_graph(len(charset), **params)\n",
    "saver = tf.train.Saver(max_to_keep=200)\n",
    "epoch_start = 0\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # tensorboard 작성을 위한 Filewriter를 만듭니다.\n",
    "    train_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/test')\n",
    "       \n",
    "    n_batches = int(train_x.shape[1]/params['time_steps'])\n",
    "    iterations = n_batches * epochs\n",
    "     # 기존의 checkpoint를 읽어서 다시 학습\n",
    "    if checkpoint:\n",
    "        try:\n",
    "            saver.restore(sess, checkpoint)\n",
    "            iteration=int(re.search(r'\\bi([\\d]+)_[\\w.]+\\b',checkpoint).group(1))\n",
    "            epoch_start = int(iteration/n_batches)\n",
    "        except:\n",
    "            print('Cannot read the checkpoint. Set it None.')\n",
    "            epoch_start = 0\n",
    "            checkpoint = None\n",
    "            \n",
    "    for e in range(epoch_start, epochs):\n",
    "        # network 학습\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(get_batch([train_x, train_y], params['time_steps']), 1):\n",
    "            iteration = e*n_batches + i\n",
    "            # training 시간을 기록\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state }\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \\\n",
    "                                                          model.final_state, model.optimizer], feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/i),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            # summary추가\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            if (iteration%checkpoint_interval == 0) or (iteration == iterations):\n",
    "                # validation loss 확인. dropout의 값을 1로 설정하여 모든 node가 동작하도록 한다.\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], params['time_steps']):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \\\n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                # summary추가\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/sherlock/i{}_l{}_{:.3f}\".format(iteration, params['lstm_size'], np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/sherlock/i5640_l1024_1.083\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i50_l1024_2.980\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i100_l1024_2.813\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i150_l1024_2.459\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i200_l1024_2.264\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i250_l1024_2.136\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i300_l1024_2.017\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i350_l1024_1.932\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i400_l1024_1.869\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i450_l1024_1.803\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i500_l1024_1.749\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i550_l1024_1.708\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i600_l1024_1.656\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i650_l1024_1.619\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i700_l1024_1.584\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i750_l1024_1.542\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i800_l1024_1.512\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i850_l1024_1.484\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i900_l1024_1.461\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i950_l1024_1.443\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1000_l1024_1.416\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1050_l1024_1.395\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1100_l1024_1.379\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1150_l1024_1.351\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1200_l1024_1.331\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1250_l1024_1.313\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1300_l1024_1.298\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1350_l1024_1.289\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1400_l1024_1.274\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1450_l1024_1.263\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1500_l1024_1.254\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1550_l1024_1.245\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1600_l1024_1.235\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1650_l1024_1.233\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1700_l1024_1.225\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1750_l1024_1.207\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1800_l1024_1.206\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1850_l1024_1.201\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1900_l1024_1.193\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i1950_l1024_1.189\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2000_l1024_1.178\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2050_l1024_1.174\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2100_l1024_1.172\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2150_l1024_1.166\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2200_l1024_1.163\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2250_l1024_1.159\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2300_l1024_1.153\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2350_l1024_1.150\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2400_l1024_1.145\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2450_l1024_1.142\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2500_l1024_1.142\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2550_l1024_1.141\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2600_l1024_1.131\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2650_l1024_1.135\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2700_l1024_1.128\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2750_l1024_1.125\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2800_l1024_1.123\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2850_l1024_1.118\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2900_l1024_1.116\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i2950_l1024_1.118\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3000_l1024_1.113\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3050_l1024_1.114\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3100_l1024_1.111\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3150_l1024_1.106\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3200_l1024_1.105\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3250_l1024_1.106\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3300_l1024_1.102\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3350_l1024_1.104\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3400_l1024_1.099\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3450_l1024_1.098\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3500_l1024_1.101\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3550_l1024_1.099\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3600_l1024_1.096\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3650_l1024_1.095\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3700_l1024_1.094\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3750_l1024_1.092\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3800_l1024_1.096\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3850_l1024_1.089\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3900_l1024_1.090\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i3950_l1024_1.095\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4000_l1024_1.086\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4050_l1024_1.089\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4100_l1024_1.089\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4150_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4200_l1024_1.093\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4250_l1024_1.086\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4300_l1024_1.084\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4350_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4400_l1024_1.086\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4450_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4500_l1024_1.087\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4550_l1024_1.087\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4600_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4650_l1024_1.084\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4700_l1024_1.085\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4750_l1024_1.084\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4800_l1024_1.085\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4850_l1024_1.081\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4900_l1024_1.091\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i4950_l1024_1.084\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5000_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5050_l1024_1.090\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5100_l1024_1.083\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5150_l1024_1.082\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5200_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5250_l1024_1.083\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5300_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5350_l1024_1.090\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5400_l1024_1.092\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5450_l1024_1.094\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5500_l1024_1.088\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5550_l1024_1.085\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5600_l1024_1.085\"\n",
       "all_model_checkpoint_paths: \"checkpoints/sherlock/i5640_l1024_1.083\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/sherlock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "이제 학습된 모델을 이용하여 문장을 만들어봅시다. 학습된 모델이 문장을 만드는 방법은 이전 글자가 주어졌을때, 다음 글자를 예측을 반복적으로 하면서 이루어집니다. 학습된 모델은 주어진 이전 글자에 대해 다음 글자를 확률 값으로 예측을 하게됩니다. 각각의 확률을 적용하여 Random sampling을 하여 새로운 글자가 추가가 되고, 새로운 글자와 이전 state를 이용하여 다음 글자를 예측합니다. 이 과정을 반복하게되면 문장을 만들 수 있습니다.\n",
    "확률값이 가장 높은 `N`가지중에 하나를 선택하도록 코드를 작성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, charset_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(charset_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, charset_size, prime=\"The \"):\n",
    "    samples = list(prime)\n",
    "    model = define_rnn_graph(charset_size, **{'lstm_size':lstm_size, 'sampling':True})\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = char_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(charset))\n",
    "        samples.append(int_to_char[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(charset))\n",
    "            samples.append(int_to_char[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Loss가 가장 작은 모델을 포함한 여러 모델을 이용하여 문장을 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_checkpoints=re.findall(r'\\b([\\w/]+_([\\d.]+))\\b',str(tf.train.get_checkpoint_state('checkpoints/sherlock')),re.IGNORECASE)\n",
    "all_checkpoints_sorted_by_valloss = sorted(all_checkpoints, key=lambda tup: float(tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('checkpoints/sherlock/i4850_l1024_1.081', '1.081'),\n",
       " ('checkpoints/sherlock/i5150_l1024_1.082', '1.082'),\n",
       " ('checkpoints/sherlock/i5640_l1024_1.083', '1.083'),\n",
       " ('checkpoints/sherlock/i5100_l1024_1.083', '1.083'),\n",
       " ('checkpoints/sherlock/i5250_l1024_1.083', '1.083'),\n",
       " ('checkpoints/sherlock/i5640_l1024_1.083', '1.083'),\n",
       " ('checkpoints/sherlock/i4300_l1024_1.084', '1.084'),\n",
       " ('checkpoints/sherlock/i4650_l1024_1.084', '1.084'),\n",
       " ('checkpoints/sherlock/i4750_l1024_1.084', '1.084'),\n",
       " ('checkpoints/sherlock/i4950_l1024_1.084', '1.084')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_checkpoints_sorted_by_valloss[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10번째 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<checkpoints/sherlock/i500_l1024_1.749>>\n",
      "The was a sarn to simper and  hers, that I was not the  some of at the mastle an thenes with and an  the contired. He was there was it help buchere to stald and andent out thome of the wilfing a mould of mad it who his  well, and to me with the  to bake whe had sint our himseres of her. I was  therl was in the dient of all this  seal the  state.  \"I, were that that have nisheld with the wanded  to seve the mere to tho gotes alle the  and thes,  was a sait the mently of insomand, and that to ser were  ars as atteller tomnint  our the digtt a stold whor was nagethall and to sellition  then all the casersed thene. This was whon  sumn and selfit through. The willon the cearing.  \"An a meate and allowed the colmers  fal the sand of at that was  the deall see of the sing thremes our thas a mostle were had house beand his asent, but it is an that wis  a teen a for our waster and him here. Ho mes wish a  colfare this aly she cherers,\" said he.\"  \"We are a tain to hourd that I had beat a site of  ther and to menter which hour  been the mistle and his  carred our tho mist which his cringeden of he were of  was the made oven to she the sond ont well, the was  as and hear the comested to sere the stile, but which we she how to be to hear  out over to tild that he same the stome our that which were wasted to soom a comticter of the wolter  trighin with his wand out at  sure ore and as the soors a filed, ard the merter on that I was sure of the wool to may, the wire and the sair of the  dard, thime which we cal to some that was a could ofting to see of the drowe of that whe heard  as the clunt in three  should be a camare of  the wooken and ang in who hive a sene the  there on the wishour ance of the chorition ond  with the conerss this wele he care that which there.  \"I was  anged he could have a minter that in wis  simsing was  tole one which what I had been it the sair ow. It mast anothen there ours in the  streses in the some of at at thes when would be will a  that have breins th\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "checkpoint = all_checkpoints[10][0]\n",
    "samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=\"The \")\n",
    "print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation loss가 가장 작은 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<checkpoints/sherlock/i4850_l1024_1.081>>\n",
      "The  Buskervilles which I heard in her case we had some seat by meening  which had been drawn and showing that it was the man who is a  dead, therefore, to fancy as a feeling at his police, and he  shone into a chair, but he was always supposed to follow into the  servants? It is a steel one, and a clatter on treasure and too  beautifully a still one. There was no one that he had an account of  making the mother, but his heart had been a palefanthed face which  showed that he had suddenly said, and a sudden and a masterful  mother, a comfortable place.  \"I am not all a strange able to see you.\"  \"We have to be seen in my mouth, and I am sure of that importance and  the police that there is a missionable son, to my companion's  sister.\"  \"I'd tell you what we have, Mr. Holmes, and I shall see that you  have already expected the singular praction of the child  at Carrith.\"  \"You must have said anything of the lady. I tried to decide. If he  would have taken any one else her has seen any such as a track, and  here is a sense of a man to spoke in her chair, and will he wish to  be at the same time. There is a son, there's no make of my place  there.\" He shrugged his shoulders. \"I should have seen some of you,  and I will be true to your plans when I see that I said that you  have been a small, dear fellow. If you will remain to take me  all round without this point which may safely be a private  cridinal infinite police, and if I would be trained to some stories in  these places in the case of the morning.\"  \"What was he?\"  \"To my wife's sort,\" said the company, laughing, \"a case were some  society and of experience to this singular police form in all passing.  What happened, how could I descend her his arrange?\"  \"It seems to me that there is a line in my manner at all. I am a  superfrient, surprised face. He could not have done.\"  \"And then you have troubled me to be crushed. There is one of the where  what I can go out for a few hundred yards again, but is it not  all as \n",
      "====================================================================================================\n",
      "<<checkpoints/sherlock/i5150_l1024_1.082>>\n",
      "The  Pork would be of interest over this middle of his bed, and the  surprise would not bring it at once between the crest windows as  fresh and the point of darkness was the service to me. What's  the chance of him if the lady how had the short at once he had  come up and stood and he took the secretary, and he could give this  criminal in the morning behind his windows. A meaning of this strange  change was an openentary attention through the household, and with  a some society was absolutely considerably admirably closened into  such a fience.  The strength of the study of the shadow of his face and high cracks and  an examination of the ring of the boxes.  \"What is that, Watson. Where is your press?\"  \"Well, well, I have no idea of told to throw my middless of the  matter. Have you already seen them?\"  \"The centre place which I have done is.\"  As he spoke a lutterly rude of a millionaired murder, his eyes  and a gangled face were thrown by his head slell over his eyes, with  his head and agrant and dark and criss of consects. \"I am sure that  you are situated?\"  \"Well, there are some contraction. The choose is a chair on the  morning behind, and his head streaming and toos. It was only thought  that it would be a climbore that I should have been tried to show that  it has been the body which had been asked or as to the solution. If it  were the service towards himself, was allowing in a choic and tried to  considerably chain to my report before I saw.  The men was the object of an active suspicion, we were all  described. It was only a claim once more that I had been doing in the  place of side. It shints some details which he had strong, and I  sat in half an hour. A lodge of heavy wails came to the shattered  sudden chance of thought when I could have strongly and the  strength of a conversation which had been suddenly commented.  \"If this woman's hose is the most incessage one,\" said Holmes, smiling,  \"I have no doubt that there is an one that was a solution which\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:2]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=\"The \")\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<checkpoints/sherlock/i4850_l1024_1.081>>\n",
      "Mr. Holmes,  that when I had some points of that way has been taking to the  boxtomy steps, he said that he is a professional buind. This man  has been correctlooking from her since I has shown myself to  bushed the little characters. It was a small scent for an amateur  of the same answer. Then all these three men was a man who will find  the situation of which he could not beat.  \"What do you think of it?\" he cried to the leaster peace of  reputation. \"I will remember your surprise and the situation to  watch him.\"  I walked out at the door of the station and was some which seemed to  hold the conversation at the station at the other end.  \"What will you do, Mr. McCarthy?\" he asked.  \"It will be a large and pretty practice that you have a lodger, sie  and here we are now to be defined to take a power.\"  \"I've hold it in to determine an abstraction. You see. There is  no man to contain it. We must see what you have seen since I have  heard of him. You will be a statement as you will account for me to  speak.\"  \"And this is a crime?\"  \"There has been any singular proofs of my presence, Mr. Holmes,\" said he. \"I should be glin in the matter of myself. I  have no doubt that you had sent a point for me that I have been  caused by the case, and it was a probable tonight, and I could  discuss this light through them at the times of her months. That is why  I had to come up to you. Instantly, as you are still assested. How  could I assure me also in your heads what is thinking of all this?  If he could help to hunt it upstairs, I will tell you in the morning,  and I will see that there was no one. I shall be strong for you.\"  \"When I arranged these, and I hear that it would be a pearly  practical thing, and we shall be surprised in meaning to you, as I  will take all his souls and sensitive monograph and the police.  The police have been taken by some sort at a singular or in  my life won't bother than any of his contracted clay with me as  they were all supposed? I am not the\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for checkpoint, _ in all_checkpoints_sorted_by_valloss[:1]:\n",
    "    samp = sample(checkpoint, n_samples, params['lstm_size'], len(charset), prime=\"Mr.\")\n",
    "    print('<<{}>>\\n'.format(checkpoint)+samp)\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 checkpoint의 prediction 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  Amprican, a delicate police that he would not take a sergeant of his  figure. There is no other possibility that they were suspicions often, and  was attached to the community of his face that he sprang up and his  back without a figure as we walked over the door and the drop and  loose at the salling place.  \"This is surely the present,\" He said the landlord insade a strong brain  beside the window.  \"What do you think of the matter?\"  \"Well, I seem to be a very common possession which has been  convinced that the man was in a parageness who had the most sense  by walking out into the study it was of the death of a difficult  interest. Then, that is the maid and that we should have a stopped  afternoon to see him, for else would be off the death of myself, and,  by heavens, takes it all right to hin heaves. I should not do it as  all as a suspicion, and I am not to be told him what was the story. That  has been making a family pass on this table. I take my word that  I could hardly tr\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints/sherlock')\n",
    "samp = sample(checkpoint, 1000, params['lstm_size'], len(charset), prime=\"The \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "log directory를 설정해주고 실행합니다.\n",
    "```bash\n",
    "$ tensorboard --logdir='logs/'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-py2]",
   "language": "python",
   "name": "conda-env-tensorflow-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
