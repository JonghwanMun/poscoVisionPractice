{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings, Sequnce to Sequence\n",
    "## <학습목표>\n",
    "1. word embeddings의 필요성과 자연어 처리(natural language processing)에서 word embeddings의 사용에 대해서 알아보자.\n",
    "2. Tensorflow의 word embeddings tutorial의 내용을 숙지하자.\n",
    "3. skip-gram architecture를 이용하여 word2vec 알고리즘을 구현해보자.\n",
    "\n",
    "#### 참고자료\n",
    "- TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n",
    "- A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "- [Distributed Representations of Words and Phrases\n",
    "and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Mikolov et al.\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) Mikolov et al.\n",
    "- An [implementation of word2vec](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/) from Thushan Ganegedara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings이란?\n",
    "<img src=\"../resources/define_embedding.png\" width=\"800\">\n",
    "<img src=\"../resources/dense_sparse.png\" width=\"800\">\n",
    "영상이나 음성데이터와는 다르게, 언어 데이터를 표현하는 문제는 수십만에서 수백만개의 단어들을 각각 분류해야하는 문제로 치환됩니다. 각 단어들을 one-hot encoding을 하는 경우, 언어 데이터를 표현하는 벡터는 크기가 매우 크지만 매우 sparse한 벡터가 됩니다. one-hot encoding 벡터는 매우 크기때문에 gpu memory에 넣기 힘들며, 한 원소만 1이고 나머지는 다 0으로 저장되는 벡터이기 때문에 매우 비효율적으로 정보를 저장하게 됩니다. 또한 각 단어간에 거리(distance)도 항상 1이 되는 문제점도 있습니다.\n",
    "\n",
    "word2vec은 이러한 문제점에 착안하여 보다 효율적인 단어의 정보를 표현하는 벡터를 만드는데에 목적을 둔 알고리즘입니다. Vector space models(VSMs)은 continuous한 vector space의 벡터들을 비슷한 문맥상 의미를 가지는 단어들은 서로 가까이하고, 서로 다른 단어는 거리가 멀어지게 학습 하는 것을 목표로 합니다. 예를들면, 검정('black'), 빨강('red'), 파랑('blue')같은 단어들은 서로 가까이 있게되는 것입니다. 이런 식으로 word를 vector로 표현하게되면 Embedding lookup matrix를 학습하게됩니다. 즉 One hot word vector들이 아래와 같은 형식으로 변환되는데, 이는 matrix의 index번째의 row를 가져오는 것과 같기 때문에, embedding lookup matrix(tensor/table/...)라고 합니다.\n",
    "\n",
    "<img src='../resources/matrix_mult_w_one_hot.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec 알고리즘으로 가장 잘 알려진 알고리즘은 Continuous Bag-of-Words model (CBOW)와 Skip-Gram model이 있습니다.\n",
    "\n",
    "<img src=\"../resources/word2vec_archs.png\" width=\"800\">\n",
    "\n",
    "알고리즘의 측면으로만 본다면, 두 모델은 매우 비슷합니다. 위 그림에서 보는 것과 같이 CBOW는 target 단어를 주변 context 단어들을 이용하여 예측하는 반면, Skip-Gram은 CBOW와 반대로 하나의 단어가 주어지고 이 단어를 이용하여 나머지 주변단어들을 예측하도록 학습됩니다. 통계적으로 CBOW는 전체 주변 단어들을 하나의 observation으로 보아서 정답을 예측하기 때문에 예측하는 과정에서 분포적 정보(distributional information)가 smoothing됩니다. 그래서 CBOW는 적은 데이터셋에서 유리합니다. 반면에 skip-gram은 context와 target쌍이 하나의 새로운 observation이 되어서 더 좋은 표현력을 가지게됩니다. \n",
    "\n",
    "이번 수업에서는 Skip-gram에 대해서 배워봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow가 GPU의 모든 memory를 사용하지 않고 dynamic하게 할당하도록 설정합니다.\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ../data/text8.zip\n"
     ]
    }
   ],
   "source": [
    "# 데이터 Download\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + os.path.basename(filename), filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('../data/text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 string의 list형태로 저장합니다.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary를 제작하고 희귀한 단어들은 UNK(Unknown)으로 치환합니다.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "# 데이터셋 작성\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)\n",
    "del words  # 필요없는 변수는 제거하여 메모리를 절약합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data\n",
      " [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋이 잘 만들어 졋는지 확인합니다.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data\\n', data[:10])\n",
    "print([reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram batch를 만드는 원리는 아래 그림과 같습니다.\n",
    "<img src='../resources/skip_gram_batch.png' width=800>\n",
    "[이미지출처](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "이번 예제에서의 skip_window는 위 그림의 (window 크기-1)/2 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 as -> 6 a\n",
      "6 a -> 3084 originated\n",
      "195 term -> 6 a\n",
      "2 of -> 46 first\n",
      "3137 abuse -> 46 first\n",
      "46 first -> 156 against\n",
      "59 used -> 3137 abuse\n",
      "156 against -> 128 early\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "# Skip-gram의 학습에 필요한 batch를 만드는 함수를 제작합니다\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=1, skip_window=2)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "          '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classification\n",
    "<img src='../resources/skip_gram_net_arch.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise-constrastive estimation (NCE) \n",
    "<img src='../resources/noise_classifier.png' width=600>\n",
    "<img src='../resources/negative_sampling.png' width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Skip-gram과 모델 코드\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # embedding vector의 dimension.\n",
    "skip_window = 1       # 왼쪽, 오른쪽의 skip_window의 크기\n",
    "num_skips = 2         # 동일한 input에 대해 얼마만큼의 output을 할당 할 것인지\n",
    "\n",
    "# 임의의 validation set을 nearest neighbor를 sample하기위해 만듭니다.  \n",
    "# validation samples은 빈도수가 높은 단어를 위주로 뽑습니다.\n",
    "valid_size = 16     # Similarity를 계산하기위한  Val set의 크기\n",
    "valid_window = 100  # 빈도가 가장 높은 valid_window순 까지의 벡터를 고릅니다.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # negative sample의 수.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data의 placeholder를 설정.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # GPU에 변수들을 넣습니다. 일반적으로 default가 /gpu:0로 되어있어서 다른 GPU에 넣고 싶을 경우 따로 지정해야합니다.\n",
    "    with tf.device('/gpu:0'):\n",
    "        # input data의 Look up embeddings.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # NCE loss 계산을 위한 variables\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # 각 batch의 Average NCE loss\n",
    "    # tf.nce_loss 함수는 새로운 negative label을 sample하여 loss를 계산합니다.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    # SGD optimizer, a learning rate 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # minibatch 샘플들과 and 모든 embeddings과의 cosine similarity를 계산합니다. \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "        valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  295.308135986\n",
      "Nearest to during: afc, gratis, compiling, hamish, disappearing, horizon, conciliation, boating,\n",
      "Nearest to when: formulate, maximizes, grays, encephalitis, captive, smp, luminance, acknowledged,\n",
      "Nearest to it: mpa, showgirl, revamp, osbourne, clogged, kegan, politic, macht,\n",
      "Nearest to has: preservative, pamphlets, cas, fungal, migration, followers, hearn, floating,\n",
      "Nearest to more: xilinx, bangladeshis, caecilius, asroc, spectral, greyhounds, tutelage, spurt,\n",
      "Nearest to all: triangulation, token, sous, stunt, corroborated, watchman, victor, davidic,\n",
      "Nearest to see: foal, salted, prefectural, wiesel, inadequacy, ascension, ccny, wied,\n",
      "Nearest to his: slimmer, indeed, waltrip, diputados, vestigial, utility, domitia, naming,\n",
      "Nearest to new: pickling, astronautica, assimilation, gemstone, galaxies, shouts, pomona, youth,\n",
      "Nearest to for: lombardo, aground, jai, cranberry, overdoses, lorre, galway, webcomic,\n",
      "Nearest to such: bluegrass, cygnus, filters, horizonte, galina, specializing, oasis, zahn,\n",
      "Nearest to about: quintet, deliver, pu, buddhas, fleece, interviewed, mobilizing, probus,\n",
      "Nearest to other: dago, bryan, recalls, torch, etymologies, symphonies, hurried, psychedelic,\n",
      "Nearest to no: fundamentals, hostile, pretensions, catholic, writeups, agamemnon, meticulous, risks,\n",
      "Nearest to d: kidney, disjointed, reticent, beautiful, paintings, fanaticism, frida, thea,\n",
      "Nearest to at: meadow, revenue, empowered, expense, engler, fallacies, inv, dreamt,\n",
      "Average loss at step  2000 :  113.265317338\n",
      "Average loss at step  4000 :  52.9555684431\n",
      "Average loss at step  6000 :  33.4819835522\n",
      "Average loss at step  8000 :  23.4854804702\n",
      "Average loss at step  10000 :  17.8203308562\n",
      "Nearest to during: afc, altenberg, coke, by, horizon, bodyline, paine, failing,\n",
      "Nearest to when: with, and, however, son, face, zero, manifesto, probably,\n",
      "Nearest to it: mpa, phi, vs, proper, tolerance, also, anniversary, kenneth,\n",
      "Nearest to has: had, preservative, have, failed, followers, migration, was, subject,\n",
      "Nearest to more: spectral, von, a, two, bring, create, kaposi, biodiesel,\n",
      "Nearest to all: sous, vs, mathbf, boat, asphalt, phi, css, archie,\n",
      "Nearest to see: altenberg, and, war, space, safin, archie, defended, my,\n",
      "Nearest to his: the, utility, indeed, this, statues, phi, instance, closer,\n",
      "Nearest to new: youth, jpg, locke, amounts, fg, prestige, properties, demanding,\n",
      "Nearest to for: and, in, of, as, archie, lorre, on, mathbf,\n",
      "Nearest to such: la, abbot, butler, input, hiv, person, value, oasis,\n",
      "Nearest to about: coke, phi, notion, citations, el, nine, deliver, ales,\n",
      "Nearest to other: tuna, atheist, environments, colombo, etymologies, mathbf, pleas, square,\n",
      "Nearest to no: hostile, catholic, bible, rfc, primarily, vs, corrupt, pursuit,\n",
      "Nearest to d: archie, frida, approved, paintings, beautiful, initiative, government, hagen,\n",
      "Nearest to at: in, declare, lymphoma, revenue, between, hogeschool, austin, phi,\n",
      "Average loss at step  12000 :  14.0464767851\n",
      "Average loss at step  14000 :  11.8636471997\n",
      "Average loss at step  16000 :  9.96458616459\n",
      "Average loss at step  18000 :  8.33383527297\n",
      "Average loss at step  20000 :  7.94589479458\n",
      "Nearest to during: by, afc, in, altenberg, paine, coke, horizon, agouti,\n",
      "Nearest to when: and, with, however, that, dasyprocta, encephalitis, circ, drake,\n",
      "Nearest to it: he, she, circ, phi, mpa, also, agouti, this,\n",
      "Nearest to has: had, have, was, is, apatosaurus, circ, failed, would,\n",
      "Nearest to more: creationism, spectral, hbox, von, aoc, abraxas, circ, bring,\n",
      "Nearest to all: sous, UNK, nn, both, boat, vs, traits, unfortunate,\n",
      "Nearest to see: and, parsons, space, safin, altenberg, defended, optics, cindy,\n",
      "Nearest to his: the, their, its, this, statues, a, circ, s,\n",
      "Nearest to new: youth, prestige, dasyprocta, jpg, fg, amounts, averaging, locke,\n",
      "Nearest to for: in, of, and, on, as, agouti, by, from,\n",
      "Nearest to such: hippolytus, generates, butler, hello, hydrophilic, input, ru, abbot,\n",
      "Nearest to about: agouti, citations, fleece, notion, dasyprocta, phi, three, billboard,\n",
      "Nearest to other: environments, tuna, colombo, dasyprocta, atheist, some, pleas, mlb,\n",
      "Nearest to no: hostile, she, bible, rfc, and, antoninus, cantatas, that,\n",
      "Nearest to d: b, archie, and, circ, newfoundland, nebraska, fanaticism, ech,\n",
      "Nearest to at: in, agouti, and, declare, pyruvate, on, lymphoma, revenue,\n",
      "Average loss at step  22000 :  7.05861825085\n",
      "Average loss at step  24000 :  6.87819337952\n",
      "Average loss at step  26000 :  6.73716685295\n",
      "Average loss at step  28000 :  6.38891325724\n",
      "Average loss at step  30000 :  5.91698963583\n",
      "Nearest to during: by, in, afc, paine, on, altenberg, coke, for,\n",
      "Nearest to when: and, with, however, where, that, hamas, in, dasyprocta,\n",
      "Nearest to it: he, she, this, which, circ, also, agouti, hippolytus,\n",
      "Nearest to has: had, have, is, was, preservative, vicu, circ, apatosaurus,\n",
      "Nearest to more: less, hbox, dayton, spectral, creationism, topped, abraxas, bring,\n",
      "Nearest to all: amalthea, sous, both, mathbf, unfortunate, tricky, look, phi,\n",
      "Nearest to see: and, parsons, space, defended, safin, altenberg, ascension, optics,\n",
      "Nearest to his: their, the, its, s, this, statues, aston, lymphoma,\n",
      "Nearest to new: youth, prestige, dasyprocta, amounts, fg, demanding, averaging, forearm,\n",
      "Nearest to for: in, and, of, on, with, agouti, by, to,\n",
      "Nearest to such: well, generates, disappointing, hippolytus, hydrophilic, ani, ru, continuity,\n",
      "Nearest to about: citations, notion, otimes, three, agouti, fleece, asbestos, adiabats,\n",
      "Nearest to other: environments, some, colombo, tuna, amalthea, dasyprocta, reserve, atheist,\n",
      "Nearest to no: hostile, ramps, or, she, antoninus, cantatas, that, metellus,\n",
      "Nearest to d: b, and, archie, circ, newfoundland, nebraska, frida, kidney,\n",
      "Nearest to at: in, and, agouti, on, pyruvate, from, declare, for,\n",
      "Average loss at step  32000 :  5.99689881253\n",
      "Average loss at step  34000 :  5.69684918964\n",
      "Average loss at step  36000 :  5.78572181785\n",
      "Average loss at step  38000 :  5.50437387311\n",
      "Average loss at step  40000 :  5.2739378767\n",
      "Nearest to during: in, by, afc, on, paine, altenberg, at, of,\n",
      "Nearest to when: however, with, where, and, that, but, though, recitative,\n",
      "Nearest to it: he, she, this, zero, which, there, that, circ,\n",
      "Nearest to has: had, have, was, is, arno, zero, circ, vicu,\n",
      "Nearest to more: less, most, dayton, creationism, topped, zero, hbox, too,\n",
      "Nearest to all: both, amalthea, rang, zero, sous, tricky, vma, mathbf,\n",
      "Nearest to see: zero, and, space, parsons, but, defended, altenberg, ascension,\n",
      "Nearest to his: their, the, its, her, zero, statues, s, working,\n",
      "Nearest to new: prestige, dasyprocta, youth, demanding, vma, zero, amounts, forearm,\n",
      "Nearest to for: of, in, zero, agouti, on, with, to, projectile,\n",
      "Nearest to such: well, bluegrass, disappointing, zero, hippolytus, hydrophilic, generates, visible,\n",
      "Nearest to about: citations, notion, otimes, agouti, adiabats, three, dasyprocta, vdc,\n",
      "Nearest to other: some, colombo, environments, tuna, zero, dasyprocta, hurried, amalthea,\n",
      "Nearest to no: recitative, hostile, that, a, ramps, she, metellus, zero,\n",
      "Nearest to d: b, archie, and, newfoundland, ech, nebraska, frida, circ,\n",
      "Nearest to at: in, on, agouti, declare, from, zero, pyruvate, asshole,\n",
      "Average loss at step  42000 :  5.36580946243\n",
      "Average loss at step  44000 :  5.24777988064\n",
      "Average loss at step  46000 :  5.23097908735\n",
      "Average loss at step  48000 :  5.2061295296\n",
      "Average loss at step  50000 :  4.97699618316\n",
      "Nearest to during: in, by, at, on, afc, for, altenberg, paine,\n",
      "Nearest to when: however, and, where, with, but, though, recitative, four,\n",
      "Nearest to it: he, she, this, which, there, kapoor, agouti, they,\n",
      "Nearest to has: had, have, was, is, arno, vicu, s, circ,\n",
      "Nearest to more: less, most, dayton, bobble, creationism, topped, mario, hbox,\n",
      "Nearest to all: both, rang, amalthea, these, two, four, vma, tricky,\n",
      "Nearest to see: and, solicitation, space, defended, but, parsons, ascension, filling,\n",
      "Nearest to his: their, the, its, her, s, this, working, statues,\n",
      "Nearest to new: prestige, dasyprocta, demanding, youth, vma, amounts, averaging, sanfl,\n",
      "Nearest to for: in, agouti, of, and, circ, from, on, against,\n",
      "Nearest to such: well, bluegrass, disappointing, these, disaccharide, hydrophilic, ani, generates,\n",
      "Nearest to about: citations, notion, over, otimes, interviewed, three, microsystems, adiabats,\n",
      "Nearest to other: some, environments, colombo, many, dasyprocta, tuna, amalthea, intestinal,\n",
      "Nearest to no: a, recitative, metellus, hostile, ramps, it, she, arises,\n",
      "Nearest to d: b, archie, newfoundland, circ, nebraska, ech, kidney, and,\n",
      "Nearest to at: in, on, agouti, and, pyruvate, declare, during, from,\n",
      "Average loss at step  52000 :  5.05054309332\n",
      "Average loss at step  54000 :  5.18643849969\n",
      "Average loss at step  56000 :  5.02569621789\n",
      "Average loss at step  58000 :  5.05179492474\n",
      "Average loss at step  60000 :  4.95657833409\n",
      "Nearest to during: in, by, at, from, on, altenberg, afc, ursus,\n",
      "Nearest to when: however, where, but, with, though, recitative, and, four,\n",
      "Nearest to it: he, she, this, there, which, they, thibetanus, ursus,\n",
      "Nearest to has: had, have, was, is, arno, ursus, s, circ,\n",
      "Nearest to more: less, most, bobble, dayton, topped, very, creationism, too,\n",
      "Nearest to all: both, these, two, three, rang, amalthea, some, what,\n",
      "Nearest to see: but, and, solicitation, space, defended, filling, parsons, ursus,\n",
      "Nearest to his: their, its, her, the, s, this, working, statues,\n",
      "Nearest to new: prestige, thibetanus, dasyprocta, youth, vma, circ, demanding, sanfl,\n",
      "Nearest to for: ursus, in, of, agouti, thibetanus, against, projectile, with,\n",
      "Nearest to such: well, these, bluegrass, disappointing, known, disaccharide, ani, hydrophilic,\n",
      "Nearest to about: ursus, citations, over, notion, otimes, microsystems, vdc, interviewed,\n",
      "Nearest to other: some, many, these, dasyprocta, environments, colombo, microsite, amalthea,\n",
      "Nearest to no: recitative, a, metellus, arises, ramps, or, what, hostile,\n",
      "Nearest to d: b, archie, newfoundland, ursus, circ, kidney, ech, nebraska,\n",
      "Nearest to at: in, agouti, on, ursus, pyruvate, declare, during, dasyprocta,\n",
      "Average loss at step  62000 :  5.00895713425\n",
      "Average loss at step  64000 :  4.8298766799\n",
      "Average loss at step  66000 :  4.58777527761\n",
      "Average loss at step  68000 :  4.98718530703\n",
      "Average loss at step  70000 :  4.9041941371\n",
      "Nearest to during: in, upanija, by, at, under, from, afc, altenberg,\n",
      "Nearest to when: however, but, where, and, with, though, upanija, recitative,\n",
      "Nearest to it: he, she, this, there, which, they, thibetanus, ursus,\n",
      "Nearest to has: had, have, was, is, upanija, arno, ursus, mishnayot,\n",
      "Nearest to more: less, most, very, dayton, creationism, topped, bobble, mario,\n",
      "Nearest to all: both, these, rang, some, amalthea, mitral, what, pulau,\n",
      "Nearest to see: but, filling, unexplored, and, computationally, defended, believer, monogamy,\n",
      "Nearest to his: their, its, her, the, s, kriol, marek, statues,\n",
      "Nearest to new: prestige, upanija, thibetanus, youth, dasyprocta, prohibitions, circ, vma,\n",
      "Nearest to for: ursus, agouti, thibetanus, of, upanija, against, projectile, in,\n",
      "Nearest to such: well, these, disappointing, bluegrass, known, disaccharide, many, some,\n",
      "Nearest to about: ursus, over, citations, five, otimes, notion, adiabats, microsystems,\n",
      "Nearest to other: some, many, these, various, dasyprocta, colombo, amalthea, microsite,\n",
      "Nearest to no: metellus, arises, belloc, recitative, ramps, what, primarily, it,\n",
      "Nearest to d: b, archie, ech, newfoundland, c, ursus, kidney, UNK,\n",
      "Nearest to at: in, upanija, agouti, declare, pyruvate, ursus, during, from,\n",
      "Average loss at step  72000 :  4.74507256103\n",
      "Average loss at step  74000 :  4.82005964553\n",
      "Average loss at step  76000 :  4.70418335247\n",
      "Average loss at step  78000 :  4.80240983897\n",
      "Average loss at step  80000 :  4.79506668973\n",
      "Nearest to during: in, by, upanija, at, under, through, pontificia, after,\n",
      "Nearest to when: however, but, where, though, upanija, with, recitative, ursus,\n",
      "Nearest to it: he, she, this, there, which, they, ursus, kapoor,\n",
      "Nearest to has: had, have, was, is, upanija, arno, mishnayot, ursus,\n",
      "Nearest to more: less, most, very, dayton, chicano, topped, creationism, too,\n",
      "Nearest to all: both, these, some, amalthea, rang, two, mitral, vma,\n",
      "Nearest to see: but, computationally, unexplored, believer, and, ursus, defended, filling,\n",
      "Nearest to his: their, its, her, the, s, statues, working, marek,\n",
      "Nearest to new: thibetanus, prestige, upanija, dasyprocta, youth, circ, vma, ursus,\n",
      "Nearest to for: ursus, agouti, thibetanus, against, of, projectile, upanija, escuela,\n",
      "Nearest to such: well, these, known, disappointing, many, bluegrass, including, disaccharide,\n",
      "Nearest to about: over, citations, ursus, adiabats, otimes, only, billboard, interviewed,\n",
      "Nearest to other: some, many, various, these, dasyprocta, amalthea, colombo, microsite,\n",
      "Nearest to no: metellus, ramps, arises, belloc, it, a, often, which,\n",
      "Nearest to d: b, archie, UNK, ursus, circ, ech, c, newfoundland,\n",
      "Nearest to at: in, upanija, during, agouti, on, declare, pyruvate, and,\n",
      "Average loss at step  82000 :  4.76712251174\n",
      "Average loss at step  84000 :  4.73770134306\n",
      "Average loss at step  86000 :  4.77705066061\n",
      "Average loss at step  88000 :  4.73107021284\n",
      "Average loss at step  90000 :  4.73532194185\n",
      "Nearest to during: in, by, upanija, under, at, after, from, through,\n",
      "Nearest to when: however, where, but, though, if, after, before, upanija,\n",
      "Nearest to it: he, she, this, there, which, they, ursus, but,\n",
      "Nearest to has: had, have, was, is, upanija, mishnayot, affects, ursus,\n",
      "Nearest to more: less, most, very, topped, chicano, dayton, too, bobble,\n",
      "Nearest to all: both, these, some, mitral, amalthea, rang, look, vma,\n",
      "Nearest to see: but, and, computationally, believer, unexplored, filling, cotangent, kapoor,\n",
      "Nearest to his: their, her, its, the, s, my, working, statues,\n",
      "Nearest to new: prestige, thibetanus, upanija, dasyprocta, circ, youth, globemaster, vma,\n",
      "Nearest to for: ursus, agouti, thibetanus, against, projectile, escuela, upanija, in,\n",
      "Nearest to such: well, these, known, many, including, disappointing, some, like,\n",
      "Nearest to about: over, citations, ursus, only, adiabats, billboard, otimes, interviewed,\n",
      "Nearest to other: some, many, various, these, dasyprocta, including, amalthea, intestinal,\n",
      "Nearest to no: any, metellus, ramps, it, a, arises, often, belloc,\n",
      "Nearest to d: b, UNK, archie, c, k, ech, ursus, circ,\n",
      "Nearest to at: in, upanija, agouti, declare, on, during, emphasised, ursus,\n",
      "Average loss at step  92000 :  4.66408310211\n",
      "Average loss at step  94000 :  4.72712691486\n",
      "Average loss at step  96000 :  4.6813763206\n",
      "Average loss at step  98000 :  4.58869922149\n",
      "Average loss at step  100000 :  4.68987471437\n",
      "Nearest to during: in, by, upanija, after, under, at, through, when,\n",
      "Nearest to when: however, if, where, though, but, after, before, with,\n",
      "Nearest to it: he, she, there, this, they, which, ursus, but,\n",
      "Nearest to has: had, have, is, was, upanija, mishnayot, affects, arno,\n",
      "Nearest to more: less, most, very, engrams, wallach, bobble, too, chicano,\n",
      "Nearest to all: both, these, some, many, mitral, those, amalthea, what,\n",
      "Nearest to see: but, and, computationally, unexplored, believer, filling, ursus, cotangent,\n",
      "Nearest to his: their, her, its, the, s, my, working, marek,\n",
      "Nearest to new: prestige, thibetanus, upanija, dasyprocta, youth, circ, asshole, globemaster,\n",
      "Nearest to for: ursus, agouti, upanija, in, thibetanus, against, including, of,\n",
      "Nearest to such: well, these, known, many, including, some, disappointing, like,\n",
      "Nearest to about: over, citations, ursus, three, only, adiabats, billboard, interviewed,\n",
      "Nearest to other: some, many, various, including, dasyprocta, amalthea, these, gunfire,\n",
      "Nearest to no: any, metellus, it, ramps, astrometric, belloc, recitative, often,\n",
      "Nearest to d: b, archie, circ, ursus, c, ech, six, k,\n",
      "Nearest to at: in, upanija, agouti, during, declare, on, emphasised, under,\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    # 모든 변수들을 초기화 합니다.\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        # batch를 가져옵니다\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000\n",
    "            # batches.\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500\n",
    "        # steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = \"Nearest to %s:\" % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding 시각화.\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='results/tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    if not os.path.isdir(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    filename='results/tsne.png'\n",
    "    plot_with_labels(low_dim_embs, labels, filename)\n",
    "    I=Image.open(filename)\n",
    "    plt.imshow(I)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Please install sklearn, matplotlib, pillow, and scipy to visualize embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-py2]",
   "language": "python",
   "name": "conda-env-tensorflow-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
